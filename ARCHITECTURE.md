# Sylk Architecture

## Overview

Sylk is a multi-agent system built in Go. It uses a **central Guide** as the universal message router for **all** inter-agent communication. The system is designed for **high concurrency**: dozens of sessions, hundreds to thousands of subagents, and workflows executed as DAGs (Directed Acyclic Graphs) with explicit execution order.

This document defines:

- Agent roles, responsibilities, and user interaction patterns
- The Guide as universal message router
- **Session management system** (creation, switching, isolation, context preservation)
- Message envelope and routing model
- Knowledge layer (three RAGs: Academic, Librarian, Archivalist)
- DAG planning and execution
- Quality assurance loop (Inspector + Tester)
- **Skills per agent** (progressive disclosure)
- **Skill definitions per agent**
- **LLM hooks per agent**
- State transitions and lifecycle tracking
- Failure handling and recovery
- Concurrency and backpressure
- Token savings model
- Implementation guide

---

## High-Level System Diagram

```
┌─────────────────────────────────────────────────────────────────────────────────────┐
│                                      USER                                           │
└───────────────────────────────────────┬─────────────────────────────────────────────┘
                                        │
                                        ▼
┌─────────────────────────────────────────────────────────────────────────────────────┐
│                             SESSION MANAGER                                          │
│                       (Session Lifecycle Control)                                    │
│                                                                                      │
│   Creates, switches, preserves, and isolates session contexts                       │
│                                                                                      │
└───────────────────────────────────────╦─────────────────────────────────────────────┘
                                        ║
                                        ▼
╔═════════════════════════════════════════════════════════════════════════════════════╗
║                                     GUIDE                                           ║
║                            (Universal Message Router)                               ║
║                                                                                     ║
║   ALL messages flow through here - user requests AND inter-agent communication     ║
║   Session-scoped routing, context injection, and message correlation               ║
║                                                                                     ║
╚═══════════════════════════════════════╦═════════════════════════════════════════════╝
                                        ║
    ┌───────────┬───────────┬───────────╫───────────┬───────────┬───────────┐
    │           │           │           ║           │           │           │
    ▼           ▼           ▼           ▼           ▼           ▼           ▼
┌───────┐ ┌──────────┐ ┌──────────┐ ┌───────┐ ┌──────────┐ ┌──────────┐ ┌───────┐
│ACADEM-│ │ARCHITECT │ │ORCHESTRA-│ │ENGINE-│ │LIBRARIAN │ │ARCHIVAL- │ │INSPECT│
│IC     │ │          │ │TOR       │ │ER(s)  │ │          │ │IST       │ │OR     │
│       │ │          │ │          │ │       │ │          │ │          │ │       │
│Extern-│ │Abstract→ │ │DAG       │ │Task   │ │Local     │ │Historic- │ │Code   │
│al RAG │ │Concrete  │ │Execution │ │Execut-│ │Code RAG  │ │al RAG    │ │Valid- │
│       │ │          │ │          │ │ion    │ │          │ │          │ │ation  │
└───────┘ └──────────┘ └──────────┘ └───────┘ └──────────┘ └──────────┘ └───────┘
                                                                              │
                                                                        ┌─────┴─────┐
                                                                        │  TESTER   │
                                                                        │           │
                                                                        │ Test Plan │
                                                                        │ + Execute │
                                                                        └───────────┘
```

---

## Message Envelope

### Session-Aware Message

```go
// core/messaging/message.go

type Message[T any] struct {
    ID            string        `json:"id"`
    CorrelationID string        `json:"correlation_id,omitempty"`
    ParentID      string        `json:"parent_id,omitempty"`

    // Session context (REQUIRED for most messages)
    SessionID     string        `json:"session_id"`

    Source string      `json:"source"`
    Target string      `json:"target,omitempty"`
    Type   MessageType `json:"type"`

    Payload   T          `json:"payload"`
    Timestamp time.Time  `json:"timestamp"`
    Deadline  *time.Time `json:"deadline,omitempty"`
    TTL       time.Duration `json:"ttl,omitempty"`

    Status      MessageStatus `json:"status"`
    Attempt     int           `json:"attempt"`
    MaxAttempts int           `json:"max_attempts,omitempty"`

    Priority  Priority       `json:"priority"`
    Metadata  map[string]any `json:"metadata,omitempty"`
    Error     string         `json:"error,omitempty"`
    ProcessedAt *time.Time   `json:"processed_at,omitempty"`

    // Intent Preservation & Confidence Propagation (carried through request lifecycle)
    StructuredIntent *StructuredIntent  `json:"structured_intent,omitempty"`
    ConfidenceChain  []ConfidenceEntry  `json:"confidence_chain,omitempty"`
    RerouteHistory   []RerouteHop       `json:"reroute_history,omitempty"`
}

// StructuredIntent captures the original user request in a structured format.
// Extracted by Guide at request intake, carried through entire lifecycle.
// Enables any agent to reference original constraints and success criteria.
type StructuredIntent struct {
    OriginalRequest string   `json:"original_request"`  // Verbatim user request (bounded)
    KeyConstraints  []string `json:"key_constraints"`   // Extracted constraints
    SuccessCriteria []string `json:"success_criteria"`  // What constitutes success
    IntentType      string   `json:"intent_type"`       // Classified intent type
}

// ConfidenceEntry records an agent's confidence after processing.
// Each agent appends to the chain when it processes a message.
type ConfidenceEntry struct {
    Agent      string  `json:"agent"`       // Agent ID
    Confidence float64 `json:"confidence"`  // 0.0 - 1.0
    Reason     string  `json:"reason"`      // Brief explanation
}

// RerouteHop records a reroute event when an agent's Step 0 rejects a request.
type RerouteHop struct {
    From   string `json:"from"`    // Agent that rejected
    Reason string `json:"reason"`  // Why it rejected
    To     string `json:"to"`      // Where it was rerouted
}
```

### Message Types

```
USER_INPUT              User message, needs intent classification
USER_RESPONSE           User response to agent question

SESSION_CREATE          Create new session
SESSION_SWITCH          Switch active session
SESSION_SUSPEND         Suspend current session
SESSION_RESUME          Resume suspended session
SESSION_COMPLETE        Complete session

RESEARCH_REQUEST        Query for Academic
RESEARCH_RESPONSE       Academic's research paper

CONTEXT_REQUEST         Query for Librarian
CONTEXT_RESPONSE        Librarian's codebase context

HISTORY_REQUEST         Query for Archivalist
HISTORY_RESPONSE        Archivalist's historical context
STORE_REQUEST           Store data in Archivalist

PLAN_REQUEST            Request Architect to create plan
PLAN_RESPONSE           Architect's plan (to User for approval)
PLAN_APPROVED           User approved plan
PLAN_MODIFIED           User wants changes

DAG_EXECUTE             Architect → Orchestrator: execute this DAG
DAG_STATUS              Orchestrator → Architect: status update

TASK_DISPATCH           Orchestrator → Engineer: do this task
TASK_COMPLETE           Engineer → Orchestrator: task done
TASK_FAILED             Engineer → Orchestrator: task failed
TASK_HELP               Engineer → Orchestrator: need clarification

CLARIFICATION_REQUEST   Architect → User: need input
CLARIFICATION_RESPONSE  User → Architect: here's the answer

VALIDATE_TASK           Orchestrator → Inspector: validate this
VALIDATION_RESULT       Inspector → Orchestrator: pass/fail
VALIDATION_FULL         Inspector → User: full validation results
VALIDATION_CORRECTIONS  Inspector → Architect: fixes needed

TEST_PLAN_REQUEST       Architect → Tester: create test plan
TEST_PLAN_RESPONSE      Tester → User: proposed tests
TEST_DAG_REQUEST        Tester → Architect: implement these tests
TESTS_READY             Orchestrator → Tester: tests implemented
TEST_RESULTS            Tester → User: test results
TEST_CORRECTIONS        Tester → Architect: impl fixes needed

USER_OVERRIDE           User → Inspector/Tester: ignore this issue
USER_INTERRUPT          User → Architect: stop, I want to change

WORKFLOW_COMPLETE       Architect → User: all done, summary

REROUTE_REQUEST         Agent → Guide: not my domain, reroute this
```

---

## Intent Preservation & Confidence Propagation

**CRITICAL: This system prevents information loss ("lossiness") as requests flow through multiple agents. The message carries all state; Guide only routes.**

### Problem Statement

Without intent preservation, information degrades at each hop:
```
User: "Add caching to improve API performance, careful about memory"
       ↓ (Guide classifies)
"Implementation request for caching"
       ↓ (Architect decomposes)
"Add Redis cache to /api/users endpoint"
       ↓ (Engineer executes)
"Added Redis cache"

❌ Original constraint "careful about memory" lost
❌ Success criterion "improve performance" never validated
```

### Solution: Message-Carried State

The message envelope carries three critical fields through the entire request lifecycle:

1. **StructuredIntent**: Original request + extracted constraints + success criteria
2. **ConfidenceChain**: Each agent's confidence after processing
3. **RerouteHistory**: Record of any domain mismatches and reroutes

### Responsibility Delineation

```
┌─────────────────────────────────────────────────────────────────────────────────────┐
│                        RESPONSIBILITY DELINEATION                                    │
├─────────────────────────────────────────────────────────────────────────────────────┤
│                                                                                     │
│  GUIDE (Stateless Router):                                                          │
│  ├── Extract StructuredIntent from initial user request (ONE TIME)                  │
│  ├── Initialize ConfidenceChain with first entry                                    │
│  ├── Route messages between agents                                                  │
│  ├── Route clarification requests to user                                           │
│  ├── Route user responses back to requesting agent                                  │
│  └── NEVER: evaluate responses, decide escalation, store state                      │
│                                                                                     │
│  REQUESTING AGENT (e.g., Architect querying Librarian):                             │
│  ├── Send query via Guide                                                           │
│  ├── Receive response via Guide                                                     │
│  ├── Evaluate response quality against StructuredIntent                             │
│  ├── Calculate cumulative confidence from ConfidenceChain                           │
│  ├── Decide: continue, query another agent, or escalate to user                     │
│  ├── If escalating: send CLARIFICATION_REQUEST to user via Guide                    │
│  └── Append own ConfidenceEntry to chain when responding                            │
│                                                                                     │
│  RESPONDING AGENT (e.g., Librarian responding to query):                            │
│  ├── Perform Step 0: Is this my domain?                                             │
│  ├── If NO: send REROUTE_REQUEST via Guide                                          │
│  ├── If YES: process request                                                        │
│  ├── Append ConfidenceEntry: {agent, confidence, reason}                            │
│  └── Send response via Guide (back to requesting agent)                             │
│                                                                                     │
│  USER:                                                                              │
│  ├── Receives clarification requests from agents (via Guide)                        │
│  ├── Provides clarification responses (via Guide, back to requesting agent)         │
│  └── Can interrupt/override at any point                                            │
│                                                                                     │
└─────────────────────────────────────────────────────────────────────────────────────┘
```

### Complete Request Lifecycle Flow

```
┌─────────────────────────────────────────────────────────────────────────────────────┐
│                         COMPLETE REQUEST LIFECYCLE                                   │
├─────────────────────────────────────────────────────────────────────────────────────┤
│                                                                                     │
│  USER: "Add caching to improve API performance, careful about memory"               │
│       │                                                                             │
│       ▼                                                                             │
│  ╔═══════════════════════════════════════════════════════════════════════════════╗  │
│  ║  GUIDE (Initial Processing)                                                   ║  │
│  ║                                                                               ║  │
│  ║  1. Extract StructuredIntent:                                                 ║  │
│  ║     {                                                                         ║  │
│  ║       original_request: "Add caching to improve API performance...",          ║  │
│  ║       key_constraints: ["memory-limited"],                                    ║  │
│  ║       success_criteria: ["improved API performance"],                         ║  │
│  ║       intent_type: "PERFORMANCE_OPTIMIZATION"                                 ║  │
│  ║     }                                                                         ║  │
│  ║                                                                               ║  │
│  ║  2. Initialize ConfidenceChain:                                               ║  │
│  ║     [{agent: "guide", confidence: 0.95, reason: "clear implementation req"}]  ║  │
│  ║                                                                               ║  │
│  ║  3. Step 0: "I'm the router. Implementation request." → Route to Architect    ║  │
│  ╚═══════════════════════════════════════════════════════════════════════════════╝  │
│       │                                                                             │
│       │  Message: {                                                                 │
│       │    target: "architect",                                                     │
│       │    structured_intent: {original_request: "...", key_constraints: [...]},   │
│       │    confidence_chain: [{guide, 0.95, "clear implementation req"}]           │
│       │  }                                                                          │
│       ▼                                                                             │
│  ┌───────────────────────────────────────────────────────────────────────────────┐  │
│  │  ARCHITECT (Receives from Guide)                                              │  │
│  │                                                                               │  │
│  │  1. Step 0: "I'm the planner. This is implementation." → YES, my domain       │  │
│  │                                                                               │  │
│  │  2. Decompose request:                                                        │  │
│  │     - Check structured_intent.key_constraints: ["memory-limited"]             │  │
│  │     - Note: must consider memory in caching approach                          │  │
│  │                                                                               │  │
│  │  3. Need codebase context → Query Librarian                                   │  │
│  │                                                                               │  │
│  │  4. Append confidence: {architect, 0.88, "decomposition complete"}            │  │
│  └───────────────────────────────────────────────────────────────────────────────┘  │
│       │                                                                             │
│       │  Message: {                                                                 │
│       │    target: "guide",                                                         │
│       │    type: "CONTEXT_REQUEST",                                                 │
│       │    payload: {query: "What caching patterns exist? Memory usage?"},          │
│       │    structured_intent: {...},                                                │
│       │    confidence_chain: [{guide, 0.95}, {architect, 0.88}]                     │
│       │  }                                                                          │
│       ▼                                                                             │
│  ╔═══════════════════════════════════════════════════════════════════════════════╗  │
│  ║  GUIDE (Routing Architect's Query)                                            ║  │
│  ║                                                                               ║  │
│  ║  1. Receive CONTEXT_REQUEST from Architect                                    ║  │
│  ║  2. Classify: codebase query → Librarian                                      ║  │
│  ║  3. Route to Librarian (message carries structured_intent, chain)             ║  │
│  ║                                                                               ║  │
│  ║  NOTE: Guide does NOT evaluate confidence. Just routes.                       ║  │
│  ╚═══════════════════════════════════════════════════════════════════════════════╝  │
│       │                                                                             │
│       │  Message: {                                                                 │
│       │    target: "librarian",                                                     │
│       │    type: "CONTEXT_REQUEST",                                                 │
│       │    payload: {query: "What caching patterns exist? Memory usage?"},          │
│       │    structured_intent: {...},                                                │
│       │    confidence_chain: [{guide, 0.95}, {architect, 0.88}]                     │
│       │  }                                                                          │
│       ▼                                                                             │
│  ┌───────────────────────────────────────────────────────────────────────────────┐  │
│  │  LIBRARIAN (Receives from Guide)                                              │  │
│  │                                                                               │  │
│  │  1. Step 0: "I'm codebase knowledge. Query asks about our patterns."          │  │
│  │     → YES, my domain                                                          │  │
│  │                                                                               │  │
│  │  2. Search codebase for caching patterns                                      │  │
│  │     - Found: Redis pattern in cache/redis.go                                  │  │
│  │     - Memory info: Limited, only found connection pooling settings            │  │
│  │                                                                               │  │
│  │  3. Append confidence: {librarian, 0.72, "found pattern, limited memory info"}│  │
│  └───────────────────────────────────────────────────────────────────────────────┘  │
│       │                                                                             │
│       │  Message: {                                                                 │
│       │    target: "guide",                                                         │
│       │    type: "CONTEXT_RESPONSE",                                                │
│       │    payload: {response: "Found Redis pattern in cache/redis.go..."},         │
│       │    structured_intent: {...},                                                │
│       │    confidence_chain: [{guide, 0.95}, {architect, 0.88}, {librarian, 0.72}]  │
│       │  }                                                                          │
│       ▼                                                                             │
│  ╔═══════════════════════════════════════════════════════════════════════════════╗  │
│  ║  GUIDE (Routing Librarian's Response)                                         ║  │
│  ║                                                                               ║  │
│  ║  1. Receive CONTEXT_RESPONSE from Librarian                                   ║  │
│  ║  2. Route back to original requester: Architect                               ║  │
│  ║                                                                               ║  │
│  ║  NOTE: Guide does NOT evaluate response. Just routes back.                    ║  │
│  ╚═══════════════════════════════════════════════════════════════════════════════╝  │
│       │                                                                             │
│       ▼                                                                             │
│  ┌───────────────────────────────────────────────────────────────────────────────┐  │
│  │  ARCHITECT (Receives Librarian's Response)                                    │  │
│  │                                                                               │  │
│  │  ** THIS IS WHERE EVALUATION HAPPENS - BY THE REQUESTING AGENT **             │  │
│  │                                                                               │  │
│  │  1. Examine response: "Found Redis pattern in cache/redis.go"                 │  │
│  │                                                                               │  │
│  │  2. Examine confidence_chain:                                                 │  │
│  │     - guide: 0.95                                                             │  │
│  │     - architect: 0.88                                                         │  │
│  │     - librarian: 0.72                                                         │  │
│  │     - cumulative: 0.95 × 0.88 × 0.72 = 0.60                                   │  │
│  │                                                                               │  │
│  │  3. Check structured_intent.key_constraints: ["memory-limited"]               │  │
│  │     - Response mentions Redis but limited memory info                         │  │
│  │     - Constraint not adequately addressed                                     │  │
│  │                                                                               │  │
│  │  4. DECISION: Cumulative confidence 0.60 < threshold 0.65                     │  │
│  │     AND key constraint "memory-limited" not satisfied                         │  │
│  │     → Escalate to user for clarification                                      │  │
│  │                                                                               │  │
│  │  5. Send CLARIFICATION_REQUEST to User (via Guide)                            │  │
│  └───────────────────────────────────────────────────────────────────────────────┘  │
│       │                                                                             │
│       │  Message: {                                                                 │
│       │    target: "guide",                                                         │
│       │    type: "CLARIFICATION_REQUEST",                                           │
│       │    payload: {                                                               │
│       │      question: "Librarian found Redis caching pattern, but I couldn't...", │
│       │      options: ["Proceed with Redis", "Research alternatives", "Clarify"],   │
│       │      context: "Your constraint was 'memory-limited'..."                     │
│       │    },                                                                       │
│       │    structured_intent: {...},                                                │
│       │    confidence_chain: [...]                                                  │
│       │  }                                                                          │
│       ▼                                                                             │
│  ╔═══════════════════════════════════════════════════════════════════════════════╗  │
│  ║  GUIDE (Routing Clarification to User)                                        ║  │
│  ║                                                                               ║  │
│  ║  Route CLARIFICATION_REQUEST to User                                          ║  │
│  ╚═══════════════════════════════════════════════════════════════════════════════╝  │
│       │                                                                             │
│       ▼                                                                             │
│  USER receives: "Librarian found Redis caching pattern, but I couldn't verify      │
│  it meets your 'memory-limited' constraint. Should I:                              │
│    A) Proceed with Redis pattern found                                             │
│    B) Research memory-efficient alternatives via Academic                          │
│    C) You clarify your memory requirements"                                        │
│                                                                                     │
└─────────────────────────────────────────────────────────────────────────────────────┘
```

### Agent-Specific Examples

#### Engineer Example: Querying Academic for Best Practices

```
┌─────────────────────────────────────────────────────────────────────────────────────┐
│                    ENGINEER EXAMPLE: QUERYING ACADEMIC                               │
├─────────────────────────────────────────────────────────────────────────────────────┤
│                                                                                     │
│  Context: Engineer implementing caching, unsure about eviction strategy             │
│                                                                                     │
│  ┌───────────────────────────────────────────────────────────────────────────────┐  │
│  │  ENGINEER (implementing task)                                                 │  │
│  │                                                                               │  │
│  │  1. Working on cache implementation                                           │  │
│  │  2. Uncertain about eviction strategy for memory-limited environment          │  │
│  │  3. Check structured_intent.key_constraints: ["memory-limited"]               │  │
│  │  4. Query Academic for best practices                                         │  │
│  └───────────────────────────────────────────────────────────────────────────────┘  │
│       │                                                                             │
│       │  Message: {type: "RESEARCH_REQUEST", query: "cache eviction strategies     │
│       │            for memory-constrained environments", ...}                       │
│       ▼                                                                             │
│  ╔═══════════════════════════════════════════════════════════════════════════════╗  │
│  ║  GUIDE → routes to → ACADEMIC                                                 ║  │
│  ╚═══════════════════════════════════════════════════════════════════════════════╝  │
│       │                                                                             │
│       ▼                                                                             │
│  ┌───────────────────────────────────────────────────────────────────────────────┐  │
│  │  ACADEMIC                                                                     │  │
│  │                                                                               │  │
│  │  1. Step 0: "I'm external knowledge. Query asks about best practices."        │  │
│  │     → YES, my domain                                                          │  │
│  │  2. Research eviction strategies                                              │  │
│  │  3. Append confidence: {academic, 0.85, "found relevant research"}            │  │
│  └───────────────────────────────────────────────────────────────────────────────┘  │
│       │                                                                             │
│       │  Response: "LRU with size limits recommended for memory-constrained..."    │
│       ▼                                                                             │
│  ╔═══════════════════════════════════════════════════════════════════════════════╗  │
│  ║  GUIDE → routes response back to → ENGINEER                                   ║  │
│  ╚═══════════════════════════════════════════════════════════════════════════════╝  │
│       │                                                                             │
│       ▼                                                                             │
│  ┌───────────────────────────────────────────────────────────────────────────────┐  │
│  │  ENGINEER (evaluates Academic's response)                                     │  │
│  │                                                                               │  │
│  │  1. Examine response: "LRU with size limits recommended..."                   │  │
│  │  2. Check against structured_intent.key_constraints: ["memory-limited"]       │  │
│  │     - Response directly addresses memory constraint ✓                         │  │
│  │  3. Calculate cumulative confidence: still acceptable                         │  │
│  │  4. DECISION: Response satisfactory → Continue implementation                 │  │
│  └───────────────────────────────────────────────────────────────────────────────┘  │
│                                                                                     │
│  ** NO ESCALATION NEEDED - Engineer continues with implementation **                │
│                                                                                     │
└─────────────────────────────────────────────────────────────────────────────────────┘
```

#### Inspector Example: Querying Archivalist for Failure Patterns

```
┌─────────────────────────────────────────────────────────────────────────────────────┐
│                  INSPECTOR EXAMPLE: QUERYING ARCHIVALIST                             │
├─────────────────────────────────────────────────────────────────────────────────────┤
│                                                                                     │
│  Context: Inspector validating code, checking if similar approach failed before     │
│                                                                                     │
│  ┌───────────────────────────────────────────────────────────────────────────────┐  │
│  │  INSPECTOR (validating implementation)                                        │  │
│  │                                                                               │  │
│  │  1. Reviewing caching implementation                                          │  │
│  │  2. Pattern looks familiar - want to check history                            │  │
│  │  3. Query Archivalist for failure patterns                                    │  │
│  └───────────────────────────────────────────────────────────────────────────────┘  │
│       │                                                                             │
│       │  Message: {type: "HISTORY_REQUEST", query: "Redis caching failures         │
│       │            or issues in past implementations", ...}                         │
│       ▼                                                                             │
│  ╔═══════════════════════════════════════════════════════════════════════════════╗  │
│  ║  GUIDE → routes to → ARCHIVALIST                                              ║  │
│  ╚═══════════════════════════════════════════════════════════════════════════════╝  │
│       │                                                                             │
│       ▼                                                                             │
│  ┌───────────────────────────────────────────────────────────────────────────────┐  │
│  │  ARCHIVALIST                                                                  │  │
│  │                                                                               │  │
│  │  1. Step 0: "I'm historical knowledge. Query asks about past failures."       │  │
│  │     → YES, my domain                                                          │  │
│  │  2. Search historical records                                                 │  │
│  │  3. Found: "Q3 incident - Redis OOM killed due to unbounded cache"            │  │
│  │  4. Append confidence: {archivalist, 0.92, "found relevant incident"}         │  │
│  └───────────────────────────────────────────────────────────────────────────────┘  │
│       │                                                                             │
│       │  Response: "Found incident: Redis OOM in Q3, unbounded cache growth..."    │
│       ▼                                                                             │
│  ╔═══════════════════════════════════════════════════════════════════════════════╗  │
│  ║  GUIDE → routes response back to → INSPECTOR                                  ║  │
│  ╚═══════════════════════════════════════════════════════════════════════════════╝  │
│       │                                                                             │
│       ▼                                                                             │
│  ┌───────────────────────────────────────────────────────────────────────────────┐  │
│  │  INSPECTOR (evaluates Archivalist's response)                                 │  │
│  │                                                                               │  │
│  │  1. Examine response: "Redis OOM in Q3, unbounded cache growth"               │  │
│  │  2. Check structured_intent.key_constraints: ["memory-limited"]               │  │
│  │  3. CRITICAL FINDING: Past failure directly relates to current constraint!    │  │
│  │  4. Check current implementation: Does it have size limits?                   │  │
│  │     - If NO → Generate VALIDATION_CORRECTIONS for Architect                   │  │
│  │     - If YES → Continue validation                                            │  │
│  └───────────────────────────────────────────────────────────────────────────────┘  │
│                                                                                     │
└─────────────────────────────────────────────────────────────────────────────────────┘
```

#### Tester Example: Unsatisfactory Response → User Escalation

```
┌─────────────────────────────────────────────────────────────────────────────────────┐
│                TESTER EXAMPLE: ESCALATION TO USER                                    │
├─────────────────────────────────────────────────────────────────────────────────────┤
│                                                                                     │
│  Context: Tester creating test plan, needs to understand performance expectations   │
│                                                                                     │
│  ┌───────────────────────────────────────────────────────────────────────────────┐  │
│  │  TESTER (creating test plan)                                                  │  │
│  │                                                                               │  │
│  │  1. Check structured_intent.success_criteria: ["improved API performance"]    │  │
│  │  2. Need concrete metrics to test against                                     │  │
│  │  3. Query Librarian for existing performance benchmarks                       │  │
│  └───────────────────────────────────────────────────────────────────────────────┘  │
│       │                                                                             │
│       │  Message: {type: "CONTEXT_REQUEST", query: "existing API performance       │
│       │            benchmarks or SLAs", ...}                                        │
│       ▼                                                                             │
│  ╔═══════════════════════════════════════════════════════════════════════════════╗  │
│  ║  GUIDE → routes to → LIBRARIAN                                                ║  │
│  ╚═══════════════════════════════════════════════════════════════════════════════╝  │
│       │                                                                             │
│       ▼                                                                             │
│  ┌───────────────────────────────────────────────────────────────────────────────┐  │
│  │  LIBRARIAN                                                                    │  │
│  │                                                                               │  │
│  │  1. Step 0: "I'm codebase knowledge. Query asks about our benchmarks."        │  │
│  │     → YES, my domain                                                          │  │
│  │  2. Search codebase for performance metrics                                   │  │
│  │  3. Found: Nothing concrete, just some TODO comments                          │  │
│  │  4. Append confidence: {librarian, 0.35, "no concrete benchmarks found"}      │  │
│  └───────────────────────────────────────────────────────────────────────────────┘  │
│       │                                                                             │
│       │  Response: "No performance benchmarks found. TODO in api/handlers.go..."   │
│       ▼                                                                             │
│  ╔═══════════════════════════════════════════════════════════════════════════════╗  │
│  ║  GUIDE → routes response back to → TESTER                                     ║  │
│  ╚═══════════════════════════════════════════════════════════════════════════════╝  │
│       │                                                                             │
│       ▼                                                                             │
│  ┌───────────────────────────────────────────────────────────────────────────────┐  │
│  │  TESTER (evaluates Librarian's response)                                      │  │
│  │                                                                               │  │
│  │  1. Examine response: "No performance benchmarks found"                       │  │
│  │  2. Examine confidence_chain: librarian confidence only 0.35                  │  │
│  │  3. Check structured_intent.success_criteria: ["improved API performance"]    │  │
│  │     - Cannot create meaningful tests without baseline metrics!                │  │
│  │  4. DECISION: Cannot proceed without user input                               │  │
│  │  5. Send CLARIFICATION_REQUEST to User (via Guide)                            │  │
│  └───────────────────────────────────────────────────────────────────────────────┘  │
│       │                                                                             │
│       │  Message: {type: "CLARIFICATION_REQUEST", payload: {...}}                   │
│       ▼                                                                             │
│  ╔═══════════════════════════════════════════════════════════════════════════════╗  │
│  ║  GUIDE → routes to → USER                                                     ║  │
│  ╚═══════════════════════════════════════════════════════════════════════════════╝  │
│       │                                                                             │
│       ▼                                                                             │
│  USER receives: "I need to create tests for 'improved API performance' but         │
│  couldn't find existing benchmarks in the codebase. Please specify:                │
│    A) Target response time (e.g., < 200ms)                                         │
│    B) Throughput requirements (e.g., 1000 req/s)                                   │
│    C) Comparison baseline (e.g., 'faster than current')"                           │
│       │                                                                             │
│       │  User responds: "Target < 100ms p99 latency"                                │
│       ▼                                                                             │
│  ╔═══════════════════════════════════════════════════════════════════════════════╗  │
│  ║  GUIDE → routes response back to → TESTER                                     ║  │
│  ╚═══════════════════════════════════════════════════════════════════════════════╝  │
│       │                                                                             │
│       ▼                                                                             │
│  ┌───────────────────────────────────────────────────────────────────────────────┐  │
│  │  TESTER (receives User's clarification)                                       │  │
│  │                                                                               │  │
│  │  1. User provided: "< 100ms p99 latency"                                      │  │
│  │  2. Now have concrete success criterion                                       │  │
│  │  3. Continue with test plan creation                                          │  │
│  └───────────────────────────────────────────────────────────────────────────────┘  │
│                                                                                     │
└─────────────────────────────────────────────────────────────────────────────────────┘
```

#### Designer Example: Step 0 Reroute

```
┌─────────────────────────────────────────────────────────────────────────────────────┐
│                      DESIGNER EXAMPLE: STEP 0 REROUTE                                │
├─────────────────────────────────────────────────────────────────────────────────────┤
│                                                                                     │
│  Context: Guide misroutes a backend query to Designer                               │
│                                                                                     │
│  ╔═══════════════════════════════════════════════════════════════════════════════╗  │
│  ║  GUIDE (misroutes)                                                            ║  │
│  ║                                                                               ║  │
│  ║  Query: "How should we style the cache response?"                             ║  │
│  ║  Guide interprets "style" as UI → Routes to Designer (INCORRECT)              ║  │
│  ╚═══════════════════════════════════════════════════════════════════════════════╝  │
│       │                                                                             │
│       ▼                                                                             │
│  ┌───────────────────────────────────────────────────────────────────────────────┐  │
│  │  DESIGNER (receives misrouted query)                                          │  │
│  │                                                                               │  │
│  │  1. Step 0: "I'm UI/UX. Query asks about 'cache response styling'."           │  │
│  │  2. Examine context: This is about API response format, not UI                │  │
│  │  3. → NO, not my domain                                                       │  │
│  │  4. Send REROUTE_REQUEST                                                      │  │
│  └───────────────────────────────────────────────────────────────────────────────┘  │
│       │                                                                             │
│       │  Message: {                                                                 │
│       │    type: "REROUTE_REQUEST",                                                 │
│       │    source: "designer",                                                      │
│       │    reason: "Query asks about API response format, not UI styling",          │
│       │    suggested_target: "engineer",                                            │
│       │    reroute_history: [{from: "designer", reason: "...", to: "engineer"}]     │
│       │  }                                                                          │
│       ▼                                                                             │
│  ╔═══════════════════════════════════════════════════════════════════════════════╗  │
│  ║  GUIDE (handles reroute)                                                      ║  │
│  ║                                                                               ║  │
│  ║  1. Receive REROUTE_REQUEST from Designer                                     ║  │
│  ║  2. Check reroute_history.length: 1 (first reroute, acceptable)               ║  │
│  ║  3. Route to suggested_target: Engineer                                       ║  │
│  ╚═══════════════════════════════════════════════════════════════════════════════╝  │
│       │                                                                             │
│       ▼                                                                             │
│  ┌───────────────────────────────────────────────────────────────────────────────┐  │
│  │  ENGINEER (receives correctly routed query)                                   │  │
│  │                                                                               │  │
│  │  1. Step 0: "I'm implementation. Query asks about response format."           │  │
│  │     → YES, my domain                                                          │  │
│  │  2. Process request normally                                                  │  │
│  └───────────────────────────────────────────────────────────────────────────────┘  │
│                                                                                     │
└─────────────────────────────────────────────────────────────────────────────────────┘
```

### Reroute Limit & User Escalation

```
┌─────────────────────────────────────────────────────────────────────────────────────┐
│                     REROUTE LIMIT & USER ESCALATION                                  │
├─────────────────────────────────────────────────────────────────────────────────────┤
│                                                                                     │
│  When reroute_history.length >= 3, Guide escalates to user:                         │
│                                                                                     │
│  reroute_history: [                                                                 │
│    {from: "librarian", reason: "asks about external practices", to: "academic"},    │
│    {from: "academic", reason: "asks about our codebase", to: "librarian"},          │
│    {from: "librarian", reason: "asks about external practices", to: "academic"}     │
│  ]                                                                                  │
│                                                                                     │
│  Guide detects ping-pong pattern → Escalate to user:                                │
│                                                                                     │
│  USER receives: "Your request has been routed between agents without resolution:    │
│                                                                                     │
│    - Librarian said: 'asks about external practices'                               │
│    - Academic said: 'asks about our codebase'                                      │
│    - Librarian said: 'asks about external practices'                               │
│                                                                                     │
│  Original request: 'How do our caching patterns compare to industry standards?'    │
│                                                                                     │
│  This seems to span both internal and external knowledge. Can you clarify:          │
│    A) Focus on our current implementation (Librarian)                              │
│    B) Focus on industry best practices (Academic)                                  │
│    C) I need both compared side-by-side"                                           │
│                                                                                     │
└─────────────────────────────────────────────────────────────────────────────────────┘
```

### Confidence Calculation Guidelines

```
┌─────────────────────────────────────────────────────────────────────────────────────┐
│                      CONFIDENCE CALCULATION GUIDELINES                               │
├─────────────────────────────────────────────────────────────────────────────────────┤
│                                                                                     │
│  AGENT CONFIDENCE REPORTING:                                                        │
│  Each agent appends a ConfidenceEntry when it processes/responds:                   │
│                                                                                     │
│  │  Confidence │ Meaning                                                           │
│  ├─────────────┼───────────────────────────────────────────────────────────────────│
│  │  0.90-1.00  │ High certainty - found exactly what was needed                    │
│  │  0.70-0.89  │ Good confidence - found relevant information                      │
│  │  0.50-0.69  │ Moderate - found partial information, gaps remain                 │
│  │  0.30-0.49  │ Low - limited relevant information found                          │
│  │  0.00-0.29  │ Very low - essentially guessing or found nothing useful           │
│  └─────────────┴───────────────────────────────────────────────────────────────────│
│                                                                                     │
│  CUMULATIVE CONFIDENCE CALCULATION:                                                 │
│  Product of all confidence values in chain:                                         │
│                                                                                     │
│  cumulative = chain[0].confidence × chain[1].confidence × ... × chain[n].confidence│
│                                                                                     │
│  Example:                                                                           │
│  chain: [{guide, 0.95}, {architect, 0.88}, {librarian, 0.72}]                       │
│  cumulative = 0.95 × 0.88 × 0.72 = 0.60                                             │
│                                                                                     │
│  THRESHOLD GUIDELINES (agent decides, not Guide):                                   │
│  │  Cumulative  │ Suggested Action                                                 │
│  ├──────────────┼──────────────────────────────────────────────────────────────────│
│  │  >= 0.65     │ Proceed with current information                                 │
│  │  0.50-0.64   │ Consider querying additional agents before proceeding            │
│  │  0.35-0.49   │ Strongly consider escalating to user                             │
│  │  < 0.35      │ Escalate to user - insufficient confidence to proceed            │
│  └──────────────┴──────────────────────────────────────────────────────────────────│
│                                                                                     │
│  IMPORTANT: These are GUIDELINES. The requesting agent makes the final decision    │
│  based on the specific context and structured_intent.                              │
│                                                                                     │
└─────────────────────────────────────────────────────────────────────────────────────┘
```

### Implicit Requirements Inference

**Problem**: User requests often contain unstated expectations. "Add logout button" implicitly assumes session cleanup, redirect behavior, and possibly confirmation dialogs. The StructuredIntent captures explicit constraints but misses domain-standard expectations.

**Solution**: Before decomposing a request, Architect queries Academic for standard expectations for the feature type/domain, then merges non-conflicting expectations into StructuredIntent.

#### Implicit Requirements Flow

```
┌─────────────────────────────────────────────────────────────────────────────────────┐
│                     IMPLICIT REQUIREMENTS INFERENCE FLOW                             │
├─────────────────────────────────────────────────────────────────────────────────────┤
│                                                                                     │
│  USER: "Add logout button"                                                          │
│       │                                                                             │
│       ▼                                                                             │
│  ╔═══════════════════════════════════════════════════════════════════════════════╗  │
│  ║  GUIDE (Initial Processing)                                                   ║  │
│  ║                                                                               ║  │
│  ║  Extract StructuredIntent:                                                    ║  │
│  ║  {                                                                            ║  │
│  ║    original_request: "Add logout button",                                     ║  │
│  ║    key_constraints: [],                                                       ║  │
│  ║    success_criteria: ["logout button exists"],   ← MINIMAL (explicit only)   ║  │
│  ║    intent_type: "CREATE"                                                      ║  │
│  ║  }                                                                            ║  │
│  ╚═══════════════════════════════════════════════════════════════════════════════╝  │
│       │                                                                             │
│       ▼                                                                             │
│  ┌───────────────────────────────────────────────────────────────────────────────┐  │
│  │  ARCHITECT (Pre-Decomposition Phase)                                         │  │
│  │                                                                               │  │
│  │  1. Receive request with StructuredIntent                                    │  │
│  │  2. Detect: intent_type=CREATE, feature_domain="authentication/logout"       │  │
│  │  3. TRIGGER: Query Academic for domain expectations                          │  │
│  └───────────────────────────────────────────────────────────────────────────────┘  │
│       │                                                                             │
│       │  Message: {                                                                 │
│       │    type: "DOMAIN_EXPECTATIONS_REQUEST",                                     │
│       │    payload: {                                                               │
│       │      intent_type: "CREATE",                                                 │
│       │      feature_domain: "authentication/logout",                               │
│       │      existing_constraints: []                                               │
│       │    }                                                                        │
│       │  }                                                                          │
│       ▼                                                                             │
│  ╔═══════════════════════════════════════════════════════════════════════════════╗  │
│  ║  GUIDE → routes to → ACADEMIC                                                 ║  │
│  ╚═══════════════════════════════════════════════════════════════════════════════╝  │
│       │                                                                             │
│       ▼                                                                             │
│  ┌───────────────────────────────────────────────────────────────────────────────┐  │
│  │  ACADEMIC (Domain Knowledge Provider)                                         │  │
│  │                                                                               │  │
│  │  1. Step 0: "I'm external knowledge. Query asks about standard practices."    │  │
│  │     → YES, my domain                                                          │  │
│  │                                                                               │  │
│  │  2. Retrieve domain expectations for "authentication/logout":                 │  │
│  │     - Session termination (server-side invalidation)                          │  │
│  │     - Token/cookie cleanup                                                    │  │
│  │     - Redirect to login or landing page                                       │  │
│  │     - Optional: confirmation for unsaved work                                 │  │
│  │     - Optional: "remember me" cleanup                                         │  │
│  │                                                                               │  │
│  │  3. Categorize expectations:                                                  │  │
│  │     - REQUIRED: Session termination, token cleanup, redirect                  │  │
│  │     - RECOMMENDED: Confirmation dialog if unsaved state possible              │  │
│  │     - OPTIONAL: "Remember me" cleanup                                         │  │
│  │                                                                               │  │
│  │  4. Append confidence: {academic, 0.88, "standard auth patterns"}             │  │
│  └───────────────────────────────────────────────────────────────────────────────┘  │
│       │                                                                             │
│       │  Response: {                                                                │
│       │    domain_expectations: [                                                   │
│       │      {expectation: "session termination", priority: "REQUIRED"},            │
│       │      {expectation: "token/cookie cleanup", priority: "REQUIRED"},           │
│       │      {expectation: "redirect to login", priority: "REQUIRED"},              │
│       │      {expectation: "confirmation if unsaved", priority: "RECOMMENDED"},     │
│       │      {expectation: "remember-me cleanup", priority: "OPTIONAL"}             │
│       │    ]                                                                        │
│       │  }                                                                          │
│       ▼                                                                             │
│  ╔═══════════════════════════════════════════════════════════════════════════════╗  │
│  ║  GUIDE → routes response back to → ARCHITECT                                  ║  │
│  ╚═══════════════════════════════════════════════════════════════════════════════╝  │
│       │                                                                             │
│       ▼                                                                             │
│  ┌───────────────────────────────────────────────────────────────────────────────┐  │
│  │  ARCHITECT (Merges Implicit Requirements)                                     │  │
│  │                                                                               │  │
│  │  1. Receive Academic's domain expectations                                    │  │
│  │                                                                               │  │
│  │  2. MERGE LOGIC (for each expectation):                                       │  │
│  │     a. Check: Does it conflict with any KeyConstraint?                        │  │
│  │        - If YES → Skip (explicit overrides implicit)                          │  │
│  │        - If NO → Continue                                                     │  │
│  │     b. Check: Is it already in SuccessCriteria?                               │  │
│  │        - If YES → Skip (already explicit)                                     │  │
│  │        - If NO → Add with source="inferred"                                   │  │
│  │                                                                               │  │
│  │  3. ENRICHED StructuredIntent:                                                │  │
│  │     {                                                                         │  │
│  │       original_request: "Add logout button",                                  │  │
│  │       key_constraints: [],                                                    │  │
│  │       success_criteria: [                                                     │  │
│  │         {criterion: "logout button exists", source: "explicit"},              │  │
│  │         {criterion: "session terminated on click", source: "inferred"},       │  │
│  │         {criterion: "tokens/cookies cleared", source: "inferred"},            │  │
│  │         {criterion: "redirect to login after logout", source: "inferred"}     │  │
│  │       ],                                                                      │  │
│  │       intent_type: "CREATE",                                                  │  │
│  │       inferred_from: "academic:authentication/logout"                         │  │
│  │     }                                                                         │  │
│  │                                                                               │  │
│  │  4. NOW decompose with enriched criteria                                      │  │
│  └───────────────────────────────────────────────────────────────────────────────┘  │
│                                                                                     │
└─────────────────────────────────────────────────────────────────────────────────────┘
```

#### Merge Precedence Rules

```
┌─────────────────────────────────────────────────────────────────────────────────────┐
│                         MERGE PRECEDENCE RULES                                       │
├─────────────────────────────────────────────────────────────────────────────────────┤
│                                                                                     │
│  PRECEDENCE ORDER (highest to lowest):                                              │
│                                                                                     │
│  1. EXPLICIT CONSTRAINT (from KeyConstraints)                                       │
│     └── Always wins. If user said "no confirmation dialog", skip that expectation  │
│                                                                                     │
│  2. EXPLICIT CRITERION (from SuccessCriteria, source="explicit")                    │
│     └── User-stated criteria are preserved exactly                                  │
│                                                                                     │
│  3. REQUIRED EXPECTATION (from Academic, priority="REQUIRED")                       │
│     └── Domain-standard requirements, merged if no conflict                         │
│                                                                                     │
│  4. RECOMMENDED EXPECTATION (from Academic, priority="RECOMMENDED")                 │
│     └── Best practices, merged if no conflict                                       │
│                                                                                     │
│  5. OPTIONAL EXPECTATION (from Academic, priority="OPTIONAL")                       │
│     └── Nice-to-haves, only merged if specifically relevant                         │
│                                                                                     │
│  ─────────────────────────────────────────────────────────────────────────────────  │
│                                                                                     │
│  CONFLICT DETECTION EXAMPLES:                                                       │
│                                                                                     │
│  User: "Add logout, skip the confirmation"                                          │
│  KeyConstraints: ["skip confirmation"]                                              │
│  Academic says: "confirmation if unsaved" (RECOMMENDED)                             │
│  Result: SKIP - explicit constraint overrides                                       │
│                                                                                     │
│  User: "Add logout with immediate redirect to home"                                 │
│  KeyConstraints: ["redirect to home"]                                               │
│  Academic says: "redirect to login" (REQUIRED)                                      │
│  Result: SKIP - user specified different redirect, respect explicit                 │
│                                                                                     │
│  User: "Add logout button"                                                          │
│  KeyConstraints: []                                                                 │
│  Academic says: "session termination" (REQUIRED)                                    │
│  Result: MERGE - no conflict, add as inferred criterion                             │
│                                                                                     │
└─────────────────────────────────────────────────────────────────────────────────────┘
```

#### State Diagram: Implicit Requirements State Machine

```
┌─────────────────────────────────────────────────────────────────────────────────────┐
│              IMPLICIT REQUIREMENTS INFERENCE STATE MACHINE                           │
├─────────────────────────────────────────────────────────────────────────────────────┤
│                                                                                     │
│                        ┌──────────────────┐                                         │
│                        │  REQUEST_RECEIVED │                                        │
│                        │  (initial state)  │                                        │
│                        └────────┬─────────┘                                         │
│                                 │                                                   │
│                    ┌────────────┴────────────┐                                      │
│                    │ Extract feature_domain  │                                      │
│                    │ from intent_type +      │                                      │
│                    │ original_request        │                                      │
│                    └────────────┬────────────┘                                      │
│                                 │                                                   │
│              ┌──────────────────┴──────────────────┐                                │
│              ▼                                     ▼                                │
│  ┌───────────────────────┐           ┌───────────────────────┐                      │
│  │  DOMAIN_IDENTIFIABLE  │           │  DOMAIN_UNKNOWN       │                      │
│  │  (can query Academic) │           │  (skip inference)     │                      │
│  └───────────┬───────────┘           └───────────┬───────────┘                      │
│              │                                   │                                  │
│              ▼                                   │                                  │
│  ┌───────────────────────┐                       │                                  │
│  │  QUERYING_ACADEMIC    │                       │                                  │
│  │  (awaiting response)  │                       │                                  │
│  └───────────┬───────────┘                       │                                  │
│              │                                   │                                  │
│   ┌──────────┴──────────┐                        │                                  │
│   ▼                     ▼                        │                                  │
│ ┌─────────────┐  ┌──────────────┐                │                                  │
│ │ EXPECTATIONS│  │ NO_EXPECTATIONS│               │                                  │
│ │ _RECEIVED   │  │ _FOUND        │               │                                  │
│ └──────┬──────┘  └───────┬──────┘                │                                  │
│        │                 │                       │                                  │
│        ▼                 │                       │                                  │
│ ┌──────────────┐         │                       │                                  │
│ │ MERGING      │         │                       │                                  │
│ │ (apply rules)│         │                       │                                  │
│ └──────┬───────┘         │                       │                                  │
│        │                 │                       │                                  │
│        └────────┬────────┴───────────────────────┘                                  │
│                 ▼                                                                   │
│        ┌───────────────────┐                                                        │
│        │  INTENT_ENRICHED  │                                                        │
│        │  (ready for       │                                                        │
│        │   decomposition)  │                                                        │
│        └───────────────────┘                                                        │
│                                                                                     │
└─────────────────────────────────────────────────────────────────────────────────────┘
```

#### Implementation: Domain Expectations Types

```go
// DomainExpectation represents a standard expectation for a feature domain.
type DomainExpectation struct {
    Expectation string             `json:"expectation"`  // What is expected
    Priority    ExpectationPriority `json:"priority"`    // REQUIRED, RECOMMENDED, OPTIONAL
    Rationale   string             `json:"rationale"`    // Why this is expected
}

type ExpectationPriority string

const (
    ExpectationRequired    ExpectationPriority = "REQUIRED"    // Domain-standard, should always do
    ExpectationRecommended ExpectationPriority = "RECOMMENDED" // Best practice, do unless conflict
    ExpectationOptional    ExpectationPriority = "OPTIONAL"    // Nice-to-have, context-dependent
)

// DomainExpectationsRequest is sent from Architect to Academic.
type DomainExpectationsRequest struct {
    IntentType          string   `json:"intent_type"`          // CREATE, MODIFY, FIX, etc.
    FeatureDomain       string   `json:"feature_domain"`       // e.g., "authentication/logout"
    ExistingConstraints []string `json:"existing_constraints"` // User's explicit constraints
}

// DomainExpectationsResponse is returned by Academic.
type DomainExpectationsResponse struct {
    Domain       string              `json:"domain"`
    Expectations []DomainExpectation `json:"expectations"`
    Confidence   float64             `json:"confidence"`
}

// EnrichedSuccessCriterion extends SuccessCriteria with source tracking.
type EnrichedSuccessCriterion struct {
    Criterion string `json:"criterion"`
    Source    string `json:"source"` // "explicit" or "inferred"
    Priority  string `json:"priority,omitempty"` // For inferred: REQUIRED, RECOMMENDED, OPTIONAL
}
```

#### Implementation: Architect Merge Logic

```go
// MergeImplicitRequirements enriches StructuredIntent with domain expectations.
func (a *Architect) MergeImplicitRequirements(
    intent *StructuredIntent,
    expectations []DomainExpectation,
) *StructuredIntent {
    enriched := &StructuredIntent{
        OriginalRequest: intent.OriginalRequest,
        KeyConstraints:  intent.KeyConstraints,
        IntentType:      intent.IntentType,
        SuccessCriteria: make([]EnrichedSuccessCriterion, 0),
    }

    // 1. Preserve all explicit criteria
    for _, criterion := range intent.SuccessCriteria {
        enriched.SuccessCriteria = append(enriched.SuccessCriteria, EnrichedSuccessCriterion{
            Criterion: criterion,
            Source:    "explicit",
        })
    }

    // 2. Merge non-conflicting expectations
    for _, exp := range expectations {
        if a.conflictsWithConstraints(exp.Expectation, intent.KeyConstraints) {
            continue // Explicit constraint overrides
        }
        if a.alreadyInCriteria(exp.Expectation, intent.SuccessCriteria) {
            continue // Already explicit
        }

        // Add inferred criterion
        enriched.SuccessCriteria = append(enriched.SuccessCriteria, EnrichedSuccessCriterion{
            Criterion: exp.Expectation,
            Source:    "inferred",
            Priority:  string(exp.Priority),
        })
    }

    return enriched
}

// conflictsWithConstraints checks if expectation contradicts any constraint.
func (a *Architect) conflictsWithConstraints(expectation string, constraints []string) bool {
    // Use semantic similarity to detect conflicts
    // e.g., "redirect to login" conflicts with "redirect to home"
    // e.g., "show confirmation" conflicts with "skip confirmation"
    for _, constraint := range constraints {
        if a.semanticConflict(expectation, constraint) {
            return true
        }
    }
    return false
}
```

### Feedback Validation Gate

**Problem**: Inspector/Tester may focus on local issues ("test X fails") without considering whether their correction aligns with the original intent. This can cause fixes that pass tests but violate constraints or miss the broader goal.

**Solution**: Inspector and Tester must validate their corrections against StructuredIntent before sending `VALIDATION_CORRECTIONS` or `TEST_CORRECTIONS`. Each correction includes an `IntentAlignment` assessment.

#### Feedback Validation Flow

```
┌─────────────────────────────────────────────────────────────────────────────────────┐
│                       FEEDBACK VALIDATION GATE FLOW                                  │
├─────────────────────────────────────────────────────────────────────────────────────┤
│                                                                                     │
│  Context: Original request was "Add caching without breaking existing tests"        │
│  StructuredIntent.KeyConstraints: ["no breaking existing tests"]                    │
│  StructuredIntent.SuccessCriteria: ["caching implemented", "existing tests pass"]   │
│                                                                                     │
│  ┌───────────────────────────────────────────────────────────────────────────────┐  │
│  │  INSPECTOR (validates implementation)                                         │  │
│  │                                                                               │  │
│  │  1. Run validation checks                                                     │  │
│  │  2. Detect issue: Test `TestUserFetch` fails after caching added              │  │
│  │  3. Draft correction: "Mock the cache in TestUserFetch"                       │  │
│  │                                                                               │  │
│  │  ════════════════════════════════════════════════════════════════════════     │  │
│  │  ║  VALIDATION GATE (MANDATORY BEFORE SENDING CORRECTION)                ║     │  │
│  │  ════════════════════════════════════════════════════════════════════════     │  │
│  │                                                                               │  │
│  │  4. For each drafted correction, evaluate against StructuredIntent:           │  │
│  │                                                                               │  │
│  │     CORRECTION: "Mock the cache in TestUserFetch"                             │  │
│  │                                                                               │  │
│  │     ┌─────────────────────────────────────────────────────────────────────┐   │  │
│  │     │ VALIDATION CHECK 1: Constraint Conflicts                           │   │  │
│  │     │                                                                     │   │  │
│  │     │ KeyConstraints: ["no breaking existing tests"]                      │   │  │
│  │     │ Does "Mock the cache" conflict?                                     │   │  │
│  │     │ → NO direct conflict (mocking doesn't break tests)                  │   │  │
│  │     │ → Result: PASS                                                      │   │  │
│  │     └─────────────────────────────────────────────────────────────────────┘   │  │
│  │                                                                               │  │
│  │     ┌─────────────────────────────────────────────────────────────────────┐   │  │
│  │     │ VALIDATION CHECK 2: Criteria Impact                                │   │  │
│  │     │                                                                     │   │  │
│  │     │ SuccessCriteria: ["caching implemented", "existing tests pass"]     │   │  │
│  │     │ Does "Mock the cache" affect criteria?                              │   │  │
│  │     │ → "caching implemented": NEGATIVE IMPACT                            │   │  │
│  │     │   (mocking means cache behavior not actually tested)                │   │  │
│  │     │ → "existing tests pass": POSITIVE IMPACT                            │   │  │
│  │     │   (test will pass with mock)                                        │   │  │
│  │     │ → Result: MIXED - flag for review                                   │   │  │
│  │     └─────────────────────────────────────────────────────────────────────┘   │  │
│  │                                                                               │  │
│  │     ┌─────────────────────────────────────────────────────────────────────┐   │  │
│  │     │ VALIDATION CHECK 3: Root Cause Assessment                          │   │  │
│  │     │                                                                     │   │  │
│  │     │ Is this correction addressing root cause or symptom?                │   │  │
│  │     │ → Symptom: Test fails                                               │   │  │
│  │     │ → Proposed fix: Mock to make test pass                              │   │  │
│  │     │ → Assessment: SYMPTOM FIX (doesn't address why cache breaks test)   │   │  │
│  │     │ → RootCauseConfidence: 0.35                                         │   │  │
│  │     │ → Result: LOW - suggest investigating actual cause                  │   │  │
│  │     └─────────────────────────────────────────────────────────────────────┘   │  │
│  │                                                                               │  │
│  │  5. Generate IntentAlignment for correction:                                  │  │
│  │     {                                                                         │  │
│  │       conflicts_with_constraints: [],                                         │  │
│  │       affected_criteria: [                                                    │  │
│  │         {criterion: "caching implemented", impact: "NEGATIVE",                │  │
│  │          reason: "mocking bypasses actual cache behavior"}                    │  │
│  │       ],                                                                      │  │
│  │       root_cause_confidence: 0.35,                                            │  │
│  │       root_cause_assessment: "Addresses symptom (test failure) not cause"     │  │
│  │     }                                                                         │  │
│  │                                                                               │  │
│  │  6. GATE DECISION:                                                            │  │
│  │     - Constraint conflicts: 0                                                 │  │
│  │     - Negative criteria impact: 1                                             │  │
│  │     - Root cause confidence: 0.35 (LOW)                                       │  │
│  │     → FLAG CORRECTION with warning                                            │  │
│  │                                                                               │  │
│  └───────────────────────────────────────────────────────────────────────────────┘  │
│       │                                                                             │
│       │  Message: {                                                                 │
│       │    type: "VALIDATION_CORRECTIONS",                                          │
│       │    corrections: [{                                                          │
│       │      issue: "TestUserFetch fails after caching added",                      │
│       │      correction: "Mock the cache in TestUserFetch",                         │
│       │      intent_alignment: {                                                    │
│       │        conflicts_with_constraints: [],                                      │
│       │        affected_criteria: [{                                                │
│       │          criterion: "caching implemented",                                  │
│       │          impact: "NEGATIVE",                                                │
│       │          reason: "mocking bypasses actual cache behavior"                   │
│       │        }],                                                                  │
│       │        root_cause_confidence: 0.35,                                         │
│       │        root_cause_assessment: "Addresses symptom not cause",                │
│       │        ⚠️ warning: "This correction may leave cache behavior untested.      │
│       │           Consider: Is the cache invalidating correctly? Root cause may    │
│       │           be cache TTL or key generation, not test setup."                 │
│       │      }                                                                      │
│       │    }]                                                                       │
│       │  }                                                                          │
│       ▼                                                                             │
│  ╔═══════════════════════════════════════════════════════════════════════════════╗  │
│  ║  GUIDE → routes to → ARCHITECT                                                ║  │
│  ╚═══════════════════════════════════════════════════════════════════════════════╝  │
│       │                                                                             │
│       ▼                                                                             │
│  ┌───────────────────────────────────────────────────────────────────────────────┐  │
│  │  ARCHITECT (Receives Flagged Correction)                                      │  │
│  │                                                                               │  │
│  │  1. See warning on correction                                                 │  │
│  │  2. Options:                                                                  │  │
│  │     a. Request Inspector investigate root cause first                         │  │
│  │     b. Escalate to user: "Inspector suggests mocking, but this may leave      │  │
│  │        cache untested. Should we investigate the actual failure cause?"       │  │
│  │     c. Accept correction with documented trade-off                            │  │
│  │                                                                               │  │
│  │  3. DECISION: Request root cause investigation                                │  │
│  └───────────────────────────────────────────────────────────────────────────────┘  │
│                                                                                     │
└─────────────────────────────────────────────────────────────────────────────────────┘
```

#### Validation Gate Decision Matrix

```
┌─────────────────────────────────────────────────────────────────────────────────────┐
│                     VALIDATION GATE DECISION MATRIX                                  │
├─────────────────────────────────────────────────────────────────────────────────────┤
│                                                                                     │
│  ┌─────────────────────┬─────────────────────┬───────────────────────────────────┐  │
│  │ Constraint Conflicts│ Criteria Impact     │ Action                            │  │
│  ├─────────────────────┼─────────────────────┼───────────────────────────────────┤  │
│  │ 0                   │ All POSITIVE/NEUTRAL│ ✅ PASS - Send correction         │  │
│  ├─────────────────────┼─────────────────────┼───────────────────────────────────┤  │
│  │ 0                   │ Any NEGATIVE        │ ⚠️ FLAG - Send with warning       │  │
│  ├─────────────────────┼─────────────────────┼───────────────────────────────────┤  │
│  │ 1+                  │ Any                 │ 🛑 BLOCK - Do not send, rethink   │  │
│  └─────────────────────┴─────────────────────┴───────────────────────────────────┘  │
│                                                                                     │
│  ROOT CAUSE CONFIDENCE THRESHOLDS:                                                  │
│  ┌──────────────────┬────────────────────────────────────────────────────────────┐  │
│  │ Confidence       │ Interpretation                                             │  │
│  ├──────────────────┼────────────────────────────────────────────────────────────┤  │
│  │ >= 0.80          │ HIGH - Likely addressing root cause                        │  │
│  ├──────────────────┼────────────────────────────────────────────────────────────┤  │
│  │ 0.50 - 0.79      │ MEDIUM - May be root cause, some uncertainty               │  │
│  ├──────────────────┼────────────────────────────────────────────────────────────┤  │
│  │ < 0.50           │ LOW - Likely symptom fix, recommend investigation          │  │
│  └──────────────────┴────────────────────────────────────────────────────────────┘  │
│                                                                                     │
│  COMBINED DECISION LOGIC:                                                           │
│                                                                                     │
│  if constraint_conflicts > 0:                                                       │
│      return BLOCK                                                                   │
│  elif any_negative_criteria_impact:                                                 │
│      if root_cause_confidence < 0.50:                                               │
│          return FLAG_WITH_STRONG_WARNING                                            │
│      else:                                                                          │
│          return FLAG_WITH_WARNING                                                   │
│  elif root_cause_confidence < 0.50:                                                 │
│      return FLAG_WITH_NOTE  # No criteria impact but low confidence                 │
│  else:                                                                              │
│      return PASS                                                                    │
│                                                                                     │
└─────────────────────────────────────────────────────────────────────────────────────┘
```

#### State Diagram: Correction Validation State Machine

```
┌─────────────────────────────────────────────────────────────────────────────────────┐
│                   CORRECTION VALIDATION STATE MACHINE                                │
├─────────────────────────────────────────────────────────────────────────────────────┤
│                                                                                     │
│                     ┌──────────────────────┐                                        │
│                     │  CORRECTION_DRAFTED  │                                        │
│                     │   (initial state)    │                                        │
│                     └──────────┬───────────┘                                        │
│                                │                                                    │
│                                ▼                                                    │
│                     ┌──────────────────────┐                                        │
│                     │ CHECKING_CONSTRAINTS │                                        │
│                     └──────────┬───────────┘                                        │
│                                │                                                    │
│              ┌─────────────────┴─────────────────┐                                  │
│              ▼                                   ▼                                  │
│  ┌───────────────────────┐         ┌───────────────────────┐                        │
│  │ CONSTRAINT_CONFLICT   │         │ NO_CONSTRAINT_CONFLICT│                        │
│  │ (conflicts > 0)       │         │ (conflicts = 0)       │                        │
│  └───────────┬───────────┘         └───────────┬───────────┘                        │
│              │                                 │                                    │
│              ▼                                 ▼                                    │
│  ┌───────────────────────┐         ┌──────────────────────┐                         │
│  │ BLOCKED              │         │ CHECKING_CRITERIA    │                         │
│  │ (cannot send)        │         │ IMPACT               │                         │
│  └───────────────────────┘         └──────────┬───────────┘                         │
│                                               │                                     │
│                         ┌─────────────────────┴─────────────────────┐               │
│                         ▼                                           ▼               │
│            ┌────────────────────────┐               ┌────────────────────────┐      │
│            │ NEGATIVE_IMPACT        │               │ NO_NEGATIVE_IMPACT     │      │
│            │ (any criteria harmed)  │               │ (all positive/neutral) │      │
│            └────────────┬───────────┘               └────────────┬───────────┘      │
│                         │                                        │                  │
│                         ▼                                        ▼                  │
│            ┌────────────────────────┐               ┌────────────────────────┐      │
│            │ ASSESSING_ROOT_CAUSE   │               │ ASSESSING_ROOT_CAUSE   │      │
│            └────────────┬───────────┘               └────────────┬───────────┘      │
│                         │                                        │                  │
│         ┌───────────────┴───────────────┐          ┌─────────────┴─────────────┐    │
│         ▼                               ▼          ▼                           ▼    │
│  ┌─────────────┐              ┌─────────────┐ ┌─────────────┐          ┌─────────┐  │
│  │ LOW_CONF    │              │ MEDIUM/HIGH │ │ LOW_CONF    │          │ HIGH    │  │
│  │ (<0.50)     │              │ (>=0.50)    │ │ (<0.50)     │          │ (>=0.50)│  │
│  └──────┬──────┘              └──────┬──────┘ └──────┬──────┘          └────┬────┘  │
│         │                            │               │                      │       │
│         ▼                            ▼               ▼                      ▼       │
│  ┌─────────────────┐     ┌─────────────────┐ ┌─────────────┐       ┌────────────┐   │
│  │ FLAGGED_STRONG  │     │ FLAGGED         │ │ FLAGGED_NOTE│       │ PASSED     │   │
│  │ ⚠️⚠️ warning    │     │ ⚠️ warning      │ │ 📝 note     │       │ ✅ send    │   │
│  └─────────────────┘     └─────────────────┘ └─────────────┘       └────────────┘   │
│                                                                                     │
└─────────────────────────────────────────────────────────────────────────────────────┘
```

#### Implementation: Correction Validation Types

```go
// CriteriaImpact represents how a correction affects a success criterion.
type CriteriaImpact struct {
    Criterion string `json:"criterion"`
    Impact    string `json:"impact"` // "POSITIVE", "NEGATIVE", "NEUTRAL"
    Reason    string `json:"reason"`
}

// IntentAlignment assesses how well a correction aligns with original intent.
type IntentAlignment struct {
    ConflictsWithConstraints []string         `json:"conflicts_with_constraints,omitempty"`
    AffectedCriteria         []CriteriaImpact `json:"affected_criteria,omitempty"`
    RootCauseConfidence      float64          `json:"root_cause_confidence"`
    RootCauseAssessment      string           `json:"root_cause_assessment"`
    Warning                  string           `json:"warning,omitempty"`
}

// CorrectionEntry represents a single correction with its validation.
type CorrectionEntry struct {
    Issue           string          `json:"issue"`
    Correction      string          `json:"correction"`
    IntentAlignment IntentAlignment `json:"intent_alignment"`
}

// ValidationGateResult represents the outcome of the validation gate.
type ValidationGateResult string

const (
    ValidationPassed       ValidationGateResult = "PASSED"        // ✅ Send correction
    ValidationFlagged      ValidationGateResult = "FLAGGED"       // ⚠️ Send with warning
    ValidationFlaggedStrong ValidationGateResult = "FLAGGED_STRONG" // ⚠️⚠️ Send with strong warning
    ValidationFlaggedNote  ValidationGateResult = "FLAGGED_NOTE"  // 📝 Send with note
    ValidationBlocked      ValidationGateResult = "BLOCKED"       // 🛑 Do not send
)
```

#### Implementation: Inspector Validation Gate

```go
// ValidateCorrection runs the correction through the validation gate.
func (i *Inspector) ValidateCorrection(
    correction string,
    issue string,
    intent *StructuredIntent,
) (*CorrectionEntry, ValidationGateResult) {
    alignment := IntentAlignment{}

    // Check 1: Constraint conflicts
    for _, constraint := range intent.KeyConstraints {
        if i.conflictsWith(correction, constraint) {
            alignment.ConflictsWithConstraints = append(
                alignment.ConflictsWithConstraints,
                constraint,
            )
        }
    }

    // If any constraint conflicts, BLOCK immediately
    if len(alignment.ConflictsWithConstraints) > 0 {
        return nil, ValidationBlocked
    }

    // Check 2: Criteria impact
    for _, criterion := range intent.SuccessCriteria {
        impact := i.assessImpact(correction, criterion)
        if impact.Impact != "NEUTRAL" {
            alignment.AffectedCriteria = append(alignment.AffectedCriteria, impact)
        }
    }

    // Check 3: Root cause assessment
    alignment.RootCauseConfidence = i.assessRootCauseConfidence(correction, issue)
    alignment.RootCauseAssessment = i.generateRootCauseAssessment(correction, issue)

    // Determine gate result
    hasNegativeImpact := false
    for _, impact := range alignment.AffectedCriteria {
        if impact.Impact == "NEGATIVE" {
            hasNegativeImpact = true
            break
        }
    }

    var result ValidationGateResult
    if hasNegativeImpact {
        if alignment.RootCauseConfidence < 0.50 {
            result = ValidationFlaggedStrong
            alignment.Warning = i.generateStrongWarning(correction, alignment)
        } else {
            result = ValidationFlagged
            alignment.Warning = i.generateWarning(correction, alignment)
        }
    } else if alignment.RootCauseConfidence < 0.50 {
        result = ValidationFlaggedNote
        alignment.Warning = i.generateNote(correction, alignment)
    } else {
        result = ValidationPassed
    }

    return &CorrectionEntry{
        Issue:           issue,
        Correction:      correction,
        IntentAlignment: alignment,
    }, result
}

// assessImpact evaluates how a correction affects a specific criterion.
func (i *Inspector) assessImpact(correction, criterion string) CriteriaImpact {
    // Semantic analysis of correction vs criterion
    // Returns POSITIVE if correction helps achieve criterion
    // Returns NEGATIVE if correction hinders or bypasses criterion
    // Returns NEUTRAL if no significant relationship

    // Example: "mock the cache" vs "caching implemented"
    // → NEGATIVE because mocking bypasses actual cache testing

    // Implementation uses semantic similarity and domain knowledge
    // ...
}
```

### Integration Guide: Combining Both Mechanisms

Both Implicit Requirements Inference and Feedback Validation Gate work together to form a closed loop of intent preservation:

```
┌─────────────────────────────────────────────────────────────────────────────────────┐
│                    INTEGRATED INTENT PRESERVATION LOOP                               │
├─────────────────────────────────────────────────────────────────────────────────────┤
│                                                                                     │
│  ┌──────────────────────────────────────────────────────────────────────────────┐   │
│  │                          USER REQUEST                                        │   │
│  │                    "Add logout button"                                       │   │
│  └──────────────────────────────────────────────────────────────────────────────┘   │
│                                      │                                              │
│                                      ▼                                              │
│  ╔══════════════════════════════════════════════════════════════════════════════╗   │
│  ║  PHASE 1: INTENT CAPTURE (Guide)                                             ║   │
│  ║  Extract StructuredIntent with explicit constraints/criteria                 ║   │
│  ╚══════════════════════════════════════════════════════════════════════════════╝   │
│                                      │                                              │
│                                      ▼                                              │
│  ╔══════════════════════════════════════════════════════════════════════════════╗   │
│  ║  PHASE 2: INTENT ENRICHMENT (Architect + Academic)                           ║   │
│  ║  ┌────────────────────────────────────────────────────────────────────────┐  ║   │
│  ║  │ IMPLICIT REQUIREMENTS INFERENCE                                        │  ║   │
│  ║  │                                                                        │  ║   │
│  ║  │ Architect queries Academic for domain expectations                     │  ║   │
│  ║  │ Academic returns: session termination, token cleanup, redirect         │  ║   │
│  ║  │ Architect merges non-conflicting expectations into SuccessCriteria     │  ║   │
│  ║  │                                                                        │  ║   │
│  ║  │ ENRICHED INTENT:                                                       │  ║   │
│  ║  │ - "logout button exists" (explicit)                                    │  ║   │
│  ║  │ - "session terminated" (inferred, REQUIRED)                            │  ║   │
│  ║  │ - "tokens cleared" (inferred, REQUIRED)                                │  ║   │
│  ║  │ - "redirect to login" (inferred, REQUIRED)                             │  ║   │
│  ║  └────────────────────────────────────────────────────────────────────────┘  ║   │
│  ╚══════════════════════════════════════════════════════════════════════════════╝   │
│                                      │                                              │
│                                      ▼                                              │
│  ╔══════════════════════════════════════════════════════════════════════════════╗   │
│  ║  PHASE 3: DECOMPOSITION & EXECUTION                                          ║   │
│  ║  Architect decomposes using enriched criteria                                ║   │
│  ║  Engineers execute with full context (StructuredIntent in every message)     ║   │
│  ╚══════════════════════════════════════════════════════════════════════════════╝   │
│                                      │                                              │
│                                      ▼                                              │
│  ╔══════════════════════════════════════════════════════════════════════════════╗   │
│  ║  PHASE 4: VALIDATION WITH GATE (Inspector/Tester)                            ║   │
│  ║  ┌────────────────────────────────────────────────────────────────────────┐  ║   │
│  ║  │ FEEDBACK VALIDATION GATE                                               │  ║   │
│  ║  │                                                                        │  ║   │
│  ║  │ Inspector detects: "No redirect implemented after logout"              │  ║   │
│  ║  │ Drafts correction: "Add redirect to /login after session clear"        │  ║   │
│  ║  │                                                                        │  ║   │
│  ║  │ VALIDATION GATE:                                                       │  ║   │
│  ║  │ - Constraint conflicts: 0 ✓                                            │  ║   │
│  ║  │ - Criteria impact: POSITIVE on "redirect to login" ✓                   │  ║   │
│  ║  │ - Root cause confidence: 0.95 ✓                                        │  ║   │
│  ║  │ → PASSED                                                               │  ║   │
│  ║  │                                                                        │  ║   │
│  ║  │ NOTE: Without enriched intent, Inspector might not have checked        │  ║   │
│  ║  │ for redirect at all! Implicit inference enables complete validation.   │  ║   │
│  ║  └────────────────────────────────────────────────────────────────────────┘  ║   │
│  ╚══════════════════════════════════════════════════════════════════════════════╝   │
│                                      │                                              │
│                                      ▼                                              │
│  ┌──────────────────────────────────────────────────────────────────────────────┐   │
│  │                     COMPLETE IMPLEMENTATION                                  │   │
│  │  All explicit AND inferred criteria validated                               │   │
│  └──────────────────────────────────────────────────────────────────────────────┘   │
│                                                                                     │
└─────────────────────────────────────────────────────────────────────────────────────┘
```

#### Integration Points

```
┌─────────────────────────────────────────────────────────────────────────────────────┐
│                         INTEGRATION DEPENDENCIES                                     │
├─────────────────────────────────────────────────────────────────────────────────────┤
│                                                                                     │
│  IMPLICIT REQUIREMENTS INFERENCE                                                    │
│  ├── Depends on: StructuredIntent (must exist in Message)                           │
│  ├── Depends on: Academic agent (provides domain expectations)                      │
│  ├── Integrates with: Architect (pre-decomposition phase)                           │
│  └── Output: EnrichedSuccessCriteria (with source tracking)                         │
│                                                                                     │
│  FEEDBACK VALIDATION GATE                                                           │
│  ├── Depends on: StructuredIntent (including enriched criteria from above)          │
│  ├── Depends on: IntentAlignment types (new)                                        │
│  ├── Integrates with: Inspector (pre-correction-send)                               │
│  ├── Integrates with: Tester (pre-correction-send)                                  │
│  └── Output: CorrectionEntry with IntentAlignment                                   │
│                                                                                     │
│  SYNERGY:                                                                           │
│  ┌────────────────────────────────────────────────────────────────────────────┐     │
│  │ Implicit Requirements → Enriches SuccessCriteria                           │     │
│  │                              ↓                                             │     │
│  │ Feedback Validation Gate → Uses enriched criteria for validation           │     │
│  │                              ↓                                             │     │
│  │ Result: Corrections validated against BOTH explicit AND inferred criteria  │     │
│  └────────────────────────────────────────────────────────────────────────────┘     │
│                                                                                     │
│  WITHOUT INTEGRATION:                                                               │
│  - Inspector only validates against explicit criteria (often minimal)               │
│  - Implicit expectations (session cleanup, redirect) never checked                  │
│  - Implementation may be "correct" but incomplete                                   │
│                                                                                     │
│  WITH INTEGRATION:                                                                  │
│  - Inspector validates against enriched criteria                                    │
│  - Implicit expectations surfaced and validated                                     │
│  - Implementation is both correct AND complete                                      │
│                                                                                     │
└─────────────────────────────────────────────────────────────────────────────────────┘
```

#### Message Types Required

```go
// New message type for domain expectations query
const (
    DOMAIN_EXPECTATIONS_REQUEST  MessageType = "DOMAIN_EXPECTATIONS_REQUEST"
    DOMAIN_EXPECTATIONS_RESPONSE MessageType = "DOMAIN_EXPECTATIONS_RESPONSE"
)
```

#### Architectural Constraints

1. **Guide remains stateless**: All enrichment happens in Architect, travels in Message
2. **Explicit always wins**: User constraints override inferred expectations
3. **Source tracking required**: All criteria must have `source` field for traceability
4. **Validation gate is mandatory**: Inspector/Tester MUST run gate before sending corrections
5. **Blocked corrections are not sent**: If gate returns BLOCKED, correction is discarded and agent must rethink

---

## Session Management System

**CRITICAL: Sessions are the fundamental unit of isolation in Sylk. Every operation happens within a session context.**

### Session Principles

1. **Maximal Independence**: Sessions maintain isolated contexts to prevent context pollution
2. **Shared Knowledge Access**: Sessions can query historical data from ANY Archivalist across ALL sessions
3. **No Cross-Contamination**: Active state (current task, blockers, in-progress work) is session-private
4. **Preservation**: Sessions can be suspended and resumed with full context restoration

### Session Architecture

```
┌─────────────────────────────────────────────────────────────────────────────────────┐
│                            SESSION MANAGEMENT LAYER                                  │
├─────────────────────────────────────────────────────────────────────────────────────┤
│                                                                                     │
│  ┌─────────────────────────────────────────────────────────────────────────────┐   │
│  │                         SESSION MANAGER                                      │   │
│  │                                                                              │   │
│  │  Responsibilities:                                                           │   │
│  │  ├── Create new sessions with unique IDs                                     │   │
│  │  ├── Switch active session for user                                          │   │
│  │  ├── List all sessions (active, suspended, completed)                        │   │
│  │  ├── Suspend sessions (preserve full context)                                │   │
│  │  ├── Resume sessions (restore full context)                                  │   │
│  │  ├── Close/archive sessions                                                  │   │
│  │  ├── Enforce session isolation                                               │   │
│  │  └── Manage session lifecycle hooks                                          │   │
│  │                                                                              │   │
│  └─────────────────────────────────────────────────────────────────────────────┘   │
│                                        │                                            │
│          ┌─────────────────────────────┼─────────────────────────────┐              │
│          │                             │                             │              │
│          ▼                             ▼                             ▼              │
│  ┌───────────────┐            ┌───────────────┐            ┌───────────────┐       │
│  │  SESSION A    │            │  SESSION B    │            │  SESSION C    │       │
│  │  (Active)     │            │  (Suspended)  │            │  (Active)     │       │
│  │               │            │               │            │               │       │
│  │ ┌───────────┐ │            │ ┌───────────┐ │            │ ┌───────────┐ │       │
│  │ │ Context   │ │            │ │ Context   │ │            │ │ Context   │ │       │
│  │ │ (Private) │ │            │ │ (Frozen)  │ │            │ │ (Private) │ │       │
│  │ └───────────┘ │            │ └───────────┘ │            │ └───────────┘ │       │
│  │               │            │               │            │               │       │
│  │ ┌───────────┐ │            │ ┌───────────┐ │            │ ┌───────────┐ │       │
│  │ │ Agents    │ │            │ │ Agents    │ │            │ │ Agents    │ │       │
│  │ │ (Scoped)  │ │            │ │ (Paused)  │ │            │ │ (Scoped)  │ │       │
│  │ └───────────┘ │            │ └───────────┘ │            │ └───────────┘ │       │
│  │               │            │               │            │               │       │
│  │ ┌───────────┐ │            │ ┌───────────┐ │            │ ┌───────────┐ │       │
│  │ │ DAG State │ │            │ │ DAG State │ │            │ │ DAG State │ │       │
│  │ │ (Active)  │ │            │ │ (Frozen)  │ │            │ │ (Active)  │ │       │
│  │ └───────────┘ │            │ └───────────┘ │            │ └───────────┘ │       │
│  └───────────────┘            └───────────────┘            └───────────────┘       │
│          │                             │                             │              │
│          └─────────────────────────────┼─────────────────────────────┘              │
│                                        │                                            │
│                                        ▼                                            │
│  ┌─────────────────────────────────────────────────────────────────────────────┐   │
│  │                     SHARED ARCHIVALIST DATABASE                              │   │
│  │                                                                              │   │
│  │  All sessions can READ from:                                                 │   │
│  │  ├── Past decisions (from any session)                                       │   │
│  │  ├── Learned patterns (from any session)                                     │   │
│  │  ├── Historical failures (from any session)                                  │   │
│  │  ├── Codebase changes (from any session)                                     │   │
│  │  └── User preferences (from any session)                                     │   │
│  │                                                                              │   │
│  │  Sessions WRITE to session-scoped partitions:                                │   │
│  │  ├── Entry.SessionID = current session ID                                    │   │
│  │  ├── Enables per-session filtering                                           │   │
│  │  └── Enables cross-session querying                                          │   │
│  │                                                                              │   │
│  └─────────────────────────────────────────────────────────────────────────────┘   │
│                                                                                     │
└─────────────────────────────────────────────────────────────────────────────────────┘
```

### Session Data Model

```go
// core/session/session.go

type SessionID string

type SessionState string

const (
    SessionStateCreated   SessionState = "created"
    SessionStateActive    SessionState = "active"
    SessionStateSuspended SessionState = "suspended"
    SessionStateCompleted SessionState = "completed"
    SessionStateFailed    SessionState = "failed"
)

// Session represents an isolated working context
type Session struct {
    ID            SessionID              `json:"id"`
    Name          string                 `json:"name,omitempty"`
    State         SessionState           `json:"state"`

    // Timestamps
    CreatedAt     time.Time              `json:"created_at"`
    ActivatedAt   *time.Time             `json:"activated_at,omitempty"`
    SuspendedAt   *time.Time             `json:"suspended_at,omitempty"`
    CompletedAt   *time.Time             `json:"completed_at,omitempty"`
    LastActiveAt  time.Time              `json:"last_active_at"`

    // Context (session-private, isolated)
    Context       *SessionContext        `json:"context"`

    // Workflow state
    ActiveDAGID   string                 `json:"active_dag_id,omitempty"`
    DAGHistory    []string               `json:"dag_history,omitempty"`

    // Git isolation
    BranchName    string                 `json:"branch_name,omitempty"`
    BaseBranch    string                 `json:"base_branch,omitempty"`

    // Resource limits
    Config        SessionConfig          `json:"config"`

    // Metadata
    Metadata      map[string]any         `json:"metadata,omitempty"`
}

// SessionContext holds session-private state (NOT shared across sessions)
type SessionContext struct {
    // Current work state
    CurrentTask       string             `json:"current_task,omitempty"`
    CurrentStep       string             `json:"current_step,omitempty"`
    CurrentObjective  string             `json:"current_objective,omitempty"`
    CompletedSteps    []string           `json:"completed_steps,omitempty"`
    NextSteps         []string           `json:"next_steps,omitempty"`
    Blockers          []string           `json:"blockers,omitempty"`

    // File tracking (session-local)
    ModifiedFiles     map[string]*FileState  `json:"modified_files,omitempty"`
    ReadFiles         map[string]*FileRead   `json:"read_files,omitempty"`
    CreatedFiles      map[string]*FileCreate `json:"created_files,omitempty"`

    // Pattern tracking (session-local discoveries)
    LocalPatterns     []*Pattern            `json:"local_patterns,omitempty"`

    // Failure tracking (session-local)
    LocalFailures     []*Failure            `json:"local_failures,omitempty"`

    // User intents (session-local)
    UserWants         []*Intent             `json:"user_wants,omitempty"`
    UserRejects       []*Intent             `json:"user_rejects,omitempty"`

    // Agent states (which agents are active in this session)
    ActiveAgents      map[string]*AgentState `json:"active_agents,omitempty"`

    // Conversation context
    ConversationID    string                 `json:"conversation_id,omitempty"`
    MessageHistory    []string               `json:"message_history,omitempty"`
}

// SessionConfig configures session resource limits
type SessionConfig struct {
    MaxConcurrentTasks  int           `json:"max_concurrent_tasks"`
    MaxEngineers        int           `json:"max_engineers"`
    TaskTimeout         time.Duration `json:"task_timeout"`
    SessionTimeout      time.Duration `json:"session_timeout"`
    AutoSuspendAfter    time.Duration `json:"auto_suspend_after"`
    EnableGitIsolation  bool          `json:"enable_git_isolation"`
}
```

### Session Manager Interface

```go
// core/session/manager.go

type SessionManager interface {
    // Lifecycle
    Create(ctx context.Context, cfg CreateSessionConfig) (*Session, error)
    Activate(ctx context.Context, id SessionID) error
    Suspend(ctx context.Context, id SessionID) error
    Resume(ctx context.Context, id SessionID) error
    Complete(ctx context.Context, id SessionID, summary string) error
    Close(ctx context.Context, id SessionID) error

    // Queries
    Get(id SessionID) (*Session, bool)
    GetActive() *Session
    GetByState(state SessionState) []*Session
    List() []*Session

    // Context management
    GetContext(id SessionID) (*SessionContext, error)
    UpdateContext(id SessionID, updates func(*SessionContext)) error

    // Switching
    Switch(ctx context.Context, toID SessionID) (*Session, error)

    // Preservation
    Snapshot(id SessionID) (*SessionSnapshot, error)
    Restore(snapshot *SessionSnapshot) (*Session, error)

    // Cross-session queries (delegates to Archivalist)
    QueryHistory(ctx context.Context, query HistoryQuery) ([]*HistoryEntry, error)

    // Resource management
    Stats() SessionManagerStats
    CloseAll() error
}

type CreateSessionConfig struct {
    Name              string
    BaseBranch        string
    EnableGitIsolation bool
    Metadata          map[string]any
    Config            *SessionConfig  // nil = use defaults
}
```

### Session Isolation Rules

```
┌─────────────────────────────────────────────────────────────────────────────────────┐
│                           SESSION ISOLATION RULES                                    │
├─────────────────────────────────────────────────────────────────────────────────────┤
│                                                                                     │
│  ISOLATED (Session-Private):                                                        │
│  ┌──────────────────────────────────────────────────────────────────────────────┐   │
│  │                                                                              │   │
│  │  ✓ Current task, step, objective                                             │   │
│  │  ✓ Completed steps, next steps, blockers                                     │   │
│  │  ✓ Modified/created files (in-progress)                                      │   │
│  │  ✓ Read file tracking                                                        │   │
│  │  ✓ Active DAG state and execution progress                                   │   │
│  │  ✓ Engineer instances and their work                                         │   │
│  │  ✓ Clarification requests pending                                            │   │
│  │  ✓ User intent for THIS session                                              │   │
│  │  ✓ Conversation context                                                      │   │
│  │  ✓ Git branch (if isolation enabled)                                         │   │
│  │                                                                              │   │
│  │  WHY: Prevents one session's in-progress work from polluting another         │   │
│  │                                                                              │   │
│  └──────────────────────────────────────────────────────────────────────────────┘   │
│                                                                                     │
│  SHARED (Cross-Session Readable):                                                   │
│  ┌──────────────────────────────────────────────────────────────────────────────┐   │
│  │                                                                              │   │
│  │  ✓ Committed decisions (with session_id for filtering)                       │   │
│  │  ✓ Learned patterns (promoted from session-local)                            │   │
│  │  ✓ Historical failures and resolutions                                       │   │
│  │  ✓ Past workflow outcomes                                                    │   │
│  │  ✓ User preferences (promoted as global)                                     │   │
│  │  ✓ Codebase patterns from Librarian                                          │   │
│  │  ✓ Research from Academic                                                    │   │
│  │                                                                              │   │
│  │  WHY: Enables learning across sessions without contamination                 │   │
│  │                                                                              │   │
│  └──────────────────────────────────────────────────────────────────────────────┘   │
│                                                                                     │
│  PROMOTION RULES:                                                                   │
│  ┌──────────────────────────────────────────────────────────────────────────────┐   │
│  │                                                                              │   │
│  │  Session-local → Shared happens when:                                        │   │
│  │  ├── Session completes successfully                                          │   │
│  │  ├── User explicitly promotes a pattern/decision                             │   │
│  │  ├── Inspector validates and approves                                        │   │
│  │  └── Pattern is used successfully N times                                    │   │
│  │                                                                              │   │
│  │  Shared data includes session_id for:                                        │   │
│  │  ├── Filtering queries to specific sessions                                  │   │
│  │  ├── Attribution and traceability                                            │   │
│  │  └── Rollback if needed                                                      │   │
│  │                                                                              │   │
│  └──────────────────────────────────────────────────────────────────────────────┘   │
│                                                                                     │
└─────────────────────────────────────────────────────────────────────────────────────┘
```

### Session Lifecycle

```
┌─────────────────────────────────────────────────────────────────────────────────────┐
│                            SESSION LIFECYCLE                                         │
├─────────────────────────────────────────────────────────────────────────────────────┤
│                                                                                     │
│                              ┌──────────┐                                           │
│                              │ CREATED  │                                           │
│                              └────┬─────┘                                           │
│                                   │ Activate()                                      │
│                                   ▼                                                 │
│                              ┌──────────┐                                           │
│                    ┌─────────│  ACTIVE  │─────────┐                                 │
│                    │         └────┬─────┘         │                                 │
│                    │              │               │                                 │
│         Suspend()  │              │ Complete()    │ Fail                            │
│                    │              │               │                                 │
│                    ▼              ▼               ▼                                 │
│              ┌──────────┐   ┌──────────┐   ┌──────────┐                             │
│              │SUSPENDED │   │COMPLETED │   │  FAILED  │                             │
│              └────┬─────┘   └──────────┘   └──────────┘                             │
│                   │                                                                 │
│         Resume()  │                                                                 │
│                   │                                                                 │
│                   └───────────────┐                                                 │
│                                   │                                                 │
│                                   ▼                                                 │
│                              ┌──────────┐                                           │
│                              │  ACTIVE  │                                           │
│                              └──────────┘                                           │
│                                                                                     │
│  State Transitions:                                                                 │
│  ├── CREATED → ACTIVE: Session initialization complete                             │
│  ├── ACTIVE → SUSPENDED: User switches to different session                        │
│  ├── ACTIVE → COMPLETED: All tasks done, user confirms                             │
│  ├── ACTIVE → FAILED: Unrecoverable error                                          │
│  ├── SUSPENDED → ACTIVE: User returns to session (Resume)                          │
│  └── SUSPENDED → COMPLETED: Cleanup of abandoned session                           │
│                                                                                     │
└─────────────────────────────────────────────────────────────────────────────────────┘
```

### Session Context Preservation

```go
// What gets preserved when a session is suspended
type SessionSnapshot struct {
    Session           *Session              `json:"session"`
    ArchivalistState  *ArchivalistSnapshot  `json:"archivalist_state"`
    GuideState        *GuideSnapshot        `json:"guide_state"`
    DAGState          *DAGSnapshot          `json:"dag_state,omitempty"`
    PendingMessages   []*PendingMessage     `json:"pending_messages,omitempty"`
    AgentStates       map[string]*AgentSnapshot `json:"agent_states,omitempty"`
    CreatedAt         time.Time             `json:"created_at"`
}

// ArchivalistSnapshot captures session-specific Archivalist state
type ArchivalistSnapshot struct {
    SessionID         string                `json:"session_id"`
    AgentContext      *AgentBriefing        `json:"agent_context"`
    ResumeState       *ResumeState          `json:"resume_state"`
    LocalEntryIDs     []string              `json:"local_entry_ids"`
    PendingWrites     []*Entry              `json:"pending_writes,omitempty"`
}

// GuideSnapshot captures session-specific Guide state
type GuideSnapshot struct {
    SessionID         string                `json:"session_id"`
    PendingRequests   []*PendingRequest     `json:"pending_requests"`
    ActiveRoutes      map[string]string     `json:"active_routes"`
}

// DAGSnapshot captures in-progress workflow state
type DAGSnapshot struct {
    DAGID             string                `json:"dag_id"`
    CurrentLayer      int                   `json:"current_layer"`
    NodeStates        map[string]NodeState  `json:"node_states"`
    CompletedResults  map[string]*NodeResult `json:"completed_results"`
    PendingNodes      []string              `json:"pending_nodes"`
}
```

### Cross-Session Queries

```go
// Archivalist supports cross-session queries with explicit session filtering
type ArchiveQuery struct {
    // Existing fields...
    Categories      []Category    `json:"categories,omitempty"`
    Sources         []SourceModel `json:"sources,omitempty"`
    Since           *time.Time    `json:"since,omitempty"`
    Until           *time.Time    `json:"until,omitempty"`
    SearchText      string        `json:"search_text,omitempty"`
    Limit           int           `json:"limit,omitempty"`

    // Session filtering (NEW)
    SessionIDs      []string      `json:"session_ids,omitempty"`      // Filter to specific sessions
    ExcludeSessions []string      `json:"exclude_sessions,omitempty"` // Exclude specific sessions
    CurrentOnly     bool          `json:"current_only"`               // Only current session
    CrossSession    bool          `json:"cross_session"`              // Explicitly query all sessions

    // Promotion status
    PromotedOnly    bool          `json:"promoted_only"`              // Only globally promoted entries
}
```

---

## Multi-Session Architecture

### Session Coordination

```
┌─────────────────────────────────────────────────────────────────────────────────────┐
│                     MULTI-SESSION ARCHITECTURE                                      │
├─────────────────────────────────────────────────────────────────────────────────────┤
│                                                                                     │
│  Session A ──┐                                                                      │
│  Session B ──┼──→ Session Manager ──→ Guide (shared) ──→ Orchestrator (shared)      │
│  Session C ──┘           │                    │                    │                │
│                          │                    │                    │                │
│                          ▼                    ▼                    ▼                │
│                    ┌─────────────────────────────────────┐                          │
│                    │      ARCHIVALIST (shared)           │                          │
│                    │                                     │                          │
│                    │  Session-scoped writes:             │                          │
│                    │  ├── Entry.SessionID = source       │                          │
│                    │  ├── Isolated active state          │                          │
│                    │  └── Per-session file tracking      │                          │
│                    │                                     │                          │
│                    │  Cross-session reads:               │                          │
│                    │  ├── Promoted patterns (any)        │                          │
│                    │  ├── Promoted decisions (any)       │                          │
│                    │  ├── Historical failures (any)      │                          │
│                    │  └── Query with session_ids filter  │                          │
│                    │                                     │                          │
│                    └─────────────────────────────────────┘                          │
│                                     │                                               │
│                                     ▼                                               │
│                    ┌─────────────────────────────────────┐                          │
│                    │      RESOURCE MANAGER (shared)      │                          │
│                    │                                     │                          │
│                    │  ├── File-level read/write locks    │                          │
│                    │  ├── Branch management per session  │                          │
│                    │  └── Merge coordination             │                          │
│                    └─────────────────────────────────────┘                          │
│                                     │                                               │
│                                     ▼                                               │
│                    ┌─────────────────────────────────────┐                          │
│                    │      WORKER POOL (shared)           │                          │
│                    │                                     │                          │
│                    │  ├── Bounded total concurrency      │                          │
│                    │  ├── Fair scheduling across sessions│                          │
│                    │  ├── Per-session task limits        │                          │
│                    │  └── Priority lanes                 │                          │
│                    └─────────────────────────────────────┘                          │
│                                                                                     │
└─────────────────────────────────────────────────────────────────────────────────────┘
```

### File Conflict Resolution

For multiple sessions operating on the same codebase:

1. **Branch-isolated workflows**: Each session operates on its own git branch (if enabled)
2. **Shared read cache**: All sessions read from same Librarian cache
3. **Deferred conflicts**: Merge conflicts handled at session completion
4. **Bounded resources**: Single worker pool prevents resource exhaustion

---

## The Guide: Universal Message Router

**CRITICAL: The Guide is the universal message router for ALL user messages and ambiguous inter-agent requests. For token-optimized direct consultations between agents with known targets, see [Direct Consultation Protocol](#direct-consultation-protocol).**

### Guide Responsibilities

```
┌─────────────────────────────────────────────────────────────────────────────────────┐
│                           GUIDE RESPONSIBILITIES                                    │
├─────────────────────────────────────────────────────────────────────────────────────┤
│                                                                                     │
│  1. MESSAGE ROUTING                                                                 │
│     ├── Receive all messages from all agents                                        │
│     ├── Classify message intent/type                                                │
│     ├── Determine target agent                                                      │
│     ├── Deliver to target agent's inbox                                             │
│     ├── Handle responses back to source                                             │
│     └── SESSION-SCOPE all messages (inject session_id)                              │
│                                                                                     │
│  2. USER INTENT CLASSIFICATION                                                      │
│     ├── Research queries → Academic                                                 │
│     ├── Codebase queries → Librarian                                                │
│     ├── History queries → Archivalist                                               │
│     ├── Implementation requests → Architect                                         │
│     ├── Inspection phase queries → Inspector                                        │
│     ├── Testing phase queries → Tester                                              │
│     └── Session commands → Session Manager                                          │
│                                                                                     │
│  3. INTER-AGENT ROUTING                                                             │
│     ├── Context requests → Librarian                                                │
│     ├── History requests → Archivalist                                              │
│     ├── Research requests → Academic                                                │
│     ├── Task dispatches → specific Engineer                                         │
│     ├── Clarifications → Architect                                                  │
│     ├── Validations → Inspector                                                     │
│     ├── Test signals → Tester                                                       │
│     └── Storage → Archivalist                                                       │
│                                                                                     │
│  4. SESSION CONTEXT INJECTION                                                       │
│     ├── Attach session_id to all messages                                           │
│     ├── Verify agent belongs to session                                             │
│     ├── Enforce session isolation                                                   │
│     └── Route cross-session queries appropriately                                   │
│                                                                                     │
│  5. WORKFLOW SIGNALS                                                                │
│     ├── Detect when workflow modification needed                                    │
│     ├── Signal Orchestrator of changes                                              │
│     ├── Track phase transitions                                                     │
│     └── Maintain conversation context                                               │
│                                                                                     │
│  6. OPTIMIZATION                                                                    │
│     ├── Cache routing decisions                                                     │
│     ├── DSL fast-path for common routes                                             │
│     ├── LLM classification only when needed                                         │
│     └── Batch similar requests                                                      │
│                                                                                     │
│  7. OBSERVABILITY                                                                   │
│     ├── Log all message flows                                                       │
│     ├── Track latencies                                                             │
│     ├── Monitor agent health                                                        │
│     └── Provide debugging info                                                      │
│                                                                                     │
└─────────────────────────────────────────────────────────────────────────────────────┘
```

### All Message Flows Through Guide

```
USER → AGENT:
┌──────────────────────────────────────────────────────────────────────────────┐
│  User                                                                        │
│    │                                                                         │
│    │ "How would I design a rate limiter?"                                    │
│    │                                                                         │
│    ▼                                                                         │
│  GUIDE ─── classifies: research query ───▶ ACADEMIC                          │
│         ─── injects: session_id ───▶                                         │
│                                                                              │
└──────────────────────────────────────────────────────────────────────────────┘

AGENT → AGENT:
┌──────────────────────────────────────────────────────────────────────────────┐
│  Architect                                                                   │
│    │                                                                         │
│    │ "I need codebase context for middleware patterns"                       │
│    │                                                                         │
│    ▼                                                                         │
│  GUIDE ─── routes: context request ───▶ LIBRARIAN                            │
│    │    ─── verifies: same session ───▶                                      │
│    │                                                                         │
│    │◀─── response: [middleware patterns] ◀─── LIBRARIAN                      │
│    │                                                                         │
│    ▼                                                                         │
│  Architect (receives context)                                                │
│                                                                              │
└──────────────────────────────────────────────────────────────────────────────┘
```

### Knowledge Agent Consultation Protocol

**ALL agents (except Guide) can consult knowledge agents (Librarian, Archivalist, Academic) via Guide.**

This is the universal mechanism for implementation agents to access codebase context, historical data, and research.

```
┌─────────────────────────────────────────────────────────────────────────────────────┐
│                    KNOWLEDGE AGENT CONSULTATION PROTOCOL                             │
├─────────────────────────────────────────────────────────────────────────────────────┤
│                                                                                     │
│  KNOWLEDGE AGENTS:                                                                  │
│  ┌──────────────────────────────────────────────────────────────────────────────┐   │
│  │  LIBRARIAN    │ Codebase context, patterns, file locations, health          │   │
│  │  ARCHIVALIST  │ Historical data, past failures, decisions, metrics          │   │
│  │  ACADEMIC     │ Research, best practices, external knowledge                │   │
│  └──────────────────────────────────────────────────────────────────────────────┘   │
│                                                                                     │
│  WHO CAN CONSULT:                                                                   │
│  ┌──────────────────────────────────────────────────────────────────────────────┐   │
│  │  ✅ Architect   - Before planning, during clarification                      │   │
│  │  ✅ Engineer    - During implementation, when stuck                          │   │
│  │  ✅ Designer    - For UI patterns, accessibility, past designs               │   │
│  │  ✅ Inspector   - For pattern validation, past failures, best practices      │   │
│  │  ✅ Tester      - For test patterns, historical flakes, testing practices    │   │
│  │  ✅ Orchestrator- For execution context (rare)                               │   │
│  │  ❌ Guide       - Routes consultations, doesn't initiate them                │   │
│  └──────────────────────────────────────────────────────────────────────────────┘   │
│                                                                                     │
│  MESSAGE TYPES:                                                                     │
│  ┌──────────────────────────────────────────────────────────────────────────────┐   │
│  │  CONTEXT_REQUEST  → Librarian   │ "What patterns exist for X?"               │   │
│  │  CONTEXT_RESPONSE ← Librarian   │ Returns codebase context                   │   │
│  │                                                                              │   │
│  │  HISTORY_REQUEST  → Archivalist │ "Have we tried X before?"                  │   │
│  │  HISTORY_RESPONSE ← Archivalist │ Returns historical data                    │   │
│  │                                                                              │   │
│  │  RESEARCH_REQUEST → Academic    │ "What's best practice for X?"              │   │
│  │  RESEARCH_RESPONSE← Academic    │ Returns research/recommendations           │   │
│  └──────────────────────────────────────────────────────────────────────────────┘   │
│                                                                                     │
│  REQUEST FORMAT:                                                                    │
│  ┌──────────────────────────────────────────────────────────────────────────────┐   │
│  │  {                                                                           │   │
│  │    "type": "CONTEXT_REQUEST",           // or HISTORY_REQUEST, RESEARCH_REQ  │   │
│  │    "from_agent": "engineer",            // requesting agent                  │   │
│  │    "session_id": "sess_abc123",         // current session                   │   │
│  │    "query": {                                                                │   │
│  │      "intent": "pattern_lookup",        // what kind of query                │   │
│  │      "subject": "middleware",           // what we're asking about           │   │
│  │      "context": "implementing auth",    // why we're asking                  │   │
│  │      "specificity": "file_level"        // how detailed (file, function, line)│  │
│  │    },                                                                        │   │
│  │    "timeout_ms": 5000                   // optional timeout                  │   │
│  │  }                                                                           │   │
│  └──────────────────────────────────────────────────────────────────────────────┘   │
│                                                                                     │
│  RESPONSE FORMAT:                                                                   │
│  ┌──────────────────────────────────────────────────────────────────────────────┐   │
│  │  {                                                                           │   │
│  │    "type": "CONTEXT_RESPONSE",                                               │   │
│  │    "from_agent": "librarian",                                                │   │
│  │    "to_agent": "engineer",                                                   │   │
│  │    "session_id": "sess_abc123",                                              │   │
│  │    "result": {                                                               │   │
│  │      "found": true,                                                          │   │
│  │      "confidence": 0.92,                                                     │   │
│  │      "data": { ... },                   // actual context/history/research   │   │
│  │      "sources": ["file.go:42", ...]     // where this came from              │   │
│  │    },                                                                        │   │
│  │    "cached": false,                     // whether from cache                │   │
│  │    "latency_ms": 120                                                         │   │
│  │  }                                                                           │   │
│  └──────────────────────────────────────────────────────────────────────────────┘   │
│                                                                                     │
│  CONSULTATION FLOW:                                                                 │
│  ┌──────────────────────────────────────────────────────────────────────────────┐   │
│  │                                                                              │   │
│  │  Agent (e.g., Engineer)                                                      │   │
│  │    │                                                                         │   │
│  │    │ 1. Emit CONTEXT_REQUEST                                                 │   │
│  │    ▼                                                                         │   │
│  │  Guide                                                                       │   │
│  │    │ 2. Validate request (session, permissions)                              │   │
│  │    │ 3. Check cache (optional optimization)                                  │   │
│  │    │ 4. Route to Librarian                                                   │   │
│  │    ▼                                                                         │   │
│  │  Librarian                                                                   │   │
│  │    │ 5. Process query                                                        │   │
│  │    │ 6. Emit CONTEXT_RESPONSE                                                │   │
│  │    ▼                                                                         │   │
│  │  Guide                                                                       │   │
│  │    │ 7. Cache response (if cacheable)                                        │   │
│  │    │ 8. Route back to Engineer                                               │   │
│  │    ▼                                                                         │   │
│  │  Engineer (receives context)                                                 │   │
│  │                                                                              │   │
│  └──────────────────────────────────────────────────────────────────────────────┘   │
│                                                                                     │
│  COMMON CONSULTATION PATTERNS:                                                      │
│  ┌──────────────────────────────────────────────────────────────────────────────┐   │
│  │                                                                              │   │
│  │  PATTERN LOOKUP (Librarian):                                                 │   │
│  │  "What middleware patterns exist in this codebase?"                          │   │
│  │  "Where is error handling implemented?"                                      │   │
│  │  "Show me existing test utilities."                                          │   │
│  │                                                                              │   │
│  │  FAILURE CHECK (Archivalist):                                                │   │
│  │  "Have we tried this approach before?"                                       │   │
│  │  "What caused similar failures in the past?"                                 │   │
│  │  "Success rate for this type of change?"                                     │   │
│  │                                                                              │   │
│  │  BEST PRACTICE (Academic):                                                   │   │
│  │  "What's the recommended approach for rate limiting?"                        │   │
│  │  "Security best practices for auth tokens?"                                  │   │
│  │  "How should we structure this API?"                                         │   │
│  │                                                                              │   │
│  └──────────────────────────────────────────────────────────────────────────────┘   │
│                                                                                     │
│  WHEN TO CONSULT:                                                                   │
│  ├── BEFORE starting implementation (get context first)                           │
│  ├── WHEN stuck or uncertain (ask for patterns/history)                           │
│  ├── AFTER failures (check if similar failures occurred)                          │
│  └── FOR validation (confirm approach matches codebase)                           │
│                                                                                     │
│  WHEN NOT TO CONSULT:                                                               │
│  ├── Trivial changes (single-line fixes, typos)                                   │
│  ├── Already have cached context for this area                                    │
│  └── Time-critical operations (use cached data instead)                           │
│                                                                                     │
└─────────────────────────────────────────────────────────────────────────────────────┘
```

### Guide Step 0: Role-Aware Skill Decision (Routing-Level)

**This is the Guide's implementation of the universal Agent Step 0 (see "Agent Step 0: Role-Aware Skill Decision" in Agent Roles section).**

**Guide's Role**: Universal message router. Guide asks: "Given I'm the router and this incoming request, SHOULD I invoke any of my routing/session skills, or should I classify and route to another agent?"

```
┌─────────────────────────────────────────────────────────────────────────────────────┐
│                  GUIDE STEP 0: ROLE-AWARE SKILL DECISION (ROUTING)                   │
├─────────────────────────────────────────────────────────────────────────────────────┤
│                                                                                     │
│  GUIDE'S ROLE: Universal message router                                             │
│  GUIDE'S DOMAIN: Routing, session management, message correlation                   │
│                                                                                     │
│  STEP 0 QUESTION:                                                                   │
│  "Given I'm the router and this request, SHOULD I invoke my skills directly,        │
│   or should I classify and route to another agent?"                                 │
│                                                                                     │
│  STEP 0 FLOW:                                                                       │
│  ┌──────────────────────────────────────────────────────────────────────────────┐   │
│  │  User Request                                                                │   │
│  │       │                                                                      │   │
│  │       ▼                                                                      │   │
│  │  ┌─────────────────────────────────────────────────────────────────┐         │   │
│  │  │  STEP 0: ROLE-AWARE SKILL DECISION                              │         │   │
│  │  │                                                                 │         │   │
│  │  │  "I'm the router. Is this request something I should handle     │         │   │
│  │  │   directly with my skills?"                                     │         │   │
│  │  │                                                                 │         │   │
│  │  │  EVALUATE:                                                      │         │   │
│  │  │  - Is this a session command? (my domain)                       │         │   │
│  │  │  - Is this a routing command? (my domain)                       │         │   │
│  │  │  - Is this a request that needs classification + routing?       │         │   │
│  │  │                                                                 │         │   │
│  │  │  DECISION:                                                      │         │   │
│  │  │  - YES, my domain → Execute skill directly                      │         │   │
│  │  │  - NO, needs routing → Continue to Step 1 (Classification)      │         │   │
│  │  └─────────────────────────────────────────────────────────────────┘         │   │
│  └──────────────────────────────────────────────────────────────────────────────┘   │
│                                                                                     │
│  GUIDE'S DIRECT SKILLS (execute immediately):                                       │
│  ├── Session commands: "/session new", "/session list", "/session switch"          │
│  ├── Status commands: "/status", "/tasks"                                          │
│  └── System commands: "/help", "/clear"                                            │
│                                                                                     │
│  NOT GUIDE'S DOMAIN (proceed to classification):                                    │
│  ├── Implementation requests → classify, route to Architect                        │
│  ├── Research questions → classify, route to Academic                              │
│  ├── Codebase questions → classify, route to Librarian                             │
│  └── Everything else → classify and route appropriately                            │
│                                                                                     │
└─────────────────────────────────────────────────────────────────────────────────────┘
```

### Intent Gate Classification (Step 1)

**After skill matching (Step 0), Guide performs intent classification to route unmatched requests.**

```
┌─────────────────────────────────────────────────────────────────────────────────────┐
│                        INTENT GATE CLASSIFICATION (STEP 1)                           │
├─────────────────────────────────────────────────────────────────────────────────────┤
│                                                                                     │
│  PRINCIPLE: Classify request type BEFORE routing to ensure correct handling.        │
│                                                                                     │
│  REQUEST TYPE TAXONOMY:                                                             │
│  ┌──────────────────────────────────────────────────────────────────────────────┐   │
│  │  TRIVIAL        │ Single-line fix, typo, obvious small change                │   │
│  │                 │ → Fast-path to Engineer (skip Architect planning)          │   │
│  ├─────────────────┼────────────────────────────────────────────────────────────┤   │
│  │  EXPLICIT       │ User specified exact approach, no ambiguity                │   │
│  │                 │ → Route to Architect with approach locked                  │   │
│  ├─────────────────┼────────────────────────────────────────────────────────────┤   │
│  │  EXPLORATORY    │ "How would I...", research/learning question               │   │
│  │                 │ → Route to Academic or Librarian first                     │   │
│  ├─────────────────┼────────────────────────────────────────────────────────────┤   │
│  │  OPEN_ENDED     │ Ambiguous scope, multiple valid interpretations            │   │
│  │                 │ → Route to Architect for scope clarification               │   │
│  ├─────────────────┼────────────────────────────────────────────────────────────┤   │
│  │  GITHUB_WORK    │ PR, issue, commit mentioned explicitly                     │   │
│  │                 │ → Full cycle expected (investigate → implement → PR)       │   │
│  ├─────────────────┼────────────────────────────────────────────────────────────┤   │
│  │  AMBIGUOUS      │ Cannot classify with confidence                            │   │
│  │                 │ → Ask clarification before routing                         │   │
│  └─────────────────┴────────────────────────────────────────────────────────────┘   │
│                                                                                     │
│  VALIDATION CHECKS (Before Routing Implementation Requests):                        │
│  ├── Are assumptions EXPLICIT or IMPLICIT? (implicit → clarify)                     │
│  ├── Is scope BOUNDED or UNBOUNDED? (unbounded → clarify)                           │
│  ├── Does request match existing skill patterns?                                    │
│  ├── Should knowledge agents (Librarian/Academic/Archivalist) be consulted first?  │
│  └── Is there enough context to route confidently?                                  │
│                                                                                     │
│  PRE-ROUTING KNOWLEDGE AGENT TRIGGERS:                                              │
│  ├── Implementation request → Check Librarian for codebase health first            │
│  ├── Novel approach mentioned → Check Archivalist for failure patterns             │
│  └── External pattern referenced → Academic validates against codebase             │
│                                                                                     │
└─────────────────────────────────────────────────────────────────────────────────────┘
```

#### Guide Request Processing Flow

```
┌─────────────────────────────────────────────────────────────────────────────────────┐
│                          GUIDE REQUEST PROCESSING FLOW                               │
├─────────────────────────────────────────────────────────────────────────────────────┤
│                                                                                     │
│  User Request                                                                       │
│       │                                                                             │
│       ▼                                                                             │
│  ┌─────────────────────────────────────────────────────────────────┐                │
│  │  STEP 0: SKILL MATCHING (BLOCKING)                              │                │
│  │     - Check if request matches registered skill patterns        │                │
│  │     - Session commands: "/session new", "/status", etc.         │                │
│  │     - Workflow commands: "/plan", "/interrupt", etc.            │                │
│  │     - GitHub commands: "/pr", "/issue", "/commit"               │                │
│  └─────────────────────────────────────────────────────────────────┘                │
│       │                                                                             │
│       ├── SKILL MATCH (confidence > 0.9) ──────────► Execute skill directly         │
│       │                                                                             │
│       ▼ (No skill match)                                                            │
│  ┌─────────────────────────────────────────────────────────────────┐                │
│  │  STEP 1: CLASSIFY REQUEST TYPE                                  │                │
│  │     Is this TRIVIAL / EXPLICIT / EXPLORATORY / OPEN_ENDED /     │                │
│  │     GITHUB_WORK / AMBIGUOUS?                                    │                │
│  └─────────────────────────────────────────────────────────────────┘                │
│       │                                                                             │
│       ├── TRIVIAL ────────────────────────────────────► Engineer (fast-path)        │
│       │                                                                             │
│       ├── EXPLORATORY ────────────────────────────────► Academic/Librarian          │
│       │                                                                             │
│       ├── AMBIGUOUS ──────────────────────────────────► Clarification Request       │
│       │                                                                             │
│       ▼ (EXPLICIT / OPEN_ENDED / GITHUB_WORK)                                       │
│  ┌─────────────────────────────────────────────────────────────────┐                │
│  │  STEP 2: VALIDATION CHECKS                                      │                │
│  │     - Assumptions explicit?                                     │                │
│  │     - Scope bounded?                                            │                │
│  └─────────────────────────────────────────────────────────────────┘                │
│       │                                                                             │
│       ├── Checks FAIL ────────────────────────────────► Clarification Request       │
│       │                                                                             │
│       ▼ (Checks PASS)                                                               │
│  ┌─────────────────────────────────────────────────────────────────┐                │
│  │  STEP 3: PRE-ROUTING CONSULTATION (for implementation requests) │                │
│  │     - Query Librarian: "Codebase health for target area?"       │                │
│  │     - Query Archivalist: "Similar failure patterns?"            │                │
│  │     - Attach context to routed message                          │                │
│  └─────────────────────────────────────────────────────────────────┘                │
│       │                                                                             │
│       ▼                                                                             │
│  Route to Architect with enriched context                                           │
│                                                                                     │
└─────────────────────────────────────────────────────────────────────────────────────┘
```

#### Guide System Prompt

```go
const GuideSystemPrompt = `
You are the Guide agent. You are the universal message router for Sylk.
ALL messages between agents flow through you. You handle routing, classification, and skill matching.

REQUEST PROCESSING PROTOCOL:

STEP 0: SKILL MATCHING (BLOCKING - do this FIRST)
Check if request matches a registered skill pattern:
- Session commands: "/session new", "/session list", "/status"
- Workflow commands: "/plan", "/interrupt", "/approve"
- GitHub commands: "/pr", "/issue", "/commit"
- Direct queries: "what is [X]", "show me [X]"

If skill matches with confidence > 0.9, execute skill directly and SKIP classification.
Skills are deterministic and don't need LLM classification overhead.

STEP 1: CLASSIFY REQUEST TYPE (only if no skill match)
- TRIVIAL: Single-line fix, typo, obvious change → Fast-path to Engineer
- EXPLICIT: User specified exact approach → Architect with locked approach
- EXPLORATORY: "How would I...", learning question → Academic/Librarian first
- OPEN_ENDED: Ambiguous scope → Architect for clarification
- GITHUB_WORK: PR/issue/commit mentioned → Full cycle (investigate → implement → PR)
- AMBIGUOUS: Cannot classify → Ask clarification before routing

STEP 2: VALIDATION CHECKS (for implementation requests)
1. Are assumptions explicit? If implicit, clarify first.
2. Is scope bounded? If unbounded, clarify first.
3. Is there enough context to route confidently?

STEP 3: PRE-ROUTING CONSULTATION
For implementation requests, BEFORE routing to Architect:
- Librarian: "What is codebase health for [target area]?"
- Archivalist: "Any failure patterns for [approach]?"
Attach responses to routed message as enriched context.

NEVER ROUTE AMBIGUOUS REQUESTS:
If confidence < 0.7, ask clarification instead of guessing.
"Your request could mean X or Y. Which do you intend?"
`
```

#### Routing Failure Tracking Protocol

**CRITICAL: Guide tracks routing failures to improve future routing decisions.**

```
┌─────────────────────────────────────────────────────────────────────────────────────┐
│                      ROUTING FAILURE TRACKING PROTOCOL                               │
├─────────────────────────────────────────────────────────────────────────────────────┤
│                                                                                     │
│  PRINCIPLE: Learn from misroutes to improve future routing accuracy.                │
│                                                                                     │
│  ROUTING FAILURE SIGNALS:                                                           │
│  ┌──────────────────────────────────────────────────────────────────────────────┐   │
│  │  1. EXPLICIT REDIRECT: Agent says "this should go to X instead"              │   │
│  │  2. USER CORRECTION: User says "I meant to ask Librarian/Academic"           │   │
│  │  3. TASK_HELP ESCALATION: Engineer fails due to wrong context source         │   │
│  │  4. CLARIFICATION LOOP: Routed agent asks >2 clarifying questions            │   │
│  │  5. EMPTY RESPONSE: Routed agent cannot help with the request                │   │
│  └──────────────────────────────────────────────────────────────────────────────┘   │
│                                                                                     │
│  FAILURE ENTRY STRUCTURE:                                                           │
│  {                                                                                  │
│    "id": "route_fail_abc123",                                                       │
│    "original_request": "user's original message",                                   │
│    "classified_as": "EXPLORATORY",                                                  │
│    "routed_to": "academic",                                                         │
│    "should_have_been": "librarian",                                                 │
│    "failure_signal": "EXPLICIT_REDIRECT",                                           │
│    "pattern_extracted": "questions about 'our code' → Librarian not Academic",     │
│    "timestamp": "2024-01-15T10:30:00Z"                                              │
│  }                                                                                  │
│                                                                                     │
│  LEARNING INTEGRATION:                                                              │
│  ├── Store failures in Archivalist with category "routing_failure"                 │
│  ├── Query past failures during Intent Gate classification                         │
│  ├── Adjust confidence scores based on similar past misroutes                      │
│  └── Periodic pattern extraction for routing prompt refinement                     │
│                                                                                     │
│  METRICS TO TRACK:                                                                  │
│  ├── Routing accuracy rate per request type                                        │
│  ├── Most common misroute pairs (e.g., Academic↔Librarian)                         │
│  ├── Failure rate by confidence threshold                                          │
│  └── Time-to-correction (how quickly misroutes are caught)                         │
│                                                                                     │
└─────────────────────────────────────────────────────────────────────────────────────┘
```

#### Guide Routing Failure Skills

```go
// Routing Failure Tracking Skills
guide_skills_failure_tracking := []Skill{
    {
        Name:        "record_routing_failure",
        Description: "Record a routing failure for learning",
        Domain:      "routing_quality",
        Keywords:    []string{"misroute", "wrong agent", "redirect"},
        Parameters: []Param{
            {Name: "original_request", Type: "string", Required: true},
            {Name: "routed_to", Type: "string", Required: true},
            {Name: "should_have_been", Type: "string", Required: true},
            {Name: "failure_signal", Type: "enum", Values: []string{
                "EXPLICIT_REDIRECT", "USER_CORRECTION", "TASK_HELP_ESCALATION",
                "CLARIFICATION_LOOP", "EMPTY_RESPONSE",
            }, Required: true},
        },
    },
    {
        Name:        "query_routing_failures",
        Description: "Query past routing failures for similar requests",
        Domain:      "routing_quality",
        Keywords:    []string{"past misroutes", "routing history"},
        Parameters: []Param{
            {Name: "request_pattern", Type: "string", Required: true},
            {Name: "limit", Type: "int", Required: false},
        },
    },
    {
        Name:        "get_routing_accuracy",
        Description: "Get routing accuracy metrics",
        Domain:      "routing_quality",
        Keywords:    []string{"accuracy", "metrics", "success rate"},
    },
}
```

#### Guide Routing Failure System Prompt Addition

```go
const GuideRoutingFailurePrompt = `
ROUTING FAILURE TRACKING:
Learn from misroutes to improve future accuracy.

FAILURE SIGNALS TO DETECT:
1. EXPLICIT_REDIRECT: Agent responds "this should go to [other agent]"
2. USER_CORRECTION: User says "I meant to ask [agent]" or re-asks differently
3. TASK_HELP_ESCALATION: Downstream failure due to wrong context source
4. CLARIFICATION_LOOP: Routed agent asks >2 clarifying questions
5. EMPTY_RESPONSE: Agent says "I cannot help with this"

WHEN FAILURE DETECTED:
1. Record failure via record_routing_failure skill
2. Extract pattern: "requests mentioning 'our code' should go to Librarian"
3. Store in Archivalist category "routing_failure"

DURING INTENT GATE:
1. Query past routing failures for similar request patterns
2. If similar request was misrouted before, adjust confidence
3. If confidence drops below 0.7 due to past failures, ask clarification

FEEDBACK INTEGRATION:
When Architect reports TASK_HELP with context_source_issue:
- Record as routing failure
- The context source (Librarian/Academic/Archivalist) was wrong
- Learn: this request type needs different routing
`
```

### Guide Intent Classification for Librarian

**CRITICAL**: The Guide already classifies intent for routing. When routing to Librarian, the Guide extracts additional metadata to enable intent-aware caching - at zero additional token cost.

```
┌─────────────────────────────────────────────────────────────────────────────────────┐
│                    GUIDE INTENT CLASSIFICATION FOR LIBRARIAN                         │
├─────────────────────────────────────────────────────────────────────────────────────┤
│                                                                                     │
│  User: "what patterns do we use for error handling"                                 │
│       │                                                                             │
│       ▼                                                                             │
│  ┌─────────────────────────────────────────────────────────────────────────────┐   │
│  │  GUIDE ROUTING (already happening)                                          │   │
│  │                                                                             │   │
│  │  Standard classification:                                                   │   │
│  │    - Is this a codebase question? → YES → Route to Librarian               │   │
│  │                                                                             │   │
│  │  NEW: Additional metadata extraction (same LLM call, no extra cost):       │   │
│  │    - Query Intent: PATTERN (strategy/approach question)                    │   │
│  │    - Subject/Concept: "error_handling"                                     │   │
│  │    - Confidence: 0.92                                                      │   │
│  └─────────────────────────────────────────────────────────────────────────────┘   │
│       │                                                                             │
│       ▼                                                                             │
│  ┌─────────────────────────────────────────────────────────────────────────────┐   │
│  │  ROUTED MESSAGE TO LIBRARIAN                                                │   │
│  │                                                                             │   │
│  │  {                                                                          │   │
│  │    "query": "what patterns do we use for error handling",                  │   │
│  │    "target": "librarian",                                                  │   │
│  │    "session_id": "sess_abc123",                                            │   │
│  │    "intent": "PATTERN",           ← Enables cache lookup by concept        │   │
│  │    "subject": "error_handling",   ← Cache key                              │   │
│  │    "confidence": 0.92             ← Use cache if > 0.8                     │   │
│  │  }                                                                          │   │
│  └─────────────────────────────────────────────────────────────────────────────┘   │
│                                                                                     │
└─────────────────────────────────────────────────────────────────────────────────────┘
```

#### Intent Types for Librarian Queries

| Intent | Trigger Patterns | Example Queries |
|--------|-----------------|-----------------|
| **LOCATE** | "where is", "find", "show me", "which file", "locate" | "where is the auth code", "find CreateUser function" |
| **PATTERN** | "strategy", "approach", "pattern", "how do we", "convention" | "what is our caching strategy", "how do we handle errors" |
| **EXPLAIN** | "how does", "explain", "what does X do", "walk through" | "how does the auth flow work", "explain the pipeline" |
| **GENERAL** | Other codebase questions | "what languages are used", "list all API endpoints" |

#### Guide Routing Prompt Enhancement

```go
// Added to Guide's system prompt for Librarian routing
const LibrarianRoutingInstructions = `
When routing to Librarian, also classify the query:

Intent (required):
- LOCATE: User wants to find where something is (file, function, struct, etc.)
- PATTERN: User asks about patterns, strategies, approaches, conventions
- EXPLAIN: User wants to understand how something works
- GENERAL: Other codebase questions

Subject (required): The primary entity or concept being asked about.
Normalize to snake_case (e.g., "auth code" → "authentication", "error handling" → "error_handling")

Examples:
- "where is the auth middleware" → LOCATE, "auth_middleware"
- "what is our caching strategy" → PATTERN, "caching"
- "how does CreateSession work" → EXPLAIN, "create_session"
- "what testing frameworks do we use" → PATTERN, "testing"
`

// Guide extracts this during routing (same LLM call)
type LibrarianRoutingMetadata struct {
    Intent     QueryIntent `json:"intent"`
    Subject    string      `json:"subject"`
    Confidence float64     `json:"confidence"`
}
```

#### Why This Works (Zero Additional Cost)

```
WITHOUT Intent Classification:
┌────────────────────────────────────────────────────────────────────────────────────┐
│ Guide LLM Call: "Route this message"              → ~200 tokens                    │
│ Librarian LLM Call: "Answer this question"        → ~3000 tokens (every time)     │
│ TOTAL: ~3200 tokens per query                                                      │
└────────────────────────────────────────────────────────────────────────────────────┘

WITH Intent Classification (cache hit):
┌────────────────────────────────────────────────────────────────────────────────────┐
│ Guide LLM Call: "Route this + classify intent"    → ~250 tokens (+50 for metadata)│
│ Librarian: Cache hit using intent + subject       → 0 tokens                       │
│ TOTAL: ~250 tokens per query (on cache hit)                                        │
└────────────────────────────────────────────────────────────────────────────────────┘

WITH Intent Classification (cache miss):
┌────────────────────────────────────────────────────────────────────────────────────┐
│ Guide LLM Call: "Route this + classify intent"    → ~250 tokens                    │
│ Librarian LLM Call: "Answer this question"        → ~3000 tokens                   │
│ TOTAL: ~3250 tokens (same as before, now cached)                                   │
└────────────────────────────────────────────────────────────────────────────────────┘

At 70% cache hit rate:
- Old: 100 queries × 3200 tokens = 320,000 tokens
- New: 30 misses × 3250 + 70 hits × 250 = 97,500 + 17,500 = 115,000 tokens
- SAVINGS: 64%
```

### Guide Intent Classification for Archivalist

**CRITICAL**: The same zero-cost intent classification approach applies to Archivalist routing. The Guide extracts intent metadata during routing to enable intent-aware historical caching.

```
┌─────────────────────────────────────────────────────────────────────────────────────┐
│                    GUIDE INTENT CLASSIFICATION FOR ARCHIVALIST                       │
├─────────────────────────────────────────────────────────────────────────────────────┤
│                                                                                     │
│  User: "how did we handle the auth migration last month"                            │
│       │                                                                             │
│       ▼                                                                             │
│  ┌─────────────────────────────────────────────────────────────────────────────┐   │
│  │  GUIDE ROUTING (already happening)                                          │   │
│  │                                                                             │   │
│  │  Standard classification:                                                   │   │
│  │    - Is this a history/past question? → YES → Route to Archivalist         │   │
│  │                                                                             │   │
│  │  NEW: Additional metadata extraction (same LLM call, no extra cost):       │   │
│  │    - Query Intent: HISTORICAL (past solution/approach)                     │   │
│  │    - Subject/Concept: "auth_migration"                                     │   │
│  │    - Time Scope: "last_month"                                              │   │
│  │    - Confidence: 0.89                                                      │   │
│  └─────────────────────────────────────────────────────────────────────────────┘   │
│       │                                                                             │
│       ▼                                                                             │
│  ┌─────────────────────────────────────────────────────────────────────────────┐   │
│  │  ROUTED MESSAGE TO ARCHIVALIST                                              │   │
│  │                                                                             │   │
│  │  {                                                                          │   │
│  │    "query": "how did we handle the auth migration last month",             │   │
│  │    "target": "archivalist",                                                │   │
│  │    "session_id": "sess_abc123",                                            │   │
│  │    "intent": "HISTORICAL",        ← Enables cache lookup by subject        │   │
│  │    "subject": "auth_migration",   ← Cache key component                    │   │
│  │    "time_scope": "last_month",    ← Temporal partition hint                │   │
│  │    "confidence": 0.89             ← Use cache if > 0.8                     │   │
│  │  }                                                                          │   │
│  └─────────────────────────────────────────────────────────────────────────────┘   │
│                                                                                     │
└─────────────────────────────────────────────────────────────────────────────────────┘
```

#### Intent Types for Archivalist Queries

| Intent | Trigger Patterns | Example Queries |
|--------|-----------------|-----------------|
| **HISTORICAL** | "how did we", "what approach", "last time", "previously" | "how did we handle auth migration", "what was the solution for X" |
| **ACTIVITY** | "what has", "recent changes", "who worked on", "history of" | "what has happened with the API", "recent changes to auth" |
| **OUTCOME** | "did it work", "result of", "status of", "how did X turn out" | "did the migration succeed", "result of the refactor" |
| **SIMILAR** | "similar to", "like before", "same as", "comparable" | "similar issues to this error", "problems like this before" |
| **RESUME** | "where was I", "continue", "pick up", "last session" | "where did we leave off", "continue from yesterday" |
| **GENERAL** | Other history questions | "what did the team work on last week" |

#### Guide Routing Prompt Enhancement for Archivalist

```go
// Added to Guide's system prompt for Archivalist routing
const ArchivalistRoutingInstructions = `
When routing to Archivalist, also classify the query:

Intent (required):
- HISTORICAL: User asks about past solutions, approaches, decisions
- ACTIVITY: User asks about recent work, changes, who did what
- OUTCOME: User asks about results, success/failure of past work
- SIMILAR: User asks about similar problems or patterns from history
- RESUME: User wants to continue previous work or session state
- GENERAL: Other history questions

Subject (required): The primary entity or concept being asked about.
Normalize to snake_case (e.g., "auth migration" → "auth_migration")

Time Scope (optional): Extract any temporal hints:
- "last month", "yesterday", "last week" → specific time range
- "recently" → last 7 days
- "before" → historical, no specific time
- Empty if no time reference

Examples:
- "how did we handle the auth migration" → HISTORICAL, "auth_migration", ""
- "what changed in the API recently" → ACTIVITY, "api", "recently"
- "did the database fix work" → OUTCOME, "database_fix", ""
- "similar errors to this before" → SIMILAR, "errors", ""
- "where did we leave off yesterday" → RESUME, "session", "yesterday"
`

// Guide extracts this during routing (same LLM call)
type ArchivalistRoutingMetadata struct {
    Intent     ArchivalistIntent `json:"intent"`
    Subject    string            `json:"subject"`
    TimeScope  string            `json:"time_scope,omitempty"`
    Confidence float64           `json:"confidence"`
}
```

#### Archivalist vs Librarian Intent Classification

```
┌────────────────────────────────────────────────────────────────────────────────────┐
│                    KEY DIFFERENCES IN INTENT CLASSIFICATION                         │
├────────────────────────────────────────────────────────────────────────────────────┤
│                                                                                    │
│  LIBRARIAN (Live Code)                 │  ARCHIVALIST (Historical Data)           │
│  ─────────────────────────────────────┼──────────────────────────────────────────│
│  Subject: File/Function/Struct names   │  Subject: Problem/Solution domains       │
│  Example: "auth_middleware"            │  Example: "auth_migration"               │
│                                        │                                          │
│  No time scope (current state)         │  Time scope (temporal partitions)        │
│  Example: "where is auth"              │  Example: "last month's auth work"       │
│                                        │                                          │
│  Invalidation: File changes            │  Invalidation: Session/TTL only          │
│  Dynamic cache keys                    │  Immutable once recorded                 │
│                                        │                                          │
│  Cache duration: 5-30 minutes          │  Cache duration: 1-60 minutes            │
│  (depends on file volatility)          │  (depends on query type)                 │
│                                        │                                          │
└────────────────────────────────────────────────────────────────────────────────────┘
```

#### Expected Token Savings for Archivalist

```
WITHOUT Intent Classification:
┌────────────────────────────────────────────────────────────────────────────────────┐
│ Guide LLM Call: "Route this message"              → ~200 tokens                    │
│ Archivalist LLM Call: "Search history"            → ~2500 tokens (every time)     │
│ TOTAL: ~2700 tokens per query                                                      │
└────────────────────────────────────────────────────────────────────────────────────┘

WITH Intent Classification (cache hit):
┌────────────────────────────────────────────────────────────────────────────────────┐
│ Guide LLM Call: "Route this + classify intent"    → ~280 tokens (+80 for metadata)│
│ Archivalist: Cache hit using intent + subject     → 0 tokens                       │
│ TOTAL: ~280 tokens per query (on cache hit)                                        │
└────────────────────────────────────────────────────────────────────────────────────┘

At 75% cache hit rate (higher than Librarian due to immutable history):
- Old: 100 queries × 2700 tokens = 270,000 tokens
- New: 25 misses × 2780 + 75 hits × 280 = 69,500 + 21,000 = 90,500 tokens
- SAVINGS: 66%
```

---

## Direct Consultation Protocol

### Token Cost Problem

When agents consult each other through Guide, the token cost accumulates:

```
Intent-Based Routing (via Guide):
┌────────────────────────────────────────────────────────────────────────────────────────┐
│  Agent A → Guide → Agent B → Guide → Agent A                                           │
│                                                                                        │
│  Step                    │ Tokens  │ Model        │ Cost Factor                        │
│  ────────────────────────┼─────────┼──────────────┼────────────────                    │
│  A → Guide (request)     │ ~500    │ Haiku        │ 1x                                 │
│  Guide processes         │ ~500    │ Haiku        │ 1x                                 │
│  Guide → B (forward)     │ ~500    │ Opus/Gemini  │ 10-15x                             │
│  B responds              │ ~1000   │ Opus/Gemini  │ 10-15x                             │
│  B → Guide (response)    │ ~1000   │ Haiku        │ 1x                                 │
│  Guide → A (forward)     │ ~1000   │ Haiku        │ 1x                                 │
│                                                                                        │
│  HIDDEN COST: Guide's context accumulates ALL consultation traffic.                    │
│  At 100 consultations/session: Guide context grows by 50-100K tokens.                  │
│  This causes more frequent Guide compaction and higher per-turn costs.                 │
└────────────────────────────────────────────────────────────────────────────────────────┘
```

### Solution: Direct Consultation for Known Targets

When an agent knows exactly which agent it needs to consult, bypass Guide:

```
Direct Consultation:
┌────────────────────────────────────────────────────────────────────────────────────────┐
│  Agent A ────────────────────────▶ Agent B                                             │
│           (direct, no Guide hop)      │                                                │
│                                       │                                                │
│  Agent A ◀────────────────────────────┘                                                │
│                                                                                        │
│  Step                    │ Tokens  │ Model        │ Cost Factor                        │
│  ────────────────────────┼─────────┼──────────────┼────────────────                    │
│  A → B (request)         │ ~500    │ Opus/Gemini  │ 10-15x                             │
│  B responds              │ ~1000   │ Opus/Gemini  │ 10-15x                             │
│                                                                                        │
│  NO Guide overhead. Context growth isolated to participating agents only.              │
│  Async logging to Archivalist for observability (fire-and-forget).                     │
└────────────────────────────────────────────────────────────────────────────────────────┘
```

### When to Use Each Approach

| Scenario | Routing | Reason |
|----------|---------|--------|
| **Known target** ("Consult Engineer about X") | **Direct** | Agent knows exactly who to ask |
| **Ambiguous request** ("I need help with this") | **Guide** | Guide determines best target |
| **Fan-out to multiple agents** | **Guide** | Coordination needed |
| **Policy check required** | **Guide** | Permissions, rate limiting |
| **Blocking/urgent** | **Direct** | Skip routing overhead |

### Direct Consultation Architecture

```
┌─────────────────────────────────────────────────────────────────────────────────────┐
│                      DIRECT CONSULTATION ARCHITECTURE                                │
├─────────────────────────────────────────────────────────────────────────────────────┤
│                                                                                     │
│  DIRECT PATH (95% of consultations):                                                │
│  ┌──────────────────────────────────────────────────────────────────────────────┐   │
│  │                                                                              │   │
│  │  ┌──────────┐    consult_engineer()    ┌──────────┐                         │   │
│  │  │ Designer │ ────────────────────────▶│ Engineer │                         │   │
│  │  └──────────┘                          └────┬─────┘                         │   │
│  │       ▲                                     │                               │   │
│  │       │         response                    │                               │   │
│  │       └─────────────────────────────────────┘                               │   │
│  │                                                                              │   │
│  │       │ (async, fire-and-forget)                                            │   │
│  │       ▼                                                                      │   │
│  │  ┌─────────────┐                                                            │   │
│  │  │ Archivalist │ ◀── Log: {from, to, question_summary, timestamp}           │   │
│  │  └─────────────┘     (truncated, minimal tokens)                            │   │
│  │                                                                              │   │
│  └──────────────────────────────────────────────────────────────────────────────┘   │
│                                                                                     │
│  GUIDE PATH (5% of consultations - ambiguous/policy-controlled):                    │
│  ┌──────────────────────────────────────────────────────────────────────────────┐   │
│  │                                                                              │   │
│  │  ┌──────────┐   "I need help"   ┌───────┐   routes to   ┌──────────┐        │   │
│  │  │ Designer │ ─────────────────▶│ Guide │ ─────────────▶│ ????     │        │   │
│  │  └──────────┘                   └───────┘               └──────────┘        │   │
│  │                                                                              │   │
│  └──────────────────────────────────────────────────────────────────────────────┘   │
│                                                                                     │
└─────────────────────────────────────────────────────────────────────────────────────┘
```

### Consultation Skills Pattern

Each agent has direct consultation skills for agents it commonly consults:

```go
// Direct consultation skill pattern
type ConsultSkill struct {
    Name        string     // e.g., "consult_engineer"
    Target      AgentType  // Explicit target agent
    Description string
    Parameters  []Param
}

// Implementation pattern
func (d *Designer) consultEngineer(ctx context.Context, req *ConsultRequest) (*ConsultResponse, error) {
    // Direct message to Engineer (no Guide hop)
    resp, err := d.bus.RequestDirect(ctx, "engineer", &Message{
        Type:      "CONSULT_REQUEST",
        From:      d.ID,
        SessionID: d.sessionID,
        Payload:   req,
    })
    if err != nil {
        return nil, err
    }

    // Async log to Archivalist (fire-and-forget, doesn't block or grow Guide context)
    go d.logConsultation(ctx, "engineer", req.Question)

    return resp.(*ConsultResponse), nil
}

func (d *Designer) logConsultation(ctx context.Context, target, question string) {
    // Truncate question for minimal storage
    summary := question
    if len(summary) > 100 {
        summary = summary[:100] + "..."
    }

    d.bus.Publish("archivalist.log", &ConsultationLog{
        From:      d.agentType,
        To:        target,
        Summary:   summary,
        Timestamp: time.Now(),
        SessionID: d.sessionID,
    })
}
```

### Consultation Request/Response Types

```go
// ConsultRequest is the standard consultation request
type ConsultRequest struct {
    Question    string         `json:"question"`
    Context     map[string]any `json:"context,omitempty"`     // Relevant context
    Priority    string         `json:"priority,omitempty"`    // "blocking", "async"
    MaxTokens   int            `json:"max_tokens,omitempty"`  // Limit response size
}

// ConsultResponse is the standard consultation response
type ConsultResponse struct {
    Answer      string         `json:"answer"`
    Confidence  float64        `json:"confidence,omitempty"`  // 0.0-1.0
    References  []string       `json:"references,omitempty"`  // File paths, docs, etc.
    Suggestions []string       `json:"suggestions,omitempty"` // Follow-up suggestions
}

// ConsultationLog for Archivalist (minimal footprint)
type ConsultationLog struct {
    From      string    `json:"from"`
    To        string    `json:"to"`
    Summary   string    `json:"summary"`    // Truncated question (max 100 chars)
    Timestamp time.Time `json:"timestamp"`
    SessionID string    `json:"session_id"`
}
```

### Agent Consultation Matrix

Which agents can directly consult which:

| From \ To | Guide | Architect | Engineer | Designer | Inspector | Tester | Librarian | Archivalist | Academic |
|-----------|-------|-----------|----------|----------|-----------|--------|-----------|-------------|----------|
| **Guide** | - | ✓ | ✓ | ✓ | ✓ | ✓ | ✓ | ✓ | ✓ |
| **Architect** | ✓ | - | ✓ | ✓ | ✓ | ✓ | ✓ | ✓ | ✓ |
| **Engineer** | ✓ | ✓ | - | ✓ | ✓ | ✓ | ✓ | ✓ | ✓ |
| **Designer** | ✓ | ✓ | ✓ | - | ✓ | ✓ | ✓ | ✓ | ✓ |
| **Inspector** | ✓ | ✓ | ✓ | ✓ | - | ✓ | ✓ | ✓ | - |
| **Tester** | ✓ | ✓ | ✓ | ✓ | ✓ | - | ✓ | ✓ | - |
| **Librarian** | ✓ | - | - | - | - | - | - | ✓ | ✓ |
| **Archivalist** | ✓ | - | - | - | - | - | ✓ | - | ✓ |
| **Academic** | ✓ | - | - | - | - | - | ✓ | ✓ | - |

**Legend**: ✓ = Has direct consultation skill | - = Must route through Guide (or not applicable)

### Token Savings Analysis

```
100 consultations/session comparison:

INTENT-BASED (via Guide):
├── Guide context growth: ~100K tokens (consultations accumulate)
├── Guide compaction: 2-3 additional compactions
├── Total Guide tokens: ~300K
├── Total Worker tokens: ~150K
└── TOTAL: ~450K tokens

DIRECT CONSULTATION:
├── Guide context growth: ~0 tokens (consultations bypass Guide)
├── Guide compaction: 0 additional compactions
├── Total Worker tokens: ~150K
├── Archivalist logs: ~5K tokens (truncated summaries)
└── TOTAL: ~155K tokens

SAVINGS: ~65% reduction in consultation-related token usage
```

---

## LLM API Management

All LLM requests flow through the Guide to a centralized **LLM Request Gate**. This layer handles provider management, rate limiting, token budgets, and context overflow—ensuring agents can function reliably without managing API complexities.

### Architecture Overview

```
┌─────────────────────────────────────────────────────────────────────────────┐
│                              Guide                                           │
│                    (All LLM requests route through)                          │
└─────────────────────────────────────────────────────────────────────────────┘
                                    │
                                    ▼
┌─────────────────────────────────────────────────────────────────────────────┐
│                         LLM Request Gate                                     │
│  ┌─────────────┐  ┌─────────────┐  ┌─────────────┐  ┌─────────────────────┐ │
│  │  Priority   │→ │   Budget    │→ │    Rate     │→ │      Provider       │ │
│  │   Queue     │  │   Check     │  │   Limiter   │  │      Router         │ │
│  └─────────────┘  └─────────────┘  └─────────────┘  └─────────────────────┘ │
└─────────────────────────────────────────────────────────────────────────────┘
                                    │
                    ┌───────────────┼───────────────┐
                    ▼               ▼               ▼
             ┌──────────┐    ┌──────────┐    ┌──────────┐
             │ Anthropic│    │  OpenAI  │    │  Google  │
             │ Adapter  │    │ Adapter  │    │ Adapter  │
             └──────────┘    └──────────┘    └──────────┘
```

### Provider Configuration

Providers are configured via environment variables with config file fallback. An explicit `auth` command handles per-provider authorization.

```go
// Credential resolution order:
// 1. Environment variable (ANTHROPIC_API_KEY, OPENAI_API_KEY, GOOGLE_API_KEY)
// 2. Config file (~/.sylk/credentials.yaml) - encrypted at rest
// 3. System keychain (future enhancement)

type ProviderConfig struct {
    Provider    string            `yaml:"provider"`    // anthropic, openai, google
    APIKey      string            `yaml:"-"`           // Never serialized, loaded at runtime
    BaseURL     string            `yaml:"base_url"`    // Optional override for proxies
    Models      map[string]string `yaml:"models"`      // Alias → actual model name
    SoftLimit   *UsageLimit       `yaml:"soft_limit"`  // User-defined quota (for Max/Pro subs)
    Enabled     bool              `yaml:"enabled"`
}

type UsageLimit struct {
    Requests    int           `yaml:"requests"`     // Max requests per period
    Tokens      int           `yaml:"tokens"`       // Max tokens per period
    Period      time.Duration `yaml:"period"`       // week, month, day
    WarnAt      float64       `yaml:"warn_at"`      // Warn at threshold (e.g., 0.8 = 80%)
}

// CLI Commands:
// sylk auth anthropic              → Interactive API key setup
// sylk auth openai --api-key <key> → Direct key input
// sylk auth google --credentials-file <path> → Service account
// sylk auth status                 → Show configured providers
```

### Provider Adapter Interface

Each provider implements a common interface, enabling new providers to be added without core changes.

```go
type ProviderAdapter interface {
    // Identification
    Name() string
    SupportedModels() []ModelInfo

    // Core operations
    Complete(ctx context.Context, req *CompletionRequest) (*CompletionResponse, error)
    Stream(ctx context.Context, req *CompletionRequest) (<-chan StreamChunk, error)

    // Token management
    CountTokens(messages []Message) (int, error)
    MaxContextTokens(model string) int

    // Health
    HealthCheck(ctx context.Context) error
}

type ModelInfo struct {
    ID              string
    Name            string
    MaxContext      int
    InputPricePerM  float64  // Price per million input tokens
    OutputPricePerM float64  // Price per million output tokens
}

type CompletionRequest struct {
    Model       string
    Messages    []Message
    MaxTokens   int
    Temperature float64
    Tools       []Tool

    // Attribution (passed through for tracking)
    SessionID   string
    PipelineID  string
    TaskID      string
    AgentID     string
}

type CompletionResponse struct {
    Content      string
    ToolCalls    []ToolCall
    InputTokens  int
    OutputTokens int
    FinishReason string

    // For attribution
    Provider     string
    Model        string
    Latency      time.Duration
}
```

### Streaming + Event Bus Integration

This section defines how streaming LLM responses integrate with the event bus for real-time, reactive agent coordination.

#### Overview

Traditional batch processing waits for complete LLM responses before acting. Sylk's streaming integration converts callback-based streaming into event bus messages, enabling:

1. **Real-time token accounting** - Budget enforcement mid-generation
2. **Progressive tool detection** - Prepare tool execution before arguments complete
3. **Backpressure propagation** - Slow consumers block producers naturally
4. **Context-aware interruption** - Clean abort on timeout/cancellation
5. **Correlation chain preservation** - Trace individual chunks through the system

```
┌─────────────────────────────────────────────────────────────────────────────────────┐
│                    STREAMING + EVENT BUS ARCHITECTURE                                │
├─────────────────────────────────────────────────────────────────────────────────────┤
│                                                                                     │
│  BEFORE (Batch Mode):                                                               │
│  ═══════════════════                                                                │
│                                                                                     │
│    Agent A                                                                          │
│       │                                                                             │
│       ▼                                                                             │
│    Provider.Generate()                                                              │
│       │                                                                             │
│       │  ← [waits 30s for complete response]                                        │
│       │                                                                             │
│       ▼                                                                             │
│    Complete Response                                                                │
│       │                                                                             │
│       ▼                                                                             │
│    Event Bus → Agent B                                                              │
│                                                                                     │
│  AFTER (Streaming Mode):                                                            │
│  ═══════════════════════                                                            │
│                                                                                     │
│    Agent A                                                                          │
│       │                                                                             │
│       ▼                                                                             │
│    Provider.Stream()                                                                │
│       │                                                                             │
│       ├──► StreamChunk[0] ──► Event Bus ──► UI renders partial                     │
│       ├──► StreamChunk[1] ──► Event Bus ──► Token counter updates                  │
│       ├──► ChunkTypeToolStart ──► Event Bus ──► Agent B prepares                   │
│       ├──► ChunkTypeToolDelta ──► Event Bus ──► Agent B accumulates args           │
│       ├──► ChunkTypeToolEnd ──► Event Bus ──► Agent B executes immediately         │
│       └──► ChunkTypeEnd ──► Event Bus ──► Final usage recorded                     │
│                                                                                     │
│  RESULT: Continuous flow, not request-response                                      │
│                                                                                     │
└─────────────────────────────────────────────────────────────────────────────────────┘
```

#### StreamChunk Type System

The `StreamChunk` represents a single chunk from a streaming response, with rich type information for routing and processing.

```go
// core/providers/stream.go

type StreamChunk struct {
    // Index is the sequence number of this chunk
    Index int `json:"index"`

    // Text is the content delta
    Text string `json:"text"`

    // Type indicates the chunk type
    Type StreamChunkType `json:"type"`

    // ToolCall if this chunk contains tool use data
    ToolCall *ToolCallChunk `json:"tool_call,omitempty"`

    // Usage is populated on the final chunk (if available)
    Usage *Usage `json:"usage,omitempty"`

    // StopReason is populated on the final chunk
    StopReason StopReason `json:"stop_reason,omitempty"`

    // Timestamp when this chunk was received
    Timestamp time.Time `json:"timestamp"`
}

type StreamChunkType string

const (
    ChunkTypeText      StreamChunkType = "text"       // Text content delta
    ChunkTypeToolStart StreamChunkType = "tool_start" // Tool call beginning (ID + Name)
    ChunkTypeToolDelta StreamChunkType = "tool_delta" // Tool arguments delta
    ChunkTypeToolEnd   StreamChunkType = "tool_end"   // Tool call complete
    ChunkTypeStart     StreamChunkType = "start"      // Stream started
    ChunkTypeEnd       StreamChunkType = "end"        // Stream complete (has Usage)
    ChunkTypeError     StreamChunkType = "error"      // Error occurred
)

type ToolCallChunk struct {
    ID             string `json:"id,omitempty"`
    Name           string `json:"name,omitempty"`
    ArgumentsDelta string `json:"arguments_delta,omitempty"`
}
```

#### Streaming State Machine

```
┌─────────────────────────────────────────────────────────────────────────────────────┐
│                         STREAMING STATE MACHINE                                      │
├─────────────────────────────────────────────────────────────────────────────────────┤
│                                                                                     │
│                              ┌─────────┐                                            │
│                              │  IDLE   │                                            │
│                              └────┬────┘                                            │
│                                   │ Provider.Stream() called                        │
│                                   ▼                                                 │
│                              ┌─────────┐                                            │
│                       ┌──────│STARTING │──────┐                                     │
│                       │      └────┬────┘      │                                     │
│                       │           │           │ Error                               │
│                       │           │ ChunkTypeStart                                  │
│                       │           ▼           │                                     │
│                       │      ┌─────────┐      │                                     │
│       ChunkTypeText ──┼─────►│STREAMING│◄─────┼── ChunkTypeText (loop)              │
│                       │      └────┬────┘      │                                     │
│                       │           │           │                                     │
│                       │           │ ChunkTypeToolStart                              │
│                       │           ▼           │                                     │
│                       │      ┌─────────┐      │                                     │
│                       │      │TOOL_CALL│◄─────┼── ChunkTypeToolDelta (loop)         │
│                       │      └────┬────┘      │                                     │
│                       │           │           │                                     │
│                       │           │ ChunkTypeToolEnd                                │
│                       │           ▼           │                                     │
│                       │      ┌─────────┐      │                                     │
│                       │      │STREAMING│      │  (may return to streaming or        │
│                       │      │  or END │      │   start another tool call)          │
│                       │      └────┬────┘      │                                     │
│                       │           │           │                                     │
│                       │           │ ChunkTypeEnd or ChunkTypeError                  │
│                       │           ▼           ▼                                     │
│                       │      ┌─────────┐ ┌─────────┐                               │
│                       └─────►│COMPLETE │ │  ERROR  │                               │
│                              └─────────┘ └─────────┘                               │
│                                                                                     │
│  State Transitions:                                                                 │
│  ─────────────────                                                                  │
│  IDLE → STARTING: Stream initiated                                                  │
│  STARTING → STREAMING: First content chunk received                                 │
│  STREAMING → STREAMING: Text chunks accumulating                                    │
│  STREAMING → TOOL_CALL: Tool use detected                                           │
│  TOOL_CALL → TOOL_CALL: Tool arguments accumulating                                 │
│  TOOL_CALL → STREAMING: Tool complete, more content follows                         │
│  STREAMING → COMPLETE: End of turn or max tokens                                    │
│  ANY → ERROR: Provider error or context cancellation                                │
│                                                                                     │
└─────────────────────────────────────────────────────────────────────────────────────┘
```

#### Event Bus Message Wrapping

StreamChunks are wrapped in the generic `Message[T]` envelope for event bus transmission, preserving correlation chains and enabling priority-based routing.

```go
// Wrapping StreamChunk for event bus transmission
func WrapStreamChunk(chunk *StreamChunk, ctx StreamContext) Message[StreamChunk] {
    return Message[StreamChunk]{
        ID:            uuid.New().String(),
        CorrelationID: ctx.CorrelationID,  // Links all chunks from one generation
        ParentID:      ctx.RequestID,       // Links to triggering request
        SessionID:     ctx.SessionID,
        Source:        ctx.ProviderName,    // e.g., "anthropic-provider"
        Target:        ctx.TargetAgent,     // e.g., "ui-renderer" or "agent-orchestrator"
        Type:          MessageTypeStream,
        Payload:       *chunk,
        Timestamp:     chunk.Timestamp,
        Priority:      ctx.Priority,
        Metadata: map[string]any{
            "model":       ctx.Model,
            "agent_id":    ctx.AgentID,
            "chunk_index": chunk.Index,
            "chunk_type":  string(chunk.Type),
        },
    }
}

type StreamContext struct {
    CorrelationID string
    RequestID     string
    SessionID     string
    ProviderName  string
    TargetAgent   string
    Model         string
    AgentID       string
    Priority      Priority
}
```

#### Stream Accumulation Patterns

```go
// StreamAccumulator collects streaming chunks into a complete response
type StreamAccumulator struct {
    mu sync.Mutex

    chunks     []StreamChunk
    text       strings.Builder
    toolCalls  map[string]*accumulatedToolCall
    usage      Usage
    stopReason StopReason
    model      string

    startTime time.Time
    endTime   time.Time
}

// Add accumulates a chunk (thread-safe)
func (a *StreamAccumulator) Add(chunk *StreamChunk) {
    a.mu.Lock()
    defer a.mu.Unlock()

    a.chunks = append(a.chunks, *chunk)

    switch chunk.Type {
    case ChunkTypeText:
        a.text.WriteString(chunk.Text)

    case ChunkTypeToolStart:
        if chunk.ToolCall != nil {
            a.toolCalls[chunk.ToolCall.ID] = &accumulatedToolCall{
                ID:   chunk.ToolCall.ID,
                Name: chunk.ToolCall.Name,
            }
        }

    case ChunkTypeToolDelta:
        if chunk.ToolCall != nil {
            if tc, ok := a.toolCalls[chunk.ToolCall.ID]; ok {
                tc.Arguments.WriteString(chunk.ToolCall.ArgumentsDelta)
            }
        }

    case ChunkTypeEnd:
        a.endTime = time.Now()
        if chunk.Usage != nil {
            a.usage = *chunk.Usage
        }
        if chunk.StopReason != "" {
            a.stopReason = chunk.StopReason
        }
    }
}

// Response builds the final response from accumulated chunks
func (a *StreamAccumulator) Response() *Response {
    a.mu.Lock()
    defer a.mu.Unlock()

    var toolCalls []ToolCall
    for _, tc := range a.toolCalls {
        toolCalls = append(toolCalls, ToolCall{
            ID:        tc.ID,
            Name:      tc.Name,
            Arguments: tc.Arguments.String(),
        })
    }

    return &Response{
        Content:    a.text.String(),
        Model:      a.model,
        StopReason: a.stopReason,
        Usage:      a.usage,
        ToolCalls:  toolCalls,
        ProviderMetadata: map[string]any{
            "chunk_count":     len(a.chunks),
            "stream_duration": a.endTime.Sub(a.startTime).String(),
        },
    }
}
```

#### Channel-Based Streaming (Event Bus Bridge)

```go
// StreamToChannel converts callback-based streaming to channels
// This is the KEY BRIDGE between providers and the event bus
func StreamToChannel(
    ctx context.Context,
    provider Provider,
    req *Request,
) (<-chan *StreamChunk, <-chan error) {
    chunks := make(chan *StreamChunk, 100)  // Buffered for backpressure
    errs := make(chan error, 1)

    go func() {
        defer close(chunks)
        defer close(errs)

        err := provider.Stream(ctx, req, func(chunk *StreamChunk) error {
            select {
            case chunks <- chunk:
                return nil
            case <-ctx.Done():
                return ctx.Err()  // Clean abort on context cancellation
            }
        })

        if err != nil {
            errs <- err
        }
    }()

    return chunks, errs
}
```

#### Real-Time Capabilities Flow

```
┌─────────────────────────────────────────────────────────────────────────────────────┐
│                      REAL-TIME STREAMING CAPABILITIES                                │
├─────────────────────────────────────────────────────────────────────────────────────┤
│                                                                                     │
│  1. PROGRESSIVE UI RENDERING                                                        │
│  ═══════════════════════════                                                        │
│                                                                                     │
│     Provider ──► StreamChunk(text) ──► EventBus ──► UI Renderer                    │
│                                            │                                        │
│                                            └──► User sees text as it generates     │
│                                                                                     │
│  2. REAL-TIME TOKEN BUDGET ENFORCEMENT                                              │
│  ═════════════════════════════════════                                              │
│                                                                                     │
│     Provider ──► StreamChunk(text) ──► EventBus ──► Token Monitor                  │
│                                            │              │                         │
│                                            │              ▼                         │
│                                            │         Estimate tokens                │
│                                            │              │                         │
│                                            │              ▼                         │
│                                            │         Budget exceeded?               │
│                                            │              │                         │
│                                            │      ┌───────┴───────┐                 │
│                                            │      │ YES           │ NO              │
│                                            │      ▼               ▼                 │
│                                            │  ctx.Cancel()    Continue              │
│                                            │      │                                 │
│                                            │      ▼                                 │
│                                            └──► Stream aborts cleanly               │
│                                                                                     │
│  3. PROGRESSIVE TOOL PREPARATION                                                    │
│  ═══════════════════════════════                                                    │
│                                                                                     │
│     Provider ──► ChunkTypeToolStart ──► EventBus ──► Tool Executor                 │
│                      (ID + Name)                          │                         │
│                                                           ▼                         │
│                                                    Prepare execution context        │
│                                                    (load resources, validate)       │
│                                                           │                         │
│     Provider ──► ChunkTypeToolDelta ──► EventBus ────────►│                         │
│                   (arguments delta)                       │                         │
│                                                    Accumulate arguments             │
│                                                           │                         │
│     Provider ──► ChunkTypeToolEnd ──► EventBus ──────────►│                         │
│                                                           ▼                         │
│                                                    Execute IMMEDIATELY              │
│                                                    (context already warm)           │
│                                                                                     │
│  4. VALIDATION EARLY ABORT                                                          │
│  ═════════════════════════                                                          │
│                                                                                     │
│     Provider ──► StreamChunk(text) ──► EventBus ──► Validator                      │
│                                            │              │                         │
│                                            │              ▼                         │
│                                            │         Detect issues                  │
│                                            │         (syntax errors, security)      │
│                                            │              │                         │
│                                            │              ▼                         │
│                                            │         Issue found?                   │
│                                            │              │                         │
│                                            │      ┌───────┴───────┐                 │
│                                            │      │ YES           │ NO              │
│                                            │      ▼               ▼                 │
│                                            │  Abort + Feedback  Continue            │
│                                            │  (save tokens)                         │
│                                            │                                        │
│                                                                                     │
└─────────────────────────────────────────────────────────────────────────────────────┘
```

#### Backpressure Handling

The streaming architecture handles backpressure at multiple levels:

```
┌─────────────────────────────────────────────────────────────────────────────────────┐
│                         BACKPRESSURE PROPAGATION                                     │
├─────────────────────────────────────────────────────────────────────────────────────┤
│                                                                                     │
│  LEVEL 1: Channel Buffer                                                            │
│  ═══════════════════════                                                            │
│                                                                                     │
│     chunks := make(chan *StreamChunk, 100)  // 100-chunk buffer                     │
│                                                                                     │
│     • Buffer absorbs temporary consumer slowdowns                                   │
│     • When buffer full, producer blocks                                             │
│     • Prevents memory explosion from unbounded queues                               │
│                                                                                     │
│  LEVEL 2: Handler Error Return                                                      │
│  ═════════════════════════════                                                      │
│                                                                                     │
│     err := provider.Stream(ctx, req, func(chunk *StreamChunk) error {               │
│         // Handler can return error to halt stream                                  │
│         if budgetExceeded {                                                         │
│             return ErrBudgetExceeded  // Stops generation                           │
│         }                                                                           │
│         return nil                                                                  │
│     })                                                                              │
│                                                                                     │
│     • Handler returning error halts generation immediately                          │
│     • Provider stops calling LLM API                                                │
│     • Partial response still usable via accumulator                                 │
│                                                                                     │
│  LEVEL 3: Context Cancellation                                                      │
│  ════════════════════════════                                                       │
│                                                                                     │
│     ctx, cancel := context.WithCancel(parentCtx)                                    │
│                                                                                     │
│     // In token monitor goroutine:                                                  │
│     if estimatedTokens > budget {                                                   │
│         cancel()  // Cancels context                                                │
│     }                                                                               │
│                                                                                     │
│     // In stream goroutine:                                                         │
│     select {                                                                        │
│     case chunks <- chunk:                                                           │
│         return nil                                                                  │
│     case <-ctx.Done():                                                              │
│         return ctx.Err()  // Propagates cancellation                                │
│     }                                                                               │
│                                                                                     │
│     • Cancellation propagates through entire call chain                             │
│     • All goroutines clean up properly                                              │
│     • No orphaned resources                                                         │
│                                                                                     │
│  LEVEL 4: Event Bus Priority                                                        │
│  ══════════════════════════                                                         │
│                                                                                     │
│     Message[StreamChunk]{                                                           │
│         Priority: PriorityHigh,  // User-interactive stream                         │
│     }                                                                               │
│                                                                                     │
│     • High-priority streams processed first                                         │
│     • Low-priority streams may be delayed                                           │
│     • User-facing streams never starved                                             │
│                                                                                     │
└─────────────────────────────────────────────────────────────────────────────────────┘
```

#### Provider-Specific Streaming Implementation

Each provider implements streaming with provider-specific event handling:

```go
// Anthropic streaming (uses event-based API)
func (p *AnthropicProvider) Stream(ctx context.Context, req *Request, handler StreamHandler) error {
    stream := p.client.Messages.NewStreaming(ctx, params)

    // Send start chunk
    handler(&StreamChunk{Type: ChunkTypeStart, Timestamp: time.Now()})

    toolCallIDForIndex := map[int64]string{}

    for stream.Next() {
        event := stream.Current()

        // Convert provider-specific events to generic StreamChunk
        chunk := p.convertStreamEvent(event, chunkIndex, toolCallIDForIndex)
        if chunk != nil {
            if err := handler(chunk); err != nil {
                return err  // Handler requested stop
            }
        }
    }

    // Send end chunk with final usage
    return handler(&StreamChunk{
        Type:       ChunkTypeEnd,
        StopReason: stopReason,
        Usage:      &finalUsage,
        Timestamp:  time.Now(),
    })
}

// Google streaming (uses iterator-based API)
func (g *GoogleProvider) Stream(ctx context.Context, req *Request, handler StreamHandler) error {
    handler(&StreamChunk{Type: ChunkTypeStart, Timestamp: time.Now()})

    for resp, err := range g.client.Models.GenerateContentStream(ctx, model, contents, config) {
        if err != nil {
            handler(&StreamChunk{Type: ChunkTypeError, Text: err.Error()})
            return err
        }

        // Extract text from response
        if text := resp.Text(); text != "" {
            handler(&StreamChunk{Type: ChunkTypeText, Text: text, Timestamp: time.Now()})
        }

        // Handle function calls
        for _, fc := range resp.FunctionCalls() {
            handler(&StreamChunk{
                Type:     ChunkTypeToolStart,
                ToolCall: &ToolCallChunk{ID: fc.ID, Name: fc.Name},
            })
            // ... tool delta and end chunks
        }
    }

    return handler(&StreamChunk{Type: ChunkTypeEnd, Usage: &usage})
}

// OpenAI streaming (uses Responses API events)
func (p *OpenAIProvider) Stream(ctx context.Context, req *Request, handler StreamHandler) error {
    stream := p.client.Responses.NewStreaming(ctx, params)

    handler(&StreamChunk{Type: ChunkTypeStart})

    for stream.Next() {
        event := stream.Current()

        switch ev := event.AsAny().(type) {
        case responses.ResponseTextDeltaEvent:
            handler(&StreamChunk{Type: ChunkTypeText, Text: ev.Delta})

        case responses.ResponseFunctionCallArgumentsDeltaEvent:
            handler(&StreamChunk{
                Type:     ChunkTypeToolDelta,
                ToolCall: &ToolCallChunk{ID: ev.ItemID, ArgumentsDelta: ev.Delta},
            })

        case responses.ResponseCompletedEvent:
            // Final response
        }
    }

    return handler(&StreamChunk{Type: ChunkTypeEnd, Usage: &usage})
}
```

#### Agent Integration Example

```go
// Agent receives streaming responses via event bus
func (a *Engineer) handleStreamingResponse(ctx context.Context, correlationID string) {
    // Subscribe to stream chunks for this correlation
    streamSub := a.eventBus.Subscribe(
        MessageTypeStream,
        WithCorrelation(correlationID),
    )
    defer streamSub.Close()

    accumulator := NewStreamAccumulator()
    var toolContext *ToolPreparationContext

    for msg := range streamSub.Messages() {
        chunk := msg.Payload  // StreamChunk

        accumulator.Add(&chunk)

        switch chunk.Type {
        case ChunkTypeText:
            // Progressive output to user
            a.ui.AppendText(chunk.Text)

            // Real-time validation
            if issue := a.validator.CheckPartial(accumulator.Text()); issue != nil {
                a.cancelStream(correlationID, issue)
                return
            }

        case ChunkTypeToolStart:
            // Prepare tool execution BEFORE arguments arrive
            toolContext = a.toolExecutor.Prepare(chunk.ToolCall.Name)

        case ChunkTypeToolDelta:
            // Accumulate arguments (accumulator handles this)

        case ChunkTypeToolEnd:
            // Execute tool immediately - context already warm
            if toolContext != nil {
                tc := accumulator.GetToolCall(chunk.ToolCall.ID)
                result := a.toolExecutor.Execute(toolContext, tc)
                a.handleToolResult(result)
            }

        case ChunkTypeEnd:
            // Final response ready
            response := accumulator.Response()
            a.processCompleteResponse(response)

        case ChunkTypeError:
            a.handleStreamError(chunk.Text)
        }
    }
}

// Cancel stream on validation failure
func (a *Engineer) cancelStream(correlationID string, issue *ValidationIssue) {
    a.eventBus.Publish(Message[StreamControl]{
        Type:          MessageTypeStreamControl,
        CorrelationID: correlationID,
        Payload:       StreamControl{Action: "cancel", Reason: issue.Message},
    })
}
```

#### Streaming Metrics and Observability

```go
// StreamMetrics collects telemetry from streaming responses
type StreamMetrics struct {
    // Timing
    TimeToFirstToken    time.Duration  // Latency until first content
    TimeToFirstToolCall time.Duration  // Latency until tool detected
    TotalDuration       time.Duration  // Total stream duration

    // Throughput
    TokensPerSecond     float64        // Average generation speed
    ChunksReceived      int            // Total chunks processed

    // Efficiency
    EarlyAborts         int            // Streams cancelled before completion
    TokensSaved         int            // Estimated tokens saved from early abort
    BackpressureEvents  int            // Times producer blocked on full buffer
}

// Collect metrics from stream
func CollectStreamMetrics(
    chunks <-chan *StreamChunk,
    done <-chan struct{},
) *StreamMetrics {
    metrics := &StreamMetrics{}
    startTime := time.Now()
    var firstTokenTime, firstToolTime time.Time

    for {
        select {
        case chunk, ok := <-chunks:
            if !ok {
                metrics.TotalDuration = time.Since(startTime)
                return metrics
            }

            metrics.ChunksReceived++

            switch chunk.Type {
            case ChunkTypeText:
                if firstTokenTime.IsZero() {
                    firstTokenTime = time.Now()
                    metrics.TimeToFirstToken = firstTokenTime.Sub(startTime)
                }

            case ChunkTypeToolStart:
                if firstToolTime.IsZero() {
                    firstToolTime = time.Now()
                    metrics.TimeToFirstToolCall = firstToolTime.Sub(startTime)
                }

            case ChunkTypeEnd:
                if chunk.Usage != nil {
                    elapsed := time.Since(startTime).Seconds()
                    if elapsed > 0 {
                        metrics.TokensPerSecond = float64(chunk.Usage.OutputTokens) / elapsed
                    }
                }
            }

        case <-done:
            metrics.EarlyAborts++
            metrics.TotalDuration = time.Since(startTime)
            return metrics
        }
    }
}
```

#### Edge Cases and Error Handling

```
┌─────────────────────────────────────────────────────────────────────────────────────┐
│                      STREAMING EDGE CASES AND ERROR HANDLING                         │
├─────────────────────────────────────────────────────────────────────────────────────┤
│                                                                                     │
│  1. NETWORK INTERRUPTION MID-STREAM                                                 │
│  ══════════════════════════════════                                                 │
│                                                                                     │
│     Symptoms: Stream stops without ChunkTypeEnd                                     │
│                                                                                     │
│     Handling:                                                                       │
│     • stream.Err() returns network error                                            │
│     • Send ChunkTypeError to handler                                                │
│     • Accumulator contains partial response (usable for retry)                      │
│     • Agent decides: retry from scratch or resume from partial                      │
│                                                                                     │
│  2. RATE LIMIT (429) DURING STREAM                                                  │
│  ═════════════════════════════════                                                  │
│                                                                                     │
│     Symptoms: Provider returns 429 mid-generation                                   │
│                                                                                     │
│     Handling:                                                                       │
│     • Provider converts to ChunkTypeError                                           │
│     • RateLimiter.OnRateLimit() called                                              │
│     • ALL streams pause (global rate limit)                                         │
│     • Partial response preserved for retry                                          │
│                                                                                     │
│  3. CONTEXT CANCELLED BY USER                                                       │
│  ═══════════════════════════                                                        │
│                                                                                     │
│     Symptoms: User types Ctrl+C or sends interrupt                                  │
│                                                                                     │
│     Handling:                                                                       │
│     • Context cancelled propagates to stream goroutine                              │
│     • Handler returns ctx.Err() (context.Canceled)                                  │
│     • Stream stops cleanly                                                          │
│     • Partial response available if needed                                          │
│     • No orphaned goroutines                                                        │
│                                                                                     │
│  4. TOOL CALL WITH MALFORMED ARGUMENTS                                              │
│  ═════════════════════════════════════                                              │
│                                                                                     │
│     Symptoms: JSON parse fails on accumulated arguments                             │
│                                                                                     │
│     Handling:                                                                       │
│     • ToolCall.Arguments contains invalid JSON                                      │
│     • Tool executor returns validation error                                        │
│     • Error result sent back to LLM (tool result message)                           │
│     • LLM may retry with corrected arguments                                        │
│                                                                                     │
│  5. STREAM TIMEOUT                                                                  │
│  ════════════════                                                                   │
│                                                                                     │
│     Symptoms: No chunks received for configurable duration                          │
│                                                                                     │
│     Handling:                                                                       │
│     • StreamContext includes deadline                                               │
│     • Watchdog goroutine monitors last chunk time                                   │
│     • If exceeded, context cancelled                                                │
│     • Partial response preserved                                                    │
│                                                                                     │
│  6. BUFFER OVERFLOW (Slow Consumer)                                                 │
│  ═══════════════════════════════════                                                │
│                                                                                     │
│     Symptoms: Channel buffer full, producer blocks                                  │
│                                                                                     │
│     Handling:                                                                       │
│     • Buffered channel (100 chunks) absorbs bursts                                  │
│     • If full, select blocks until consumer catches up                              │
│     • Context cancellation still respected (no deadlock)                            │
│     • Metrics track backpressure events                                             │
│                                                                                     │
│  7. MULTIPLE TOOL CALLS IN ONE STREAM                                               │
│  ═════════════════════════════════════                                              │
│                                                                                     │
│     Symptoms: LLM returns multiple tool calls before completion                     │
│                                                                                     │
│     Handling:                                                                       │
│     • toolCallIDForIndex map tracks each tool call by index                         │
│     • Accumulator maintains separate builder per tool call ID                       │
│     • All tool calls available in final Response.ToolCalls                          │
│     • Parallel tool execution possible if independent                               │
│                                                                                     │
└─────────────────────────────────────────────────────────────────────────────────────┘
```

### Request Priority Queue

Requests are prioritized based on context. User-interactive requests (direct agent invocation) get highest priority; otherwise, task phase determines priority.

```go
type RequestPriority int

const (
    PriorityUserInteractive RequestPriority = 100  // User directly invoked agent via command
    PriorityPlanning        RequestPriority = 80   // Architect planning phase
    PriorityExecution       RequestPriority = 60   // Engineer/Designer execution phase
    PriorityValidation      RequestPriority = 40   // Inspector/Tester validation phase
    PriorityBackground      RequestPriority = 20   // Archivalist background indexing
)

type LLMRequest struct {
    ID            string
    SessionID     string
    PipelineID    string
    TaskID        string
    AgentID       string
    AgentType     AgentType
    Provider      string
    Model         string
    Messages      []Message
    Priority      RequestPriority
    CreatedAt     time.Time
    TokenEstimate int  // Pre-estimated for budget checking
}

type PriorityQueue struct {
    mu      sync.Mutex
    heap    *requestHeap  // Min-heap by priority (higher = processed first)
    cond    *sync.Cond
    closed  bool
}

func (pq *PriorityQueue) Push(req *LLMRequest) {
    pq.mu.Lock()
    defer pq.mu.Unlock()
    heap.Push(pq.heap, req)
    pq.cond.Signal()
}

func (pq *PriorityQueue) Pop(ctx context.Context) (*LLMRequest, error) {
    pq.mu.Lock()
    defer pq.mu.Unlock()

    for pq.heap.Len() == 0 && !pq.closed {
        pq.cond.Wait()
    }

    if pq.closed {
        return nil, ErrQueueClosed
    }

    return heap.Pop(pq.heap).(*LLMRequest), nil
}

// Priority determination logic
func DeterminePriority(req *LLMRequest, isUserInvoked bool) RequestPriority {
    if isUserInvoked {
        return PriorityUserInteractive
    }

    switch req.AgentType {
    case AgentTypeArchitect:
        return PriorityPlanning
    case AgentTypeEngineer, AgentTypeDesigner:
        return PriorityExecution
    case AgentTypeInspector, AgentTypeTester:
        return PriorityValidation
    case AgentTypeArchivalist, AgentTypeLibrarian:
        return PriorityBackground
    default:
        return PriorityExecution
    }
}
```

### Adaptive Rate Limiting

Rate limiting uses adaptive exponential backoff. On 429 response, ALL workflow execution pauses until the backoff period ends.

```go
type RateLimitState int

const (
    RateLimitOK RateLimitState = iota
    RateLimitWarning           // Approaching soft limit
    RateLimitExceeded          // 429 received, backing off
)

type ProviderRateLimiter struct {
    mu              sync.RWMutex
    provider        string
    state           RateLimitState
    signalBus       *SignalBus

    // Exponential backoff state
    backoffUntil    time.Time
    backoffAttempt  int
    baseBackoff     time.Duration  // Starting at 1 second
    maxBackoff      time.Duration  // Capped at 5 minutes

    // Local usage tracking (for soft limits)
    requestCount    int64
    tokenCount      int64
    periodStart     time.Time
    softLimit       *UsageLimit
}

func NewProviderRateLimiter(provider string, softLimit *UsageLimit, bus *SignalBus) *ProviderRateLimiter {
    return &ProviderRateLimiter{
        provider:    provider,
        state:       RateLimitOK,
        signalBus:   bus,
        baseBackoff: 1 * time.Second,
        maxBackoff:  5 * time.Minute,
        periodStart: time.Now(),
        softLimit:   softLimit,
    }
}

func (r *ProviderRateLimiter) OnResponse(statusCode int, headers http.Header) {
    r.mu.Lock()
    defer r.mu.Unlock()

    if statusCode == 429 {
        r.state = RateLimitExceeded
        r.backoffAttempt++

        // Exponential backoff: 1s, 2s, 4s, 8s, 16s, ... capped at 5m
        backoff := r.baseBackoff * time.Duration(1<<r.backoffAttempt)
        if backoff > r.maxBackoff {
            backoff = r.maxBackoff
        }

        // Check for Retry-After header
        if retryAfter := headers.Get("Retry-After"); retryAfter != "" {
            if seconds, err := strconv.Atoi(retryAfter); err == nil {
                backoff = time.Duration(seconds) * time.Second
            }
        }

        r.backoffUntil = time.Now().Add(backoff)

        // CRITICAL: Broadcast pause signal to ALL agents
        r.signalBus.Broadcast(SignalMessage{
            Signal:      SignalPauseAll,
            Reason:      fmt.Sprintf("Rate limited by %s", r.provider),
            Payload:     PausePayload{
                Provider:    r.provider,
                ResumeAt:    r.backoffUntil,
                Attempt:     r.backoffAttempt,
            },
            RequiresAck: true,
            Timeout:     5 * time.Second,
        })
    } else if statusCode >= 200 && statusCode < 300 {
        // Success - reset backoff on successful request
        if r.backoffAttempt > 0 {
            r.backoffAttempt = 0
            r.state = RateLimitOK
        }
    }
}

func (r *ProviderRateLimiter) CanProceed() bool {
    r.mu.Lock()
    defer r.mu.Unlock()

    if r.state == RateLimitExceeded {
        if time.Now().After(r.backoffUntil) {
            // Backoff period ended, resume
            r.state = RateLimitOK
            r.signalBus.Broadcast(SignalMessage{
                Signal:      SignalResumeAll,
                Reason:      fmt.Sprintf("Rate limit backoff complete for %s", r.provider),
                RequiresAck: true,
                Timeout:     5 * time.Second,
            })
            return true
        }
        return false
    }

    // Check soft limits if configured
    if r.softLimit != nil {
        r.checkSoftLimit()
    }

    return true
}

func (r *ProviderRateLimiter) checkSoftLimit() {
    // Reset period if needed
    if time.Since(r.periodStart) > r.softLimit.Period {
        r.requestCount = 0
        r.tokenCount = 0
        r.periodStart = time.Now()
    }

    // Check thresholds
    requestRatio := float64(r.requestCount) / float64(r.softLimit.Requests)
    tokenRatio := float64(r.tokenCount) / float64(r.softLimit.Tokens)

    if requestRatio >= r.softLimit.WarnAt || tokenRatio >= r.softLimit.WarnAt {
        if r.state != RateLimitWarning {
            r.state = RateLimitWarning
            // Emit warning (does not pause, just warns)
            r.signalBus.Broadcast(SignalMessage{
                Signal:      SignalQuotaWarning,
                Reason:      fmt.Sprintf("Approaching %s quota: %.0f%% requests, %.0f%% tokens",
                    r.provider, requestRatio*100, tokenRatio*100),
                RequiresAck: false,
            })
        }
    }
}

func (r *ProviderRateLimiter) RecordUsage(tokens int) {
    r.mu.Lock()
    defer r.mu.Unlock()
    r.requestCount++
    r.tokenCount += int64(tokens)
}
```

### Hierarchical Token Budget

Token budgets are enforced hierarchically: Global → Session → Task. Users with subscription quotas (Max/Pro) can set soft limits; without limits, the system handles 429s gracefully.

```go
type TokenBudget struct {
    mu sync.RWMutex

    // Hierarchical limits (-1 = unlimited)
    GlobalLimit     int64
    SessionLimits   map[string]int64  // sessionID → limit
    TaskLimits      map[string]int64  // taskID → limit
    ProviderLimits  map[string]int64  // provider → limit

    // Usage tracking
    tracker         *UsageTracker
}

type UsageTracker struct {
    mu      sync.RWMutex
    records []UsageRecord

    // Aggregated counters for fast lookup
    bySession  map[string]*UsageAggregate
    byTask     map[string]*UsageAggregate
    byProvider map[string]*UsageAggregate
    byAgent    map[AgentType]*UsageAggregate
}

type UsageRecord struct {
    Timestamp    time.Time
    SessionID    string
    PipelineID   string
    TaskID       string
    AgentID      string
    AgentType    AgentType
    Provider     string
    Model        string
    InputTokens  int
    OutputTokens int
    TotalTokens  int
    Cost         float64
    Currency     string  // Always USD
    Latency      time.Duration
}

type UsageAggregate struct {
    Requests     int64
    InputTokens  int64
    OutputTokens int64
    TotalTokens  int64
    TotalCost    float64
}

func (tb *TokenBudget) CheckBudget(req *LLMRequest) error {
    tb.mu.RLock()
    defer tb.mu.RUnlock()

    // Check global limit
    if tb.GlobalLimit > 0 {
        globalUsage := tb.tracker.TotalTokens()
        if globalUsage+int64(req.TokenEstimate) > tb.GlobalLimit {
            return ErrGlobalBudgetExceeded
        }
    }

    // Check session limit
    if limit, ok := tb.SessionLimits[req.SessionID]; ok && limit > 0 {
        sessionUsage := tb.tracker.TokensBySession(req.SessionID)
        if sessionUsage+int64(req.TokenEstimate) > limit {
            return ErrSessionBudgetExceeded
        }
    }

    // Check task limit
    if limit, ok := tb.TaskLimits[req.TaskID]; ok && limit > 0 {
        taskUsage := tb.tracker.TokensByTask(req.TaskID)
        if taskUsage+int64(req.TokenEstimate) > limit {
            return ErrTaskBudgetExceeded
        }
    }

    // Check provider limit
    if limit, ok := tb.ProviderLimits[req.Provider]; ok && limit > 0 {
        providerUsage := tb.tracker.TokensByProvider(req.Provider)
        if providerUsage+int64(req.TokenEstimate) > limit {
            return ErrProviderBudgetExceeded
        }
    }

    return nil
}

func (tb *TokenBudget) RecordUsage(record UsageRecord) {
    tb.mu.Lock()
    defer tb.mu.Unlock()
    tb.tracker.Record(record)
}
```

### Full Attribution Tracking

Every LLM call is fully attributed for cost analysis and debugging.

```go
type AttributionReport struct {
    SessionID   string
    Period      TimeRange

    // Totals
    TotalRequests   int
    TotalTokens     int64
    TotalCost       float64

    // Per-provider breakdown
    ByProvider      map[string]*ProviderUsage

    // Per-agent breakdown
    ByAgent         map[AgentType]*AgentUsage

    // Per-pipeline breakdown
    ByPipeline      map[string]*PipelineUsage

    // Per-task breakdown
    ByTask          map[string]*TaskUsage
}

type ProviderUsage struct {
    Provider     string
    Requests     int
    InputTokens  int64
    OutputTokens int64
    TotalCost    float64
    ByModel      map[string]*ModelUsage
}

type AgentUsage struct {
    AgentType    AgentType
    Requests     int
    InputTokens  int64
    OutputTokens int64
    TotalCost    float64
    ByProvider   map[string]int64  // provider → tokens
}

// Query interface
func (t *UsageTracker) GenerateReport(filter UsageFilter) *AttributionReport
func (t *UsageTracker) BySession(sessionID string) []UsageRecord
func (t *UsageTracker) ByProvider(provider string) []UsageRecord
func (t *UsageTracker) ByAgent(agentType AgentType) []UsageRecord
func (t *UsageTracker) ByTask(taskID string) []UsageRecord
func (t *UsageTracker) TotalCost(filter UsageFilter) float64

// CLI Commands:
// sylk usage                          → Current session summary
// sylk usage --session <id>           → Specific session breakdown
// sylk usage --provider anthropic     → Provider-specific usage
// sylk usage --period week --detailed → Detailed weekly report
```

### Context Window Management

Context overflow is handled via smart selection, Archivalist handoff, and summarization—leveraging existing agent compaction mechanisms.

```go
type ContextManager struct {
    model            string
    maxContextTokens int
    reserveTokens    int  // Reserve for response (e.g., 4096)

    // Integration points
    compactor        AgentCompactor  // Agent-specific compaction
    archivalist      *Archivalist    // For context handoff
    tokenCounter     TokenCounter
}

type ContextOverflowStrategy int

const (
    StrategySmartSelect   ContextOverflowStrategy = iota  // Keep semantically relevant
    StrategyHandoff                                        // Store in Archivalist
    StrategySummarize                                      // Summarize older context
)

func (cm *ContextManager) PrepareContext(messages []Message, query string) ([]Message, error) {
    available := cm.maxContextTokens - cm.reserveTokens
    totalTokens := cm.tokenCounter.Count(messages)

    if totalTokens <= available {
        return messages, nil  // Fits, no action needed
    }

    // Separate system prompt (always preserved) from conversation
    systemPrompt, conversation := cm.splitSystemPrompt(messages)
    systemTokens := cm.tokenCounter.Count(systemPrompt)
    availableForConversation := available - systemTokens

    // Strategy 1: Smart selection - keep semantically relevant messages
    relevant := cm.selectRelevant(conversation, query, availableForConversation)
    if cm.tokenCounter.Count(relevant) <= availableForConversation {
        return append(systemPrompt, relevant...), nil
    }

    // Strategy 2: Handoff to Archivalist - store overflow for later retrieval
    recent, overflow := cm.splitByRecency(conversation, availableForConversation/2)
    if err := cm.archivalist.StoreContextOverflow(overflow); err != nil {
        // Log but continue - summarization fallback
    }

    // Strategy 3: Summarize what won't fit
    summary, err := cm.compactor.CompactMessages(overflow)
    if err != nil {
        // Last resort: truncate oldest
        return append(systemPrompt, recent...), nil
    }

    // Reconstruct: system + summary + recent
    summaryMsg := Message{
        Role:    "system",
        Content: fmt.Sprintf("[Context summary of %d earlier messages]\n%s", len(overflow), summary),
    }

    result := append(systemPrompt, summaryMsg)
    result = append(result, recent...)

    return result, nil
}

func (cm *ContextManager) selectRelevant(messages []Message, query string, maxTokens int) []Message {
    // Score messages by relevance to current query
    scored := make([]scoredMessage, len(messages))
    for i, msg := range messages {
        scored[i] = scoredMessage{
            message: msg,
            score:   cm.relevanceScore(msg, query),
            index:   i,
        }
    }

    // Sort by score descending
    sort.Slice(scored, func(i, j int) bool {
        return scored[i].score > scored[j].score
    })

    // Take highest scoring messages that fit
    selected := []scoredMessage{}
    tokens := 0
    for _, sm := range scored {
        msgTokens := cm.tokenCounter.Count([]Message{sm.message})
        if tokens+msgTokens > maxTokens {
            break
        }
        selected = append(selected, sm)
        tokens += msgTokens
    }

    // Re-sort by original order (preserve conversation flow)
    sort.Slice(selected, func(i, j int) bool {
        return selected[i].index < selected[j].index
    })

    result := make([]Message, len(selected))
    for i, sm := range selected {
        result[i] = sm.message
    }

    return result
}
```

---

## Workflow Control Signals

The signal system enables coordinated workflow control across all agents. Signals propagate via the Guide's SignalBus using pub/sub with required acknowledgment.

### Signal Types

```go
type Signal int

const (
    // Pause/Resume
    SignalPauseAll       Signal = iota  // Rate limit: pause everything
    SignalResumeAll                      // Rate limit cleared: resume
    SignalPausePipeline                  // Pause specific pipeline
    SignalResumePipeline                 // Resume specific pipeline

    // Cancellation
    SignalCancelTask                     // Cancel current task
    SignalAbortSession                   // Abort entire session

    // Warnings (informational, no pause)
    SignalQuotaWarning                   // Approaching quota limit
)

type SignalMessage struct {
    ID          string          // Unique signal ID
    Signal      Signal          // Signal type
    TargetID    string          // Session/Pipeline/Task ID (empty = broadcast)
    Reason      string          // Human-readable reason
    Payload     any             // Signal-specific data
    RequiresAck bool            // Whether agents must acknowledge
    Timeout     time.Duration   // Ack timeout (0 = no timeout)
    SentAt      time.Time
}

type PausePayload struct {
    Provider    string
    ResumeAt    time.Time
    Attempt     int
    Message     string
}
```

### Signal Bus (Pub/Sub via Guide)

```go
type SignalBus struct {
    mu          sync.RWMutex
    subscribers map[Signal][]SignalSubscriber
    pending     map[string]*PendingAck
    ackChan     chan SignalAck
}

type SignalSubscriber struct {
    ID       string
    AgentID  string
    Signals  []Signal           // Which signals this subscriber handles
    Channel  chan SignalMessage
}

type PendingAck struct {
    SignalID    string
    Signal      SignalMessage
    Expected    int
    Received    int
    Acks        []SignalAck
    Timeout     <-chan time.Time
    AckChan     chan SignalAck
}

type SignalAck struct {
    SignalID     string
    SubscriberID string
    AgentID      string
    ReceivedAt   time.Time
    State        AgentState
    Checkpoint   *Checkpoint  // State snapshot if pausing
}

func NewSignalBus() *SignalBus {
    return &SignalBus{
        subscribers: make(map[Signal][]SignalSubscriber),
        pending:     make(map[string]*PendingAck),
        ackChan:     make(chan SignalAck, 100),
    }
}

func (sb *SignalBus) Subscribe(sub SignalSubscriber) {
    sb.mu.Lock()
    defer sb.mu.Unlock()

    for _, sig := range sub.Signals {
        sb.subscribers[sig] = append(sb.subscribers[sig], sub)
    }
}

func (sb *SignalBus) Unsubscribe(subscriberID string) {
    sb.mu.Lock()
    defer sb.mu.Unlock()

    for sig, subs := range sb.subscribers {
        filtered := make([]SignalSubscriber, 0, len(subs))
        for _, sub := range subs {
            if sub.ID != subscriberID {
                filtered = append(filtered, sub)
            }
        }
        sb.subscribers[sig] = filtered
    }
}

func (sb *SignalBus) Broadcast(msg SignalMessage) error {
    msg.ID = uuid.New().String()
    msg.SentAt = time.Now()

    sb.mu.RLock()
    subscribers := sb.subscribers[msg.Signal]
    sb.mu.RUnlock()

    if len(subscribers) == 0 {
        return nil  // No subscribers, nothing to do
    }

    // Setup pending ack tracking if required
    var pending *PendingAck
    if msg.RequiresAck {
        pending = &PendingAck{
            SignalID: msg.ID,
            Signal:   msg,
            Expected: len(subscribers),
            Received: 0,
            Acks:     make([]SignalAck, 0, len(subscribers)),
            AckChan:  make(chan SignalAck, len(subscribers)),
        }
        if msg.Timeout > 0 {
            pending.Timeout = time.After(msg.Timeout)
        }

        sb.mu.Lock()
        sb.pending[msg.ID] = pending
        sb.mu.Unlock()
    }

    // Broadcast to all subscribers
    for _, sub := range subscribers {
        select {
        case sub.Channel <- msg:
            // Sent successfully
        default:
            // Channel full - log warning, subscriber may be stuck
            log.Warnf("Signal channel full for subscriber %s", sub.ID)
        }
    }

    // Wait for acks if required
    if msg.RequiresAck {
        return sb.waitForAcks(pending)
    }

    return nil
}

func (sb *SignalBus) waitForAcks(pending *PendingAck) error {
    for {
        select {
        case <-pending.Timeout:
            if pending.Received < pending.Expected {
                // Identify non-acked subscribers
                nonAcked := sb.identifyNonAcked(pending)

                // Retry once
                if err := sb.retryNonAcked(pending, nonAcked); err != nil {
                    return fmt.Errorf("signal ack timeout: %d/%d responded, non-responsive: %v",
                        pending.Received, pending.Expected, nonAcked)
                }
            }
            return nil

        case ack := <-pending.AckChan:
            pending.Acks = append(pending.Acks, ack)
            pending.Received++

            if pending.Received >= pending.Expected {
                // All acked
                sb.mu.Lock()
                delete(sb.pending, pending.SignalID)
                sb.mu.Unlock()
                return nil
            }
        }
    }
}

func (sb *SignalBus) Acknowledge(ack SignalAck) {
    sb.mu.RLock()
    pending, ok := sb.pending[ack.SignalID]
    sb.mu.RUnlock()

    if ok {
        pending.AckChan <- ack
    }
}
```

### Agent Signal Handler

Each agent has a signal handler that manages pause/resume with immediate checkpointing.

```go
type AgentState int

const (
    AgentStateIdle AgentState = iota
    AgentStateRunning
    AgentStatePaused
    AgentStateCheckpointing
    AgentStateResuming
)

type SignalHandler struct {
    agentID     string
    agentType   AgentType
    signalBus   *SignalBus

    mu          sync.RWMutex
    state       AgentState
    checkpoint  *Checkpoint

    signalChan  chan SignalMessage
    stopChan    chan struct{}
}

func NewSignalHandler(agentID string, agentType AgentType, bus *SignalBus) *SignalHandler {
    h := &SignalHandler{
        agentID:    agentID,
        agentType:  agentType,
        signalBus:  bus,
        state:      AgentStateIdle,
        signalChan: make(chan SignalMessage, 10),
        stopChan:   make(chan struct{}),
    }

    // Subscribe to relevant signals
    bus.Subscribe(SignalSubscriber{
        ID:      agentID,
        AgentID: agentID,
        Signals: []Signal{
            SignalPauseAll,
            SignalResumeAll,
            SignalPausePipeline,
            SignalResumePipeline,
            SignalCancelTask,
            SignalAbortSession,
        },
        Channel: h.signalChan,
    })

    return h
}

func (h *SignalHandler) Start() {
    go h.listen()
}

func (h *SignalHandler) listen() {
    for {
        select {
        case msg := <-h.signalChan:
            h.handleSignal(msg)
        case <-h.stopChan:
            return
        }
    }
}

func (h *SignalHandler) handleSignal(msg SignalMessage) {
    h.mu.Lock()
    defer h.mu.Unlock()

    switch msg.Signal {
    case SignalPauseAll, SignalPausePipeline:
        h.handlePause(msg)

    case SignalResumeAll, SignalResumePipeline:
        h.handleResume(msg)

    case SignalCancelTask:
        h.handleCancel(msg)

    case SignalAbortSession:
        h.handleAbort(msg)
    }
}

func (h *SignalHandler) handlePause(msg SignalMessage) {
    // Immediately transition to checkpointing
    h.state = AgentStateCheckpointing

    // Create checkpoint of current state
    h.checkpoint = h.createCheckpoint()

    // Transition to paused
    h.state = AgentStatePaused

    // Send acknowledgment
    if msg.RequiresAck {
        h.signalBus.Acknowledge(SignalAck{
            SignalID:     msg.ID,
            SubscriberID: h.agentID,
            AgentID:      h.agentID,
            ReceivedAt:   time.Now(),
            State:        h.state,
            Checkpoint:   h.checkpoint,
        })
    }
}

func (h *SignalHandler) handleResume(msg SignalMessage) {
    if h.state != AgentStatePaused {
        return  // Not paused, ignore
    }

    h.state = AgentStateResuming

    // Re-evaluate checkpoint to decide how to resume
    decision := h.evaluateCheckpoint()

    switch decision {
    case ResumeDecisionContinue:
        // Context unchanged, continue from checkpoint
        h.resumeFromCheckpoint()

    case ResumeDecisionRetry:
        // Context changed or was mid-operation, retry last op
        h.retryLastOperation()

    case ResumeDecisionAbort:
        // Unrecoverable state, abort current task
        h.abortCurrentTask()
    }

    h.state = AgentStateRunning

    if msg.RequiresAck {
        h.signalBus.Acknowledge(SignalAck{
            SignalID:     msg.ID,
            SubscriberID: h.agentID,
            AgentID:      h.agentID,
            ReceivedAt:   time.Now(),
            State:        h.state,
        })
    }
}
```

### Checkpoint & Resume

```go
type Checkpoint struct {
    ID              string
    AgentID         string
    AgentType       AgentType
    SessionID       string
    PipelineID      string
    TaskID          string
    CreatedAt       time.Time

    // Operation state
    LastOperation   *Operation
    OperationState  OperationState  // Pending, InProgress, Completed

    // Context state
    MessagesHash    string          // Hash for detecting context changes
    MessageCount    int

    // In-flight work
    ToolsInProgress []ToolCall
    PendingActions  []Action

    // For context reconstruction
    RecentMessages  []Message       // Last N messages for re-evaluation
}

type OperationState int

const (
    OperationStatePending OperationState = iota
    OperationStateInProgress
    OperationStateCompleted
)

type ResumeDecision int

const (
    ResumeDecisionContinue ResumeDecision = iota  // Safe to continue
    ResumeDecisionRetry                            // Retry last operation
    ResumeDecisionAbort                            // Cannot recover
)

func (h *SignalHandler) createCheckpoint() *Checkpoint {
    return &Checkpoint{
        ID:              uuid.New().String(),
        AgentID:         h.agentID,
        AgentType:       h.agentType,
        SessionID:       h.currentSession(),
        PipelineID:      h.currentPipeline(),
        TaskID:          h.currentTask(),
        CreatedAt:       time.Now(),
        LastOperation:   h.lastOperation(),
        OperationState:  h.operationState(),
        MessagesHash:    h.computeMessagesHash(),
        MessageCount:    h.messageCount(),
        ToolsInProgress: h.toolsInProgress(),
        PendingActions:  h.pendingActions(),
        RecentMessages:  h.recentMessages(10),  // Last 10 for context
    }
}

func (h *SignalHandler) evaluateCheckpoint() ResumeDecision {
    cp := h.checkpoint
    if cp == nil {
        return ResumeDecisionContinue  // No checkpoint, just continue
    }

    // Check if context has changed since pause
    currentHash := h.computeMessagesHash()
    if currentHash != cp.MessagesHash {
        // Context changed while paused - retry to incorporate changes
        return ResumeDecisionRetry
    }

    // Check if we were mid-operation
    if cp.OperationState == OperationStateInProgress {
        // Interrupted mid-operation - must retry
        return ResumeDecisionRetry
    }

    // Check if tools were in progress
    if len(cp.ToolsInProgress) > 0 {
        // Tools may have partial state - safer to retry
        return ResumeDecisionRetry
    }

    // Everything looks stable - safe to continue
    return ResumeDecisionContinue
}

func (h *SignalHandler) resumeFromCheckpoint() {
    cp := h.checkpoint

    // Restore pending actions
    for _, action := range cp.PendingActions {
        h.queueAction(action)
    }

    // Clear checkpoint
    h.checkpoint = nil
}

func (h *SignalHandler) retryLastOperation() {
    cp := h.checkpoint

    if cp.LastOperation != nil {
        // Re-queue the last operation
        h.queueOperation(cp.LastOperation)
    }

    // Clear checkpoint
    h.checkpoint = nil
}
```

---

## Concurrency Architecture

This section defines how goroutines, pipelines, and resources are managed for a robust, correct, and responsive user experience.

### Goroutine Model

```
┌─────────────────────────────────────────────────────────────────────────────────────┐
│                              SESSION                                                 │
├─────────────────────────────────────────────────────────────────────────────────────┤
│                                                                                     │
│  STANDALONE AGENTS (One goroutine each, long-lived, NOT in pipeline pool)           │
│  ┌───────────┐ ┌───────────┐ ┌───────────┐ ┌───────────┐ ┌───────────┐ ┌─────────┐│
│  │   Guide   │ │ Architect │ │Orchestrator│ │ Librarian │ │Archivalist│ │ Academic││
│  │ goroutine │ │ goroutine │ │ goroutine │ │ goroutine │ │ goroutine │ │goroutine││
│  │◄─channel  │ │◄─channel  │ │◄─channel  │ │◄─channel  │ │◄─channel  │ │◄─channel││
│  │  (async)  │ │           │ │           │ │           │ │           │ │         ││
│  └───────────┘ └───────────┘ └───────────┘ └───────────┘ └───────────┘ └─────────┘│
│                                                                                     │
│  • Each standalone agent has dedicated goroutine for session lifetime              │
│  • Guide is ASYNC-ONLY: never blocks, fire-and-forget routing                      │
│  • No concurrency limit on standalone agents                                        │
│  • Separate LLM queue (user-interactive, absolute priority)                        │
│                                                                                     │
├─────────────────────────────────────────────────────────────────────────────────────┤
│                                                                                     │
│  PIPELINE POOL (N_CPU_CORES concurrent, one goroutine per pipeline)                 │
│  Designer + Engineer pipelines share this pool                                      │
│                                                                                     │
│  ┌─────────────────────────────────────────────────────────────────────────────┐   │
│  │ Pipeline 1 - Engineer (goroutine)         TDD: RED → GREEN → REFACTOR       │   │
│  │                                                                              │   │
│  │   ┌──────────┐     ┌──────────┐     ┌──────────┐                            │   │
│  │   │ Inspector│ ──► │  Tester  │ ──► │ Engineer │ ──► [Verify Loop]          │   │
│  │   │  (RED)   │     │  (RED)   │     │ (GREEN)  │                            │   │
│  │   │ Criteria │     │  Write   │     │Implement │                            │   │
│  │   └──────────┘     │  Tests   │     └──────────┘                            │   │
│  │                    └──────────┘           │                                  │   │
│  │                         ▲                 ▼                                  │   │
│  │                    ┌──────────┐     ┌──────────┐                            │   │
│  │                    │  Tester  │ ◄── │ Inspector│                            │   │
│  │                    │ (verify) │     │(REFACTOR)│                            │   │
│  │                    └──────────┘     └──────────┘                            │   │
│  └─────────────────────────────────────────────────────────────────────────────┘   │
│                                                                                     │
│  ┌─────────────────────────────────────────────────────────────────────────────┐   │
│  │ Pipeline 2 - Designer (goroutine)                                            │   │
│  │   Inspector ──► Tester ──► Designer ──► [Verify Loop]                       │   │
│  └─────────────────────────────────────────────────────────────────────────────┘   │
│                                                                                     │
│  ... up to N_CPU_CORES concurrent pipelines ...                                    │
│                                                                                     │
│  ┌─────────────────────────────────────────────────────────────────────────────┐   │
│  │ Pipeline N+1 - QUEUED (waiting for slot)                                     │   │
│  │ Pipeline N+2 - QUEUED                                                        │   │
│  └─────────────────────────────────────────────────────────────────────────────┘   │
│                                                                                     │
│  • Pipeline agents (Inspector, Tester, Engineer, Designer) execute sequentially   │
│  • One goroutine per pipeline, agents share it                                     │
│  • Pipeline concurrency capped at N_CPU_CORES                                      │
│  • Separate LLM queue (pipeline queue, lower priority than user)                  │
│                                                                                     │
└─────────────────────────────────────────────────────────────────────────────────────┘
```

### Pipeline Execution Flow

Within a single pipeline goroutine, agents execute **sequentially** following TDD:

```
┌─────────────────────────────────────────────────────────────────────────────────────┐
│                    PIPELINE EXECUTION (Sequential within goroutine)                  │
├─────────────────────────────────────────────────────────────────────────────────────┤
│                                                                                     │
│  1. RED PHASE                                                                       │
│     ┌──────────────────────────────────────────────────────────────────────────┐   │
│     │ Inspector: Define acceptance criteria, what must be validated             │   │
│     │     │                                                                     │   │
│     │     ▼                                                                     │   │
│     │ Tester: Write failing tests based on criteria                            │   │
│     └──────────────────────────────────────────────────────────────────────────┘   │
│                                                                                     │
│  2. GREEN PHASE                                                                     │
│     ┌──────────────────────────────────────────────────────────────────────────┐   │
│     │ Worker (Engineer/Designer): Implement code to make tests pass            │   │
│     └──────────────────────────────────────────────────────────────────────────┘   │
│                                                                                     │
│  3. REFACTOR PHASE                                                                  │
│     ┌──────────────────────────────────────────────────────────────────────────┐   │
│     │ Inspector: Review implementation quality                                  │   │
│     │     │                                                                     │   │
│     │     ▼                                                                     │   │
│     │ Tester: Verify tests still pass                                          │   │
│     └──────────────────────────────────────────────────────────────────────────┘   │
│                                                                                     │
│  4. FEEDBACK LOOP (if issues found)                                                │
│     ┌──────────────────────────────────────────────────────────────────────────┐   │
│     │ Return to Worker with feedback → Re-implement → Re-verify                │   │
│     └──────────────────────────────────────────────────────────────────────────┘   │
│                                                                                     │
│  5. COMPLETE                                                                        │
│     ┌──────────────────────────────────────────────────────────────────────────┐   │
│     │ All tests pass, Inspector approves → Pipeline complete                   │   │
│     │ Merge staging to working tree → Release pipeline slot                    │   │
│     └──────────────────────────────────────────────────────────────────────────┘   │
│                                                                                     │
└─────────────────────────────────────────────────────────────────────────────────────┘
```

### Orchestrator: Pipeline Scheduling

The Orchestrator manages pipeline lifecycle, enforcing N_CPU_CORES limit and eager scheduling.

```go
type PipelineScheduler struct {
    mu              sync.Mutex
    maxConcurrent   int                    // N_CPU_CORES
    active          map[string]*Pipeline   // Currently running
    ready           *PriorityQueue         // Ready to run (priority, then spawn time)
    waiting         map[string]*Pipeline   // Waiting on dependencies

    slotAvailable   chan struct{}          // Signaled when pipeline completes
}

func NewPipelineScheduler() *PipelineScheduler {
    return &PipelineScheduler{
        maxConcurrent: runtime.NumCPU(),
        active:        make(map[string]*Pipeline),
        ready:         NewPriorityQueue(),
        waiting:       make(map[string]*Pipeline),
        slotAvailable: make(chan struct{}, 1),
    }
}

// Schedule adds a pipeline to the scheduler
func (s *PipelineScheduler) Schedule(p *Pipeline) {
    s.mu.Lock()
    defer s.mu.Unlock()

    // Check if dependencies satisfied
    if s.dependenciesSatisfied(p) {
        s.ready.Push(p)
        s.tryDispatch()
    } else {
        s.waiting[p.ID] = p
    }
}

// tryDispatch starts pipelines if slots available
func (s *PipelineScheduler) tryDispatch() {
    for len(s.active) < s.maxConcurrent && s.ready.Len() > 0 {
        p := s.ready.Pop()
        s.active[p.ID] = p
        go s.runPipeline(p)
    }
}

// OnPipelineComplete handles pipeline completion
func (s *PipelineScheduler) OnPipelineComplete(pipelineID string, success bool) {
    s.mu.Lock()
    defer s.mu.Unlock()

    delete(s.active, pipelineID)

    // Check waiting pipelines - eager scheduling
    for id, p := range s.waiting {
        if s.dependenciesSatisfied(p) {
            delete(s.waiting, id)
            s.ready.Push(p)
        }
    }

    s.tryDispatch()
}

// Pipeline priority queue ordered by:
// 1. Architect-assigned priority (higher first)
// 2. Spawn time (earlier first, for same priority)
type PipelinePriority struct {
    Priority  int       // Architect-assigned
    SpawnTime time.Time // For FIFO within same priority
}

func (a PipelinePriority) Less(b PipelinePriority) bool {
    if a.Priority != b.Priority {
        return a.Priority > b.Priority // Higher priority first
    }
    return a.SpawnTime.Before(b.SpawnTime) // Earlier spawn first
}
```

### LLM Queue Architecture

Two separate queues with user-interactive having absolute priority.

```
┌─────────────────────────────────────────────────────────────────────────────────────┐
│                         LLM QUEUE ARCHITECTURE                                       │
├─────────────────────────────────────────────────────────────────────────────────────┤
│                                                                                     │
│  ┌─────────────────────────────────────────────────────────────────────────────┐   │
│  │  USER-INTERACTIVE QUEUE                                                      │   │
│  │  ════════════════════════                                                    │   │
│  │  • Guide, Architect, Orchestrator, Librarian, Archivalist, Academic         │   │
│  │  • User-invoked agent queries (user asks Librarian mid-workflow)            │   │
│  │                                                                              │   │
│  │  Properties:                                                                 │   │
│  │  • Unbounded (never reject user requests)                                   │   │
│  │  • ABSOLUTE PRIORITY (always processed first)                               │   │
│  │  • Can preempt pipeline requests                                            │   │
│  │  • No concurrency limit                                                     │   │
│  └─────────────────────────────────────────────────────────────────────────────┘   │
│                      │                                                              │
│                      │ User queue ALWAYS drains first                              │
│                      ▼                                                              │
│  ┌─────────────────────────────────────────────────────────────────────────────┐   │
│  │  LLM REQUEST GATE                                                            │   │
│  │  ═══════════════════                                                         │   │
│  │                                                                              │   │
│  │  Dispatch Order:                                                             │   │
│  │  1. ALL user-interactive requests (drain completely)                        │   │
│  │  2. Pipeline requests (by priority, then spawn time)                        │   │
│  │                                                                              │   │
│  │  Preemption:                                                                 │   │
│  │  • User request arrives, all LLM slots used by pipelines                    │   │
│  │  • Cancel lowest-priority pipeline's pending LLM request                    │   │
│  │  • Give slot to user request immediately                                    │   │
│  │  • Cancelled pipeline request re-queued (will retry)                        │   │
│  │                                                                              │   │
│  │  Rate Limit Recovery:                                                        │   │
│  │  • Rate limit clears → user queue drains first                             │   │
│  │  • Pipelines get ZERO slots until user queue empty                         │   │
│  └─────────────────────────────────────────────────────────────────────────────┘   │
│                      │                                                              │
│                      ▼                                                              │
│  ┌─────────────────────────────────────────────────────────────────────────────┐   │
│  │  PIPELINE QUEUE                                                              │   │
│  │  ══════════════                                                              │   │
│  │  • Inspector, Tester, Engineer, Designer (within pipelines)                 │   │
│  │                                                                              │   │
│  │  Properties:                                                                 │   │
│  │  • Bounded by N_CPU_CORES concurrent pipelines                              │   │
│  │  • Ordered by Architect-assigned priority                                   │   │
│  │  • Then by pipeline spawn time                                              │   │
│  │  • Can be preempted by user requests                                        │   │
│  └─────────────────────────────────────────────────────────────────────────────┘   │
│                                                                                     │
└─────────────────────────────────────────────────────────────────────────────────────┘
```

```go
type DualQueueGate struct {
    userQueue     *UnboundedQueue  // Never reject, always accept
    pipelineQueue *PriorityQueue   // Bounded, priority-ordered

    activeRequests map[string]*ActiveRequest
    mu             sync.Mutex

    rateLimiter    *ProviderRateLimiter
    signalBus      *SignalBus
}

type ActiveRequest struct {
    Request    *LLMRequest
    CancelFunc context.CancelFunc
    IsUser     bool
}

func (g *DualQueueGate) Submit(ctx context.Context, req *LLMRequest) (*CompletionResponse, error) {
    isUser := g.isUserInteractive(req)

    if isUser {
        g.userQueue.Push(req)
        // Check if preemption needed
        g.maybePreemptForUser()
    } else {
        g.pipelineQueue.Push(req)
    }

    return g.waitForCompletion(ctx, req)
}

func (g *DualQueueGate) maybePreemptForUser() {
    g.mu.Lock()
    defer g.mu.Unlock()

    // Find lowest-priority pipeline request to preempt
    var lowestPriority *ActiveRequest
    for _, ar := range g.activeRequests {
        if !ar.IsUser {
            if lowestPriority == nil || ar.Request.Priority < lowestPriority.Request.Priority {
                lowestPriority = ar
            }
        }
    }

    if lowestPriority != nil {
        // Cancel the pipeline request
        lowestPriority.CancelFunc()
        // It will be re-queued automatically on cancellation
    }
}

func (g *DualQueueGate) dispatchLoop() {
    for {
        // ALWAYS drain user queue first
        for g.userQueue.Len() > 0 {
            req := g.userQueue.Pop()
            g.executeRequest(req, true)
        }

        // Then process pipeline queue
        if g.pipelineQueue.Len() > 0 {
            req := g.pipelineQueue.Pop()
            g.executeRequest(req, false)
        }

        // Wait for next request or slot available
        g.waitForWork()
    }
}

func (g *DualQueueGate) isUserInteractive(req *LLMRequest) bool {
    switch req.AgentType {
    case AgentTypeGuide, AgentTypeArchitect, AgentTypeOrchestrator,
         AgentTypeLibrarian, AgentTypeArchivalist, AgentTypeAcademic:
        return true
    default:
        return req.UserInvoked // User explicitly asked this agent
    }
}
```

### File Access: Staging Isolation

Pipelines work on isolated staging areas. No file locking required.

```
┌─────────────────────────────────────────────────────────────────────────────────────┐
│                         FILE ACCESS: STAGING ISOLATION                               │
├─────────────────────────────────────────────────────────────────────────────────────┤
│                                                                                     │
│  USER WORKING TREE                    PIPELINE STAGING AREAS                        │
│  ════════════════                     ══════════════════════                        │
│  ~/.sylk/sessions/{id}/work/          ~/.sylk/sessions/{id}/staging/{pipeline_id}/ │
│                                                                                     │
│  • Authoritative source               • Copy-on-read from working tree             │
│  • User edits here directly           • Pipeline writes only to its staging        │
│  • Never modified by pipelines        • Complete isolation between pipelines       │
│                                       • Isolated from user edits                    │
│                                                                                     │
│  ┌─────────────────────────────────────────────────────────────────────────────┐   │
│  │                          STAGING LIFECYCLE                                   │   │
│  │                                                                              │   │
│  │  1. Pipeline starts                                                          │   │
│  │     └─► Create staging directory: staging/{pipeline_id}/                    │   │
│  │                                                                              │   │
│  │  2. Pipeline reads file                                                      │   │
│  │     └─► Copy from work/ to staging/{pipeline_id}/ (copy-on-read)           │   │
│  │     └─► Record file hash for conflict detection                             │   │
│  │                                                                              │   │
│  │  3. Pipeline writes file                                                     │   │
│  │     └─► Write ONLY to staging/{pipeline_id}/                                │   │
│  │     └─► Never touch work/ directly                                          │   │
│  │                                                                              │   │
│  │  4. Pipeline completes                                                       │   │
│  │     └─► Generate diff: staging vs current work/                             │   │
│  │     └─► Check for conflicts (hash mismatch = file changed)                  │   │
│  │     └─► If clean: apply diff to work/                                       │   │
│  │     └─► If conflict: present to user for resolution                         │   │
│  │                                                                              │   │
│  │  5. Cleanup                                                                  │   │
│  │     └─► Delete staging/{pipeline_id}/                                       │   │
│  └─────────────────────────────────────────────────────────────────────────────┘   │
│                                                                                     │
│  CONFLICT RESOLUTION:                                                               │
│  ───────────────────────────────────────────────────────────────────────────────── │
│  • User's direct edits ALWAYS win                                                  │
│  • Pipeline changes presented as diff for user approval                            │
│  • Three-way merge attempted automatically                                          │
│  • If auto-merge fails: show conflict markers, user decides                        │
│  • Git is final arbiter for persistence                                            │
│                                                                                     │
│  GUARANTEES:                                                                        │
│  ───────────────────────────────────────────────────────────────────────────────── │
│  • Zero race conditions (complete isolation)                                        │
│  • User work never corrupted by pipelines                                          │
│  • Conflicts explicit, never silent                                                │
│  • No locking needed, no deadlock possible                                         │
│  • Pipelines can run fully parallel on same files                                  │
│                                                                                     │
└─────────────────────────────────────────────────────────────────────────────────────┘
```

```go
type StagingManager struct {
    sessionID   string
    workDir     string  // ~/.sylk/sessions/{id}/work/
    stagingRoot string  // ~/.sylk/sessions/{id}/staging/
}

type PipelineStaging struct {
    pipelineID  string
    stagingDir  string                    // staging/{pipeline_id}/
    fileHashes  map[string]string         // path → hash at read time
    manager     *StagingManager
}

func (m *StagingManager) CreateStaging(pipelineID string) *PipelineStaging {
    stagingDir := filepath.Join(m.stagingRoot, pipelineID)
    os.MkdirAll(stagingDir, 0755)

    return &PipelineStaging{
        pipelineID: pipelineID,
        stagingDir: stagingDir,
        fileHashes: make(map[string]string),
        manager:    m,
    }
}

func (ps *PipelineStaging) ReadFile(relativePath string) ([]byte, error) {
    // Check if already in staging
    stagingPath := filepath.Join(ps.stagingDir, relativePath)
    if exists(stagingPath) {
        return os.ReadFile(stagingPath)
    }

    // Copy from work dir (copy-on-read)
    workPath := filepath.Join(ps.manager.workDir, relativePath)
    content, err := os.ReadFile(workPath)
    if err != nil {
        return nil, err
    }

    // Record hash for conflict detection
    ps.fileHashes[relativePath] = sha256Hash(content)

    // Copy to staging
    os.MkdirAll(filepath.Dir(stagingPath), 0755)
    os.WriteFile(stagingPath, content, 0644)

    return content, nil
}

func (ps *PipelineStaging) WriteFile(relativePath string, content []byte) error {
    // Always write to staging only
    stagingPath := filepath.Join(ps.stagingDir, relativePath)
    os.MkdirAll(filepath.Dir(stagingPath), 0755)
    return os.WriteFile(stagingPath, content, 0644)
}

func (ps *PipelineStaging) Merge() (*MergeResult, error) {
    result := &MergeResult{
        Applied:   []string{},
        Conflicts: []Conflict{},
    }

    // Walk staging directory
    filepath.Walk(ps.stagingDir, func(path string, info os.FileInfo, err error) error {
        if info.IsDir() {
            return nil
        }

        relativePath, _ := filepath.Rel(ps.stagingDir, path)
        workPath := filepath.Join(ps.manager.workDir, relativePath)

        // Check for conflicts
        originalHash, wasRead := ps.fileHashes[relativePath]
        if wasRead {
            currentContent, err := os.ReadFile(workPath)
            if err == nil {
                currentHash := sha256Hash(currentContent)
                if currentHash != originalHash {
                    // File changed since we read it - conflict!
                    result.Conflicts = append(result.Conflicts, Conflict{
                        Path:     relativePath,
                        Ours:     mustReadFile(path),
                        Theirs:   currentContent,
                        Original: originalHash,
                    })
                    return nil
                }
            }
        }

        // No conflict - apply change
        stagedContent, _ := os.ReadFile(path)
        os.MkdirAll(filepath.Dir(workPath), 0755)
        os.WriteFile(workPath, stagedContent, 0644)
        result.Applied = append(result.Applied, relativePath)

        return nil
    })

    return result, nil
}

func (ps *PipelineStaging) Cleanup() {
    os.RemoveAll(ps.stagingDir)
}
```

### Channel Architecture

Adaptive buffers with unbounded user path.

```go
// Channel types by purpose
type ChannelConfig struct {
    // User-facing: NEVER block, NEVER drop
    UserInput      *UnboundedChannel  // User commands, always accepted

    // Standalone agents: adaptive with short timeout
    AgentChannels  map[string]*AdaptiveChannel

    // Pipeline internal: adaptive with longer timeout
    PipelineChannels map[string]*AdaptiveChannel

    // Signals: NEVER block (critical path)
    SignalChannels map[Signal]*BufferedChannel
}

// UnboundedChannel: slice-backed, never blocks sender
type UnboundedChannel struct {
    ch       chan Message
    overflow []Message
    mu       sync.Mutex
    cond     *sync.Cond
}

func NewUnboundedChannel() *UnboundedChannel {
    uc := &UnboundedChannel{
        ch:       make(chan Message, 64),  // Small buffer for fast path
        overflow: make([]Message, 0),
    }
    uc.cond = sync.NewCond(&uc.mu)
    return uc
}

func (uc *UnboundedChannel) Send(msg Message) {
    // Try fast path (non-blocking)
    select {
    case uc.ch <- msg:
        return
    default:
    }

    // Overflow to slice (never blocks)
    uc.mu.Lock()
    uc.overflow = append(uc.overflow, msg)
    uc.cond.Signal()
    uc.mu.Unlock()
}

func (uc *UnboundedChannel) Receive(ctx context.Context) (Message, error) {
    // Try fast path
    select {
    case msg := <-uc.ch:
        return msg, nil
    case <-ctx.Done():
        return Message{}, ctx.Err()
    default:
    }

    // Check overflow
    uc.mu.Lock()
    for len(uc.overflow) == 0 {
        uc.cond.Wait()
    }
    msg := uc.overflow[0]
    uc.overflow = uc.overflow[1:]
    uc.mu.Unlock()

    return msg, nil
}

// AdaptiveChannel: auto-resizing buffer
type AdaptiveChannel struct {
    ch          chan Message
    overflow    []Message
    mu          sync.Mutex

    // Adaptive sizing
    currentSize int
    minSize     int  // 16
    maxSize     int  // 4096

    // Metrics for adaptation
    highWaterCount int  // Consecutive checks >80% full
    lowWaterCount  int  // Consecutive checks <20% full

    // Blocking behavior
    sendTimeout time.Duration  // 100ms for agents, 1s for pipelines
}

func NewAdaptiveChannel(sendTimeout time.Duration) *AdaptiveChannel {
    initialSize := 64
    ac := &AdaptiveChannel{
        ch:          make(chan Message, initialSize),
        overflow:    make([]Message, 0),
        currentSize: initialSize,
        minSize:     16,
        maxSize:     4096,
        sendTimeout: sendTimeout,
    }
    go ac.adaptLoop()
    return ac
}

func (ac *AdaptiveChannel) Send(ctx context.Context, msg Message) error {
    // Try non-blocking first
    select {
    case ac.ch <- msg:
        return nil
    default:
    }

    // Block with timeout
    select {
    case ac.ch <- msg:
        return nil
    case <-time.After(ac.sendTimeout):
        // Overflow to secondary buffer
        ac.mu.Lock()
        ac.overflow = append(ac.overflow, msg)
        ac.mu.Unlock()
        return nil  // Never fail, just overflow
    case <-ctx.Done():
        return ctx.Err()
    }
}

func (ac *AdaptiveChannel) adaptLoop() {
    ticker := time.NewTicker(1 * time.Second)
    defer ticker.Stop()

    for range ticker.C {
        ac.mu.Lock()
        utilization := float64(len(ac.ch)) / float64(ac.currentSize)

        if utilization > 0.8 {
            ac.highWaterCount++
            ac.lowWaterCount = 0
            if ac.highWaterCount >= 3 && ac.currentSize < ac.maxSize {
                ac.resize(ac.currentSize * 2)
            }
        } else if utilization < 0.2 {
            ac.lowWaterCount++
            ac.highWaterCount = 0
            if ac.lowWaterCount >= 10 && ac.currentSize > ac.minSize {
                ac.resize(ac.currentSize / 2)
            }
        } else {
            ac.highWaterCount = 0
            ac.lowWaterCount = 0
        }

        ac.mu.Unlock()
    }
}

func (ac *AdaptiveChannel) resize(newSize int) {
    // Create new channel, drain old to new
    newCh := make(chan Message, newSize)

    // Drain without blocking
    for {
        select {
        case msg := <-ac.ch:
            newCh <- msg
        default:
            ac.ch = newCh
            ac.currentSize = newSize
            return
        }
    }
}
```

### Crash Recovery

Continuous checkpointing and WAL for durability.

```
┌─────────────────────────────────────────────────────────────────────────────────────┐
│                         CRASH RECOVERY                                               │
├─────────────────────────────────────────────────────────────────────────────────────┤
│                                                                                     │
│  WRITE-AHEAD LOG (WAL):                                                             │
│  ───────────────────────────────────────────────────────────────────────────────── │
│  Location: ~/.sylk/sessions/{id}/wal/                                              │
│                                                                                     │
│  • Every state-changing operation logged BEFORE execution                          │
│  • Log entry: {seq, timestamp, operation_type, agent_id, params, checksum}        │
│  • fsync after each write (durability guarantee)                                   │
│  • WAL truncated after checkpoint                                                  │
│                                                                                     │
│  CONTINUOUS CHECKPOINTING:                                                          │
│  ───────────────────────────────────────────────────────────────────────────────── │
│  Location: ~/.sylk/sessions/{id}/checkpoints/                                      │
│  Interval: Every 5 seconds                                                         │
│                                                                                     │
│  Checkpoint includes:                                                               │
│  • All standalone agent states                                                      │
│  • All active pipeline states                                                      │
│  • Pipeline queue contents                                                         │
│  • User queue contents                                                             │
│  • Message history                                                                 │
│  • Staging area manifests                                                          │
│                                                                                     │
│  Checkpoint procedure:                                                              │
│  1. Write to temp file: checkpoint_{timestamp}.tmp                                 │
│  2. fsync temp file                                                                │
│  3. Atomic rename: checkpoint_{timestamp}.tmp → checkpoint_{timestamp}             │
│  4. Delete checkpoints older than last 3                                           │
│  5. Truncate WAL entries before checkpoint                                         │
│                                                                                     │
│  RECOVERY PROCEDURE:                                                                │
│  ───────────────────────────────────────────────────────────────────────────────── │
│  1. On startup: check for lock file (~/.sylk/sessions/{id}/lock)                  │
│  2. If lock exists: unclean shutdown detected                                      │
│  3. Find most recent valid checkpoint                                              │
│  4. Load checkpoint state                                                          │
│  5. Replay WAL entries after checkpoint timestamp                                  │
│  6. Resume all pipelines from recovered state                                      │
│  7. Notify user: "Recovered from crash, N pipelines resumed"                       │
│  8. Create new lock file                                                           │
│                                                                                     │
│  GUARANTEES:                                                                        │
│  ───────────────────────────────────────────────────────────────────────────────── │
│  • Maximum data loss: 5 seconds of work                                            │
│  • No silent corruption (checksums verified)                                       │
│  • User always informed of recovery                                                │
│  • Pipelines resume automatically                                                  │
│  • Staging areas preserved                                                         │
│                                                                                     │
└─────────────────────────────────────────────────────────────────────────────────────┘
```

```go
type RecoveryManager struct {
    sessionID     string
    sessionDir    string
    walDir        string
    checkpointDir string
    lockFile      string

    wal           *WriteAheadLog
    checkpointer  *Checkpointer
}

type WriteAheadLog struct {
    file    *os.File
    seq     uint64
    mu      sync.Mutex
}

type WALEntry struct {
    Seq           uint64    `json:"seq"`
    Timestamp     time.Time `json:"ts"`
    OperationType string    `json:"op"`
    AgentID       string    `json:"agent"`
    PipelineID    string    `json:"pipeline,omitempty"`
    Params        any       `json:"params"`
    Checksum      string    `json:"checksum"`
}

func (w *WriteAheadLog) Append(entry WALEntry) error {
    w.mu.Lock()
    defer w.mu.Unlock()

    w.seq++
    entry.Seq = w.seq
    entry.Timestamp = time.Now()
    entry.Checksum = computeChecksum(entry)

    data, _ := json.Marshal(entry)
    data = append(data, '\n')

    if _, err := w.file.Write(data); err != nil {
        return err
    }

    return w.file.Sync()  // fsync for durability
}

type Checkpointer struct {
    sessionDir    string
    checkpointDir string
    interval      time.Duration

    getState      func() *SessionState
    stopCh        chan struct{}
}

type SessionState struct {
    Timestamp       time.Time                    `json:"ts"`
    WALSeq          uint64                       `json:"wal_seq"`
    AgentStates     map[string]*AgentState       `json:"agents"`
    PipelineStates  map[string]*PipelineState    `json:"pipelines"`
    UserQueue       []Message                    `json:"user_queue"`
    PipelineQueue   []PipelineQueueEntry         `json:"pipeline_queue"`
    MessageHistory  []Message                    `json:"messages"`
    StagingManifest map[string][]string          `json:"staging"`  // pipeline → files
}

func (c *Checkpointer) Start() {
    go func() {
        ticker := time.NewTicker(c.interval)
        defer ticker.Stop()

        for {
            select {
            case <-ticker.C:
                c.saveCheckpoint()
            case <-c.stopCh:
                c.saveCheckpoint()  // Final checkpoint on shutdown
                return
            }
        }
    }()
}

func (c *Checkpointer) saveCheckpoint() error {
    state := c.getState()
    state.Timestamp = time.Now()

    // Write to temp file
    tempPath := filepath.Join(c.checkpointDir,
        fmt.Sprintf("checkpoint_%d.tmp", state.Timestamp.UnixNano()))

    data, _ := json.Marshal(state)
    if err := os.WriteFile(tempPath, data, 0644); err != nil {
        return err
    }

    // fsync
    f, _ := os.Open(tempPath)
    f.Sync()
    f.Close()

    // Atomic rename
    finalPath := strings.TrimSuffix(tempPath, ".tmp")
    if err := os.Rename(tempPath, finalPath); err != nil {
        return err
    }

    // Cleanup old checkpoints (keep last 3)
    c.cleanupOldCheckpoints(3)

    return nil
}

func (rm *RecoveryManager) Recover() (*SessionState, error) {
    // Check for unclean shutdown
    if !exists(rm.lockFile) {
        return nil, nil  // Clean shutdown, no recovery needed
    }

    // Find most recent checkpoint
    checkpoint, err := rm.loadLatestCheckpoint()
    if err != nil {
        return nil, fmt.Errorf("recovery failed: %w", err)
    }

    // Replay WAL entries after checkpoint
    entries, err := rm.wal.EntriesAfter(checkpoint.WALSeq)
    if err != nil {
        return nil, fmt.Errorf("WAL replay failed: %w", err)
    }

    for _, entry := range entries {
        if err := rm.replayEntry(checkpoint, entry); err != nil {
            // Log but continue - partial recovery better than none
            log.Warnf("Failed to replay WAL entry %d: %v", entry.Seq, err)
        }
    }

    return checkpoint, nil
}
```

### Deadlock Prevention Summary

```
┌─────────────────────────────────────────────────────────────────────────────────────┐
│                    DEADLOCK PREVENTION GUARANTEES                                    │
├─────────────────────────────────────────────────────────────────────────────────────┤
│                                                                                     │
│  1. NO CIRCULAR WAITS                                                               │
│     • Guide is async-only, never blocks                                            │
│     • Message flow is acyclic: User → Guide → Agents → (responses via Guide)       │
│     • File access via staging (no locks, no circular dependencies)                 │
│                                                                                     │
│  2. NO HOLD AND WAIT                                                                │
│     • All channel operations have timeouts                                          │
│     • Overflow to secondary buffer on timeout (never deadlock)                     │
│     • LLM requests cancellable for preemption                                      │
│                                                                                     │
│  3. NO RESOURCE STARVATION                                                          │
│     • User-interactive has absolute LLM priority                                   │
│     • User can preempt any pipeline LLM request                                    │
│     • Pipelines use fair scheduling (priority + spawn time)                        │
│     • User queue always drains first on rate limit recovery                        │
│                                                                                     │
│  4. NO DATA LOSS                                                                    │
│     • User input unbounded (never dropped)                                          │
│     • Staging isolation (user work never corrupted)                                │
│     • WAL for all state changes                                                    │
│     • Continuous checkpointing (5s interval)                                       │
│     • Automatic crash recovery with WAL replay                                     │
│                                                                                     │
│  5. NO CORRUPTION                                                                   │
│     • Optimistic concurrency for staging merges                                    │
│     • Hash-based conflict detection                                                │
│     • Git as final arbiter                                                         │
│     • Conflicts surfaced to user (never silent)                                    │
│     • Atomic checkpoint writes                                                     │
│                                                                                     │
│  6. ALWAYS RESPONSIVE                                                               │
│     • Standalone agents always available (no pool limit)                           │
│     • User queue unbounded, always accepts                                         │
│     • Preemption ensures user never waits for pipelines                            │
│     • Adaptive channel buffers prevent bottlenecks                                 │
│                                                                                     │
└─────────────────────────────────────────────────────────────────────────────────────┘
```

---

## Error Propagation & Recovery

### Error Type Taxonomy

Sylk uses a 5-tier error classification system to determine handling behavior:

```
┌─────────────────────────────────────────────────────────────────────────────────────┐
│                              ERROR TYPE TAXONOMY                                     │
├─────────────────────────────────────────────────────────────────────────────────────┤
│                                                                                     │
│  TIER 1: TRANSIENT                                                                  │
│  ┌────────────────────────────────────────────────────────────────────────────────┐ │
│  │ Examples: Network blip, DNS hiccup, momentary timeout, connection reset        │ │
│  │ Behavior: Silent retry with backoff                                            │ │
│  │ Notification: ONLY if high frequency detected (configurable threshold)         │ │
│  │ Rationale: Normal internet noise, don't interrupt user for transient issues    │ │
│  └────────────────────────────────────────────────────────────────────────────────┘ │
│                                                                                     │
│  TIER 2: PERMANENT                                                                  │
│  ┌────────────────────────────────────────────────────────────────────────────────┐ │
│  │ Examples: File not found, invalid config, parse error, schema violation        │ │
│  │ Behavior: No retry (would fail again)                                          │ │
│  │ Notification: Immediate + proposed fixes                                       │ │
│  │ Rationale: Retrying won't help, user needs to know and fix                     │ │
│  └────────────────────────────────────────────────────────────────────────────────┘ │
│                                                                                     │
│  TIER 3: USER-FIXABLE                                                               │
│  ┌────────────────────────────────────────────────────────────────────────────────┐ │
│  │ Examples: Missing API key, invalid credentials, permission denied, quota       │ │
│  │ Behavior: No retry until user takes action                                     │ │
│  │ Notification: Immediate + exact steps to fix                                   │ │
│  │ Rationale: Only user can resolve, provide clear guidance                       │ │
│  └────────────────────────────────────────────────────────────────────────────────┘ │
│                                                                                     │
│  TIER 4: EXTERNAL-RATELIMIT                                                         │
│  ┌────────────────────────────────────────────────────────────────────────────────┐ │
│  │ Examples: 429 from Anthropic/OpenAI/Google, rate limit headers                 │ │
│  │ Behavior: Exponential backoff with countdown display                           │ │
│  │ Notification: Countdown timer shown to user (expected behavior)                │ │
│  │ Rationale: Normal backpressure, not broken, user sees progress                 │ │
│  └────────────────────────────────────────────────────────────────────────────────┘ │
│                                                                                     │
│  TIER 5: EXTERNAL-DEGRADING                                                         │
│  ┌────────────────────────────────────────────────────────────────────────────────┐ │
│  │ Examples: 500/502/503, auth failure, provider down, connection refused         │ │
│  │ Behavior: IMMEDIATE notification → working on remedy → proposed fixes          │ │
│  │ Notification: Instant alert, continuous status, user approves/rejects fix      │ │
│  │ Rationale: App functionality impaired, user must know NOW                      │ │
│  └────────────────────────────────────────────────────────────────────────────────┘ │
│                                                                                     │
└─────────────────────────────────────────────────────────────────────────────────────┘
```

### Error Type Detection

```go
// ErrorTier represents the classification of an error
type ErrorTier int

const (
    ErrorTierTransient ErrorTier = iota
    ErrorTierPermanent
    ErrorTierUserFixable
    ErrorTierExternalRateLimit
    ErrorTierExternalDegrading
)

// ErrorClassifier determines the tier for a given error
type ErrorClassifier struct {
    config ErrorClassifierConfig
}

type ErrorClassifierConfig struct {
    // Transient error patterns (regex)
    TransientPatterns []string `yaml:"transient_patterns"`

    // Permanent error patterns
    PermanentPatterns []string `yaml:"permanent_patterns"`

    // User-fixable error codes/patterns
    UserFixablePatterns []string `yaml:"user_fixable_patterns"`

    // HTTP status codes that indicate rate limiting
    RateLimitStatuses []int `yaml:"rate_limit_statuses"` // default: [429]

    // HTTP status codes that indicate degradation
    DegradingStatuses []int `yaml:"degrading_statuses"` // default: [500, 502, 503, 504]
}

// Classify determines the error tier
func (c *ErrorClassifier) Classify(err error) ErrorTier {
    // Check for HTTP status codes first
    if httpErr, ok := err.(*HTTPError); ok {
        if contains(c.config.RateLimitStatuses, httpErr.StatusCode) {
            return ErrorTierExternalRateLimit
        }
        if contains(c.config.DegradingStatuses, httpErr.StatusCode) {
            return ErrorTierExternalDegrading
        }
    }

    // Check for known user-fixable errors
    if c.matchesPatterns(err, c.config.UserFixablePatterns) {
        return ErrorTierUserFixable
    }

    // Check for known permanent errors
    if c.matchesPatterns(err, c.config.PermanentPatterns) {
        return ErrorTierPermanent
    }

    // Check for known transient errors
    if c.matchesPatterns(err, c.config.TransientPatterns) {
        return ErrorTierTransient
    }

    // Default: treat unknown errors as permanent (fail fast, don't mask)
    return ErrorTierPermanent
}
```

### Error Handling Flow

```
┌─────────────────────────────────────────────────────────────────────────────────────┐
│                              ERROR HANDLING FLOW                                     │
├─────────────────────────────────────────────────────────────────────────────────────┤
│                                                                                     │
│  Error Occurs                                                                       │
│       │                                                                             │
│       ▼                                                                             │
│  ┌─────────────┐                                                                    │
│  │  Classify   │                                                                    │
│  │   Error     │                                                                    │
│  └──────┬──────┘                                                                    │
│         │                                                                           │
│         ├─────────────────┬─────────────────┬─────────────────┬──────────────────┐  │
│         ▼                 ▼                 ▼                 ▼                  ▼  │
│    TRANSIENT         PERMANENT        USER-FIXABLE      RATE-LIMIT        DEGRADING │
│         │                 │                 │                 │                  │  │
│         ▼                 ▼                 ▼                 ▼                  ▼  │
│  ┌─────────────┐   ┌─────────────┐   ┌─────────────┐   ┌─────────────┐   ┌─────────┐│
│  │Silent Retry │   │  Escalate   │   │  Escalate   │   │  Backoff +  │   │IMMEDIATE││
│  │ + Frequency │   │  Immediate  │   │  Immediate  │   │  Countdown  │   │  ALERT  ││
│  │  Tracking   │   │             │   │             │   │             │   │         ││
│  └──────┬──────┘   └──────┬──────┘   └──────┬──────┘   └──────┬──────┘   └────┬────┘│
│         │                 │                 │                 │                │    │
│         ▼                 │                 │                 │                ▼    │
│  Frequency High?          │                 │                 │         ┌──────────┐│
│    │      │               │                 │                 │         │ Architect ││
│   Yes     No              │                 │                 │         │ Attempts  ││
│    │      │               │                 │                 │         │  Remedy   ││
│    │      ▼               │                 │                 │         └─────┬────┘│
│    │   Continue           │                 │                 │               │     │
│    │                      │                 │                 │               ▼     │
│    ▼                      ▼                 ▼                 ▼         Remedy OK?  │
│  ┌───────────────────────────────────────────────────────────────────┐   │      │   │
│  │                     ESCALATION CHAIN                              │  Yes     No  │
│  │                                                                   │   │      │   │
│  │  Agent → Architect (try workaround, token budget limited)         │   │      ▼   │
│  │      │                                                            │   │  ┌──────┐│
│  │      ├── Workaround succeeds → Continue (transparent to user)     │   │  │Notify││
│  │      │                                                            │   │  │ User ││
│  │      ├── Critical error → IMMEDIATE user notification             │   │  │+Fixes││
│  │      │                    + proposed remedy                       │   │  └──────┘│
│  │      │                    + user accepts/rejects                  │   │         │
│  │      │                                                            │   ▼         │
│  │      └── Budget exceeded → Escalate to user with options          │  Continue   │
│  │                                                                   │             │
│  └───────────────────────────────────────────────────────────────────┘             │
│                                                                                     │
└─────────────────────────────────────────────────────────────────────────────────────┘
```

### Transient Error Frequency Detection

```go
// TransientTracker monitors transient error frequency
type TransientTracker struct {
    mu       sync.Mutex
    errors   []time.Time
    config   TransientTrackerConfig
}

type TransientTrackerConfig struct {
    // Number of errors to trigger notification
    FrequencyCount int `yaml:"frequency_count"` // default: 3

    // Time window for frequency detection
    FrequencyWindow time.Duration `yaml:"frequency_window"` // default: 10s

    // Cooldown after notification before re-alerting
    NotificationCooldown time.Duration `yaml:"notification_cooldown"` // default: 60s
}

func DefaultTransientTrackerConfig() TransientTrackerConfig {
    return TransientTrackerConfig{
        FrequencyCount:       3,
        FrequencyWindow:      10 * time.Second,
        NotificationCooldown: 60 * time.Second,
    }
}

// Record tracks a transient error and returns whether notification is needed
func (t *TransientTracker) Record(err error) bool {
    t.mu.Lock()
    defer t.mu.Unlock()

    now := time.Now()
    t.errors = append(t.errors, now)

    // Prune old errors outside window
    cutoff := now.Add(-t.config.FrequencyWindow)
    pruned := make([]time.Time, 0)
    for _, errTime := range t.errors {
        if errTime.After(cutoff) {
            pruned = append(pruned, errTime)
        }
    }
    t.errors = pruned

    // Check if frequency threshold exceeded
    return len(t.errors) >= t.config.FrequencyCount
}
```

### Escalation Chain

```
┌─────────────────────────────────────────────────────────────────────────────────────┐
│                              ESCALATION CHAIN                                        │
├─────────────────────────────────────────────────────────────────────────────────────┤
│                                                                                     │
│  LEVEL 1: AGENT SELF-RECOVERY                                                       │
│  ┌────────────────────────────────────────────────────────────────────────────────┐ │
│  │ • Agent attempts retry based on error tier                                     │ │
│  │ • Transient: silent retry with backoff                                         │ │
│  │ • Rate limit: wait for countdown                                               │ │
│  │ • If recovery succeeds: continue (transparent to user)                         │ │
│  │ • If recovery fails: escalate to Architect                                     │ │
│  └────────────────────────────────────────────────────────────────────────────────┘ │
│                                         │                                           │
│                                         ▼                                           │
│  LEVEL 2: ARCHITECT WORKAROUND                                                      │
│  ┌────────────────────────────────────────────────────────────────────────────────┐ │
│  │ • Architect receives error context from agent                                  │ │
│  │ • Token budget for workaround attempts (configurable, default: 1000)           │ │
│  │ • Architect may:                                                               │ │
│  │   - Retry with different parameters                                            │ │
│  │   - Use alternative approach                                                   │ │
│  │   - Query Archivalist for similar past failures + resolutions                  │ │
│  │ • CRITICAL: If error would cause app degradation → skip to user immediately    │ │
│  │ • If workaround succeeds: continue (notify user of what was done)              │ │
│  │ • If budget exceeded or no workaround: escalate to user                        │ │
│  └────────────────────────────────────────────────────────────────────────────────┘ │
│                                         │                                           │
│                                         ▼                                           │
│  LEVEL 3: USER DECISION                                                             │
│  ┌────────────────────────────────────────────────────────────────────────────────┐ │
│  │ • User receives:                                                               │ │
│  │   - Clear explanation of what failed                                           │ │
│  │   - What was attempted to fix it                                               │ │
│  │   - Proposed remedies (ranked by likelihood of success)                        │ │
│  │ • User can:                                                                    │ │
│  │   - Accept a proposed remedy                                                   │ │
│  │   - Reject and provide alternative instruction                                 │ │
│  │   - Abort the operation                                                        │ │
│  │   - Retry with modifications                                                   │ │
│  └────────────────────────────────────────────────────────────────────────────────┘ │
│                                                                                     │
└─────────────────────────────────────────────────────────────────────────────────────┘
```

### Architect Workaround Budget

```go
// WorkaroundBudget tracks token spending on error recovery
type WorkaroundBudget struct {
    mu        sync.Mutex
    spent     int
    config    WorkaroundBudgetConfig
}

type WorkaroundBudgetConfig struct {
    // Maximum tokens for workaround attempts
    MaxTokens int `yaml:"max_tokens"` // default: 1000

    // Reset budget after successful recovery
    ResetOnSuccess bool `yaml:"reset_on_success"` // default: true

    // Per-error-type budgets (optional override)
    PerTypeBudgets map[ErrorTier]int `yaml:"per_type_budgets"`
}

func DefaultWorkaroundBudgetConfig() WorkaroundBudgetConfig {
    return WorkaroundBudgetConfig{
        MaxTokens:      1000,
        ResetOnSuccess: true,
        PerTypeBudgets: nil, // use MaxTokens for all
    }
}

// CanSpend checks if there's budget for an operation
func (w *WorkaroundBudget) CanSpend(tokens int, tier ErrorTier) bool {
    w.mu.Lock()
    defer w.mu.Unlock()

    limit := w.config.MaxTokens
    if tierLimit, ok := w.config.PerTypeBudgets[tier]; ok {
        limit = tierLimit
    }

    return w.spent + tokens <= limit
}

// Spend deducts tokens from budget
func (w *WorkaroundBudget) Spend(tokens int) {
    w.mu.Lock()
    defer w.mu.Unlock()
    w.spent += tokens
}

// Reset clears spent tokens (call on successful recovery)
func (w *WorkaroundBudget) Reset() {
    w.mu.Lock()
    defer w.mu.Unlock()
    w.spent = 0
}
```

### Circuit Breaker

```
┌─────────────────────────────────────────────────────────────────────────────────────┐
│                              CIRCUIT BREAKER STATE MACHINE                           │
├─────────────────────────────────────────────────────────────────────────────────────┤
│                                                                                     │
│                         ┌──────────────────────────────────┐                        │
│                         │                                  │                        │
│                         ▼                                  │                        │
│                    ┌─────────┐                             │                        │
│         ┌─────────│ CLOSED  │◄────────────────────────────┐│                        │
│         │         └────┬────┘                             ││                        │
│         │              │                                  ││                        │
│         │              │ Failure threshold                ││                        │
│         │              │ exceeded                         ││                        │
│         │              ▼                                  ││                        │
│         │         ┌─────────┐                             ││                        │
│    Success        │  OPEN   │─────────────────┐           ││                        │
│    resets         └────┬────┘                 │           ││                        │
│    counter             │                      │           ││                        │
│         │              │ Cooldown             │ All       ││                        │
│         │              │ expires              │ requests  ││                        │
│         │              ▼                      │ fail-fast ││                        │
│         │         ┌───────────┐               │           ││                        │
│         │         │HALF-OPEN  │               │           ││                        │
│         │         └─────┬─────┘               │           ││                        │
│         │               │                     │           ││                        │
│         │    ┌──────────┼──────────┐          │           ││                        │
│         │    │          │          │          │           ││                        │
│         │  Probe      Probe      Probe        │           ││                        │
│         │  Success    Fails      Timeout      │           ││                        │
│         │    │          │          │          │           ││                        │
│         │    ▼          ▼          ▼          │           ││                        │
│         └────┘     Back to OPEN ──────────────┘           ││                        │
│                                                           ││                        │
│                     User Manual Reset ────────────────────┘│                        │
│                                                            │                        │
└─────────────────────────────────────────────────────────────────────────────────────┘
```

### Circuit Breaker Implementation

```go
// CircuitState represents the circuit breaker state
type CircuitState int

const (
    CircuitClosed CircuitState = iota
    CircuitOpen
    CircuitHalfOpen
)

// CircuitBreaker implements per-resource circuit breaking
type CircuitBreaker struct {
    mu              sync.RWMutex
    state           CircuitState
    failures        int
    successes       int
    lastFailure     time.Time
    lastStateChange time.Time
    config          CircuitBreakerConfig
    resourceID      string

    // Sliding window for rate-based detection
    recentResults   []bool // true = success, false = failure
    windowIndex     int
}

type CircuitBreakerConfig struct {
    // Consecutive failures to trip (count-based)
    ConsecutiveFailures int `yaml:"consecutive_failures"` // varies by resource

    // Failure rate to trip (rate-based)
    FailureRateThreshold float64 `yaml:"failure_rate_threshold"` // default: 0.5

    // Window size for rate calculation
    RateWindowSize int `yaml:"rate_window_size"` // default: 20

    // Cooldown before half-open probe
    CooldownDuration time.Duration `yaml:"cooldown_duration"` // default: 30s

    // Number of successful probes to close circuit
    SuccessThreshold int `yaml:"success_threshold"` // default: 3

    // Notify user when circuit state changes
    NotifyOnStateChange bool `yaml:"notify_on_state_change"` // default: true
}

// Per-resource default configurations
func DefaultCircuitBreakerConfigs() map[string]CircuitBreakerConfig {
    return map[string]CircuitBreakerConfig{
        "llm": {
            ConsecutiveFailures:  5,  // LLM failures are expensive, more tolerance
            FailureRateThreshold: 0.5,
            RateWindowSize:       20,
            CooldownDuration:     30 * time.Second,
            SuccessThreshold:     3,
            NotifyOnStateChange:  true,
        },
        "file": {
            ConsecutiveFailures:  2,  // File ops should work, fail fast
            FailureRateThreshold: 0.3,
            RateWindowSize:       10,
            CooldownDuration:     10 * time.Second,
            SuccessThreshold:     2,
            NotifyOnStateChange:  true,
        },
        "network": {
            ConsecutiveFailures:  3,
            FailureRateThreshold: 0.5,
            RateWindowSize:       20,
            CooldownDuration:     30 * time.Second,
            SuccessThreshold:     3,
            NotifyOnStateChange:  true,
        },
        "subprocess": {
            ConsecutiveFailures:  3,
            FailureRateThreshold: 0.4,
            RateWindowSize:       15,
            CooldownDuration:     20 * time.Second,
            SuccessThreshold:     2,
            NotifyOnStateChange:  true,
        },
    }
}

// RecordResult records a success or failure
func (cb *CircuitBreaker) RecordResult(success bool) {
    cb.mu.Lock()
    defer cb.mu.Unlock()

    // Update sliding window
    if len(cb.recentResults) < cb.config.RateWindowSize {
        cb.recentResults = append(cb.recentResults, success)
    } else {
        cb.recentResults[cb.windowIndex] = success
        cb.windowIndex = (cb.windowIndex + 1) % cb.config.RateWindowSize
    }

    if success {
        cb.recordSuccess()
    } else {
        cb.recordFailure()
    }
}

func (cb *CircuitBreaker) recordFailure() {
    cb.failures++
    cb.successes = 0
    cb.lastFailure = time.Now()

    // Check if should trip
    if cb.state == CircuitClosed {
        if cb.shouldTrip() {
            cb.tripCircuit()
        }
    } else if cb.state == CircuitHalfOpen {
        // Probe failed, back to open
        cb.setState(CircuitOpen)
    }
}

func (cb *CircuitBreaker) shouldTrip() bool {
    // Count-based: consecutive failures
    if cb.failures >= cb.config.ConsecutiveFailures {
        return true
    }

    // Rate-based: failure rate in window
    if len(cb.recentResults) >= cb.config.RateWindowSize {
        failures := 0
        for _, success := range cb.recentResults {
            if !success {
                failures++
            }
        }
        rate := float64(failures) / float64(len(cb.recentResults))
        if rate >= cb.config.FailureRateThreshold {
            return true
        }
    }

    return false
}

// Allow checks if a request should be allowed
func (cb *CircuitBreaker) Allow() bool {
    cb.mu.Lock()
    defer cb.mu.Unlock()

    switch cb.state {
    case CircuitClosed:
        return true

    case CircuitOpen:
        // Check if cooldown expired
        if time.Since(cb.lastStateChange) >= cb.config.CooldownDuration {
            cb.setState(CircuitHalfOpen)
            return true // Allow probe request
        }
        return false

    case CircuitHalfOpen:
        // Only allow limited probes
        return true
    }

    return false
}

// ForceReset allows user to manually reset the circuit
func (cb *CircuitBreaker) ForceReset() {
    cb.mu.Lock()
    defer cb.mu.Unlock()

    cb.setState(CircuitClosed)
    cb.failures = 0
    cb.successes = 0
    cb.recentResults = nil
}
```

### Partial Failure Handling

```
┌─────────────────────────────────────────────────────────────────────────────────────┐
│                           PARTIAL FAILURE HANDLING                                   │
├─────────────────────────────────────────────────────────────────────────────────────┤
│                                                                                     │
│  Pipeline Failure Mid-Execution:                                                    │
│                                                                                     │
│  ┌─────────┐    ┌─────────┐    ┌─────────┐    ┌─────────┐                          │
│  │Inspector│───▶│ Tester  │───▶│ Worker  │───▶│ Verify  │                          │
│  │   ✓     │    │   ✓     │    │   ✗     │    │         │                          │
│  └─────────┘    └─────────┘    └────┬────┘    └─────────┘                          │
│                                     │                                               │
│                                     ▼                                               │
│                              ┌─────────────┐                                        │
│                              │  PRESERVE   │                                        │
│                              │   WORK IN   │                                        │
│                              │   STAGING   │                                        │
│                              └──────┬──────┘                                        │
│                                     │                                               │
│                                     ▼                                               │
│                           ┌─────────────────┐                                       │
│                           │ Present to User │                                       │
│                           └────────┬────────┘                                       │
│                                    │                                                │
│      ┌─────────────────────────────┼─────────────────────────────┐                  │
│      │                             │                             │                  │
│      ▼                             ▼                             ▼                  │
│ ┌──────────┐                ┌──────────────┐              ┌──────────────┐          │
│ │  RETRY   │                │   ROLLBACK   │              │    KEEP +    │          │
│ │  FROM    │                │     ALL      │              │   CONTINUE   │          │
│ │ FAILURE  │                │              │              │   MANUALLY   │          │
│ └────┬─────┘                └──────┬───────┘              └──────┬───────┘          │
│      │                             │                             │                  │
│      │ (Default for               │ (Default for                │                  │
│      │  Transient/External)       │  Permanent)                 │                  │
│      │                             │                             │                  │
│      ▼                             ▼                             ▼                  │
│ Archivalist                  Discard staging              Merge partial             │
│ briefs agent                 Reset context                User takes over           │
│ on prior attempt             Clean slate                                            │
│                                                                                     │
└─────────────────────────────────────────────────────────────────────────────────────┘
```

### Retry with Context Briefing

```go
// RetryBriefing contains context for a retry attempt
type RetryBriefing struct {
    OriginalRequest   string            `json:"original_request"`
    AttemptNumber     int               `json:"attempt_number"`
    PriorAttempts     []AttemptSummary  `json:"prior_attempts"`
    FailureAnalysis   string            `json:"failure_analysis"`
    SuggestedApproach string            `json:"suggested_approach"`
    AvoidPatterns     []string          `json:"avoid_patterns"`
}

type AttemptSummary struct {
    Timestamp    time.Time `json:"timestamp"`
    Approach     string    `json:"approach"`
    Error        string    `json:"error"`
    ErrorTier    ErrorTier `json:"error_tier"`
    TokensSpent  int       `json:"tokens_spent"`
    PhasesComplete []string `json:"phases_complete"`
}

// PrepareRetryBriefing queries Archivalist for context
func (rm *RecoveryManager) PrepareRetryBriefing(
    ctx context.Context,
    pipelineID string,
    currentError error,
) (*RetryBriefing, error) {

    // Query Archivalist for prior attempts on this task
    priorAttempts, err := rm.archivalist.QueryFailures(ctx, pipelineID)
    if err != nil {
        // Non-fatal: proceed without historical context
        priorAttempts = nil
    }

    // Analyze failure patterns
    avoidPatterns := rm.analyzeFailurePatterns(priorAttempts)

    // Generate suggested approach (avoiding prior failures)
    suggested := rm.suggestAlternativeApproach(priorAttempts, currentError)

    return &RetryBriefing{
        AttemptNumber:     len(priorAttempts) + 1,
        PriorAttempts:     priorAttempts,
        FailureAnalysis:   currentError.Error(),
        SuggestedApproach: suggested,
        AvoidPatterns:     avoidPatterns,
    }, nil
}
```

### Rollback Strategy

```
┌─────────────────────────────────────────────────────────────────────────────────────┐
│                              ROLLBACK STRATEGY                                       │
├─────────────────────────────────────────────────────────────────────────────────────┤
│                                                                                     │
│  LAYER 1: FILE STAGING                                                              │
│  ┌────────────────────────────────────────────────────────────────────────────────┐ │
│  │ • Discard staging directory (rm -rf ~/.sylk/staging/<pipeline-id>)             │ │
│  │ • If already merged: git reset --soft to uncommit                              │ │
│  │ • Working directory restored to pre-pipeline state                             │ │
│  └────────────────────────────────────────────────────────────────────────────────┘ │
│                                                                                     │
│  LAYER 2: AGENT STATE                                                               │
│  ┌────────────────────────────────────────────────────────────────────────────────┐ │
│  │ • Reset agent conversation context to pre-attempt checkpoint                   │ │
│  │ • Clear agent's working memory of failed attempt                               │ │
│  │ • BUT: Archivalist MARKS attempt as rolled-back (does not forget)              │ │
│  │   - Failure record preserved for learning                                      │ │
│  │   - Available for retry briefing                                               │ │
│  │   - Queryable: "what approaches failed for similar tasks?"                     │ │
│  └────────────────────────────────────────────────────────────────────────────────┘ │
│                                                                                     │
│  LAYER 3: GIT STATE                                                                 │
│  ┌────────────────────────────────────────────────────────────────────────────────┐ │
│  │ • Local commits only: git reset --soft HEAD~N                                  │ │
│  │ • Already pushed: NEVER auto-revert                                            │ │
│  │   - Present to user: "N commits were pushed. Options:"                         │ │
│  │     1. Create revert commit (safe)                                             │ │
│  │     2. Leave as-is (user handles manually)                                     │ │
│  │     3. Force push (dangerous, requires explicit confirmation)                  │ │
│  └────────────────────────────────────────────────────────────────────────────────┘ │
│                                                                                     │
│  LAYER 4: EXTERNAL STATE                                                            │
│  ┌────────────────────────────────────────────────────────────────────────────────┐ │
│  │ • Log all external API calls made during attempt                               │ │
│  │ • Flag as "potentially inconsistent" if rollback occurs                        │ │
│  │ • Notify user: "The following external calls were made and cannot be undone:"  │ │
│  │   - POST /api/deploy (status: completed)                                       │ │
│  │   - PUT /api/config (status: completed)                                        │ │
│  │ • User decides how to handle external state                                    │ │
│  └────────────────────────────────────────────────────────────────────────────────┘ │
│                                                                                     │
│  ROLLBACK RECEIPT (always shown to user):                                           │
│  ┌────────────────────────────────────────────────────────────────────────────────┐ │
│  │ Rollback Complete:                                                             │ │
│  │   ✓ Staged files discarded (3 files)                                           │ │
│  │   ✓ Agent context reset                                                        │ │
│  │   ✓ Local commits removed (2 commits)                                          │ │
│  │   ⚠ External calls logged (cannot undo):                                       │ │
│  │     - POST /api/webhook                                                        │ │
│  │   ℹ Failure recorded in Archivalist for future reference                       │ │
│  └────────────────────────────────────────────────────────────────────────────────┘ │
│                                                                                     │
└─────────────────────────────────────────────────────────────────────────────────────┘
```

### Error Configuration Schema

```yaml
# ~/.sylk/config.yaml - Error handling configuration

error:
  # Transient error detection
  transient:
    frequency_count: 3          # Errors within window to notify
    frequency_window: "10s"     # Time window for frequency detection
    notification_cooldown: "60s" # Cooldown after notification

  # Escalation settings
  escalation:
    workaround_budget: 1000     # Max tokens for Architect workarounds
    reset_on_success: true      # Reset budget after successful recovery

  # Retry policies by error tier
  retry:
    transient:
      max_attempts: 5
      initial_delay: "100ms"
      max_delay: "5s"
      multiplier: 2.0
    external_rate_limit:
      max_attempts: 10
      initial_delay: "1s"
      max_delay: "60s"
      multiplier: 2.0
      use_retry_after: true     # Respect Retry-After header
    external_degrading:
      max_attempts: 3
      initial_delay: "5s"
      max_delay: "30s"
      multiplier: 2.0

# Circuit breaker configuration
circuit:
  llm:
    consecutive_failures: 5
    failure_rate_threshold: 0.5
    rate_window_size: 20
    cooldown_duration: "30s"
    success_threshold: 3
  file:
    consecutive_failures: 2
    failure_rate_threshold: 0.3
    rate_window_size: 10
    cooldown_duration: "10s"
    success_threshold: 2
  network:
    consecutive_failures: 3
    failure_rate_threshold: 0.5
    rate_window_size: 20
    cooldown_duration: "30s"
    success_threshold: 3
  subprocess:
    consecutive_failures: 3
    failure_rate_threshold: 0.4
    rate_window_size: 15
    cooldown_duration: "20s"
    success_threshold: 2
```

---

## Resource Constraints

### Memory Management

```
┌─────────────────────────────────────────────────────────────────────────────────────┐
│                              MEMORY MANAGEMENT                                       │
├─────────────────────────────────────────────────────────────────────────────────────┤
│                                                                                     │
│  PER-COMPONENT BUDGETS (configurable):                                              │
│                                                                                     │
│  ┌─────────────────────────────────────────────────────────────────────────────┐    │
│  │                         MEMORY ALLOCATION                                   │    │
│  │                                                                             │    │
│  │  ┌─────────────┐ ┌─────────────┐ ┌─────────────┐ ┌─────────────┐           │    │
│  │  │ QueryCache  │ │  Staging    │ │   Agent     │ │ WAL/Chkpt   │           │    │
│  │  │   500MB     │ │    1GB      │ │  Contexts   │ │   200MB     │           │    │
│  │  │  (default)  │ │  (default)  │ │   500MB     │ │  (default)  │           │    │
│  │  └─────────────┘ └─────────────┘ └─────────────┘ └─────────────┘           │    │
│  │                                                                             │    │
│  │  Global Ceiling: 80% of available system memory (configurable)              │    │
│  │                                                                             │    │
│  └─────────────────────────────────────────────────────────────────────────────┘    │
│                                                                                     │
│  EVICTION CASCADE:                                                                  │
│                                                                                     │
│  70% component budget ──▶ Evict oldest items (LRU)                                  │
│           │                   │                                                     │
│           │                   └── QueryCache: token-weighted eviction               │
│           │                       (high-token-cost responses evicted LAST)          │
│           │                                                                         │
│  90% component budget ──▶ Aggressive eviction + warn user                           │
│           │                                                                         │
│  80% global ceiling ────▶ Pause new pipelines + evict across all components         │
│           │                                                                         │
│  95% global ────────────▶ PAUSE ALL WORK + emergency eviction + notify user         │
│                                                                                     │
│  NEVER CRASH. NEVER OOM. Always pause + evict + notify.                             │
│                                                                                     │
└─────────────────────────────────────────────────────────────────────────────────────┘
```

### Token-Weighted Cache Eviction

```go
// CacheEntry represents a cached item with token cost
type CacheEntry struct {
    Key         string
    Value       []byte
    Size        int64     // Memory size in bytes
    TokenCost   int       // Tokens spent to generate this
    CreatedAt   time.Time
    LastAccess  time.Time
    AccessCount int64
}

// EvictionScore calculates eviction priority (lower = evict first)
func (e *CacheEntry) EvictionScore() float64 {
    // Score = TokenCost / MemorySize * RecencyFactor
    // High token cost + low memory = high score (keep)
    // Low token cost + high memory = low score (evict)

    age := time.Since(e.LastAccess).Seconds()
    recencyFactor := 1.0 / (1.0 + age/3600) // Decay over hours

    if e.Size == 0 {
        return 0
    }

    return float64(e.TokenCost) / float64(e.Size) * recencyFactor
}

// TokenWeightedEviction evicts entries with lowest score
func (c *QueryCache) TokenWeightedEviction(targetFreeBytes int64) []string {
    c.mu.Lock()
    defer c.mu.Unlock()

    // Score all entries
    type scored struct {
        key   string
        entry *CacheEntry
        score float64
    }

    entries := make([]scored, 0, len(c.entries))
    for key, entry := range c.entries {
        entries = append(entries, scored{
            key:   key,
            entry: entry,
            score: entry.EvictionScore(),
        })
    }

    // Sort by score ascending (lowest first)
    sort.Slice(entries, func(i, j int) bool {
        return entries[i].score < entries[j].score
    })

    // Evict until we free enough memory
    var freed int64
    var evicted []string
    for _, e := range entries {
        if freed >= targetFreeBytes {
            break
        }
        delete(c.entries, e.key)
        freed += e.entry.Size
        evicted = append(evicted, e.key)
    }

    return evicted
}
```

### Memory Monitor

```go
// MemoryMonitor tracks and enforces memory limits
type MemoryMonitor struct {
    mu          sync.RWMutex
    config      MemoryConfig
    components  map[string]*ComponentMemory
    signalBus   *SignalBus

    // Metrics
    totalUsed   int64
    systemTotal int64
    systemFree  int64
}

type MemoryConfig struct {
    // Per-component budgets
    QueryCacheBudget    int64 `yaml:"query_cache"`      // default: 500MB
    StagingBudget       int64 `yaml:"staging"`          // default: 1GB
    AgentContextBudget  int64 `yaml:"agent_context"`    // default: 500MB
    WALBudget           int64 `yaml:"wal"`              // default: 200MB

    // Global limits
    GlobalCeilingPercent float64 `yaml:"global_ceiling_percent"` // default: 0.8

    // Eviction thresholds
    EvictionTriggerPercent  float64 `yaml:"eviction_trigger_percent"`  // default: 0.7
    AggressiveEvictPercent  float64 `yaml:"aggressive_evict_percent"`  // default: 0.9
    EmergencyEvictPercent   float64 `yaml:"emergency_evict_percent"`   // default: 0.95

    // Monitoring interval
    MonitorInterval time.Duration `yaml:"monitor_interval"` // default: 1s
}

func DefaultMemoryConfig() MemoryConfig {
    return MemoryConfig{
        QueryCacheBudget:       500 * 1024 * 1024,  // 500MB
        StagingBudget:          1024 * 1024 * 1024, // 1GB
        AgentContextBudget:     500 * 1024 * 1024,  // 500MB
        WALBudget:              200 * 1024 * 1024,  // 200MB
        GlobalCeilingPercent:   0.8,
        EvictionTriggerPercent: 0.7,
        AggressiveEvictPercent: 0.9,
        EmergencyEvictPercent:  0.95,
        MonitorInterval:        time.Second,
    }
}

// Run starts the memory monitor loop
func (m *MemoryMonitor) Run(ctx context.Context) {
    ticker := time.NewTicker(m.config.MonitorInterval)
    defer ticker.Stop()

    for {
        select {
        case <-ctx.Done():
            return
        case <-ticker.C:
            m.check()
        }
    }
}

func (m *MemoryMonitor) check() {
    m.updateSystemMemory()

    globalUsage := m.globalUsagePercent()

    // Emergency: pause everything
    if globalUsage >= m.config.EmergencyEvictPercent {
        m.signalBus.Publish(Signal{
            Type:    SignalPauseAll,
            Reason:  "Memory emergency",
            Payload: map[string]interface{}{"usage": globalUsage},
        })
        m.emergencyEvict()
        return
    }

    // High pressure: pause new pipelines
    if globalUsage >= m.config.GlobalCeilingPercent {
        m.signalBus.Publish(Signal{
            Type:   SignalPauseNewPipelines,
            Reason: "Memory pressure",
        })
    }

    // Check each component
    for name, comp := range m.components {
        usage := comp.UsagePercent()

        if usage >= m.config.AggressiveEvictPercent {
            comp.AggressiveEvict()
            m.signalBus.Publish(Signal{
                Type:    SignalMemoryWarning,
                Reason:  fmt.Sprintf("Component %s at %.0f%% memory", name, usage*100),
            })
        } else if usage >= m.config.EvictionTriggerPercent {
            comp.Evict()
        }
    }
}
```

### Concurrent Resource Pools

```
┌─────────────────────────────────────────────────────────────────────────────────────┐
│                           RESOURCE POOL ARCHITECTURE                                 │
├─────────────────────────────────────────────────────────────────────────────────────┤
│                                                                                     │
│  ALL POOLS RESERVE CAPACITY FOR USER-INTERACTIVE REQUESTS                           │
│                                                                                     │
│  ┌─────────────────────────────────────────────────────────────────────────────┐    │
│  │                          FILE HANDLE POOL                                   │    │
│  │                                                                             │    │
│  │  Total: 50% of OS ulimit (queried at startup)                               │    │
│  │                                                                             │    │
│  │  ┌─────────────────────────────┐  ┌─────────────────────────────┐           │    │
│  │  │     USER RESERVED (20%)    │  │     PIPELINE POOL (80%)     │           │    │
│  │  │                             │  │                             │           │    │
│  │  │  • Never denied             │  │  • Queued when exhausted    │           │    │
│  │  │  • Instant allocation       │  │  • Priority ordering        │           │    │
│  │  │  • Preempts pipeline if     │  │  • Timeout on wait          │           │    │
│  │  │    needed                   │  │                             │           │    │
│  │  └─────────────────────────────┘  └─────────────────────────────┘           │    │
│  └─────────────────────────────────────────────────────────────────────────────┘    │
│                                                                                     │
│  ┌─────────────────────────────────────────────────────────────────────────────┐    │
│  │                        NETWORK CONNECTION POOL                              │    │
│  │                                                                             │    │
│  │  Per-Provider: 10 connections (configurable)                                │    │
│  │  Global Max: 50 connections (configurable)                                  │    │
│  │                                                                             │    │
│  │  ┌─────────────────────────────┐  ┌─────────────────────────────┐           │    │
│  │  │     USER RESERVED (20%)    │  │     PIPELINE POOL (80%)     │           │    │
│  │  └─────────────────────────────┘  └─────────────────────────────┘           │    │
│  └─────────────────────────────────────────────────────────────────────────────┘    │
│                                                                                     │
│  ┌─────────────────────────────────────────────────────────────────────────────┐    │
│  │                         SUBPROCESS POOL                                     │    │
│  │                                                                             │    │
│  │  Max: 2 × N_CPU_CORES (configurable)                                        │    │
│  │                                                                             │    │
│  │  ┌─────────────────────────────┐  ┌─────────────────────────────┐           │    │
│  │  │     USER RESERVED (20%)    │  │     PIPELINE POOL (80%)     │           │    │
│  │  └─────────────────────────────┘  └─────────────────────────────┘           │    │
│  └─────────────────────────────────────────────────────────────────────────────┘    │
│                                                                                     │
└─────────────────────────────────────────────────────────────────────────────────────┘
```

### Resource Pool Implementation

```go
// ResourcePool manages a pool of limited resources
type ResourcePool struct {
    mu              sync.Mutex
    name            string
    total           int
    userReserved    int
    available       int
    userInUse       int
    pipelineInUse   int
    waitQueue       *PriorityQueue
    config          ResourcePoolConfig
}

type ResourcePoolConfig struct {
    // Total pool size
    Total int `yaml:"total"`

    // Percentage reserved for user-interactive
    UserReservedPercent float64 `yaml:"user_reserved_percent"` // default: 0.2

    // Maximum wait time for pipeline requests
    PipelineWaitTimeout time.Duration `yaml:"pipeline_wait_timeout"` // default: 30s

    // Allow preemption of pipeline resources for user
    AllowPreemption bool `yaml:"allow_preemption"` // default: true
}

func DefaultResourcePoolConfig(total int) ResourcePoolConfig {
    return ResourcePoolConfig{
        Total:               total,
        UserReservedPercent: 0.2,
        PipelineWaitTimeout: 30 * time.Second,
        AllowPreemption:     true,
    }
}

// AcquireUser acquires a resource for user-interactive request (never fails)
func (p *ResourcePool) AcquireUser(ctx context.Context) (*ResourceHandle, error) {
    p.mu.Lock()

    // User reserved capacity available?
    if p.userInUse < p.userReserved {
        p.userInUse++
        p.available--
        p.mu.Unlock()
        return &ResourceHandle{pool: p, isUser: true}, nil
    }

    // General pool available?
    if p.available > 0 {
        p.userInUse++
        p.available--
        p.mu.Unlock()
        return &ResourceHandle{pool: p, isUser: true}, nil
    }

    // Preempt from pipeline if allowed
    if p.config.AllowPreemption && p.pipelineInUse > 0 {
        // Signal preemption to lowest-priority pipeline
        p.preemptLowestPriority()
        p.userInUse++
        // available stays same (we preempted, didn't free)
        p.mu.Unlock()
        return &ResourceHandle{pool: p, isUser: true}, nil
    }

    p.mu.Unlock()
    return nil, fmt.Errorf("resource pool exhausted (should not happen for user)")
}

// AcquirePipeline acquires a resource for pipeline (may queue)
func (p *ResourcePool) AcquirePipeline(ctx context.Context, priority int) (*ResourceHandle, error) {
    p.mu.Lock()

    // Available in pipeline portion?
    pipelineAvailable := p.available - (p.userReserved - p.userInUse)
    if pipelineAvailable > 0 {
        p.pipelineInUse++
        p.available--
        p.mu.Unlock()
        return &ResourceHandle{pool: p, isUser: false}, nil
    }

    // Queue and wait
    waiter := &ResourceWaiter{
        priority: priority,
        ready:    make(chan struct{}),
    }
    p.waitQueue.Push(waiter)
    p.mu.Unlock()

    // Wait with timeout
    select {
    case <-ctx.Done():
        p.removeWaiter(waiter)
        return nil, ctx.Err()
    case <-time.After(p.config.PipelineWaitTimeout):
        p.removeWaiter(waiter)
        return nil, fmt.Errorf("timeout waiting for resource")
    case <-waiter.ready:
        return &ResourceHandle{pool: p, isUser: false}, nil
    }
}

// Release returns a resource to the pool
func (p *ResourcePool) Release(h *ResourceHandle) {
    p.mu.Lock()
    defer p.mu.Unlock()

    if h.isUser {
        p.userInUse--
    } else {
        p.pipelineInUse--
    }
    p.available++

    // Wake highest-priority waiter
    if p.waitQueue.Len() > 0 {
        waiter := p.waitQueue.Pop().(*ResourceWaiter)
        p.pipelineInUse++
        p.available--
        close(waiter.ready)
    }
}
```

### Disk Quota Management

```go
// DiskQuotaManager enforces disk usage limits
type DiskQuotaManager struct {
    mu        sync.RWMutex
    config    DiskQuotaConfig
    basePath  string
    usage     int64
    signalBus *SignalBus
}

type DiskQuotaConfig struct {
    // Minimum quota (floor)
    QuotaMin int64 `yaml:"quota_min"` // default: 1GB

    // Maximum quota (ceiling)
    QuotaMax int64 `yaml:"quota_max"` // default: 20GB

    // Percentage of free space
    QuotaPercent float64 `yaml:"quota_percent"` // default: 0.1

    // Warning threshold
    WarningThreshold float64 `yaml:"warning_threshold"` // default: 0.8

    // Cleanup threshold
    CleanupThreshold float64 `yaml:"cleanup_threshold"` // default: 0.9
}

func DefaultDiskQuotaConfig() DiskQuotaConfig {
    return DiskQuotaConfig{
        QuotaMin:         1 * 1024 * 1024 * 1024,  // 1GB
        QuotaMax:         20 * 1024 * 1024 * 1024, // 20GB
        QuotaPercent:     0.1,
        WarningThreshold: 0.8,
        CleanupThreshold: 0.9,
    }
}

// CalculateQuota determines the effective quota
func (d *DiskQuotaManager) CalculateQuota() int64 {
    // Get free space on disk
    var stat syscall.Statfs_t
    if err := syscall.Statfs(d.basePath, &stat); err != nil {
        // Fallback to minimum on error
        return d.config.QuotaMin
    }

    freeSpace := int64(stat.Bavail) * int64(stat.Bsize)
    percentQuota := int64(float64(freeSpace) * d.config.QuotaPercent)

    // Apply bounds: max(min, min(max, percent))
    quota := percentQuota
    if quota < d.config.QuotaMin {
        quota = d.config.QuotaMin
    }
    if quota > d.config.QuotaMax {
        quota = d.config.QuotaMax
    }

    return quota
}

// CanWrite checks if a write of the given size is allowed
func (d *DiskQuotaManager) CanWrite(size int64) bool {
    d.mu.RLock()
    defer d.mu.RUnlock()

    quota := d.CalculateQuota()
    return d.usage + size <= quota
}

// RecordWrite tracks a write
func (d *DiskQuotaManager) RecordWrite(size int64) error {
    d.mu.Lock()
    defer d.mu.Unlock()

    quota := d.CalculateQuota()
    newUsage := d.usage + size

    if newUsage > quota {
        return fmt.Errorf("disk quota exceeded: %d > %d", newUsage, quota)
    }

    d.usage = newUsage

    // Check thresholds
    usagePercent := float64(d.usage) / float64(quota)

    if usagePercent >= d.config.CleanupThreshold {
        go d.triggerCleanup()
    } else if usagePercent >= d.config.WarningThreshold {
        d.signalBus.Publish(Signal{
            Type:   SignalDiskWarning,
            Reason: fmt.Sprintf("Disk usage at %.0f%%", usagePercent*100),
        })
    }

    return nil
}
```

### Resource Broker

```
┌─────────────────────────────────────────────────────────────────────────────────────┐
│                              RESOURCE BROKER                                         │
├─────────────────────────────────────────────────────────────────────────────────────┤
│                                                                                     │
│  Central coordinator for all resource allocation                                    │
│                                                                                     │
│  ┌─────────────────────────────────────────────────────────────────────────────┐    │
│  │                                                                             │    │
│  │                         ┌─────────────────┐                                 │    │
│  │                         │ RESOURCE BROKER │                                 │    │
│  │                         └────────┬────────┘                                 │    │
│  │                                  │                                          │    │
│  │        ┌─────────────────────────┼─────────────────────────┐                │    │
│  │        │                         │                         │                │    │
│  │        ▼                         ▼                         ▼                │    │
│  │  ┌───────────┐            ┌───────────┐            ┌───────────┐            │    │
│  │  │  Memory   │            │   Pools   │            │   Disk    │            │    │
│  │  │  Monitor  │            │ (FH/Net/  │            │   Quota   │            │    │
│  │  │           │            │  Process) │            │           │            │    │
│  │  └───────────┘            └───────────┘            └───────────┘            │    │
│  │                                                                             │    │
│  └─────────────────────────────────────────────────────────────────────────────┘    │
│                                                                                     │
│  ALLOCATION FLOW:                                                                   │
│                                                                                     │
│  1. Pipeline requests resource bundle from Broker                                   │
│  2. Broker checks all required resources available                                  │
│  3. ALL-OR-NOTHING: Grant all or queue (no partial allocation)                      │
│  4. Pipeline holds resources until completion                                       │
│  5. On completion/failure: release all resources atomically                         │
│                                                                                     │
│  DEADLOCK PREVENTION:                                                               │
│  • All-or-nothing allocation eliminates hold-and-wait                               │
│  • User requests bypass broker (reserved capacity)                                  │
│  • Timeout on all acquisitions                                                      │
│  • Broker can detect potential deadlocks (cycle in wait graph)                      │
│                                                                                     │
└─────────────────────────────────────────────────────────────────────────────────────┘
```

### Resource Broker Implementation

```go
// ResourceBroker coordinates all resource allocation
type ResourceBroker struct {
    mu            sync.Mutex
    memoryMonitor *MemoryMonitor
    filePool      *ResourcePool
    networkPool   *ResourcePool
    processPool   *ResourcePool
    diskQuota     *DiskQuotaManager

    // Allocation tracking for deadlock detection
    allocations   map[string]*AllocationSet // pipelineID -> resources held
    waitGraph     map[string][]string       // who's waiting for whom

    signalBus     *SignalBus
    config        ResourceBrokerConfig
}

type ResourceBrokerConfig struct {
    // Timeout for bundle acquisition
    AcquisitionTimeout time.Duration `yaml:"acquisition_timeout"` // default: 30s

    // Enable deadlock detection
    DeadlockDetection bool `yaml:"deadlock_detection"` // default: true

    // Deadlock detection interval
    DetectionInterval time.Duration `yaml:"detection_interval"` // default: 5s
}

// ResourceBundle specifies required resources
type ResourceBundle struct {
    FileHandles     int
    NetworkConns    int
    Subprocesses    int
    MemoryEstimate  int64
    DiskEstimate    int64
}

// AllocationSet tracks resources held by a pipeline
type AllocationSet struct {
    PipelineID   string
    FileHandles  []*ResourceHandle
    NetworkConns []*ResourceHandle
    Subprocesses []*ResourceHandle
    AcquiredAt   time.Time
}

// AcquireBundle atomically acquires all resources in a bundle
func (b *ResourceBroker) AcquireBundle(
    ctx context.Context,
    pipelineID string,
    bundle ResourceBundle,
    priority int,
) (*AllocationSet, error) {

    // Check memory and disk first (non-blocking)
    if !b.memoryMonitor.CanAllocate(bundle.MemoryEstimate) {
        return nil, fmt.Errorf("insufficient memory")
    }
    if !b.diskQuota.CanWrite(bundle.DiskEstimate) {
        return nil, fmt.Errorf("insufficient disk quota")
    }

    // Create timeout context
    ctx, cancel := context.WithTimeout(ctx, b.config.AcquisitionTimeout)
    defer cancel()

    // Try to acquire all resources
    alloc := &AllocationSet{
        PipelineID: pipelineID,
        AcquiredAt: time.Now(),
    }

    // Acquire in fixed order to prevent deadlock
    var err error

    // 1. File handles
    alloc.FileHandles, err = b.acquireN(ctx, b.filePool, bundle.FileHandles, priority)
    if err != nil {
        return nil, fmt.Errorf("file handles: %w", err)
    }

    // 2. Network connections
    alloc.NetworkConns, err = b.acquireN(ctx, b.networkPool, bundle.NetworkConns, priority)
    if err != nil {
        b.releaseHandles(alloc.FileHandles)
        return nil, fmt.Errorf("network connections: %w", err)
    }

    // 3. Subprocesses
    alloc.Subprocesses, err = b.acquireN(ctx, b.processPool, bundle.Subprocesses, priority)
    if err != nil {
        b.releaseHandles(alloc.FileHandles)
        b.releaseHandles(alloc.NetworkConns)
        return nil, fmt.Errorf("subprocesses: %w", err)
    }

    // Track allocation
    b.mu.Lock()
    b.allocations[pipelineID] = alloc
    b.mu.Unlock()

    return alloc, nil
}

// ReleaseBundle releases all resources in an allocation
func (b *ResourceBroker) ReleaseBundle(alloc *AllocationSet) {
    b.mu.Lock()
    delete(b.allocations, alloc.PipelineID)
    b.mu.Unlock()

    b.releaseHandles(alloc.FileHandles)
    b.releaseHandles(alloc.NetworkConns)
    b.releaseHandles(alloc.Subprocesses)
}

// AcquireUser acquires resources for user-interactive (bypasses broker)
func (b *ResourceBroker) AcquireUser(ctx context.Context, bundle ResourceBundle) (*AllocationSet, error) {
    alloc := &AllocationSet{
        PipelineID: "user",
        AcquiredAt: time.Now(),
    }

    var err error

    // User acquisition uses reserved capacity, never queues
    for i := 0; i < bundle.FileHandles; i++ {
        h, err := b.filePool.AcquireUser(ctx)
        if err != nil {
            b.releaseHandles(alloc.FileHandles)
            return nil, err
        }
        alloc.FileHandles = append(alloc.FileHandles, h)
    }

    for i := 0; i < bundle.NetworkConns; i++ {
        h, err := b.networkPool.AcquireUser(ctx)
        if err != nil {
            b.releaseHandles(alloc.FileHandles)
            b.releaseHandles(alloc.NetworkConns)
            return nil, err
        }
        alloc.NetworkConns = append(alloc.NetworkConns, h)
    }

    for i := 0; i < bundle.Subprocesses; i++ {
        h, err := b.processPool.AcquireUser(ctx)
        if err != nil {
            b.releaseHandles(alloc.FileHandles)
            b.releaseHandles(alloc.NetworkConns)
            b.releaseHandles(alloc.Subprocesses)
            return nil, err
        }
        alloc.Subprocesses = append(alloc.Subprocesses, h)
    }

    return alloc, nil
}
```

### Graceful Degradation Under Pressure

```
┌─────────────────────────────────────────────────────────────────────────────────────┐
│                        GRACEFUL DEGRADATION STRATEGY                                 │
├─────────────────────────────────────────────────────────────────────────────────────┤
│                                                                                     │
│  PRESSURE DETECTED (memory, CPU, disk, or connections)                              │
│                                                                                     │
│  ┌────────────────────────────────────────────────────────────────────────────────┐ │
│  │                                                                                │ │
│  │  STEP 1: Identify lowest-value work                                            │ │
│  │                                                                                │ │
│  │    User-Interactive ──────────────────────────────── NEVER TOUCH               │ │
│  │          │                                                                     │ │
│  │          ▼                                                                     │ │
│  │    High Priority Pipelines ───────────────────────── Pause LAST                │ │
│  │          │                                                                     │ │
│  │          ▼                                                                     │ │
│  │    Medium Priority Pipelines ─────────────────────── Pause if needed           │ │
│  │          │                                                                     │ │
│  │          ▼                                                                     │ │
│  │    Low Priority Pipelines ────────────────────────── Pause FIRST               │ │
│  │          │                                                                     │ │
│  │          ▼                                                                     │ │
│  │    Same Priority? ────────────────────────────────── Newest paused first       │ │
│  │                                                                                │ │
│  └────────────────────────────────────────────────────────────────────────────────┘ │
│                                                                                     │
│  ┌────────────────────────────────────────────────────────────────────────────────┐ │
│  │                                                                                │ │
│  │  STEP 2: Pause selected pipelines                                              │ │
│  │                                                                                │ │
│  │    • Signal pause via SignalBus                                                │ │
│  │    • Pipeline checkpoints current state                                        │ │
│  │    • Pipeline releases resources to broker                                     │ │
│  │    • User notified: "Pipeline X paused due to resource pressure"               │ │
│  │                                                                                │ │
│  └────────────────────────────────────────────────────────────────────────────────┘ │
│                                                                                     │
│  ┌────────────────────────────────────────────────────────────────────────────────┐ │
│  │                                                                                │ │
│  │  STEP 3: Resume when pressure relieved                                         │ │
│  │                                                                                │ │
│  │    • Monitor detects resources available                                       │ │
│  │    • Resume highest-priority paused pipeline first                             │ │
│  │    • Pipeline restores from checkpoint                                         │ │
│  │    • User notified: "Pipeline X resumed"                                       │ │
│  │                                                                                │ │
│  └────────────────────────────────────────────────────────────────────────────────┘ │
│                                                                                     │
└─────────────────────────────────────────────────────────────────────────────────────┘
```

### Resource Configuration Schema

```yaml
# ~/.sylk/config.yaml - Resource constraints configuration

memory:
  # Per-component budgets
  query_cache: "500MB"
  staging: "1GB"
  agent_context: "500MB"
  wal: "200MB"

  # Global limits
  global_ceiling_percent: 0.8

  # Eviction thresholds
  eviction_trigger_percent: 0.7
  aggressive_evict_percent: 0.9
  emergency_evict_percent: 0.95

  # Monitoring
  monitor_interval: "1s"

resources:
  # File handles (percent of ulimit)
  file_handle_percent: 0.5

  # User-reserved capacity (applies to all pools)
  user_reserved_percent: 0.2

  # Network connections
  connections_per_provider: 10
  connections_global: 50

  # Subprocesses
  max_subprocesses_multiplier: 2  # × N_CPU_CORES

  # Acquisition settings
  pipeline_wait_timeout: "30s"
  allow_preemption: true

disk:
  # Quota bounds
  quota_min: "1GB"
  quota_max: "20GB"
  quota_percent: 0.1

  # Thresholds
  warning_threshold: 0.8
  cleanup_threshold: 0.9

broker:
  acquisition_timeout: "30s"
  deadlock_detection: true
  detection_interval: "5s"
```

### Integration with Existing Systems

```
┌─────────────────────────────────────────────────────────────────────────────────────┐
│                    ERROR & RESOURCE SYSTEM INTEGRATION                               │
├─────────────────────────────────────────────────────────────────────────────────────┤
│                                                                                     │
│  ┌─────────────────────────────────────────────────────────────────────────────┐    │
│  │                           SIGNAL BUS (0.11)                                 │    │
│  │                                                                             │    │
│  │  Error signals:                Resource signals:                            │    │
│  │  • SignalErrorOccurred         • SignalMemoryWarning                        │    │
│  │  • SignalCircuitTripped        • SignalMemoryEmergency                      │    │
│  │  • SignalCircuitRecovered      • SignalDiskWarning                          │    │
│  │  • SignalEscalationRequired    • SignalResourcePressure                     │    │
│  │                                • SignalPauseForResources                    │    │
│  │                                • SignalResumeAfterResources                 │    │
│  └─────────────────────────────────────────────────────────────────────────────┘    │
│                                         │                                           │
│                    ┌────────────────────┼────────────────────┐                      │
│                    │                    │                    │                      │
│                    ▼                    ▼                    ▼                      │
│  ┌─────────────────────┐  ┌─────────────────────┐  ┌─────────────────────┐         │
│  │   Guide (0.1-0.6)   │  │  Checkpointer (0.23)│  │ Pipeline Scheduler  │         │
│  │                     │  │                     │  │      (0.18)         │         │
│  │ • Routes errors to  │  │ • Checkpoint on     │  │                     │         │
│  │   user via terminal │  │   resource pause    │  │ • Pause/resume      │         │
│  │ • Displays warnings │  │ • Preserve state    │  │   based on signals  │         │
│  │ • Shows countdowns  │  │   for recovery      │  │ • Priority ordering │         │
│  │ • Presents remedies │  │                     │  │   for degradation   │         │
│  └─────────────────────┘  └─────────────────────┘  └─────────────────────┘         │
│                                                                                     │
│  ┌─────────────────────────────────────────────────────────────────────────────┐    │
│  │                        ARCHIVALIST INTEGRATION                              │    │
│  │                                                                             │    │
│  │  • Stores all error occurrences with context                                │    │
│  │  • Stores all recovery attempts and outcomes                                │    │
│  │  • Queryable: "What fixes worked for similar errors?"                       │    │
│  │  • Queryable: "What approaches failed for this type of task?"               │    │
│  │  • Provides retry briefings from historical data                            │    │
│  │  • Token-weighted eviction respects Archivalist query patterns              │    │
│  │                                                                             │    │
│  └─────────────────────────────────────────────────────────────────────────────┘    │
│                                                                                     │
│  ┌─────────────────────────────────────────────────────────────────────────────┐    │
│  │                       DUAL QUEUE GATE INTEGRATION (0.19)                    │    │
│  │                                                                             │    │
│  │  • Error tier affects queue priority                                        │    │
│  │  • Rate limit errors trigger backoff in gate                                │    │
│  │  • Circuit breaker state affects gate acceptance                            │    │
│  │  • Resource pressure can pause pipeline queue entirely                      │    │
│  │                                                                             │    │
│  └─────────────────────────────────────────────────────────────────────────────┘    │
│                                                                                     │
└─────────────────────────────────────────────────────────────────────────────────────┘
```

---

## Tier 1 Multi-Session Amendments

The original Tier 1 specifications were designed with single-session assumptions. This section documents the amendments required for multi-session coordination across all Tier 1 components.

### 1.1 LLM API Management - Multi-Session

#### Rate Limiting: Per-Session with Global Subscription Ceiling

Users may use subscription plans (Max/Pro) with **weekly** or **monthly** usage limits. The rate limiter must track per-session usage while enforcing a global ceiling across all concurrent sessions.

```
┌─────────────────────────────────────────────────────────────────────────────────────┐
│                    MULTI-SESSION RATE LIMITING                                        │
├─────────────────────────────────────────────────────────────────────────────────────┤
│                                                                                     │
│  ┌──────────────────────────────────────────────────────────────────────────────┐  │
│  │                        GLOBAL SUBSCRIPTION TRACKER                            │  │
│  │                                                                               │  │
│  │  SQLite WAL (shared across sessions)                                          │  │
│  │  ┌─────────────────────────────────────────────────────────────────────────┐ │  │
│  │  │ provider | period_start | period_end | requests_used | tokens_used     │ │  │
│  │  │ anthropic| 2025-01-13   | 2025-01-20 | 1,234         | 2,456,789       │ │  │
│  │  │ openai   | 2025-01-01   | 2025-01-31 | 567           | 1,234,567       │ │  │
│  │  └─────────────────────────────────────────────────────────────────────────┘ │  │
│  │                                                                               │  │
│  │  Soft Limits (from config):                                                   │  │
│  │  • Weekly: 10,000 requests OR 50M tokens (configurable)                       │  │
│  │  • Monthly: 40,000 requests OR 200M tokens (configurable)                     │  │
│  │  • Warn at: 80% (configurable)                                                │  │
│  │  • Period: week | month | day (configurable)                                  │  │
│  │                                                                               │  │
│  └──────────────────────────────────────────────────────────────────────────────┘  │
│                              │                                                      │
│                              ▼                                                      │
│  ┌───────────────┐  ┌───────────────┐  ┌───────────────┐                          │
│  │  Session A    │  │  Session B    │  │  Session C    │                          │
│  │               │  │               │  │               │                          │
│  │ Per-session:  │  │ Per-session:  │  │ Per-session:  │                          │
│  │ • Usage count │  │ • Usage count │  │ • Usage count │                          │
│  │ • Rate state  │  │ • Rate state  │  │ • Rate state  │                          │
│  │ • Backoff     │  │ • Backoff     │  │ • Backoff     │                          │
│  └───────┬───────┘  └───────┬───────┘  └───────┬───────┘                          │
│          │                  │                  │                                   │
│          └──────────────────┼──────────────────┘                                   │
│                             ▼                                                      │
│  ┌──────────────────────────────────────────────────────────────────────────────┐  │
│  │                         RATE LIMIT DECISION                                   │  │
│  │                                                                               │  │
│  │  1. Check global subscription ceiling:                                        │  │
│  │     IF global_usage >= soft_limit THEN warn ALL sessions                      │  │
│  │     IF global_usage >= hard_limit THEN block ALL sessions                     │  │
│  │                                                                               │  │
│  │  2. Check provider 429:                                                       │  │
│  │     IF 429 received THEN backoff THIS session (provider may per-key limit)    │  │
│  │     Broadcast warning to other sessions (they may hit same limit)             │  │
│  │                                                                               │  │
│  │  3. Per-session rate state:                                                   │  │
│  │     Each session tracks its own backoff (may differ by timing)                │  │
│  │     Global ceiling applies equally to all                                     │  │
│  │                                                                               │  │
│  └──────────────────────────────────────────────────────────────────────────────┘  │
│                                                                                     │
└─────────────────────────────────────────────────────────────────────────────────────┘
```

#### Implementation

```go
// GlobalSubscriptionTracker tracks usage across all sessions
type GlobalSubscriptionTracker struct {
    db        *sql.DB           // Shared SQLite WAL
    mu        sync.RWMutex
    config    SubscriptionConfig
    signalBus *CrossSessionSignalDispatcher
}

type SubscriptionConfig struct {
    // Per-provider limits (nil = no limit, rely on 429)
    ProviderLimits map[string]*SubscriptionLimit `yaml:"provider_limits"`
}

type SubscriptionLimit struct {
    Period        string  `yaml:"period"`          // "week", "month", "day"
    MaxRequests   *int64  `yaml:"max_requests"`    // nil = unlimited
    MaxTokens     *int64  `yaml:"max_tokens"`      // nil = unlimited
    WarnThreshold float64 `yaml:"warn_threshold"`  // 0.8 = 80%
}

// RecordUsage atomically records usage and checks limits
func (t *GlobalSubscriptionTracker) RecordUsage(
    ctx context.Context,
    provider string,
    sessionID string,
    tokens int,
) (*UsageStatus, error) {
    t.mu.Lock()
    defer t.mu.Unlock()

    // Atomic update in SQLite
    _, err := t.db.ExecContext(ctx, `
        INSERT INTO subscription_usage (provider, period_start, period_end, requests_used, tokens_used)
        VALUES (?, ?, ?, 1, ?)
        ON CONFLICT (provider, period_start) DO UPDATE SET
            requests_used = requests_used + 1,
            tokens_used = tokens_used + ?
    `, provider, t.periodStart(provider), t.periodEnd(provider), tokens, tokens)
    if err != nil {
        return nil, err
    }

    // Check limits
    status := t.checkLimits(ctx, provider)

    // Broadcast warnings if needed
    if status.WarningLevel > 0 && status.WarningLevel != status.PreviousWarningLevel {
        t.signalBus.Broadcast(CrossSessionSignal{
            Type:    SignalSubscriptionWarning,
            Payload: status,
        })
    }

    return status, nil
}

type UsageStatus struct {
    Provider             string  `json:"provider"`
    RequestsUsed         int64   `json:"requests_used"`
    TokensUsed           int64   `json:"tokens_used"`
    RequestsRemaining    *int64  `json:"requests_remaining"` // nil if unlimited
    TokensRemaining      *int64  `json:"tokens_remaining"`   // nil if unlimited
    WarningLevel         int     `json:"warning_level"`      // 0=ok, 1=warning, 2=critical
    PreviousWarningLevel int     `json:"-"`
    PeriodEndsAt         time.Time `json:"period_ends_at"`
}
```

#### Token Budgets: Per-Session with Global Ceiling

```
┌─────────────────────────────────────────────────────────────────────────────────────┐
│                    TOKEN BUDGET HIERARCHY (MULTI-SESSION)                            │
├─────────────────────────────────────────────────────────────────────────────────────┤
│                                                                                     │
│  Level 1: GLOBAL SUBSCRIPTION CEILING                                               │
│  ═══════════════════════════════════                                                │
│  • Enforced by GlobalSubscriptionTracker                                            │
│  • Applies across ALL sessions                                                      │
│  • Based on subscription plan (weekly/monthly)                                      │
│                                                                                     │
│          │                                                                          │
│          ▼                                                                          │
│                                                                                     │
│  Level 2: PER-SESSION BUDGET                                                        │
│  ═══════════════════════════                                                        │
│  • Optional per-session limit (config or runtime)                                   │
│  • Prevents runaway sessions                                                        │
│  • Fair share: Global remaining ÷ active sessions                                   │
│                                                                                     │
│  ┌─────────────────┐  ┌─────────────────┐  ┌─────────────────┐                     │
│  │  Session A      │  │  Session B      │  │  Session C      │                     │
│  │  Budget: 500K   │  │  Budget: 500K   │  │  Budget: 500K   │                     │
│  │  Used: 123K     │  │  Used: 456K     │  │  Used: 78K      │                     │
│  └────────┬────────┘  └────────┬────────┘  └────────┬────────┘                     │
│           │                    │                    │                              │
│           ▼                    ▼                    ▼                              │
│                                                                                     │
│  Level 3: PER-TASK BUDGET (unchanged)                                               │
│  ═══════════════════════════════════                                                │
│  • Architect assigns per-task budgets                                               │
│  • Within session budget                                                            │
│                                                                                     │
└─────────────────────────────────────────────────────────────────────────────────────┘
```

#### Priority Queue: Cross-Session User Preemption

User-interactive requests from **ANY** session preempt pipeline requests from **ALL** sessions.

```
┌─────────────────────────────────────────────────────────────────────────────────────┐
│                    CROSS-SESSION LLM QUEUE                                           │
├─────────────────────────────────────────────────────────────────────────────────────┤
│                                                                                     │
│  ┌─────────────────────────────────────────────────────────────────────────────┐    │
│  │  GLOBAL USER-INTERACTIVE QUEUE (HIGHEST PRIORITY)                            │    │
│  │                                                                              │    │
│  │  Session A: User query  ───┐                                                 │    │
│  │  Session B: User query  ───┼──► FIFO within user tier                        │    │
│  │  Session C: User query  ───┘    (all have absolute priority)                 │    │
│  │                                                                              │    │
│  │  Properties:                                                                 │    │
│  │  • Unbounded (never reject user)                                            │    │
│  │  • ANY user request preempts ALL pipeline requests                          │    │
│  │  • FIFO ordering among user requests (fair across sessions)                 │    │
│  │                                                                              │    │
│  └─────────────────────────────────────────────────────────────────────────────┘    │
│                      │                                                              │
│                      │ Always drain user queue first                               │
│                      ▼                                                              │
│  ┌─────────────────────────────────────────────────────────────────────────────┐    │
│  │  GLOBAL PIPELINE QUEUE (LOWER PRIORITY)                                       │    │
│  │                                                                              │    │
│  │  ┌─────────────────────────────────────────────────────────────────────┐     │    │
│  │  │ Session A: Pipeline 1 (priority 80)  ─┐                              │     │    │
│  │  │ Session B: Pipeline 2 (priority 60)  ─┼─► Ordered by:                │     │    │
│  │  │ Session A: Pipeline 3 (priority 60)  ─┤   1. Priority (higher first) │     │    │
│  │  │ Session C: Pipeline 4 (priority 40)  ─┘   2. Spawn time (earlier)    │     │    │
│  │  └─────────────────────────────────────────────────────────────────────┘     │    │
│  │                                                                              │    │
│  │  Properties:                                                                 │    │
│  │  • Bounded by global fair share slots                                       │    │
│  │  • Cross-session priority ordering                                          │    │
│  │  • Can be preempted by ANY session's user request                           │    │
│  │                                                                              │    │
│  └─────────────────────────────────────────────────────────────────────────────┘    │
│                                                                                     │
└─────────────────────────────────────────────────────────────────────────────────────┘
```

```go
// CrossSessionDualQueueGate extends DualQueueGate for multi-session
type CrossSessionDualQueueGate struct {
    // Shared across sessions (SQLite-backed)
    globalUserQueue     *GlobalPriorityQueue  // User requests from all sessions
    globalPipelineQueue *GlobalPriorityQueue  // Pipeline requests from all sessions

    // Per-session state
    sessionStates       map[string]*SessionQueueState

    // Coordination
    db                  *sql.DB
    signalDispatcher    *CrossSessionSignalDispatcher
    fairShare           *FairShareCalculator

    mu                  sync.RWMutex
}

type SessionQueueState struct {
    SessionID       string
    ActiveRequests  int
    QueuedRequests  int
    LastActivity    time.Time
}

// Submit handles request from any session
func (g *CrossSessionDualQueueGate) Submit(
    ctx context.Context,
    sessionID string,
    req *LLMRequest,
) (*CompletionResponse, error) {
    isUser := g.isUserInteractive(req)

    if isUser {
        // User requests go to global user queue
        g.globalUserQueue.Push(sessionID, req)
        // Preempt ANY pipeline request if needed
        g.maybePreemptAnyPipeline()
    } else {
        // Pipeline requests go to global pipeline queue
        g.globalPipelineQueue.Push(sessionID, req)
    }

    return g.waitForCompletion(ctx, sessionID, req)
}

// maybePreemptAnyPipeline preempts lowest-priority pipeline from ANY session
func (g *CrossSessionDualQueueGate) maybePreemptAnyPipeline() {
    g.mu.Lock()
    defer g.mu.Unlock()

    // Find lowest-priority active pipeline request across ALL sessions
    var lowestPriority *ActiveRequest
    var lowestSession string

    for sessionID, state := range g.sessionStates {
        for _, ar := range state.ActiveRequests {
            if !ar.IsUser {
                if lowestPriority == nil || ar.Request.Priority < lowestPriority.Request.Priority {
                    lowestPriority = ar
                    lowestSession = sessionID
                }
            }
        }
    }

    if lowestPriority != nil {
        // Cancel and notify the session
        lowestPriority.CancelFunc()
        g.signalDispatcher.Send(lowestSession, CrossSessionSignal{
            Type:    SignalRequestPreempted,
            Payload: lowestPriority.Request.ID,
        })
    }
}
```

### 1.2 Concurrency Model - Multi-Session

#### Global N_CPU_CORES Pool

The N_CPU_CORES limit for pipelines is **GLOBAL** across all sessions. Sessions share this pool via the Fair Share Calculator (TODO 0.40).

```
┌─────────────────────────────────────────────────────────────────────────────────────┐
│                    GLOBAL PIPELINE SLOT ALLOCATION                                    │
├─────────────────────────────────────────────────────────────────────────────────────┤
│                                                                                     │
│  Total Pipeline Slots: N_CPU_CORES (e.g., 8)                                        │
│                                                                                     │
│  ┌──────────────────────────────────────────────────────────────────────────────┐   │
│  │                        FAIR SHARE ALLOCATION                                  │   │
│  │                                                                               │   │
│  │  Tier 1: User Reserved (20% = ~2 slots)                                       │   │
│  │  ════════════════════════════════════════                                     │   │
│  │  • Always available for user-invoked pipelines                                │   │
│  │  • ANY session can use if they have user activity                             │   │
│  │  • Preempts background pipelines if needed                                    │   │
│  │                                                                               │   │
│  │  Tier 2: Active Sessions (proportional)                                       │   │
│  │  ══════════════════════════════════════                                       │   │
│  │  • Remaining slots divided by activity score                                  │   │
│  │  • Activity: running_pipelines + recent_invocations × decay                   │   │
│  │                                                                               │   │
│  │  Tier 3: Idle Sessions (baseline)                                             │   │
│  │  ════════════════════════════════════                                         │   │
│  │  • Minimum 1 slot if available                                                │   │
│  │  • Can be preempted by active sessions                                        │   │
│  │                                                                               │   │
│  └──────────────────────────────────────────────────────────────────────────────┘   │
│                                                                                     │
│  Example with 8 cores, 3 sessions:                                                  │
│                                                                                     │
│  ┌─────────────────────────────────────────────────────────────────────────────┐    │
│  │ Session A (active, user typing): 3 slots (reserved + activity share)        │    │
│  │ Session B (active, pipelines):   3 slots (activity share)                   │    │
│  │ Session C (idle):                1 slot (baseline)                          │    │
│  │ Reserved for spikes:             1 slot (user buffer)                       │    │
│  └─────────────────────────────────────────────────────────────────────────────┘    │
│                                                                                     │
└─────────────────────────────────────────────────────────────────────────────────────┘
```

#### Pipeline Scheduler Amendment

```go
// GlobalPipelineScheduler manages pipelines across all sessions
type GlobalPipelineScheduler struct {
    mu              sync.Mutex

    // Global state
    maxConcurrent   int                    // N_CPU_CORES (GLOBAL)
    totalActive     int                    // Currently running across all sessions

    // Per-session state
    sessionActive   map[string]int         // Running pipelines per session
    sessionQueued   map[string]*PriorityQueue // Queued pipelines per session

    // Coordination
    fairShare       *FairShareCalculator
    signalDispatcher *CrossSessionSignalDispatcher
    db              *sql.DB                // Shared state

    slotAvailable   chan struct{}
}

func NewGlobalPipelineScheduler() *GlobalPipelineScheduler {
    return &GlobalPipelineScheduler{
        maxConcurrent:   runtime.NumCPU(), // GLOBAL limit
        sessionActive:   make(map[string]int),
        sessionQueued:   make(map[string]*PriorityQueue),
        slotAvailable:   make(chan struct{}, 1),
    }
}

// Schedule adds a pipeline respecting global limits and fair share
func (s *GlobalPipelineScheduler) Schedule(sessionID string, p *Pipeline) {
    s.mu.Lock()
    defer s.mu.Unlock()

    // Get session's fair share allocation
    allocations, _ := s.fairShare.Calculate(s.maxConcurrent)
    sessionAlloc := allocations[sessionID]

    // Check if session can run more
    if s.sessionActive[sessionID] < sessionAlloc.PipelineSlots && s.totalActive < s.maxConcurrent {
        s.sessionActive[sessionID]++
        s.totalActive++
        go s.runPipeline(sessionID, p)
    } else {
        // Queue for later
        if s.sessionQueued[sessionID] == nil {
            s.sessionQueued[sessionID] = NewPriorityQueue()
        }
        s.sessionQueued[sessionID].Push(p)
    }
}

// OnPipelineComplete handles completion with cross-session rebalancing
func (s *GlobalPipelineScheduler) OnPipelineComplete(sessionID string, pipelineID string) {
    s.mu.Lock()
    defer s.mu.Unlock()

    s.sessionActive[sessionID]--
    s.totalActive--

    // Recalculate fair share and dispatch
    allocations, _ := s.fairShare.Calculate(s.maxConcurrent)

    // Try to dispatch from any session that has quota
    for sid, alloc := range allocations {
        if s.sessionActive[sid] < alloc.PipelineSlots && s.totalActive < s.maxConcurrent {
            if queue := s.sessionQueued[sid]; queue != nil && queue.Len() > 0 {
                p := queue.Pop()
                s.sessionActive[sid]++
                s.totalActive++
                go s.runPipeline(sid, p)
            }
        }
    }
}
```

### 1.3 State Persistence - Multi-Session

#### WAL: Per-Session Files with Shared Metadata

```
┌─────────────────────────────────────────────────────────────────────────────────────┐
│                    MULTI-SESSION WAL STRATEGY                                         │
├─────────────────────────────────────────────────────────────────────────────────────┤
│                                                                                     │
│  ~/.sylk/                                                                           │
│  ├── sessions/                                                                      │
│  │   ├── session_a/                                                                 │
│  │   │   ├── wal/                    ← Per-session WAL files                       │
│  │   │   │   ├── 000001.wal                                                        │
│  │   │   │   └── 000002.wal                                                        │
│  │   │   └── checkpoints/            ← Per-session checkpoints                     │
│  │   │       └── checkpoint_1234.json                                              │
│  │   │                                                                              │
│  │   ├── session_b/                                                                 │
│  │   │   ├── wal/                                                                  │
│  │   │   └── checkpoints/                                                          │
│  │   │                                                                              │
│  │   └── session_c/                                                                 │
│  │       ├── wal/                                                                  │
│  │       └── checkpoints/                                                          │
│  │                                                                                  │
│  └── shared/                                                                        │
│      ├── sessions.db                 ← SQLite: session registry (TODO 0.39)        │
│      ├── subscription.db             ← SQLite: global usage tracking               │
│      └── circuit_breakers.db         ← SQLite: global circuit states               │
│                                                                                     │
│  Isolation:                                                                         │
│  • WAL files: Per-session (crash recovery isolated)                                │
│  • Checkpoints: Per-session (state snapshots isolated)                             │
│  • Session metadata: Shared (cross-session coordination)                           │
│  • Resource tracking: Shared (fair allocation)                                     │
│                                                                                     │
└─────────────────────────────────────────────────────────────────────────────────────┘
```

```go
// MultiSessionWALManager handles per-session WAL with shared coordination
type MultiSessionWALManager struct {
    baseDir          string                      // ~/.sylk/sessions
    sharedDB         *sql.DB                     // Shared metadata

    sessionWALs      map[string]*WriteAheadLog   // Per-session WAL instances
    mu               sync.RWMutex
}

// GetOrCreateWAL gets/creates WAL for a session
func (m *MultiSessionWALManager) GetOrCreateWAL(sessionID string) (*WriteAheadLog, error) {
    m.mu.Lock()
    defer m.mu.Unlock()

    if wal, ok := m.sessionWALs[sessionID]; ok {
        return wal, nil
    }

    // Create session directory
    sessionDir := filepath.Join(m.baseDir, sessionID, "wal")
    if err := os.MkdirAll(sessionDir, 0700); err != nil {
        return nil, err
    }

    // Register in shared DB
    _, err := m.sharedDB.Exec(`
        INSERT INTO sessions (id, wal_dir, created_at, last_active)
        VALUES (?, ?, ?, ?)
        ON CONFLICT (id) DO UPDATE SET last_active = ?
    `, sessionID, sessionDir, time.Now(), time.Now(), time.Now())
    if err != nil {
        return nil, err
    }

    wal, err := NewWriteAheadLog(sessionDir)
    if err != nil {
        return nil, err
    }

    m.sessionWALs[sessionID] = wal
    return wal, nil
}

// RecoverAllSessions recovers all sessions on startup
func (m *MultiSessionWALManager) RecoverAllSessions(ctx context.Context) error {
    rows, err := m.sharedDB.QueryContext(ctx, `
        SELECT id, wal_dir FROM sessions WHERE recovered = false
    `)
    if err != nil {
        return err
    }
    defer rows.Close()

    for rows.Next() {
        var sessionID, walDir string
        if err := rows.Scan(&sessionID, &walDir); err != nil {
            continue
        }

        wal, err := m.GetOrCreateWAL(sessionID)
        if err != nil {
            continue
        }

        // Recover this session
        if err := wal.Recover(ctx); err != nil {
            // Log but continue with other sessions
            continue
        }

        // Mark as recovered
        m.sharedDB.Exec(`UPDATE sessions SET recovered = true WHERE id = ?`, sessionID)
    }

    return nil
}
```

### 1.4 Circuit Breakers - Global Per-Resource

Circuit breakers are **GLOBAL** per resource type. If Anthropic is down, ALL sessions know immediately.

```
┌─────────────────────────────────────────────────────────────────────────────────────┐
│                    GLOBAL CIRCUIT BREAKERS                                            │
├─────────────────────────────────────────────────────────────────────────────────────┤
│                                                                                     │
│  ┌──────────────────────────────────────────────────────────────────────────────┐   │
│  │                    SHARED CIRCUIT BREAKER STATE                               │   │
│  │                                                                               │   │
│  │  SQLite (circuit_breakers.db):                                                │   │
│  │  ┌─────────────────────────────────────────────────────────────────────────┐ │   │
│  │  │ resource_id | state      | failures | last_failure | last_state_change │ │   │
│  │  │ llm:anthro  | OPEN       | 5        | 2025-01-16   | 2025-01-16 10:00  │ │   │
│  │  │ llm:openai  | CLOSED     | 0        | NULL         | 2025-01-15 14:00  │ │   │
│  │  │ network     | HALF-OPEN  | 2        | 2025-01-16   | 2025-01-16 10:05  │ │   │
│  │  └─────────────────────────────────────────────────────────────────────────┘ │   │
│  │                                                                               │   │
│  └──────────────────────────────────────────────────────────────────────────────┘   │
│                              │                                                      │
│                              │ fsnotify signals on state change                    │
│                              ▼                                                      │
│  ┌───────────────┐  ┌───────────────┐  ┌───────────────┐                          │
│  │  Session A    │  │  Session B    │  │  Session C    │                          │
│  │               │  │               │  │               │                          │
│  │ Local cache:  │  │ Local cache:  │  │ Local cache:  │                          │
│  │ llm:anthro=   │  │ llm:anthro=   │  │ llm:anthro=   │                          │
│  │   OPEN        │  │   OPEN        │  │   OPEN        │                          │
│  │               │  │               │  │               │                          │
│  │ On state      │  │ On state      │  │ On state      │                          │
│  │ change signal │  │ change signal │  │ change signal │                          │
│  │ → refresh     │  │ → refresh     │  │ → refresh     │                          │
│  │   cache       │  │   cache       │  │   cache       │                          │
│  └───────────────┘  └───────────────┘  └───────────────┘                          │
│                                                                                     │
│  Benefits:                                                                          │
│  • Session A hits 429 → ALL sessions immediately know                              │
│  • Prevents stampede when provider recovers                                        │
│  • Shared cooldown prevents duplicate probes                                       │
│                                                                                     │
└─────────────────────────────────────────────────────────────────────────────────────┘
```

```go
// GlobalCircuitBreakerRegistry manages circuit breakers across sessions
type GlobalCircuitBreakerRegistry struct {
    db              *sql.DB                     // Shared state
    signalDispatcher *CrossSessionSignalDispatcher

    // Local cache (refreshed on signals)
    localCache      map[string]*CircuitBreakerState
    cacheMu         sync.RWMutex
}

type CircuitBreakerState struct {
    ResourceID      string        `json:"resource_id"`
    State           CircuitState  `json:"state"`
    Failures        int           `json:"failures"`
    LastFailure     *time.Time    `json:"last_failure"`
    LastStateChange time.Time     `json:"last_state_change"`
    CooldownEnds    *time.Time    `json:"cooldown_ends"`
}

// RecordResult records result and broadcasts state changes
func (r *GlobalCircuitBreakerRegistry) RecordResult(
    resourceID string,
    success bool,
) (*CircuitBreakerState, error) {
    r.cacheMu.Lock()
    defer r.cacheMu.Unlock()

    // Atomic update in SQLite
    tx, err := r.db.Begin()
    if err != nil {
        return nil, err
    }
    defer tx.Rollback()

    // Get current state
    var state CircuitBreakerState
    err = tx.QueryRow(`
        SELECT resource_id, state, failures, last_failure, last_state_change
        FROM circuit_breakers WHERE resource_id = ?
    `, resourceID).Scan(&state.ResourceID, &state.State, &state.Failures,
                        &state.LastFailure, &state.LastStateChange)

    if err == sql.ErrNoRows {
        state = CircuitBreakerState{
            ResourceID: resourceID,
            State:      CircuitClosed,
        }
    }

    // Apply result
    oldState := state.State
    if success {
        state.Failures = 0
        if state.State == CircuitHalfOpen {
            state.State = CircuitClosed
        }
    } else {
        state.Failures++
        now := time.Now()
        state.LastFailure = &now

        // Check if should trip
        config := getCircuitConfig(resourceID)
        if state.State == CircuitClosed && state.Failures >= config.ConsecutiveFailures {
            state.State = CircuitOpen
            cooldownEnd := now.Add(config.CooldownDuration)
            state.CooldownEnds = &cooldownEnd
        } else if state.State == CircuitHalfOpen {
            state.State = CircuitOpen
        }
    }

    if state.State != oldState {
        state.LastStateChange = time.Now()
    }

    // Persist
    _, err = tx.Exec(`
        INSERT INTO circuit_breakers (resource_id, state, failures, last_failure, last_state_change, cooldown_ends)
        VALUES (?, ?, ?, ?, ?, ?)
        ON CONFLICT (resource_id) DO UPDATE SET
            state = ?, failures = ?, last_failure = ?, last_state_change = ?, cooldown_ends = ?
    `, resourceID, state.State, state.Failures, state.LastFailure, state.LastStateChange, state.CooldownEnds,
       state.State, state.Failures, state.LastFailure, state.LastStateChange, state.CooldownEnds)
    if err != nil {
        return nil, err
    }

    if err := tx.Commit(); err != nil {
        return nil, err
    }

    // Update local cache
    r.localCache[resourceID] = &state

    // Broadcast state change to all sessions
    if state.State != oldState {
        r.signalDispatcher.Broadcast(CrossSessionSignal{
            Type:    SignalCircuitStateChange,
            Payload: state,
        })
    }

    return &state, nil
}

// Allow checks if request should be allowed (uses local cache)
func (r *GlobalCircuitBreakerRegistry) Allow(resourceID string) bool {
    r.cacheMu.RLock()
    state, ok := r.localCache[resourceID]
    r.cacheMu.RUnlock()

    if !ok {
        // No state = allowed (first request)
        return true
    }

    switch state.State {
    case CircuitClosed:
        return true
    case CircuitOpen:
        if state.CooldownEnds != nil && time.Now().After(*state.CooldownEnds) {
            // Cooldown expired, allow probe
            return true
        }
        return false
    case CircuitHalfOpen:
        return true
    }

    return false
}

// OnSignalReceived refreshes local cache when notified
func (r *GlobalCircuitBreakerRegistry) OnSignalReceived(signal CrossSessionSignal) {
    if signal.Type != SignalCircuitStateChange {
        return
    }

    state := signal.Payload.(CircuitBreakerState)

    r.cacheMu.Lock()
    r.localCache[state.ResourceID] = &state
    r.cacheMu.Unlock()
}
```

### 1.5 Resource Constraints - Multi-Session

Already addressed in Tier 2 with:
- **TODO 0.39**: Session Registry (SQLite)
- **TODO 0.40**: Fair Share Calculator
- **TODO 0.41**: Signal Dispatcher
- **TODO 0.42**: Cross-Session Resource Pool

The original Tier 1 resource pool designs (TODO 0.34) are extended by the Cross-Session Resource Pool which wraps them with session awareness and fair allocation.

### Multi-Session Configuration

```yaml
# ~/.sylk/config.yaml

multi_session:
  # Enable multi-session coordination
  enabled: true

  # Shared database locations
  sessions_db: "~/.sylk/shared/sessions.db"
  subscription_db: "~/.sylk/shared/subscription.db"
  circuit_breakers_db: "~/.sylk/shared/circuit_breakers.db"

  # Signal coordination
  signal_dir: "~/.sylk/shared/signals"
  signal_debounce: "100ms"

  # Fair share defaults
  fair_share:
    user_reserved_percent: 0.2     # 20% reserved for user-interactive
    idle_baseline_slots: 1         # Minimum slots for idle sessions
    activity_decay_rate: 0.1       # Decay per minute for activity score
    rebalance_interval: "5s"       # How often to recalculate allocations

subscription:
  # Provider-specific limits (optional - if not set, rely on 429)
  anthropic:
    period: "week"                 # week, month, day
    max_requests: 10000            # nil = unlimited
    max_tokens: 50000000           # 50M
    warn_threshold: 0.8

  openai:
    period: "month"
    max_requests: null             # Unlimited requests
    max_tokens: 100000000          # 100M
    warn_threshold: 0.8

llm:
  # Cross-session queue settings
  cross_session_queue:
    user_preemption: true          # User requests preempt pipelines
    global_rate_limit: true        # Share rate limit state across sessions

  # Per-session settings
  per_session:
    max_concurrent_requests: 10    # Per-session limit (within fair share)

pipelines:
  # Global settings
  global_max_concurrent: 0         # 0 = N_CPU_CORES (auto-detect)

  # Per-session settings
  per_session:
    max_concurrent: 0              # 0 = fair share allocation
```

### Integration with Existing TODO Tasks

The multi-session amendments integrate with existing TODO items:

```
Tier 1 Multi-Session Dependencies:
═══════════════════════════════════

NEW TASKS:
• TODO 0.55: GlobalSubscriptionTracker - Cross-session usage tracking
• TODO 0.56: CrossSessionDualQueueGate - Multi-session LLM queue
• TODO 0.57: GlobalPipelineScheduler - Cross-session pipeline slots
• TODO 0.58: MultiSessionWALManager - Per-session WAL with shared metadata
• TODO 0.59: GlobalCircuitBreakerRegistry - Shared circuit breaker state

EXISTING TASK UPDATES:
• TODO 0.39 (Session Registry) - Already covers session tracking
• TODO 0.40 (Fair Share Calculator) - Already covers allocation
• TODO 0.41 (Signal Dispatcher) - Already covers cross-session signals
• TODO 0.42 (Cross-Session Resource Pool) - Already covers resource sharing

INTEGRATION:
• TODO 0.55-0.59 DEPEND ON 0.39-0.41 (must implement session coordination first)
• TODO 0.56 (Queue) depends on 0.55 (Subscription) for budget checks
• TODO 0.57 (Scheduler) depends on 0.40 (Fair Share) for allocation
• TODO 0.59 (Circuit) depends on 0.41 (Signal) for state broadcasts
```

---

## Tool Execution Layer

The Tool Execution Layer provides infrastructure for agents to execute external tools (compilers, linters, test runners, git, etc.) with proper process management, output handling, and multi-session coordination.

### Multi-Session Resource Coordination

```
┌─────────────────────────────────────────────────────────────────────────────────────┐
│                    MULTI-SESSION RESOURCE COORDINATION                               │
├─────────────────────────────────────────────────────────────────────────────────────┤
│                                                                                     │
│  ┌─────────────┐     ┌─────────────┐     ┌─────────────┐                           │
│  │  Session A  │     │  Session B  │     │  Session C  │                           │
│  │  (active)   │     │  (active)   │     │   (idle)    │                           │
│  └──────┬──────┘     └──────┬──────┘     └──────┬──────┘                           │
│         │                   │                   │                                   │
│         └───────────────────┼───────────────────┘                                   │
│                             │                                                       │
│                             ▼                                                       │
│         ┌───────────────────────────────────────────────────┐                       │
│         │              SESSION REGISTRY                      │                       │
│         │              (SQLite WAL)                          │                       │
│         │                                                    │                       │
│         │  ┌──────────────────────────────────────────────┐  │                       │
│         │  │ session_id │ pid │ heartbeat │ activity_score│  │                       │
│         │  ├──────────────────────────────────────────────┤  │                       │
│         │  │ sess-abc   │ 123 │ 10:30:05  │ 3.5          │  │                       │
│         │  │ sess-def   │ 456 │ 10:30:04  │ 1.2          │  │                       │
│         │  │ sess-ghi   │ 789 │ 10:29:50  │ 0.0 (idle)   │  │                       │
│         │  └──────────────────────────────────────────────┘  │                       │
│         └───────────────────────────────────────────────────┘                       │
│                             │                                                       │
│                             ▼                                                       │
│         ┌───────────────────────────────────────────────────┐                       │
│         │           FAIR SHARE CALCULATOR                    │                       │
│         │                                                    │                       │
│         │  Total Pool: 16 subprocess slots                   │                       │
│         │  User Reserved (global): 3 slots (20%)             │                       │
│         │  Remaining: 13 slots                               │                       │
│         │                                                    │                       │
│         │  Session A (score 3.5): 3.5/4.7 × 13 = 9 slots    │                       │
│         │  Session B (score 1.2): 1.2/4.7 × 13 = 3 slots    │                       │
│         │  Session C (idle):      baseline      = 1 slot     │                       │
│         │                                                    │                       │
│         └───────────────────────────────────────────────────┘                       │
│                             │                                                       │
│                             ▼                                                       │
│         ┌───────────────────────────────────────────────────┐                       │
│         │              SIGNAL DISPATCHER                     │                       │
│         │              (fsnotify)                            │                       │
│         │                                                    │                       │
│         │  ~/.sylk/signals/                                  │                       │
│         │    ├── sess-abc/                                   │                       │
│         │    │   └── preempt-1234.signal                     │                       │
│         │    ├── sess-def/                                   │                       │
│         │    └── sess-ghi/                                   │                       │
│         │                                                    │                       │
│         │  Signal Types:                                     │                       │
│         │    • preempt: User needs resources NOW             │                       │
│         │    • pressure: Memory/disk pressure alert          │                       │
│         │    • rebalance: Fair share changed                 │                       │
│         │    • shutdown: Session ending                      │                       │
│         │                                                    │                       │
│         └───────────────────────────────────────────────────┘                       │
│                                                                                     │
└─────────────────────────────────────────────────────────────────────────────────────┘
```

### Session Registry

```go
// SessionRegistry manages cross-session coordination via SQLite
type SessionRegistry struct {
    db     *sql.DB
    config SessionRegistryConfig
}

type SessionRegistryConfig struct {
    // Database path
    DBPath string `yaml:"db_path"` // default: ~/.sylk/state.db

    // Heartbeat interval
    HeartbeatInterval time.Duration `yaml:"heartbeat_interval"` // default: 1s

    // Session considered stale after
    StaleThreshold time.Duration `yaml:"stale_threshold"` // default: 10s

    // Activity score decay rate
    ActivityDecayRate float64 `yaml:"activity_decay_rate"` // default: 0.9 per second
}

type SessionRecord struct {
    SessionID     string    `db:"session_id"`
    PID           int       `db:"pid"`
    StartTime     time.Time `db:"start_time"`
    LastHeartbeat time.Time `db:"last_heartbeat"`
    ActivityScore float64   `db:"activity_score"`
    RunningPipes  int       `db:"running_pipelines"`
    AllocatedSlots int      `db:"allocated_slots"`
}

// Register adds this session to the registry
func (r *SessionRegistry) Register(sessionID string) error {
    _, err := r.db.Exec(`
        INSERT INTO sessions (session_id, pid, start_time, last_heartbeat, activity_score)
        VALUES (?, ?, ?, ?, 0.0)
    `, sessionID, os.Getpid(), time.Now(), time.Now())
    return err
}

// Heartbeat updates session liveness and activity score
func (r *SessionRegistry) Heartbeat(sessionID string, runningPipelines int, recentInvocations int) error {
    // Activity score = running_pipelines + recent_invocations * decay
    activityScore := float64(runningPipelines) + float64(recentInvocations)*r.config.ActivityDecayRate

    _, err := r.db.Exec(`
        UPDATE sessions
        SET last_heartbeat = ?, activity_score = ?, running_pipelines = ?
        WHERE session_id = ?
    `, time.Now(), activityScore, runningPipelines, sessionID)
    return err
}

// GetActiveSessions returns all non-stale sessions
func (r *SessionRegistry) GetActiveSessions() ([]SessionRecord, error) {
    cutoff := time.Now().Add(-r.config.StaleThreshold)
    rows, err := r.db.Query(`
        SELECT session_id, pid, start_time, last_heartbeat, activity_score,
               running_pipelines, allocated_slots
        FROM sessions
        WHERE last_heartbeat > ?
        ORDER BY activity_score DESC
    `, cutoff)
    // ... scan rows
}

// CleanupStaleSessions removes dead sessions
func (r *SessionRegistry) CleanupStaleSessions() error {
    cutoff := time.Now().Add(-r.config.StaleThreshold)
    _, err := r.db.Exec(`DELETE FROM sessions WHERE last_heartbeat < ?`, cutoff)
    return err
}
```

### Fair Share Allocation

```
┌─────────────────────────────────────────────────────────────────────────────────────┐
│                         FAIR SHARE ALLOCATION MODEL                                  │
├─────────────────────────────────────────────────────────────────────────────────────┤
│                                                                                     │
│  TIER 1: USER-INTERACTIVE (Absolute Priority)                                       │
│  ┌────────────────────────────────────────────────────────────────────────────────┐ │
│  │ • User typing in ANY session gets immediate resources                          │ │
│  │ • Reserved capacity: 20% of all pools (configurable)                           │ │
│  │ • Can preempt pipelines from ANY session (not just own)                        │ │
│  │ • Cross-session preemption via signal dispatcher                               │ │
│  └────────────────────────────────────────────────────────────────────────────────┘ │
│                                                                                     │
│  TIER 2: ACTIVE SESSIONS (Proportional Share)                                       │
│  ┌────────────────────────────────────────────────────────────────────────────────┐ │
│  │ • Sessions with activity_score > 0                                             │ │
│  │ • Share = (session_score / total_score) × remaining_capacity                   │ │
│  │ • Activity score = running_pipelines + recent_invocations × decay              │ │
│  │ • Rebalanced on: session join, session leave, activity change                  │ │
│  └────────────────────────────────────────────────────────────────────────────────┘ │
│                                                                                     │
│  TIER 3: IDLE SESSIONS (Baseline)                                                   │
│  ┌────────────────────────────────────────────────────────────────────────────────┐ │
│  │ • Sessions with activity_score = 0                                             │ │
│  │ • Baseline allocation: 1 slot per pool (for quick startup)                     │ │
│  │ • Excess released to active sessions                                           │ │
│  │ • Promoted to Tier 2 when activity resumes                                     │ │
│  └────────────────────────────────────────────────────────────────────────────────┘ │
│                                                                                     │
└─────────────────────────────────────────────────────────────────────────────────────┘
```

### Fair Share Calculator

```go
// FairShareCalculator computes per-session resource allocations
type FairShareCalculator struct {
    registry *SessionRegistry
    config   FairShareConfig
}

type FairShareConfig struct {
    // Global user-interactive reservation (percent)
    UserReservedPercent float64 `yaml:"user_reserved_percent"` // default: 0.2

    // Minimum baseline for idle sessions
    IdleBaseline int `yaml:"idle_baseline"` // default: 1

    // Rebalance interval
    RebalanceInterval time.Duration `yaml:"rebalance_interval"` // default: 500ms
}

type SessionAllocation struct {
    SessionID       string
    SubprocessSlots int
    FileHandleSlots int
    NetworkSlots    int
    MemoryBudget    int64
}

// Calculate computes fair share for all active sessions
func (f *FairShareCalculator) Calculate(totalSlots int) (map[string]SessionAllocation, error) {
    sessions, err := f.registry.GetActiveSessions()
    if err != nil {
        return nil, err
    }

    // Reserve user-interactive capacity
    userReserved := int(float64(totalSlots) * f.config.UserReservedPercent)
    remaining := totalSlots - userReserved

    // Calculate total activity score
    var totalScore float64
    var activeSessions []SessionRecord
    var idleSessions []SessionRecord

    for _, s := range sessions {
        if s.ActivityScore > 0 {
            totalScore += s.ActivityScore
            activeSessions = append(activeSessions, s)
        } else {
            idleSessions = append(idleSessions, s)
        }
    }

    // Reserve baseline for idle sessions
    idleReserved := len(idleSessions) * f.config.IdleBaseline
    forActive := remaining - idleReserved

    allocations := make(map[string]SessionAllocation)

    // Allocate to active sessions proportionally
    for _, s := range activeSessions {
        share := int(float64(forActive) * (s.ActivityScore / totalScore))
        if share < 1 {
            share = 1 // Minimum 1 slot
        }
        allocations[s.SessionID] = SessionAllocation{
            SessionID:       s.SessionID,
            SubprocessSlots: share,
            // ... other resources scaled similarly
        }
    }

    // Allocate baseline to idle sessions
    for _, s := range idleSessions {
        allocations[s.SessionID] = SessionAllocation{
            SessionID:       s.SessionID,
            SubprocessSlots: f.config.IdleBaseline,
        }
    }

    return allocations, nil
}
```

### Signal Dispatcher

```go
// SignalDispatcher handles cross-session signaling via filesystem
type SignalDispatcher struct {
    signalDir string
    sessionID string
    watcher   *fsnotify.Watcher
    handlers  map[SignalType]SignalHandler
}

type SignalType string

const (
    SignalPreempt   SignalType = "preempt"   // User needs resources NOW
    SignalPressure  SignalType = "pressure"  // Memory/disk pressure
    SignalRebalance SignalType = "rebalance" // Fair share changed
    SignalShutdown  SignalType = "shutdown"  // Session ending
)

type CrossSessionSignal struct {
    Type       SignalType `json:"type"`
    FromSession string    `json:"from_session"`
    ToSession   string    `json:"to_session"` // empty = broadcast
    Timestamp  time.Time  `json:"timestamp"`
    Payload    string     `json:"payload"`
}

// SendSignal sends a signal to another session (or broadcast)
func (d *SignalDispatcher) SendSignal(sig CrossSessionSignal) error {
    sig.FromSession = d.sessionID
    sig.Timestamp = time.Now()

    data, _ := json.Marshal(sig)
    filename := fmt.Sprintf("%s-%d.signal", sig.Type, time.Now().UnixNano())

    if sig.ToSession != "" {
        // Targeted signal
        path := filepath.Join(d.signalDir, sig.ToSession, filename)
        return os.WriteFile(path, data, 0644)
    }

    // Broadcast to all sessions
    sessions, _ := filepath.Glob(filepath.Join(d.signalDir, "*"))
    for _, sessDir := range sessions {
        if filepath.Base(sessDir) == d.sessionID {
            continue // Don't signal self
        }
        path := filepath.Join(sessDir, filename)
        os.WriteFile(path, data, 0644)
    }
    return nil
}

// Watch starts watching for incoming signals
func (d *SignalDispatcher) Watch(ctx context.Context) error {
    myDir := filepath.Join(d.signalDir, d.sessionID)
    os.MkdirAll(myDir, 0755)

    d.watcher.Add(myDir)

    for {
        select {
        case <-ctx.Done():
            return nil
        case event := <-d.watcher.Events:
            if event.Op&fsnotify.Create != 0 {
                d.handleSignalFile(event.Name)
            }
        }
    }
}

func (d *SignalDispatcher) handleSignalFile(path string) {
    data, err := os.ReadFile(path)
    if err != nil {
        return
    }
    os.Remove(path) // Consume signal

    var sig CrossSessionSignal
    if json.Unmarshal(data, &sig) != nil {
        return
    }

    if handler, ok := d.handlers[sig.Type]; ok {
        handler(sig)
    }
}
```

### Subprocess Management

```
┌─────────────────────────────────────────────────────────────────────────────────────┐
│                          SUBPROCESS MANAGEMENT                                       │
├─────────────────────────────────────────────────────────────────────────────────────┤
│                                                                                     │
│  SPAWN MODEL: Hybrid (Direct + Shell)                                               │
│                                                                                     │
│  ┌────────────────────────────────────────────────────────────────────────────────┐ │
│  │                                                                                │ │
│  │  Agent invokes tool ──▶ Tool Executor ──▶ Spawn Decision                       │ │
│  │                              │                  │                              │ │
│  │                              │           ┌──────┴──────┐                       │ │
│  │                              │           │             │                       │ │
│  │                              │      Simple cmd    Complex cmd                  │ │
│  │                              │      (no pipes,   (pipes, env                   │ │
│  │                              │       no glob)    expansion, glob)              │ │
│  │                              │           │             │                       │ │
│  │                              │           ▼             ▼                       │ │
│  │                              │     exec.Command   sh -c "cmd"                  │ │
│  │                              │      (direct)       (shell)                     │ │
│  │                              │           │             │                       │ │
│  │                              │           └──────┬──────┘                       │ │
│  │                              │                  │                              │ │
│  │                              │                  ▼                              │ │
│  │                              │         Process Group                           │ │
│  │                              │         (setpgid on Unix,                       │ │
│  │                              │          Job Object on Windows)                 │ │
│  │                              │                  │                              │ │
│  │                              │                  ▼                              │ │
│  │                              └────────▶ Output Streamer                        │ │
│  │                                         (tee to user + buffer)                 │ │
│  │                                                                                │ │
│  └────────────────────────────────────────────────────────────────────────────────┘ │
│                                                                                     │
│  CONCURRENCY: Agent goroutines unbounded, actual processes via global pool          │
│                                                                                     │
└─────────────────────────────────────────────────────────────────────────────────────┘
```

### Tool Executor

```go
// ToolExecutor manages subprocess execution for agents
type ToolExecutor struct {
    config        ToolExecutorConfig
    processPool   *CrossSessionPool
    outputHandler *OutputHandler
    processGroups map[int]*ProcessGroup // PID -> group
    mu            sync.Mutex
}

type ToolExecutorConfig struct {
    // Shell detection patterns (requires shell execution)
    ShellPatterns []string `yaml:"shell_patterns"` // default: ["|", "&&", "||", ";", "*", "?", "$"]

    // Environment blocklist (sensitive vars to strip)
    EnvBlocklist []string `yaml:"env_blocklist"` // default: ["*_API_KEY", "*_SECRET", ...]

    // Default timeout (extended adaptively)
    DefaultTimeout time.Duration `yaml:"default_timeout"` // default: 60s

    // Per-tool timeout overrides
    ToolTimeouts map[string]time.Duration `yaml:"tool_timeouts"`

    // Kill sequence timings
    SIGINTGrace  time.Duration `yaml:"sigint_grace"`  // default: 5s
    SIGTERMGrace time.Duration `yaml:"sigterm_grace"` // default: 3s
}

// ToolInvocation represents a tool execution request
type ToolInvocation struct {
    Tool       string            // Tool name (for timeout lookup)
    Command    string            // Full command string
    Args       []string          // Arguments (if not using shell)
    WorkingDir string            // Execution directory
    Env        map[string]string // Additional env vars
    Stdin      io.Reader         // Optional stdin
    StreamTo   io.Writer         // Where to stream output (typically Guide)
}

// ToolResult contains execution results
type ToolResult struct {
    ExitCode     int
    Stdout       []byte
    Stderr       []byte
    Duration     time.Duration
    Killed       bool   // Was process killed (vs exited normally)
    KillSignal   string // Which signal killed it
    Partial      bool   // Was output truncated/incomplete
    ParsedOutput interface{} // Optional parsed structure
}

// Execute runs a tool with proper process management
func (e *ToolExecutor) Execute(ctx context.Context, inv ToolInvocation) (*ToolResult, error) {
    // Validate working directory
    if err := e.validateWorkingDir(inv.WorkingDir); err != nil {
        return nil, fmt.Errorf("invalid working directory: %w", err)
    }

    // Acquire subprocess slot from cross-session pool
    slot, err := e.processPool.Acquire(ctx)
    if err != nil {
        return nil, fmt.Errorf("subprocess pool exhausted: %w", err)
    }
    defer slot.Release()

    // Determine if shell needed
    useShell := e.needsShell(inv.Command)

    // Build command
    var cmd *exec.Cmd
    if useShell {
        cmd = exec.CommandContext(ctx, "sh", "-c", inv.Command)
    } else {
        cmd = exec.CommandContext(ctx, inv.Command, inv.Args...)
    }

    // Set working directory
    cmd.Dir = inv.WorkingDir

    // Set environment (inherit with blocklist)
    cmd.Env = e.buildEnvironment(inv.Env)

    // Create process group for orphan prevention
    pg := e.createProcessGroup(cmd)

    // Set up output streaming
    stdout, stderr := e.outputHandler.CreateStreams(inv.StreamTo)
    cmd.Stdout = stdout
    cmd.Stderr = stderr

    // Start process
    startTime := time.Now()
    if err := cmd.Start(); err != nil {
        return nil, err
    }

    // Track process group
    e.mu.Lock()
    e.processGroups[cmd.Process.Pid] = pg
    e.mu.Unlock()

    // Wait with adaptive timeout
    result := e.waitWithAdaptiveTimeout(ctx, cmd, inv.Tool, stdout, stderr)
    result.Duration = time.Since(startTime)

    // Cleanup process group
    e.mu.Lock()
    delete(e.processGroups, cmd.Process.Pid)
    e.mu.Unlock()

    return result, nil
}

func (e *ToolExecutor) needsShell(command string) bool {
    for _, pattern := range e.config.ShellPatterns {
        if strings.Contains(command, pattern) {
            return true
        }
    }
    return false
}

func (e *ToolExecutor) buildEnvironment(extra map[string]string) []string {
    env := os.Environ()

    // Filter blocklist
    filtered := make([]string, 0, len(env))
    for _, e := range env {
        key := strings.SplitN(e, "=", 2)[0]
        if !e.matchesBlocklist(key) {
            filtered = append(filtered, e)
        }
    }

    // Add extra vars
    for k, v := range extra {
        filtered = append(filtered, fmt.Sprintf("%s=%s", k, v))
    }

    return filtered
}

func (e *ToolExecutor) validateWorkingDir(dir string) error {
    // Must be absolute
    if !filepath.IsAbs(dir) {
        return fmt.Errorf("must be absolute path")
    }

    // Resolve symlinks and check boundaries
    resolved, err := filepath.EvalSymlinks(dir)
    if err != nil {
        return err
    }

    // Check against allowed boundaries
    allowed := []string{
        e.config.ProjectRoot,
        e.config.StagingRoot,
        e.config.TempRoot,
    }

    for _, boundary := range allowed {
        if strings.HasPrefix(resolved, boundary) {
            return nil
        }
    }

    return fmt.Errorf("directory outside allowed boundaries")
}
```

### Adaptive Timeout

```go
// AdaptiveTimeout extends timeout while output is being produced
type AdaptiveTimeout struct {
    baseTimeout     time.Duration
    extendOnOutput  time.Duration
    maxTimeout      time.Duration
    noisePatterns   []*regexp.Regexp
    lastOutputTime  time.Time
    mu              sync.Mutex
}

type AdaptiveTimeoutConfig struct {
    // Base timeout (from tool config or default)
    BaseTimeout time.Duration `yaml:"base_timeout"`

    // Extension on meaningful output
    ExtendOnOutput time.Duration `yaml:"extend_on_output"` // default: 30s

    // Maximum total timeout
    MaxTimeout time.Duration `yaml:"max_timeout"` // default: 30m

    // Patterns considered noise (don't extend)
    NoisePatterns []string `yaml:"noise_patterns"` // default: ["^\\.*$", "^[|/\\-\\\\]+$", "^\\d+%$"]
}

// OnOutput called when output received
func (a *AdaptiveTimeout) OnOutput(line string) {
    // Check if noise
    for _, pattern := range a.noisePatterns {
        if pattern.MatchString(strings.TrimSpace(line)) {
            return // Noise, don't extend
        }
    }

    // Meaningful output, extend timeout
    a.mu.Lock()
    a.lastOutputTime = time.Now()
    a.mu.Unlock()
}

// ShouldTimeout checks if process should be killed
func (a *AdaptiveTimeout) ShouldTimeout(started time.Time) bool {
    a.mu.Lock()
    lastOutput := a.lastOutputTime
    a.mu.Unlock()

    elapsed := time.Since(started)

    // Hard max timeout
    if elapsed > a.maxTimeout {
        return true
    }

    // If recent output, extend
    if time.Since(lastOutput) < a.extendOnOutput {
        return false
    }

    // Base timeout
    return elapsed > a.baseTimeout
}
```

### Kill Sequence

```
┌─────────────────────────────────────────────────────────────────────────────────────┐
│                              KILL SEQUENCE                                           │
├─────────────────────────────────────────────────────────────────────────────────────┤
│                                                                                     │
│  Cancel/Timeout Triggered                                                           │
│         │                                                                           │
│         ▼                                                                           │
│  ┌─────────────┐                                                                    │
│  │   SIGINT    │ ──▶ User sees: "Stopping..."                                       │
│  │  (Ctrl-C)   │                                                                    │
│  └──────┬──────┘                                                                    │
│         │                                                                           │
│         │ Wait SIGINT_GRACE (default: 5s)                                           │
│         │                                                                           │
│         ├──────────────────────────────────────┐                                    │
│         │                                      │                                    │
│    Process exited?                        Still running                             │
│         │                                      │                                    │
│        Yes                                     ▼                                    │
│         │                               ┌─────────────┐                             │
│         │                               │   SIGTERM   │ ──▶ "Force stopping..."     │
│         │                               │             │                             │
│         │                               └──────┬──────┘                             │
│         │                                      │                                    │
│         │                          Wait SIGTERM_GRACE (default: 3s)                 │
│         │                                      │                                    │
│         │                ┌─────────────────────┼─────────────────────┐              │
│         │                │                     │                     │              │
│         │           Process exited?       Still running         Still running       │
│         │                │                     │                     │              │
│         │               Yes                    ▼                     │              │
│         │                │              ┌─────────────┐              │              │
│         │                │              │   SIGKILL   │ ──▶ "Killed" │              │
│         │                │              │  (force)    │              │              │
│         │                │              └──────┬──────┘              │              │
│         │                │                     │                     │              │
│         ▼                ▼                     ▼                     │              │
│  ┌─────────────────────────────────────────────────────────────────┐ │              │
│  │                      CLEANUP PHASE                              │ │              │
│  │                                                                 │ │              │
│  │  1. Kill entire process group (catch orphans)                   │ │              │
│  │  2. Remove temp files created by this invocation                │ │              │
│  │  3. Release any file locks                                      │ │              │
│  │  4. Release subprocess slot to pool                             │ │              │
│  │                                                                 │ │              │
│  │  Cleanup timeout: 5s (best-effort, don't block)                 │ │              │
│  │                                                                 │ │              │
│  └─────────────────────────────────────────────────────────────────┘ │              │
│                                                                      │              │
│  All signals sent to PROCESS GROUP (not just parent PID)             │              │
│  Unix: kill(-pgid, signal)                                           │              │
│  Windows: TerminateJobObject()                                       │              │
│                                                                                     │
└─────────────────────────────────────────────────────────────────────────────────────┘
```

### Process Group Management

```go
// ProcessGroup manages a process and all its children
type ProcessGroup struct {
    pgid    int
    mainPID int
    mu      sync.Mutex
    killed  bool
}

// Unix implementation
func (pg *ProcessGroup) Setup(cmd *exec.Cmd) error {
    cmd.SysProcAttr = &syscall.SysProcAttr{
        Setpgid: true,
    }
    return nil
}

func (pg *ProcessGroup) Signal(sig syscall.Signal) error {
    pg.mu.Lock()
    defer pg.mu.Unlock()

    if pg.killed {
        return nil
    }

    // Negative PID = send to process group
    return syscall.Kill(-pg.pgid, sig)
}

func (pg *ProcessGroup) Kill() error {
    pg.mu.Lock()
    pg.killed = true
    pg.mu.Unlock()

    return pg.Signal(syscall.SIGKILL)
}

// KillSequence executes the full kill sequence
func (e *ToolExecutor) KillSequence(pg *ProcessGroup) KillResult {
    result := KillResult{}

    // SIGINT
    pg.Signal(syscall.SIGINT)
    result.SentSIGINT = true

    if pg.WaitExit(e.config.SIGINTGrace) {
        result.ExitedAfter = "SIGINT"
        return result
    }

    // SIGTERM
    pg.Signal(syscall.SIGTERM)
    result.SentSIGTERM = true

    if pg.WaitExit(e.config.SIGTERMGrace) {
        result.ExitedAfter = "SIGTERM"
        return result
    }

    // SIGKILL
    pg.Kill()
    result.SentSIGKILL = true
    result.ExitedAfter = "SIGKILL"

    return result
}
```

### Output Handling

```
┌─────────────────────────────────────────────────────────────────────────────────────┐
│                              OUTPUT HANDLING                                         │
├─────────────────────────────────────────────────────────────────────────────────────┤
│                                                                                     │
│  Process Output                                                                     │
│       │                                                                             │
│       ▼                                                                             │
│  ┌─────────────────────────────────────────────────────────────────────────────┐    │
│  │                         OUTPUT STREAMER                                     │    │
│  │                                                                             │    │
│  │  Tee: Stream to user + buffer for agent                                     │    │
│  │                                                                             │    │
│  │     ┌─────────────────┐      ┌─────────────────┐                            │    │
│  │     │  User Stream    │      │  Agent Buffer   │                            │    │
│  │     │  (real-time)    │      │  (full capture) │                            │    │
│  │     └────────┬────────┘      └────────┬────────┘                            │    │
│  │              │                        │                                     │    │
│  │              ▼                        ▼                                     │    │
│  │         Guide ──▶ Terminal       Smart Truncator                            │    │
│  │                                       │                                     │    │
│  │                                       ▼                                     │    │
│  │                              ┌─────────────────┐                            │    │
│  │                              │ Importance      │                            │    │
│  │                              │ Detector        │                            │    │
│  │                              │                 │                            │    │
│  │                              │ • error/warning │                            │    │
│  │                              │ • stack traces  │                            │    │
│  │                              │ • file:line:col │                            │    │
│  │                              │ • first N lines │                            │    │
│  │                              │ • last M lines  │                            │    │
│  │                              └────────┬────────┘                            │    │
│  │                                       │                                     │    │
│  │                                       ▼                                     │    │
│  │                              ┌─────────────────┐                            │    │
│  │                              │ Parser          │                            │    │
│  │                              │ (if available)  │                            │    │
│  │                              │                 │                            │    │
│  │                              │ • go build      │                            │    │
│  │                              │ • npm/yarn      │                            │    │
│  │                              │ • pytest/jest   │                            │    │
│  │                              │ • eslint        │                            │    │
│  │                              │ • git           │                            │    │
│  │                              └────────┬────────┘                            │    │
│  │                                       │                                     │    │
│  │                                       ▼                                     │    │
│  │                              Agent receives:                                │    │
│  │                              • Parsed result (if parser)                    │    │
│  │                              • OR smart-truncated raw                       │    │
│  │                              • Full raw on request                          │    │
│  │                                                                             │    │
│  └─────────────────────────────────────────────────────────────────────────────┘    │
│                                                                                     │
└─────────────────────────────────────────────────────────────────────────────────────┘
```

### Output Handler

```go
// OutputHandler manages output streaming and parsing
type OutputHandler struct {
    config      OutputHandlerConfig
    parsers     map[string]OutputParser
    truncator   *SmartTruncator
    parseCache  *ParseTemplateCache
}

type OutputHandlerConfig struct {
    // Maximum buffer size before truncation
    MaxBufferSize int `yaml:"max_buffer_size"` // default: 1MB

    // Lines to always keep
    KeepFirstLines int `yaml:"keep_first_lines"` // default: 50
    KeepLastLines  int `yaml:"keep_last_lines"`  // default: 100

    // Importance patterns
    ImportancePatterns []string `yaml:"importance_patterns"`
}

func DefaultOutputHandlerConfig() OutputHandlerConfig {
    return OutputHandlerConfig{
        MaxBufferSize:  1024 * 1024,
        KeepFirstLines: 50,
        KeepLastLines:  100,
        ImportancePatterns: []string{
            `(?i)error`,
            `(?i)warning`,
            `(?i)failed`,
            `(?i)exception`,
            `(?i)panic`,
            `\w+\.\w+:\d+:\d+`, // file:line:col
            `^\s+at\s+`,        // stack trace
        },
    }
}

// ProcessOutput handles output for agent consumption
func (h *OutputHandler) ProcessOutput(tool string, stdout, stderr []byte) *ProcessedOutput {
    // Try tool-specific parser first
    if parser, ok := h.parsers[tool]; ok {
        if parsed, err := parser.Parse(stdout, stderr); err == nil {
            return &ProcessedOutput{
                Type:   OutputTypeParsed,
                Parsed: parsed,
            }
        }
    }

    // Try cached LLM parse template
    if template := h.parseCache.Get(tool); template != nil {
        if parsed, err := template.Apply(stdout, stderr); err == nil {
            return &ProcessedOutput{
                Type:   OutputTypeParsed,
                Parsed: parsed,
            }
        }
    }

    // Fall back to smart truncation
    truncated := h.truncator.Truncate(stdout, stderr)
    return &ProcessedOutput{
        Type:      OutputTypeTruncated,
        Summary:   truncated.Summary,
        Important: truncated.ImportantLines,
        FirstN:    truncated.FirstLines,
        LastM:     truncated.LastLines,
        FullSize:  len(stdout) + len(stderr),
    }
}
```

### Smart Truncator

```go
// SmartTruncator extracts important content from large output
type SmartTruncator struct {
    patterns   []*regexp.Regexp
    keepFirst  int
    keepLast   int
    maxSize    int
}

type TruncatedOutput struct {
    Summary        string   // One-line summary
    ImportantLines []string // Lines matching importance patterns
    FirstLines     []string // First N lines
    LastLines      []string // Last M lines
    TotalLines     int
    Truncated      bool
}

func (t *SmartTruncator) Truncate(stdout, stderr []byte) *TruncatedOutput {
    combined := append(stdout, stderr...)
    lines := strings.Split(string(combined), "\n")

    result := &TruncatedOutput{
        TotalLines: len(lines),
        Truncated:  len(combined) > t.maxSize,
    }

    // Always keep first N
    if len(lines) > t.keepFirst {
        result.FirstLines = lines[:t.keepFirst]
    } else {
        result.FirstLines = lines
    }

    // Always keep last M
    if len(lines) > t.keepLast {
        result.LastLines = lines[len(lines)-t.keepLast:]
    }

    // Extract important lines
    for _, line := range lines {
        for _, pattern := range t.patterns {
            if pattern.MatchString(line) {
                result.ImportantLines = append(result.ImportantLines, line)
                break
            }
        }
    }

    // Deduplicate (important lines might overlap with first/last)
    result.ImportantLines = t.dedupe(result.ImportantLines, result.FirstLines, result.LastLines)

    // Generate summary
    result.Summary = t.generateSummary(result)

    return result
}

func (t *SmartTruncator) generateSummary(r *TruncatedOutput) string {
    errorCount := 0
    warningCount := 0
    for _, line := range r.ImportantLines {
        if strings.Contains(strings.ToLower(line), "error") {
            errorCount++
        }
        if strings.Contains(strings.ToLower(line), "warning") {
            warningCount++
        }
    }

    if errorCount > 0 || warningCount > 0 {
        return fmt.Sprintf("%d errors, %d warnings in %d lines", errorCount, warningCount, r.TotalLines)
    }
    return fmt.Sprintf("%d lines of output", r.TotalLines)
}
```

### Filesystem Abstraction

```
┌─────────────────────────────────────────────────────────────────────────────────────┐
│                          FILESYSTEM ABSTRACTION                                      │
├─────────────────────────────────────────────────────────────────────────────────────┤
│                                                                                     │
│  READ PATH (Direct)                   WRITE PATH (Abstracted)                       │
│                                                                                     │
│  ┌─────────────┐                     ┌─────────────┐                               │
│  │ Agent reads │                     │ Agent writes│                               │
│  └──────┬──────┘                     └──────┬──────┘                               │
│         │                                   │                                       │
│         ▼                                   ▼                                       │
│  ┌─────────────┐                     ┌─────────────────────┐                       │
│  │  Direct OS  │                     │   Write Abstraction │                       │
│  │   read()    │                     │                     │                       │
│  └─────────────┘                     │  ┌───────────────┐  │                       │
│                                      │  │ Permission    │  │                       │
│                                      │  │ Pre-check     │  │                       │
│                                      │  └───────┬───────┘  │                       │
│                                      │          │          │                       │
│                                      │          ▼          │                       │
│                                      │  ┌───────────────┐  │                       │
│                                      │  │ Symlink       │  │                       │
│                                      │  │ Boundary Check│  │                       │
│                                      │  └───────┬───────┘  │                       │
│                                      │          │          │                       │
│                                      │          ▼          │                       │
│                                      │  ┌───────────────┐  │                       │
│                                      │  │ Route to      │  │                       │
│                                      │  │ Staging (if   │  │                       │
│                                      │  │ in pipeline)  │  │                       │
│                                      │  └───────┬───────┘  │                       │
│                                      │          │          │                       │
│                                      │          ▼          │                       │
│                                      │  ┌───────────────┐  │                       │
│                                      │  │ Audit Log     │  │                       │
│                                      │  └───────┬───────┘  │                       │
│                                      │          │          │                       │
│                                      │          ▼          │                       │
│                                      │  ┌───────────────┐  │                       │
│                                      │  │ Actual Write  │  │                       │
│                                      │  └───────────────┘  │                       │
│                                      │                     │                       │
│                                      └─────────────────────┘                       │
│                                                                                     │
│  TEMP FILE HIERARCHY:                                                               │
│                                                                                     │
│  ~/.sylk/tmp/                                                                       │
│    ├── <session-id-A>/                   # Session A's temp                         │
│    │   ├── pipeline-<id-1>/              # Pipeline 1's isolated temp               │
│    │   ├── pipeline-<id-2>/              # Pipeline 2's isolated temp               │
│    │   └── standalone/                   # Standalone agent temp                    │
│    ├── <session-id-B>/                   # Session B's temp                         │
│    │   └── ...                                                                      │
│    └── shared/                           # Cross-session temp (rare)                │
│                                                                                     │
│  SYMLINK BOUNDARIES (configurable):                                                 │
│    ✓ Project root (git root or cwd)                                                 │
│    ✓ Staging directories                                                            │
│    ✓ Temp directories                                                               │
│    ✗ Home directory (too broad)                                                     │
│    ✗ System directories                                                             │
│                                                                                     │
└─────────────────────────────────────────────────────────────────────────────────────┘
```

### Filesystem Manager

```go
// FilesystemManager provides safe file operations for agents
type FilesystemManager struct {
    config     FilesystemConfig
    sessionID  string
    pipelineID string // empty for standalone
    auditLog   *AuditLog
}

type FilesystemConfig struct {
    // Project root (typically git root)
    ProjectRoot string `yaml:"project_root"`

    // Staging root
    StagingRoot string `yaml:"staging_root"` // default: ~/.sylk/staging

    // Temp root
    TempRoot string `yaml:"temp_root"` // default: ~/.sylk/tmp

    // Additional allowed paths
    AllowedPaths []string `yaml:"allowed_paths"`

    // Symlink resolution
    FollowSymlinks bool `yaml:"follow_symlinks"` // default: true (with boundary check)
}

// Write writes a file through the abstraction layer
func (f *FilesystemManager) Write(path string, content []byte, perm os.FileMode) error {
    // Pre-check permission
    if err := f.checkWritePermission(path); err != nil {
        // Return user-fixable error with guidance
        return &UserFixableError{
            Err:     err,
            Message: fmt.Sprintf("Cannot write to %s: %v", path, err),
            Fix:     fmt.Sprintf("Check permissions: ls -la %s", filepath.Dir(path)),
        }
    }

    // Resolve and validate symlinks
    resolved, err := f.resolveWithBoundaryCheck(path)
    if err != nil {
        return err
    }

    // Route to staging if in pipeline
    if f.pipelineID != "" {
        resolved = f.routeToStaging(resolved)
    }

    // Audit log
    f.auditLog.LogWrite(f.sessionID, f.pipelineID, path, len(content))

    // Actual write
    return os.WriteFile(resolved, content, perm)
}

func (f *FilesystemManager) resolveWithBoundaryCheck(path string) (string, error) {
    if !f.config.FollowSymlinks {
        return path, nil
    }

    resolved, err := filepath.EvalSymlinks(path)
    if err != nil {
        // Path doesn't exist yet, check parent
        dir := filepath.Dir(path)
        resolvedDir, err := filepath.EvalSymlinks(dir)
        if err != nil {
            return "", err
        }
        resolved = filepath.Join(resolvedDir, filepath.Base(path))
    }

    // Check boundaries
    allowed := append([]string{
        f.config.ProjectRoot,
        f.config.StagingRoot,
        f.config.TempRoot,
    }, f.config.AllowedPaths...)

    for _, boundary := range allowed {
        if strings.HasPrefix(resolved, boundary) {
            return resolved, nil
        }
    }

    return "", fmt.Errorf("path %s resolves to %s which is outside allowed boundaries", path, resolved)
}

// GetTempDir returns the appropriate temp directory
func (f *FilesystemManager) GetTempDir() string {
    base := filepath.Join(f.config.TempRoot, f.sessionID)

    if f.pipelineID != "" {
        return filepath.Join(base, "pipeline-"+f.pipelineID)
    }
    return filepath.Join(base, "standalone")
}
```

### Tool Cancellation

```
┌─────────────────────────────────────────────────────────────────────────────────────┐
│                           TOOL CANCELLATION FLOW                                     │
├─────────────────────────────────────────────────────────────────────────────────────┤
│                                                                                     │
│  User presses Ctrl-C (or context cancelled)                                         │
│         │                                                                           │
│         ▼                                                                           │
│  ┌─────────────────────────────────────────────────────────────────────────────┐    │
│  │                    CASCADING TIMEOUT BUDGET                                 │    │
│  │                                                                             │    │
│  │  Total budget: 30s (configurable)                                           │    │
│  │                                                                             │    │
│  │  ┌─────────────────────────────────────────────────────────────────────┐    │    │
│  │  │ Agent Level (25s)                                                   │    │    │
│  │  │                                                                     │    │    │
│  │  │   • Agent receives cancel                                           │    │    │
│  │  │   • Propagates to all running tools                                 │    │    │
│  │  │   • User sees: "Cancelling operation..."                            │    │    │
│  │  │                                                                     │    │    │
│  │  │  ┌─────────────────────────────────────────────────────────┐        │    │    │
│  │  │  │ Tool Level (20s)                                        │        │    │    │
│  │  │  │                                                         │        │    │    │
│  │  │  │   • Tool executor receives cancel                       │        │    │    │
│  │  │  │   • Initiates kill sequence                             │        │    │    │
│  │  │  │   • User sees: "Stopping [tool name]..."                │        │    │    │
│  │  │  │                                                         │        │    │    │
│  │  │  │  ┌─────────────────────────────────────────────┐        │        │    │    │
│  │  │  │  │ Process Level (8s)                          │        │        │    │    │
│  │  │  │  │                                             │        │        │    │    │
│  │  │  │  │   • SIGINT (5s) → SIGTERM (3s) → SIGKILL    │        │        │    │    │
│  │  │  │  │   • User sees progress through sequence     │        │        │    │    │
│  │  │  │  │                                             │        │        │    │    │
│  │  │  │  └─────────────────────────────────────────────┘        │        │    │    │
│  │  │  │                                                         │        │    │    │
│  │  │  └─────────────────────────────────────────────────────────┘        │    │    │
│  │  │                                                                     │    │    │
│  │  └─────────────────────────────────────────────────────────────────────┘    │    │
│  │                                                                             │    │
│  │  ┌─────────────────────────────────────────────────────────────────────┐    │    │
│  │  │ Cleanup Phase (5s budget, best-effort)                              │    │    │
│  │  │                                                                     │    │    │
│  │  │   • Kill process group (orphans)                                    │    │    │
│  │  │   • Remove temp files                                               │    │    │
│  │  │   • Release file locks                                              │    │    │
│  │  │   • Release pool slots                                              │    │    │
│  │  │   • Mark output as partial                                          │    │    │
│  │  │                                                                     │    │    │
│  │  │   If cleanup times out:                                             │    │    │
│  │  │   • Log warning                                                     │    │    │
│  │  │   • User sees: "Cleanup incomplete, some temp files may remain"     │    │    │
│  │  │   • Continue (don't block)                                          │    │    │
│  │  │                                                                     │    │    │
│  │  └─────────────────────────────────────────────────────────────────────┘    │    │
│  │                                                                             │    │
│  └─────────────────────────────────────────────────────────────────────────────┘    │
│                                                                                     │
│  PARTIAL RESULTS:                                                                   │
│    • Output captured before cancellation is PRESERVED                               │
│    • Marked as partial: result.Partial = true                                       │
│    • Agent sees what happened before cancel                                         │
│    • Can decide: retry, use partial, discard                                        │
│                                                                                     │
└─────────────────────────────────────────────────────────────────────────────────────┘
```

### Tool Output Caching

```go
// ToolOutputCache caches deterministic tool outputs
type ToolOutputCache struct {
    cache  map[string]*CachedToolOutput
    config ToolCacheConfig
    mu     sync.RWMutex
}

type ToolCacheConfig struct {
    // Maximum cache entries
    MaxEntries int `yaml:"max_entries"` // default: 1000

    // Maximum cache size
    MaxSize int64 `yaml:"max_size"` // default: 100MB

    // TTL for cache entries
    TTL time.Duration `yaml:"ttl"` // default: 5m

    // Tools that are cacheable (deterministic)
    CacheableTools map[string]CachePolicy `yaml:"cacheable_tools"`
}

type CachePolicy struct {
    Cacheable    bool          `yaml:"cacheable"`
    TTL          time.Duration `yaml:"ttl"`
    InvalidateOn []string      `yaml:"invalidate_on"` // file patterns that invalidate
}

type CachedToolOutput struct {
    Tool       string
    InputHash  string // Hash of command + args + relevant file contents
    Output     *ToolResult
    CachedAt   time.Time
    HitCount   int
}

// Get retrieves cached output if available
func (c *ToolOutputCache) Get(tool, inputHash string) (*ToolResult, bool) {
    c.mu.RLock()
    defer c.mu.RUnlock()

    key := tool + ":" + inputHash
    if cached, ok := c.cache[key]; ok {
        if time.Since(cached.CachedAt) < c.config.TTL {
            cached.HitCount++
            return cached.Output, true
        }
    }
    return nil, false
}

// ComputeInputHash generates cache key from tool invocation
func (c *ToolOutputCache) ComputeInputHash(inv ToolInvocation) string {
    h := sha256.New()
    h.Write([]byte(inv.Command))
    for _, arg := range inv.Args {
        h.Write([]byte(arg))
    }
    h.Write([]byte(inv.WorkingDir))

    // Include relevant file hashes for file-based tools
    // e.g., for "eslint file.js", hash file.js contents
    // This is tool-specific logic

    return hex.EncodeToString(h.Sum(nil))
}
```

### Tool Configuration Schema

```yaml
# ~/.sylk/config.yaml - Tool execution configuration

session:
  # Session registry
  db_path: "~/.sylk/state.db"
  heartbeat_interval: "1s"
  stale_threshold: "10s"
  activity_decay_rate: 0.9

  # Fair share
  user_reserved_percent: 0.2
  idle_baseline: 1
  rebalance_interval: "500ms"

  # Signal dispatcher
  signal_dir: "~/.sylk/signals"

tools:
  # Subprocess execution
  shell_patterns: ["|", "&&", "||", ";", "*", "?", "$", "`"]

  env_blocklist:
    - "*_API_KEY"
    - "*_SECRET"
    - "*_TOKEN"
    - "*_PASSWORD"
    - "AWS_*"
    - "ANTHROPIC_API_KEY"
    - "OPENAI_API_KEY"
    - "GOOGLE_API_KEY"
    - "GITHUB_TOKEN"
    - "NPM_TOKEN"

  # Default timeout
  default_timeout: "60s"

  # Per-tool timeouts
  tool_timeouts:
    "go build": "10m"
    "go test": "5m"
    "npm install": "5m"
    "npm run build": "10m"
    "pytest": "5m"
    "jest": "5m"
    "eslint": "60s"
    "prettier": "30s"
    "git": "30s"

  # Adaptive timeout
  extend_on_output: "30s"
  max_timeout: "30m"
  noise_patterns:
    - "^\\.*$"           # Dots
    - "^[|/\\-\\\\]+$"   # Spinners
    - "^\\d+%$"          # Percentages alone

  # Kill sequence
  sigint_grace: "5s"
  sigterm_grace: "3s"
  cleanup_timeout: "5s"

  # Cancellation
  cancel_budget: "30s"
  agent_budget: "25s"
  tool_budget: "20s"

output:
  # Buffer limits
  max_buffer_size: "1MB"
  keep_first_lines: 50
  keep_last_lines: 100

  # Importance patterns
  importance_patterns:
    - "(?i)error"
    - "(?i)warning"
    - "(?i)failed"
    - "(?i)exception"
    - "(?i)panic"
    - "\\w+\\.\\w+:\\d+:\\d+"
    - "^\\s+at\\s+"

  # Tool output caching
  cache:
    max_entries: 1000
    max_size: "100MB"
    ttl: "5m"
    cacheable_tools:
      eslint:
        cacheable: true
        ttl: "5m"
        invalidate_on: ["*.js", "*.ts", ".eslintrc*"]
      prettier:
        cacheable: true
        ttl: "5m"
        invalidate_on: ["*"]
      "go vet":
        cacheable: true
        ttl: "5m"
        invalidate_on: ["*.go"]

filesystem:
  # Symlink handling
  follow_symlinks: true

  # Allowed boundaries (in addition to project/staging/temp)
  allowed_paths: []

  # Temp directory structure
  temp_root: "~/.sylk/tmp"
```

### Integration with Existing Systems

```
┌─────────────────────────────────────────────────────────────────────────────────────┐
│                    TOOL EXECUTION SYSTEM INTEGRATION                                 │
├─────────────────────────────────────────────────────────────────────────────────────┤
│                                                                                     │
│  ┌─────────────────────────────────────────────────────────────────────────────┐    │
│  │                    RESOURCE BROKER (0.36)                                   │    │
│  │                                                                             │    │
│  │  Tool Executor requests subprocess slots from broker                        │    │
│  │  Broker checks:                                                             │    │
│  │    • Global pool availability                                               │    │
│  │    • Session fair share allocation                                          │    │
│  │    • User-interactive reservation                                           │    │
│  │                                                                             │    │
│  │  Cross-session preemption:                                                  │    │
│  │    • User request → broker checks all sessions                              │    │
│  │    • Signals lowest-priority pipeline in ANY session                        │    │
│  │    • That session's tool executor receives preempt signal                   │    │
│  │    • Pauses tool, releases slot                                             │    │
│  │                                                                             │    │
│  └─────────────────────────────────────────────────────────────────────────────┘    │
│                                         │                                           │
│                    ┌────────────────────┼────────────────────┐                      │
│                    │                    │                    │                      │
│                    ▼                    ▼                    ▼                      │
│  ┌─────────────────────┐  ┌─────────────────────┐  ┌─────────────────────┐         │
│  │ Error System (0.26+)│  │  Staging (0.20)     │  │  Signal Bus (0.11)  │         │
│  │                     │  │                     │  │                     │         │
│  │ • Tool errors get   │  │ • Writes during     │  │ • Publish tool      │         │
│  │   classified        │  │   pipeline go to    │  │   events            │         │
│  │ • Transient → retry │  │   staging           │  │ • Cancel signals    │         │
│  │ • Permanent → fail  │  │ • Filesystem mgr    │  │ • Preempt signals   │         │
│  │ • Circuit breaker   │  │   routes writes     │  │ • Progress updates  │         │
│  │   per tool          │  │                     │  │                     │         │
│  └─────────────────────┘  └─────────────────────┘  └─────────────────────┘         │
│                                                                                     │
│  ┌─────────────────────────────────────────────────────────────────────────────┐    │
│  │                         ARCHIVALIST                                         │    │
│  │                                                                             │    │
│  │  • Stores tool invocation history                                           │    │
│  │  • Stores parse templates learned by LLM                                    │    │
│  │  • Queryable: "What commands worked for similar tasks?"                     │    │
│  │  • Queryable: "What's the typical output for [tool]?"                       │    │
│  │                                                                             │    │
│  └─────────────────────────────────────────────────────────────────────────────┘    │
│                                                                                     │
│  ┌─────────────────────────────────────────────────────────────────────────────┐    │
│  │                            GUIDE                                            │    │
│  │                                                                             │    │
│  │  • Receives streamed output for user display                                │    │
│  │  • Routes cancel signals from user to tools                                 │    │
│  │  • Displays tool progress (starting, running, stopping)                     │    │
│  │  • Shows countdown for rate-limited tools                                   │    │
│  │                                                                             │    │
│  └─────────────────────────────────────────────────────────────────────────────┘    │
│                                                                                     │
└─────────────────────────────────────────────────────────────────────────────────────┘
```

---

## Storage & Configuration

This section defines the storage layout, configuration schema, credential management, and database operations for Sylk.

### Directory Layout

Sylk uses XDG semantics with platform-native default paths. This provides consistent internal structure while respecting each platform's conventions.

#### Directory Resolution

```
┌─────────────────────────────────────────────────────────────────────────────────────┐
│                         DIRECTORY RESOLUTION ORDER                                   │
├─────────────────────────────────────────────────────────────────────────────────────┤
│                                                                                     │
│  1. Check XDG environment variables (works on ALL platforms if user sets them):    │
│     • $XDG_CONFIG_HOME/sylk  → config                                              │
│     • $XDG_DATA_HOME/sylk    → data (sessions, DB)                                 │
│     • $XDG_CACHE_HOME/sylk   → cache (regenerable)                                 │
│     • $XDG_STATE_HOME/sylk   → state (logs, runtime)                               │
│                                                                                     │
│  2. Fall back to platform-native defaults:                                          │
│                                                                                     │
│     LINUX:                                                                          │
│     • ~/.config/sylk         → config                                              │
│     • ~/.local/share/sylk    → data                                                │
│     • ~/.cache/sylk          → cache                                               │
│     • ~/.local/state/sylk    → state                                               │
│                                                                                     │
│     MACOS:                                                                          │
│     • ~/Library/Preferences/sylk           → config                                │
│     • ~/Library/Application Support/sylk   → data                                  │
│     • ~/Library/Caches/sylk                → cache                                 │
│     • ~/Library/Logs/sylk                  → state                                 │
│                                                                                     │
│     WINDOWS:                                                                        │
│     • %APPDATA%\sylk\config                → config                                │
│     • %APPDATA%\sylk\data                  → data                                  │
│     • %LOCALAPPDATA%\sylk\cache            → cache                                 │
│     • %LOCALAPPDATA%\sylk\state            → state                                 │
│                                                                                     │
└─────────────────────────────────────────────────────────────────────────────────────┘
```

#### Directory Purpose & Backup Priority

| Directory | Contents | Backup Priority | Can Delete Safely? |
|-----------|----------|-----------------|-------------------|
| **config** | User settings, credentials (encrypted), provider config | HIGH | No |
| **data** | Sessions, SQLite DBs, WAL, checkpoints, per-project knowledge | HIGH | No |
| **cache** | Query cache, embeddings cache, tool output cache | LOW | Yes |
| **state** | Logs, lock files, PID files, runtime signals, temp files | MEDIUM | Yes (after shutdown) |

#### Complete Directory Structure

```
config/
├── config.yaml                    # User configuration (YAML)
├── config.schema.json             # JSON Schema for validation
├── credentials.enc                # Encrypted credentials (fallback)
└── profiles/
    ├── default.yaml               # Default profile
    ├── work.yaml                  # Named profile
    └── personal.yaml              # Named profile

data/
├── shared/
│   └── system.db                  # GLOBAL: sessions, subscription_usage, circuit_breakers
│
└── projects/
    └── {project_hash}/
        ├── knowledge.db           # PER-PROJECT: query_cache, failure_patterns, embeddings
        └── index.db               # PER-PROJECT: Librarian codebase index (VectorGraphDB)

data/sessions/{id}/
├── session.db                     # Session-specific context, routing cache
├── wal/                           # Write-ahead log files
└── checkpoints/                   # Checkpoint files

cache/
├── shared/                        # Cross-session cache (deterministic tool output)
│   ├── hot/                       # Frequently accessed, never auto-evict
│   ├── warm/                      # Recent, evict under pressure
│   └── cold/                      # Old, evict first
│
└── sessions/{id}/                 # Session-scoped cache (query cache)
    ├── hot/
    ├── warm/
    └── cold/

state/
├── logs/
│   ├── sessions/{id}/
│   │   ├── session.log            # Human-readable, all components
│   │   └── session.jsonl          # Structured JSONL, queryable
│   │
│   ├── global.jsonl               # ALL sessions, ALL components, structured
│   └── recent.log                 # Rolling "last N minutes" human-readable
│
├── runtime/
│   ├── sylk.pid                   # Process ID file
│   └── locks/                     # Lock files
│
├── signals/                       # Cross-session signal files (fsnotify)
│
└── temp/
    └── sessions/{id}/             # Controlled temp (staging, working files)

$TMPDIR/sylk-{session_id}/         # OS-managed temp (large build artifacts)
```

#### Project-Local Directory

Projects can have local configuration in `.sylk/`:

```
project_root/
└── .sylk/
    ├── config.yaml                # Committed - team settings
    │                              # Provider preferences, tool configs
    │
    └── local/                     # Gitignored - personal overrides
        ├── config.yaml            # Personal setting overrides
        └── credentials.enc        # Project-specific credentials (encrypted)
```

**.gitignore entry:**
```
.sylk/local/
```

### Directory Manager Implementation

```go
// Dirs provides platform-native directory resolution with XDG support
type Dirs struct {
    Config string // User configuration
    Data   string // Persistent data (sessions, databases)
    Cache  string // Regenerable cache
    State  string // Runtime state (logs, temp, locks)
}

// ResolveDirs returns platform-appropriate directories
func ResolveDirs() (*Dirs, error) {
    return &Dirs{
        Config: resolveDir("XDG_CONFIG_HOME", platformConfigDefault()),
        Data:   resolveDir("XDG_DATA_HOME", platformDataDefault()),
        Cache:  resolveDir("XDG_CACHE_HOME", platformCacheDefault()),
        State:  resolveDir("XDG_STATE_HOME", platformStateDefault()),
    }, nil
}

func resolveDir(envVar, fallback string) string {
    if dir := os.Getenv(envVar); dir != "" {
        return filepath.Join(dir, "sylk")
    }
    return fallback
}

func platformConfigDefault() string {
    switch runtime.GOOS {
    case "darwin":
        return filepath.Join(os.Getenv("HOME"), "Library", "Preferences", "sylk")
    case "windows":
        return filepath.Join(os.Getenv("APPDATA"), "sylk", "config")
    default: // linux and others
        return filepath.Join(os.Getenv("HOME"), ".config", "sylk")
    }
}

func platformDataDefault() string {
    switch runtime.GOOS {
    case "darwin":
        return filepath.Join(os.Getenv("HOME"), "Library", "Application Support", "sylk")
    case "windows":
        return filepath.Join(os.Getenv("APPDATA"), "sylk", "data")
    default:
        return filepath.Join(os.Getenv("HOME"), ".local", "share", "sylk")
    }
}

func platformCacheDefault() string {
    switch runtime.GOOS {
    case "darwin":
        return filepath.Join(os.Getenv("HOME"), "Library", "Caches", "sylk")
    case "windows":
        return filepath.Join(os.Getenv("LOCALAPPDATA"), "sylk", "cache")
    default:
        return filepath.Join(os.Getenv("HOME"), ".cache", "sylk")
    }
}

func platformStateDefault() string {
    switch runtime.GOOS {
    case "darwin":
        return filepath.Join(os.Getenv("HOME"), "Library", "Logs", "sylk")
    case "windows":
        return filepath.Join(os.Getenv("LOCALAPPDATA"), "sylk", "state")
    default:
        return filepath.Join(os.Getenv("HOME"), ".local", "state", "sylk")
    }
}

// ProjectDirs returns project-local directories
type ProjectDirs struct {
    Root      string // .sylk/
    Config    string // .sylk/config.yaml (committed)
    Local     string // .sylk/local/ (gitignored)
}

func ResolveProjectDirs(projectRoot string) *ProjectDirs {
    sylkDir := filepath.Join(projectRoot, ".sylk")
    return &ProjectDirs{
        Root:   sylkDir,
        Config: filepath.Join(sylkDir, "config.yaml"),
        Local:  filepath.Join(sylkDir, "local"),
    }
}

// ProjectHash generates consistent hash for project path
func ProjectHash(projectRoot string) string {
    absPath, _ := filepath.Abs(projectRoot)
    hash := sha256.Sum256([]byte(absPath))
    return hex.EncodeToString(hash[:8]) // 16 chars
}
```

---

### Configuration Schema

#### Format & Validation

Configuration uses YAML for human editing with JSON Schema for validation and IDE support.

```
┌─────────────────────────────────────────────────────────────────────────────────────┐
│                         CONFIGURATION LAYERING                                       │
├─────────────────────────────────────────────────────────────────────────────────────┤
│                                                                                     │
│  Precedence (lowest to highest):                                                    │
│                                                                                     │
│  ┌─────────────┐                                                                    │
│  │  DEFAULTS   │  Built-in defaults (in code)                                       │
│  └──────┬──────┘                                                                    │
│         │ deep merge                                                                │
│         ▼                                                                           │
│  ┌─────────────┐                                                                    │
│  │   PROJECT   │  .sylk/config.yaml (team settings, committed)                      │
│  └──────┬──────┘                                                                    │
│         │ deep merge (USER WINS on conflict)                                        │
│         ▼                                                                           │
│  ┌─────────────┐                                                                    │
│  │    USER     │  ~/Library/Preferences/sylk/config.yaml                            │
│  │             │  + .sylk/local/config.yaml (personal project overrides)            │
│  └──────┬──────┘                                                                    │
│         │ override                                                                  │
│         ▼                                                                           │
│  ┌─────────────┐                                                                    │
│  │ ENVIRONMENT │  SYLK_* environment variables                                      │
│  └─────────────┘                                                                    │
│                                                                                     │
│  Deep merge: Maps are recursively merged, higher precedence wins on conflict        │
│  User > Project: User preferences always override team settings                     │
│                                                                                     │
└─────────────────────────────────────────────────────────────────────────────────────┘
```

#### Environment Variable Format

```
┌─────────────────────────────────────────────────────────────────────────────────────┐
│                         ENVIRONMENT VARIABLE MAPPING                                 │
├─────────────────────────────────────────────────────────────────────────────────────┤
│                                                                                     │
│  Simple values (flat with prefix):                                                  │
│  ─────────────────────────────────                                                  │
│  SYLK_LLM_TIMEOUT=30s              → llm.timeout: 30s                               │
│  SYLK_LLM_DEFAULT_PROVIDER=openai  → llm.default_provider: openai                   │
│  SYLK_LOG_LEVEL=debug              → log.level: debug                               │
│                                                                                     │
│  Complex values (file reference):                                                   │
│  ────────────────────────────────                                                   │
│  SYLK_LLM_PROVIDERS_FILE=/path/to/providers.yaml                                    │
│  SYLK_SUBSCRIPTION_FILE=/etc/sylk/subscription.yaml                                 │
│                                                                                     │
│  File reference loads YAML and merges at that config path.                          │
│                                                                                     │
│  Provider credentials (standard env vars):                                          │
│  ─────────────────────────────────────────                                          │
│  ANTHROPIC_API_KEY=sk-...          → Checked first for Anthropic                    │
│  OPENAI_API_KEY=sk-...             → Checked first for OpenAI                       │
│  GOOGLE_API_KEY=...                → Checked first for Google                       │
│                                                                                     │
└─────────────────────────────────────────────────────────────────────────────────────┘
```

#### Validation Strategy

```
On startup:
1. Load all config sources (defaults → project → user → env)
2. Deep merge with precedence rules
3. Validate against JSON Schema
4. For each validation error:
   ├── Log warning with details
   ├── Use default value for that field
   └── Continue (do NOT refuse to start)
5. Emit summary: "Config loaded with N warnings" (if any)
```

#### Hot Reload

```
┌─────────────────────────────────────────────────────────────────────────────────────┐
│                         CONFIG HOT RELOAD                                            │
├─────────────────────────────────────────────────────────────────────────────────────┤
│                                                                                     │
│  Triggers:                                                                          │
│  ─────────                                                                          │
│  1. SIGHUP signal         → Explicit reload request                                 │
│  2. File watch (fsnotify) → Auto-detect config file changes                         │
│                                                                                     │
│  On reload:                                                                         │
│  ──────────                                                                         │
│  1. Re-read all config sources                                                      │
│  2. Validate new config                                                             │
│  3. If valid:                                                                       │
│     ├── Apply changes atomically                                                    │
│     ├── Notify user: "Config reloaded"                                              │
│     └── Broadcast to active sessions                                                │
│  4. If invalid:                                                                     │
│     ├── Keep current config                                                         │
│     ├── Log errors                                                                  │
│     └── Notify user: "Config reload failed: [errors]"                               │
│                                                                                     │
│  Watched files:                                                                     │
│  ──────────────                                                                     │
│  • {config}/config.yaml                                                             │
│  • .sylk/config.yaml                                                                │
│  • .sylk/local/config.yaml                                                          │
│                                                                                     │
│  Debounce: 500ms (prevent rapid reloads on editor save)                             │
│                                                                                     │
└─────────────────────────────────────────────────────────────────────────────────────┘
```

#### Configuration Schema Definition

```yaml
# config.schema.json (JSON Schema)
# Provides validation + IDE autocomplete

# Example config.yaml with all sections:

# Profile selection
profile: default                     # Named credential profile to use

# LLM Configuration
llm:
  default_provider: anthropic
  timeout: 120s

  providers:
    anthropic:
      enabled: true
      models:
        default: claude-sonnet-4-20250514
        fast: claude-haiku-4-20250514
        powerful: claude-opus-4-20250514

    openai:
      enabled: false
      base_url: null                 # Optional proxy URL
      models:
        default: gpt-4o
        fast: gpt-4o-mini

# Subscription tracking (optional - if not set, rely on 429)
subscription:
  anthropic:
    period: week                     # week | month | day
    max_requests: 10000              # null = unlimited
    max_tokens: 50000000             # 50M
    warn_threshold: 0.8

# Pipeline configuration
pipelines:
  global_max_concurrent: 0           # 0 = N_CPU_CORES
  per_session_max: 0                 # 0 = fair share

# Multi-session configuration
multi_session:
  enabled: true
  fair_share:
    user_reserved_percent: 0.2
    idle_baseline_slots: 1
    activity_decay_rate: 0.1
    rebalance_interval: 5s

# Resource constraints
resources:
  memory:
    warning_threshold: 0.7
    critical_threshold: 0.85
  disk:
    quota_min: 1GB
    quota_max: 20GB
    quota_percent: 0.1

# Error handling
errors:
  transient:
    max_retries: 3
    high_frequency_threshold: 5
    high_frequency_window: 1m
  circuit_breaker:
    llm:
      consecutive_failures: 5
      cooldown_duration: 30s

# Tool execution
tools:
  subprocess:
    default_timeout: 120s
    max_output_size: 10MB
  kill_sequence:
    sigint_timeout: 5s
    sigterm_timeout: 3s

# Logging
log:
  level: info                        # debug | info | warn | error
  format: text                       # text | json
  retention_days: 30

# Backup
backup:
  periodic:
    enabled: true
    interval: 24h
  pre_session: true
  pre_migration: true
  retention_count: 10
```

#### Config Manager Implementation

```go
type ConfigManager struct {
    mu            sync.RWMutex
    current       *Config
    schema        *jsonschema.Schema
    watcher       *fsnotify.Watcher
    dirs          *Dirs
    projectDirs   *ProjectDirs
    onChange      []func(*Config)
}

// Load loads and merges all config sources
func (cm *ConfigManager) Load() (*Config, []ValidationWarning, error) {
    // 1. Start with defaults
    cfg := DefaultConfig()

    // 2. Deep merge project config (.sylk/config.yaml)
    if projectCfg, err := cm.loadYAML(cm.projectDirs.Config); err == nil {
        cfg = deepMerge(cfg, projectCfg)
    }

    // 3. Deep merge user config (user wins over project)
    if userCfg, err := cm.loadYAML(filepath.Join(cm.dirs.Config, "config.yaml")); err == nil {
        cfg = deepMerge(cfg, userCfg)
    }

    // 4. Deep merge project-local user overrides
    localConfig := filepath.Join(cm.projectDirs.Local, "config.yaml")
    if localCfg, err := cm.loadYAML(localConfig); err == nil {
        cfg = deepMerge(cfg, localCfg)
    }

    // 5. Apply environment variables
    cfg = cm.applyEnvironment(cfg)

    // 6. Validate against schema
    warnings := cm.validate(cfg)

    // 7. Apply defaults for invalid fields
    cfg = cm.applyDefaults(cfg, warnings)

    cm.mu.Lock()
    cm.current = cfg
    cm.mu.Unlock()

    return cfg, warnings, nil
}

func (cm *ConfigManager) applyEnvironment(cfg *Config) *Config {
    // Simple values: SYLK_LLM_TIMEOUT -> llm.timeout
    if v := os.Getenv("SYLK_LLM_TIMEOUT"); v != "" {
        if d, err := time.ParseDuration(v); err == nil {
            cfg.LLM.Timeout = d
        }
    }

    // File references: SYLK_LLM_PROVIDERS_FILE -> load and merge
    if path := os.Getenv("SYLK_LLM_PROVIDERS_FILE"); path != "" {
        if fileCfg, err := cm.loadYAML(path); err == nil {
            cfg.LLM.Providers = deepMerge(cfg.LLM.Providers, fileCfg)
        }
    }

    return cfg
}

// StartWatching starts file watch for hot reload
func (cm *ConfigManager) StartWatching(ctx context.Context) error {
    watcher, err := fsnotify.NewWatcher()
    if err != nil {
        return err
    }
    cm.watcher = watcher

    // Watch config files
    paths := []string{
        filepath.Join(cm.dirs.Config, "config.yaml"),
        cm.projectDirs.Config,
        filepath.Join(cm.projectDirs.Local, "config.yaml"),
    }

    for _, p := range paths {
        if _, err := os.Stat(p); err == nil {
            watcher.Add(filepath.Dir(p))
        }
    }

    go cm.watchLoop(ctx)
    return nil
}

func (cm *ConfigManager) watchLoop(ctx context.Context) {
    debounce := time.NewTimer(0)
    <-debounce.C // Drain initial

    for {
        select {
        case <-ctx.Done():
            return
        case event := <-cm.watcher.Events:
            if event.Op&(fsnotify.Write|fsnotify.Create) != 0 {
                debounce.Reset(500 * time.Millisecond)
            }
        case <-debounce.C:
            cm.reload()
        }
    }
}

func (cm *ConfigManager) reload() {
    cfg, warnings, err := cm.Load()
    if err != nil {
        log.Warn("Config reload failed", "error", err)
        return
    }

    if len(warnings) > 0 {
        log.Info("Config reloaded with warnings", "count", len(warnings))
    } else {
        log.Info("Config reloaded")
    }

    // Notify listeners
    for _, fn := range cm.onChange {
        fn(cfg)
    }
}
```

---

### Credential Storage

#### Resolution Order

```
┌─────────────────────────────────────────────────────────────────────────────────────┐
│                         CREDENTIAL RESOLUTION ORDER                                  │
├─────────────────────────────────────────────────────────────────────────────────────┤
│                                                                                     │
│  For each provider (anthropic, openai, google):                                     │
│                                                                                     │
│  1. ENVIRONMENT VARIABLE                                                            │
│     ├── ANTHROPIC_API_KEY                                                           │
│     ├── OPENAI_API_KEY                                                              │
│     └── GOOGLE_API_KEY                                                              │
│     Used by: CI/CD, containers, scripts                                             │
│                                                                                     │
│          │ not set                                                                  │
│          ▼                                                                          │
│                                                                                     │
│  2. SYSTEM KEYCHAIN                                                                 │
│     ├── macOS: Keychain Access (service: "sylk", account: "anthropic")              │
│     ├── Linux: Secret Service API (GNOME Keyring, KWallet)                          │
│     └── Windows: Credential Manager                                                 │
│     Used by: Interactive users (most secure)                                        │
│                                                                                     │
│          │ not available / not found                                                │
│          ▼                                                                          │
│                                                                                     │
│  3. ENCRYPTED FILE FALLBACK                                                         │
│     └── {config}/credentials.enc                                                    │
│     Used by: Headless servers, containers without keychain                          │
│                                                                                     │
└─────────────────────────────────────────────────────────────────────────────────────┘
```

#### Encryption Key Derivation (Fallback File)

```
┌─────────────────────────────────────────────────────────────────────────────────────┐
│                         ENCRYPTION KEY DERIVATION                                    │
├─────────────────────────────────────────────────────────────────────────────────────┤
│                                                                                     │
│  Machine-bound, NOT portable (by design):                                           │
│                                                                                     │
│  1. HARDWARE-BACKED (if available)                                                  │
│     ├── macOS: Secure Enclave                                                       │
│     ├── Linux: TPM 2.0                                                              │
│     └── Windows: TPM 2.0                                                            │
│                                                                                     │
│          │ not available                                                            │
│          ▼                                                                          │
│                                                                                     │
│  2. MACHINE-BOUND KEY                                                               │
│     ├── Inputs:                                                                     │
│     │   • Machine ID (/etc/machine-id, IOPlatformUUID, MachineGuid)                 │
│     │   • Install-time salt (generated once, stored in config dir)                  │
│     │   • Username (additional entropy)                                             │
│     │                                                                               │
│     ├── Derivation: Argon2id(machine_id || salt || username)                        │
│     │                                                                               │
│     └── Properties:                                                                 │
│         • No password required                                                      │
│         • NOT portable between machines (intentional)                               │
│         • User runs `sylk auth <provider>` on each machine                          │
│                                                                                     │
│  Encryption: AES-256-GCM with derived key                                           │
│                                                                                     │
└─────────────────────────────────────────────────────────────────────────────────────┘
```

#### Credential Update Flow

```
sylk auth anthropic [--profile work]
         │
         ▼
┌─────────────────┐
│ Prompt for key  │ (or --api-key flag)
└────────┬────────┘
         │
         ▼
┌─────────────────┐
│ Verify key      │ → Lightweight API call (e.g., list models)
└────────┬────────┘
         │
    ┌────┴────┐
    │         │
  Valid    Invalid
    │         │
    ▼         ▼
┌─────────┐  ┌─────────────────────────┐
│ Store   │  │ Error: "Invalid API key │
│ key     │  │ for Anthropic"          │
└─────────┘  └─────────────────────────┘
```

#### Named Profiles

```
┌─────────────────────────────────────────────────────────────────────────────────────┐
│                         NAMED CREDENTIAL PROFILES                                    │
├─────────────────────────────────────────────────────────────────────────────────────┤
│                                                                                     │
│  User-level profiles ({config}/profiles/):                                          │
│  ──────────────────────────────────────────                                         │
│                                                                                     │
│  profiles/                                                                          │
│  ├── default.yaml     # Default profile (used when no --profile specified)          │
│  ├── work.yaml        # Work account credentials                                    │
│  └── personal.yaml    # Personal account credentials                                │
│                                                                                     │
│  Profile content:                                                                   │
│  ────────────────                                                                   │
│  # work.yaml (credentials stored in keychain/encrypted file, this just names it)   │
│  name: work                                                                         │
│  providers:                                                                         │
│    anthropic: true     # Has credentials for this provider                          │
│    openai: false       # No credentials                                             │
│                                                                                     │
│  Usage:                                                                             │
│  ──────                                                                             │
│  sylk auth anthropic --profile work     # Store key in "work" profile               │
│  sylk --profile work                    # Use work profile for session              │
│  SYLK_PROFILE=work sylk                 # Environment override                      │
│                                                                                     │
│  Project-scoped profile selection:                                                  │
│  ─────────────────────────────────                                                  │
│  # .sylk/config.yaml (committed - team default)                                     │
│  profile: work                                                                      │
│                                                                                     │
│  # .sylk/local/config.yaml (gitignored - personal override)                         │
│  profile: personal                                                                  │
│                                                                                     │
└─────────────────────────────────────────────────────────────────────────────────────┘
```

#### Credential Manager Implementation

```go
type CredentialManager struct {
    dirs        *Dirs
    keychain    KeychainProvider   // Platform-specific
    encFile     *EncryptedFileStore
    profile     string
}

type KeychainProvider interface {
    Get(service, account string) (string, error)
    Set(service, account, secret string) error
    Delete(service, account string) error
    Available() bool
}

// GetAPIKey retrieves API key with fallback chain
func (cm *CredentialManager) GetAPIKey(provider string) (string, error) {
    // 1. Environment variable
    envVar := providerEnvVar(provider) // e.g., "ANTHROPIC_API_KEY"
    if key := os.Getenv(envVar); key != "" {
        return key, nil
    }

    // 2. System keychain
    if cm.keychain.Available() {
        service := fmt.Sprintf("sylk:%s", cm.profile)
        if key, err := cm.keychain.Get(service, provider); err == nil && key != "" {
            return key, nil
        }
    }

    // 3. Encrypted file fallback
    if key, err := cm.encFile.Get(cm.profile, provider); err == nil && key != "" {
        return key, nil
    }

    return "", fmt.Errorf("no credentials found for %s", provider)
}

// SetAPIKey stores API key after verification
func (cm *CredentialManager) SetAPIKey(ctx context.Context, provider, key string) error {
    // Verify key first
    if err := cm.verifyKey(ctx, provider, key); err != nil {
        return fmt.Errorf("invalid API key: %w", err)
    }

    // Store in keychain if available, else encrypted file
    if cm.keychain.Available() {
        service := fmt.Sprintf("sylk:%s", cm.profile)
        return cm.keychain.Set(service, provider, key)
    }

    return cm.encFile.Set(cm.profile, provider, key)
}

func (cm *CredentialManager) verifyKey(ctx context.Context, provider, key string) error {
    ctx, cancel := context.WithTimeout(ctx, 10*time.Second)
    defer cancel()

    switch provider {
    case "anthropic":
        // Lightweight verification call
        client := anthropic.NewClient(key)
        _, err := client.ListModels(ctx)
        return err
    case "openai":
        client := openai.NewClient(key)
        _, err := client.ListModels(ctx)
        return err
    case "google":
        // Similar verification
        return nil
    default:
        return fmt.Errorf("unknown provider: %s", provider)
    }
}

// EncryptedFileStore handles fallback encrypted storage
type EncryptedFileStore struct {
    path      string
    keyDerive func() ([]byte, error)
}

func (e *EncryptedFileStore) deriveKey() ([]byte, error) {
    // Try hardware-backed first
    if key, err := hardwareKey(); err == nil {
        return key, nil
    }

    // Fall back to machine-bound
    machineID := getMachineID()
    salt := getOrCreateSalt(e.path)
    username := os.Getenv("USER")

    return argon2.IDKey(
        []byte(machineID+username),
        salt,
        1,      // time
        64*1024, // memory (64MB)
        4,      // parallelism
        32,     // key length
    ), nil
}
```

#### Credential Broker (Secure Agent Access)

Agents never see raw credentials. The Credential Broker provides secure, audited access through opaque handles and just-in-time injection.

##### Design Principles

```
┌─────────────────────────────────────────────────────────────────────────────────────┐
│                    CREDENTIAL BROKER DESIGN PRINCIPLES                                │
├─────────────────────────────────────────────────────────────────────────────────────┤
│                                                                                     │
│  1. AGENTS NEVER SEE RAW VALUES                                                     │
│     ├── Agents reference credentials by symbolic name ("openai", "github")          │
│     ├── Broker returns opaque handles, not actual secrets                           │
│     ├── Tools receive credentials via context injection, not parameters             │
│     └── Credentials never appear in LLM-visible locations                           │
│                                                                                     │
│  2. SCOPED ACCESS                                                                   │
│     ├── Each agent type has explicit credential permissions                         │
│     ├── Permissions defined at system level, overridable per-project                │
│     ├── Unknown credentials default to DENY                                         │
│     └── Scope violations logged and rejected                                        │
│                                                                                     │
│  3. JUST-IN-TIME INJECTION                                                          │
│     ├── Credentials resolved at tool execution boundary                             │
│     ├── Tool implementation fetches from context, not parameters                    │
│     ├── Credential lifetime scoped to single tool execution                         │
│     └── No credential caching in agent memory                                       │
│                                                                                     │
│  4. COMPLETE AUDIT TRAIL                                                            │
│     ├── Every credential access logged (who, what, when, why)                       │
│     ├── Failed access attempts logged with reason                                   │
│     ├── Audit log tamper-evident (hash chain)                                       │
│     └── Queryable for security review                                               │
│                                                                                     │
└─────────────────────────────────────────────────────────────────────────────────────┘
```

##### Credential Broker Architecture

```
┌─────────────────────────────────────────────────────────────────────────────────────┐
│                         CREDENTIAL BROKER FLOW                                        │
├─────────────────────────────────────────────────────────────────────────────────────┤
│                                                                                     │
│  AGENT PERSPECTIVE (What agents see)                                                │
│  ══════════════════════════════════                                                 │
│                                                                                     │
│  Agent: "I need to call the OpenAI API to generate embeddings"                      │
│         │                                                                           │
│         ▼                                                                           │
│  ┌─────────────────────────────────────────────────────────────────────────────┐    │
│  │  tool_call: generate_embeddings                                             │    │
│  │  parameters: { text: "hello world", model: "text-embedding-3-small" }       │    │
│  │                                                                             │    │
│  │  NOTE: No API key in parameters! Agent doesn't know the key.                │    │
│  └─────────────────────────────────────────────────────────────────────────────┘    │
│                                                                                     │
│  SYSTEM PERSPECTIVE (What actually happens)                                         │
│  ══════════════════════════════════════════                                         │
│                                                                                     │
│         Tool Call                                                                   │
│             │                                                                       │
│             ▼                                                                       │
│  ┌──────────────────┐     ┌──────────────────┐     ┌──────────────────┐            │
│  │   PreTool Hook   │────►│ Credential       │────►│ Scope Check      │            │
│  │   (injection)    │     │ Broker           │     │                  │            │
│  └──────────────────┘     └──────────────────┘     └────────┬─────────┘            │
│                                                              │                      │
│                                              ┌───────────────┴───────────────┐      │
│                                              │                               │      │
│                                           Allowed                         Denied    │
│                                              │                               │      │
│                                              ▼                               ▼      │
│                                   ┌──────────────────┐           ┌──────────────┐   │
│                                   │ Inject into      │           │ Reject with  │   │
│                                   │ tool context     │           │ audit log    │   │
│                                   └────────┬─────────┘           └──────────────┘   │
│                                            │                                        │
│                                            ▼                                        │
│                                   ┌──────────────────┐                              │
│                                   │ Tool Execution   │                              │
│                                   │                  │                              │
│                                   │ key := ctx.      │                              │
│                                   │   Credential(    │                              │
│                                   │   "openai")      │                              │
│                                   └────────┬─────────┘                              │
│                                            │                                        │
│                                            ▼                                        │
│                                   ┌──────────────────┐                              │
│                                   │ PostTool Hook    │                              │
│                                   │ (clear from ctx) │                              │
│                                   └──────────────────┘                              │
│                                                                                     │
└─────────────────────────────────────────────────────────────────────────────────────┘
```

##### Agent Credential Scopes

```go
// CredentialScope defines which credentials an agent can access
type CredentialScope struct {
    AgentType   string   `yaml:"agent_type"`
    Allowed     []string `yaml:"allowed"`      // Provider names: "openai", "anthropic", etc.
    Denied      []string `yaml:"denied"`       // Explicit denials (override allowed)
    RequireAuth bool     `yaml:"require_auth"` // Require user confirmation for first use
}

// DefaultCredentialScopes defines system-wide defaults
var DefaultCredentialScopes = map[string]*CredentialScope{
    // Knowledge agents - need LLM access for embeddings/queries
    "librarian": {
        AgentType: "librarian",
        Allowed:   []string{"openai", "anthropic", "voyage"},  // Embedding providers
        Denied:    []string{"github", "aws"},                   // No repo/cloud access
    },
    "academic": {
        AgentType: "academic",
        Allowed:   []string{"openai", "anthropic", "google", "serpapi"}, // LLM + search
        Denied:    []string{"github", "aws"},
    },
    "archivalist": {
        AgentType: "archivalist",
        Allowed:   []string{"openai", "anthropic"},  // For summarization
        Denied:    []string{"github", "aws"},
    },

    // Execution agents - need LLM + potentially external services
    "engineer": {
        AgentType:   "engineer",
        Allowed:     []string{"openai", "anthropic", "github"},
        RequireAuth: true,  // Confirm before first GitHub access
    },
    "designer": {
        AgentType:   "designer",
        Allowed:     []string{"openai", "anthropic", "figma"},
        RequireAuth: true,
    },

    // Quality agents - read-only, limited scope
    "inspector": {
        AgentType: "inspector",
        Allowed:   []string{"openai", "anthropic"},
        Denied:    []string{"github", "aws", "figma"},  // No external mutations
    },
    "tester": {
        AgentType: "tester",
        Allowed:   []string{"openai", "anthropic"},
        Denied:    []string{"github", "aws"},
    },

    // Coordination agents - LLM only
    "guide": {
        AgentType: "guide",
        Allowed:   []string{"openai", "anthropic"},
        Denied:    []string{"github", "aws", "figma"},
    },
    "architect": {
        AgentType: "architect",
        Allowed:   []string{"openai", "anthropic"},
        Denied:    []string{"github", "aws"},
    },
    "orchestrator": {
        AgentType: "orchestrator",
        Allowed:   []string{},  // No direct credential access - coordinates others
        Denied:    []string{"*"},
    },
}

// ProjectCredentialOverrides allows per-project customization
// Stored in .sylk/config.yaml
type ProjectCredentialOverrides struct {
    Scopes map[string]*CredentialScope `yaml:"credential_scopes"`
}
```

##### Credential Broker Implementation

```go
// CredentialBroker manages secure credential access for agents
type CredentialBroker struct {
    manager    *CredentialManager
    scopes     map[string]*CredentialScope
    overrides  *ProjectCredentialOverrides
    auditLog   *CredentialAuditLog

    // Active handles (short-lived, scoped to tool execution)
    mu         sync.RWMutex
    handles    map[string]*CredentialHandle  // handleID -> handle
}

// CredentialHandle is an opaque reference to a credential
type CredentialHandle struct {
    ID          string        `json:"id"`
    Provider    string        `json:"provider"`
    AgentID     string        `json:"agent_id"`
    AgentType   string        `json:"agent_type"`
    ToolCallID  string        `json:"tool_call_id"`
    CreatedAt   time.Time     `json:"created_at"`
    ExpiresAt   time.Time     `json:"expires_at"`
    Used        bool          `json:"used"`

    // Internal - never serialized
    value       string
}

// HandleLifetime is how long a credential handle is valid
const HandleLifetime = 30 * time.Second

func NewCredentialBroker(manager *CredentialManager, projectOverrides *ProjectCredentialOverrides) *CredentialBroker {
    broker := &CredentialBroker{
        manager:   manager,
        scopes:    DefaultCredentialScopes,
        overrides: projectOverrides,
        auditLog:  NewCredentialAuditLog(),
        handles:   make(map[string]*CredentialHandle),
    }

    // Apply project overrides
    if projectOverrides != nil {
        for agentType, scope := range projectOverrides.Scopes {
            broker.scopes[agentType] = scope
        }
    }

    // Start handle cleanup goroutine
    go broker.cleanupExpiredHandles()

    return broker
}

// RequestCredential creates a handle for credential access
func (b *CredentialBroker) RequestCredential(
    ctx context.Context,
    agentID string,
    agentType string,
    provider string,
    toolCallID string,
    reason string,
) (*CredentialHandle, error) {

    // 1. Check scope permissions
    scope := b.getScope(agentType)
    if !b.isAllowed(scope, provider) {
        b.auditLog.LogDenied(agentID, agentType, provider, toolCallID, "scope_violation")
        return nil, &CredentialAccessDeniedError{
            AgentType: agentType,
            Provider:  provider,
            Reason:    fmt.Sprintf("agent type '%s' is not permitted to access '%s' credentials", agentType, provider),
        }
    }

    // 2. Check if user confirmation required (first-time access)
    if scope.RequireAuth && !b.hasUserAuthorization(agentType, provider) {
        b.auditLog.LogPending(agentID, agentType, provider, toolCallID, "awaiting_user_auth")
        return nil, &CredentialAuthRequiredError{
            AgentType: agentType,
            Provider:  provider,
            Message:   fmt.Sprintf("First-time access to '%s' requires user confirmation", provider),
        }
    }

    // 3. Fetch actual credential
    value, err := b.manager.GetAPIKey(provider)
    if err != nil {
        b.auditLog.LogDenied(agentID, agentType, provider, toolCallID, "credential_not_found")
        return nil, fmt.Errorf("credential not available: %w", err)
    }

    // 4. Create time-limited handle
    handle := &CredentialHandle{
        ID:         generateHandleID(),
        Provider:   provider,
        AgentID:    agentID,
        AgentType:  agentType,
        ToolCallID: toolCallID,
        CreatedAt:  time.Now(),
        ExpiresAt:  time.Now().Add(HandleLifetime),
        Used:       false,
        value:      value,
    }

    b.mu.Lock()
    b.handles[handle.ID] = handle
    b.mu.Unlock()

    // 5. Log successful grant
    b.auditLog.LogGranted(agentID, agentType, provider, toolCallID, handle.ID, reason)

    return handle, nil
}

// ResolveHandle exchanges a handle for the actual credential value
// Called by tool implementation, not by agent
func (b *CredentialBroker) ResolveHandle(handleID string) (string, error) {
    b.mu.Lock()
    defer b.mu.Unlock()

    handle, exists := b.handles[handleID]
    if !exists {
        return "", fmt.Errorf("invalid or expired credential handle")
    }

    if time.Now().After(handle.ExpiresAt) {
        delete(b.handles, handleID)
        return "", fmt.Errorf("credential handle expired")
    }

    if handle.Used {
        return "", fmt.Errorf("credential handle already used")
    }

    // Mark as used (single-use)
    handle.Used = true

    // Log resolution
    b.auditLog.LogResolved(handle.AgentID, handle.AgentType, handle.Provider, handle.ToolCallID, handleID)

    return handle.value, nil
}

// RevokeHandle invalidates a handle before expiration
func (b *CredentialBroker) RevokeHandle(handleID string) {
    b.mu.Lock()
    defer b.mu.Unlock()

    if handle, exists := b.handles[handleID]; exists {
        b.auditLog.LogRevoked(handle.AgentID, handle.AgentType, handle.Provider, handle.ToolCallID, handleID)
        delete(b.handles, handleID)
    }
}

func (b *CredentialBroker) isAllowed(scope *CredentialScope, provider string) bool {
    // Check explicit deny first
    for _, denied := range scope.Denied {
        if denied == "*" || denied == provider {
            return false
        }
    }

    // Check allow list
    for _, allowed := range scope.Allowed {
        if allowed == "*" || allowed == provider {
            return true
        }
    }

    return false  // Default deny
}

func (b *CredentialBroker) cleanupExpiredHandles() {
    ticker := time.NewTicker(10 * time.Second)
    for range ticker.C {
        b.mu.Lock()
        now := time.Now()
        for id, handle := range b.handles {
            if now.After(handle.ExpiresAt) {
                delete(b.handles, id)
            }
        }
        b.mu.Unlock()
    }
}
```

##### Just-in-Time Credential Injection

```go
// CredentialContext is injected into tool execution context
type CredentialContext struct {
    broker   *CredentialBroker
    handleID string
    provider string
    resolved bool
    value    string
}

// Credential retrieves the credential value (single use)
func (cc *CredentialContext) Credential() (string, error) {
    if cc.resolved {
        return cc.value, nil
    }

    value, err := cc.broker.ResolveHandle(cc.handleID)
    if err != nil {
        return "", err
    }

    cc.resolved = true
    cc.value = value
    return value, nil
}

// PreToolCredentialHook injects credentials before tool execution
var PreToolCredentialHook = &Hook{
    Name:     "credential_injection",
    Type:     PreTool,
    Priority: HookPriorityEarly,  // After env var isolation, before execution
    Handler: func(ctx context.Context, data *ToolCallHookData) (*ToolCallHookData, error) {
        // Check if tool requires credentials
        toolMeta := GetToolMetadata(data.ToolName)
        if toolMeta == nil || len(toolMeta.RequiredCredentials) == 0 {
            return data, nil  // Tool doesn't need credentials
        }

        broker := GetCredentialBroker(ctx)
        agentID := GetAgentID(ctx)
        agentType := GetAgentType(ctx)

        // Request handles for each required credential
        credCtxs := make(map[string]*CredentialContext)
        for _, provider := range toolMeta.RequiredCredentials {
            handle, err := broker.RequestCredential(
                ctx,
                agentID,
                agentType,
                provider,
                data.ToolCallID,
                fmt.Sprintf("Tool '%s' requires '%s' credential", data.ToolName, provider),
            )
            if err != nil {
                return nil, err
            }

            credCtxs[provider] = &CredentialContext{
                broker:   broker,
                handleID: handle.ID,
                provider: provider,
            }
        }

        // Inject into context (not tool parameters!)
        data.Context = context.WithValue(data.Context, credentialContextKey, credCtxs)

        return data, nil
    },
}

// PostToolCredentialHook cleans up after tool execution
var PostToolCredentialHook = &Hook{
    Name:     "credential_cleanup",
    Type:     PostTool,
    Priority: HookPriorityLast,
    Handler: func(ctx context.Context, data *ToolCallHookData) (*ToolCallHookData, error) {
        // Revoke any unused handles
        credCtxs, ok := data.Context.Value(credentialContextKey).(map[string]*CredentialContext)
        if !ok {
            return data, nil
        }

        broker := GetCredentialBroker(ctx)
        for _, cc := range credCtxs {
            if !cc.resolved {
                broker.RevokeHandle(cc.handleID)
            }
        }

        // Clear credential values from memory
        for _, cc := range credCtxs {
            cc.value = ""
        }

        return data, nil
    },
}
```

##### Tool Implementation Pattern

```go
// ToolMetadata declares credential requirements
type ToolMetadata struct {
    Name                string
    RequiredCredentials []string  // e.g., ["openai"] or ["github", "openai"]
}

// Tool registry with credential requirements
var ToolRegistry = map[string]*ToolMetadata{
    "generate_embeddings": {
        Name:                "generate_embeddings",
        RequiredCredentials: []string{"openai"},
    },
    "create_pr": {
        Name:                "create_pr",
        RequiredCredentials: []string{"github"},
    },
    "web_search": {
        Name:                "web_search",
        RequiredCredentials: []string{"serpapi"},
    },
    // Tools that don't need credentials
    "read_file": {
        Name:                "read_file",
        RequiredCredentials: []string{},
    },
}

// CORRECT: Tool implementation fetches from context
func (t *GenerateEmbeddingsTool) Execute(ctx context.Context, params EmbeddingsParams) (*EmbeddingsResult, error) {
    // Get credential from injected context - NEVER from params
    credCtxs := ctx.Value(credentialContextKey).(map[string]*CredentialContext)
    openaiCred := credCtxs["openai"]

    apiKey, err := openaiCred.Credential()
    if err != nil {
        return nil, fmt.Errorf("failed to get OpenAI credential: %w", err)
    }

    // Use the credential
    client := openai.NewClient(apiKey)
    resp, err := client.CreateEmbedding(ctx, openai.EmbeddingRequest{
        Model: params.Model,
        Input: params.Text,
    })

    // Credential is automatically cleaned up by PostToolCredentialHook
    return &EmbeddingsResult{Embedding: resp.Data[0].Embedding}, nil
}

// WRONG: Never accept credentials as parameters
func (t *BadTool) Execute(ctx context.Context, params struct {
    APIKey string `json:"api_key"`  // ❌ NEVER DO THIS
    Text   string `json:"text"`
}) (*Result, error) {
    // This pattern is blocked by PreToolEnvVarHook
}
```

##### Credential Audit Log

```go
// CredentialAuditLog provides tamper-evident logging of credential access
type CredentialAuditLog struct {
    mu       sync.Mutex
    entries  []*CredentialAuditEntry
    hashChain string  // Running hash for tamper detection
    storage  AuditStorage
}

type CredentialAuditEntry struct {
    ID          string                 `json:"id"`
    Timestamp   time.Time              `json:"timestamp"`
    Action      CredentialAuditAction  `json:"action"`
    AgentID     string                 `json:"agent_id"`
    AgentType   string                 `json:"agent_type"`
    Provider    string                 `json:"provider"`
    ToolCallID  string                 `json:"tool_call_id,omitempty"`
    HandleID    string                 `json:"handle_id,omitempty"`
    Reason      string                 `json:"reason,omitempty"`
    PrevHash    string                 `json:"prev_hash"`
    EntryHash   string                 `json:"entry_hash"`
}

type CredentialAuditAction string

const (
    AuditActionGranted  CredentialAuditAction = "granted"   // Handle created
    AuditActionDenied   CredentialAuditAction = "denied"    // Access rejected
    AuditActionResolved CredentialAuditAction = "resolved"  // Handle exchanged for value
    AuditActionRevoked  CredentialAuditAction = "revoked"   // Handle invalidated
    AuditActionPending  CredentialAuditAction = "pending"   // Awaiting user auth
    AuditActionExpired  CredentialAuditAction = "expired"   // Handle timed out
)

func (l *CredentialAuditLog) log(action CredentialAuditAction, agentID, agentType, provider, toolCallID, handleID, reason string) {
    l.mu.Lock()
    defer l.mu.Unlock()

    entry := &CredentialAuditEntry{
        ID:         generateAuditID(),
        Timestamp:  time.Now().UTC(),
        Action:     action,
        AgentID:    agentID,
        AgentType:  agentType,
        Provider:   provider,
        ToolCallID: toolCallID,
        HandleID:   handleID,
        Reason:     reason,
        PrevHash:   l.hashChain,
    }

    // Compute entry hash for tamper detection
    entry.EntryHash = l.computeHash(entry)
    l.hashChain = entry.EntryHash

    l.entries = append(l.entries, entry)
    l.storage.Append(entry)
}

func (l *CredentialAuditLog) computeHash(entry *CredentialAuditEntry) string {
    data := fmt.Sprintf("%s|%s|%s|%s|%s|%s|%s|%s|%s",
        entry.ID,
        entry.Timestamp.Format(time.RFC3339Nano),
        entry.Action,
        entry.AgentID,
        entry.AgentType,
        entry.Provider,
        entry.ToolCallID,
        entry.HandleID,
        entry.PrevHash,
    )
    hash := sha256.Sum256([]byte(data))
    return hex.EncodeToString(hash[:])
}

// Verify checks the integrity of the audit log
func (l *CredentialAuditLog) Verify() error {
    l.mu.Lock()
    defer l.mu.Unlock()

    prevHash := ""
    for i, entry := range l.entries {
        if entry.PrevHash != prevHash {
            return fmt.Errorf("hash chain broken at entry %d", i)
        }

        expectedHash := l.computeHash(entry)
        if entry.EntryHash != expectedHash {
            return fmt.Errorf("entry %d has been tampered with", i)
        }

        prevHash = entry.EntryHash
    }

    return nil
}

// Query returns audit entries matching filters
func (l *CredentialAuditLog) Query(filters AuditFilters) []*CredentialAuditEntry {
    l.mu.Lock()
    defer l.mu.Unlock()

    var results []*CredentialAuditEntry
    for _, entry := range l.entries {
        if filters.Match(entry) {
            results = append(results, entry)
        }
    }
    return results
}

type AuditFilters struct {
    AgentType *string
    Provider  *string
    Action    *CredentialAuditAction
    Since     *time.Time
    Until     *time.Time
}

func (f *AuditFilters) Match(entry *CredentialAuditEntry) bool {
    if f.AgentType != nil && entry.AgentType != *f.AgentType {
        return false
    }
    if f.Provider != nil && entry.Provider != *f.Provider {
        return false
    }
    if f.Action != nil && entry.Action != *f.Action {
        return false
    }
    if f.Since != nil && entry.Timestamp.Before(*f.Since) {
        return false
    }
    if f.Until != nil && entry.Timestamp.After(*f.Until) {
        return false
    }
    return true
}
```

##### User Authorization Flow

```go
// UserAuthorizationManager handles first-time credential access confirmations
type UserAuthorizationManager struct {
    mu            sync.RWMutex
    authorizations map[string]bool  // "agentType:provider" -> authorized
    storage       AuthStorage
}

// RequestAuthorization prompts user to approve first-time access
func (m *UserAuthorizationManager) RequestAuthorization(
    ctx context.Context,
    agentType string,
    provider string,
) error {
    key := fmt.Sprintf("%s:%s", agentType, provider)

    m.mu.RLock()
    if m.authorizations[key] {
        m.mu.RUnlock()
        return nil  // Already authorized
    }
    m.mu.RUnlock()

    // Prompt user
    guide := GetGuide(ctx)
    approved, err := guide.PromptUserAuthorization(ctx, UserAuthRequest{
        Message: fmt.Sprintf(
            "Agent '%s' is requesting access to '%s' credentials for the first time.\n"+
            "This will allow the agent to make API calls using your %s account.\n\n"+
            "Allow this access?",
            agentType, provider, provider,
        ),
        AgentType: agentType,
        Provider:  provider,
        Options:   []string{"Allow", "Allow for this session only", "Deny"},
    })
    if err != nil {
        return err
    }

    switch approved {
    case "Allow":
        m.mu.Lock()
        m.authorizations[key] = true
        m.storage.Save(key, true)  // Persist
        m.mu.Unlock()
        return nil

    case "Allow for this session only":
        m.mu.Lock()
        m.authorizations[key] = true  // Don't persist
        m.mu.Unlock()
        return nil

    default:
        return &CredentialAccessDeniedError{
            AgentType: agentType,
            Provider:  provider,
            Reason:    "user denied access",
        }
    }
}
```

##### Complete Flow Example

```
┌─────────────────────────────────────────────────────────────────────────────────────┐
│                    COMPLETE CREDENTIAL ACCESS FLOW EXAMPLE                            │
├─────────────────────────────────────────────────────────────────────────────────────┤
│                                                                                     │
│  SCENARIO: Engineer agent needs to create a GitHub PR                               │
│                                                                                     │
│  1. AGENT MAKES TOOL CALL                                                           │
│     ────────────────────────                                                        │
│     Engineer: "Create a PR with these changes"                                      │
│     Tool call: create_pr(title: "Fix bug", body: "...", branch: "fix-123")          │
│                                                                                     │
│     NOTE: No GitHub token in parameters!                                            │
│                                                                                     │
│  2. PRE-TOOL HOOK RUNS                                                              │
│     ─────────────────────                                                           │
│     PreToolCredentialHook:                                                          │
│       ├── Looks up tool metadata: create_pr requires ["github"]                     │
│       ├── Calls broker.RequestCredential("engineer", "github", toolCallID)          │
│       │                                                                             │
│       │   Broker:                                                                   │
│       │     ├── Check scope: engineer allowed github? YES                           │
│       │     ├── Check auth: first time? YES → prompt user                           │
│       │     │                                                                       │
│       │     │   User sees: "Engineer requesting GitHub access. Allow?"              │
│       │     │   User clicks: "Allow"                                                │
│       │     │                                                                       │
│       │     ├── Fetch credential from CredentialManager                             │
│       │     ├── Create handle (expires in 30s)                                      │
│       │     ├── Log: GRANTED engineer→github handle=abc123                          │
│       │     └── Return handle                                                       │
│       │                                                                             │
│       └── Inject CredentialContext into tool context                                │
│                                                                                     │
│  3. TOOL EXECUTES                                                                   │
│     ─────────────────                                                               │
│     create_pr tool:                                                                 │
│       ├── credCtx := ctx.Value(credentialContextKey)["github"]                      │
│       ├── token, _ := credCtx.Credential()  // Resolves handle                      │
│       │                                                                             │
│       │   Broker:                                                                   │
│       │     ├── Validate handle exists, not expired, not used                       │
│       │     ├── Mark handle as used                                                 │
│       │     ├── Log: RESOLVED handle=abc123                                         │
│       │     └── Return actual token value                                           │
│       │                                                                             │
│       ├── client := github.NewClient(token)                                         │
│       ├── client.CreatePullRequest(...)                                             │
│       └── Return result                                                             │
│                                                                                     │
│  4. POST-TOOL HOOK RUNS                                                             │
│     ──────────────────────                                                          │
│     PostToolCredentialHook:                                                         │
│       ├── Check for unused handles → revoke                                         │
│       └── Clear credential values from memory                                       │
│                                                                                     │
│  5. RESULT RETURNED TO AGENT                                                        │
│     ──────────────────────────                                                      │
│     Agent receives: { "pr_url": "https://github.com/...", "pr_number": 42 }         │
│                                                                                     │
│     NOTE: Agent never saw the GitHub token!                                         │
│                                                                                     │
│  AUDIT LOG ENTRIES:                                                                 │
│  ──────────────────                                                                 │
│  [2024-01-15T10:30:00Z] GRANTED  engineer github tool=create_pr handle=abc123       │
│  [2024-01-15T10:30:01Z] RESOLVED engineer github tool=create_pr handle=abc123       │
│                                                                                     │
└─────────────────────────────────────────────────────────────────────────────────────┘
```

##### Integration with Secret Management

The Credential Broker integrates with the Secret Management system (see Secret Management section):

1. **PreToolEnvVarHook** blocks any attempt to pass credentials as parameters
2. **PostToolSecretHook** redacts any credentials that leak into tool output
3. **InterAgentSecretHook** redacts credentials in agent-to-agent messages
4. **PrePromptSecretHook** rejects user prompts containing credentials

```go
// Integration verification
func VerifySecurityIntegration() error {
    // Ensure all credential-requiring tools are registered
    for toolName, meta := range ToolRegistry {
        if len(meta.RequiredCredentials) > 0 {
            // Tool must NOT have credential-like parameters
            if toolHasCredentialParams(toolName) {
                return fmt.Errorf("tool %s has credential parameters but should use injection", toolName)
            }
        }
    }

    // Ensure all hooks are registered in correct order
    hooks := GetRegisteredHooks()

    // env_var_isolation must run before credential_injection
    if hooks.IndexOf("env_var_isolation") > hooks.IndexOf("credential_injection") {
        return fmt.Errorf("env_var_isolation must run before credential_injection")
    }

    // credential_cleanup must run before secret_redaction_output
    if hooks.IndexOf("credential_cleanup") > hooks.IndexOf("secret_redaction_output") {
        return fmt.Errorf("credential_cleanup must run before secret_redaction_output")
    }

    return nil
}
```

---

### Database Management

#### Database Organization

```
┌─────────────────────────────────────────────────────────────────────────────────────┐
│                         DATABASE ORGANIZATION                                        │
├─────────────────────────────────────────────────────────────────────────────────────┤
│                                                                                     │
│  data/shared/                                                                       │
│  └── system.db                       # GLOBAL across all projects                   │
│      ├── sessions                    # Session registry                             │
│      ├── subscription_usage          # Global rate limit tracking                   │
│      └── circuit_breakers            # Global circuit breaker state                 │
│                                                                                     │
│  data/projects/{project_hash}/                                                      │
│  ├── knowledge.db                    # PER-PROJECT knowledge base                   │
│  │   ├── query_cache                 # Cached query responses                       │
│  │   ├── failure_patterns            # Error patterns + resolutions                 │
│  │   ├── learned_patterns            # LLM-learned patterns                         │
│  │   └── embeddings                  # Query embeddings for similarity              │
│  │                                                                                  │
│  └── index.db                        # PER-PROJECT VectorGraphDB                    │
│      ├── nodes                       # Code, history, academic nodes                │
│      ├── edges                       # Relationships between nodes                  │
│      └── vectors                     # Embeddings as BLOBs                          │
│                                                                                     │
│  data/sessions/{id}/                                                                │
│  └── session.db                      # PER-SESSION state                            │
│      ├── context                     # Session context/preferences                  │
│      └── routing_cache               # Guide routing cache                          │
│                                                                                     │
│  In-memory (rebuilt on startup):                                                    │
│  ├── HNSW index                      # Built from vectors table                     │
│  ├── XOR filters                     # Built from VectorGraphDB nodes               │
│  └── Pending bloom filters           # Recent additions before XOR rebuild          │
│                                                                                     │
└─────────────────────────────────────────────────────────────────────────────────────┘
```

#### Migration Strategy

```
┌─────────────────────────────────────────────────────────────────────────────────────┐
│                         EMBEDDED SCHEMA VERSIONING                                   │
├─────────────────────────────────────────────────────────────────────────────────────┤
│                                                                                     │
│  On startup (per database):                                                         │
│                                                                                     │
│  1. Read PRAGMA user_version                                                        │
│  2. Compare to expected version in code                                             │
│  3. If current < expected:                                                          │
│                                                                                     │
│     For each pending migration:                                                     │
│     ┌─────────────────────────────────────────────────────────────────────┐         │
│     │                                                                     │         │
│     │  Additive change?                                                   │         │
│     │  (new table, new column, new index)                                 │         │
│     │       │                                                             │         │
│     │   ┌───┴───┐                                                         │         │
│     │   │       │                                                         │         │
│     │  YES     NO (destructive)                                           │         │
│     │   │       │                                                         │         │
│     │   ▼       ▼                                                         │         │
│     │ Apply   Backup DB first                                             │         │
│     │ directly  │                                                         │         │
│     │   │       ▼                                                         │         │
│     │   │     Rebuild table / apply change                                │         │
│     │   │       │                                                         │         │
│     │   └───────┴──────────────────────────────────────────────┐          │         │
│     │                                                          │          │         │
│     │                                                          ▼          │         │
│     │                                             Update PRAGMA user_version        │
│     │                                                                     │         │
│     └─────────────────────────────────────────────────────────────────────┘         │
│                                                                                     │
│  4. Rebuild in-memory structures (HNSW, XOR filters)                                │
│                                                                                     │
└─────────────────────────────────────────────────────────────────────────────────────┘
```

#### Migration Implementation

```go
type Migration struct {
    Version     int
    Description string
    Destructive bool
    Up          func(tx *sql.Tx) error
}

type MigrationManager struct {
    db         *sql.DB
    dbPath     string
    migrations []Migration
    backupDir  string
}

func (m *MigrationManager) Migrate(ctx context.Context) error {
    currentVersion, _ := m.getCurrentVersion()

    for _, migration := range m.migrations {
        if migration.Version <= currentVersion {
            continue
        }

        // Backup before destructive migrations
        if migration.Destructive {
            if err := m.backupDatabase(); err != nil {
                return fmt.Errorf("backup failed before migration %d: %w",
                    migration.Version, err)
            }
        }

        // Apply migration in transaction
        tx, err := m.db.BeginTx(ctx, nil)
        if err != nil {
            return err
        }

        if err := migration.Up(tx); err != nil {
            tx.Rollback()
            return fmt.Errorf("migration %d failed: %w", migration.Version, err)
        }

        // Update version
        if _, err := tx.Exec(fmt.Sprintf("PRAGMA user_version = %d", migration.Version)); err != nil {
            tx.Rollback()
            return err
        }

        if err := tx.Commit(); err != nil {
            return err
        }

        log.Info("Applied migration", "version", migration.Version,
            "description", migration.Description)
    }

    return nil
}

// Example migrations
var SystemDBMigrations = []Migration{
    {
        Version:     1,
        Description: "Initial schema",
        Destructive: false,
        Up: func(tx *sql.Tx) error {
            _, err := tx.Exec(`
                CREATE TABLE sessions (
                    id TEXT PRIMARY KEY,
                    created_at TIMESTAMP,
                    last_active TIMESTAMP,
                    wal_dir TEXT,
                    recovered BOOLEAN DEFAULT FALSE
                );

                CREATE TABLE subscription_usage (
                    provider TEXT,
                    period_start DATE,
                    period_end DATE,
                    requests_used INTEGER DEFAULT 0,
                    tokens_used INTEGER DEFAULT 0,
                    PRIMARY KEY (provider, period_start)
                );

                CREATE TABLE circuit_breakers (
                    resource_id TEXT PRIMARY KEY,
                    state INTEGER,
                    failures INTEGER,
                    last_failure TIMESTAMP,
                    last_state_change TIMESTAMP,
                    cooldown_ends TIMESTAMP
                );
            `)
            return err
        },
    },
    {
        Version:     2,
        Description: "Add session activity tracking",
        Destructive: false,
        Up: func(tx *sql.Tx) error {
            _, err := tx.Exec(`
                ALTER TABLE sessions ADD COLUMN activity_score REAL DEFAULT 0;
                ALTER TABLE sessions ADD COLUMN running_pipelines INTEGER DEFAULT 0;
            `)
            return err
        },
    },
}
```

#### Backup Strategy

```
┌─────────────────────────────────────────────────────────────────────────────────────┐
│                         BACKUP STRATEGY                                              │
├─────────────────────────────────────────────────────────────────────────────────────┤
│                                                                                     │
│  Backup triggers:                                                                   │
│  ────────────────                                                                   │
│  1. PERIODIC         │ Configurable interval (default: 24h)                         │
│  2. PRE-MIGRATION    │ Before any destructive schema change                         │
│  3. PRE-SESSION      │ Before each new session starts                               │
│                                                                                     │
│  Backup location:                                                                   │
│  ────────────────                                                                   │
│  {data}/backups/                                                                    │
│  ├── system/                                                                        │
│  │   ├── system_20250116_100000.db                                                  │
│  │   ├── system_20250115_100000.db                                                  │
│  │   └── ...                                                                        │
│  └── projects/{hash}/                                                               │
│      ├── knowledge_20250116_100000.db                                               │
│      ├── index_20250116_100000.db                                                   │
│      └── ...                                                                        │
│                                                                                     │
│  Retention:                                                                         │
│  ──────────                                                                         │
│  • Keep last N backups (configurable, default: 10)                                  │
│  • Automatic cleanup of older backups                                               │
│                                                                                     │
│  Backup method:                                                                     │
│  ──────────────                                                                     │
│  • SQLite Online Backup API (consistent snapshot)                                   │
│  • Non-blocking (doesn't lock DB during backup)                                     │
│                                                                                     │
└─────────────────────────────────────────────────────────────────────────────────────┘
```

```go
type BackupManager struct {
    dataDir    string
    backupDir  string
    config     BackupConfig
    scheduler  *time.Ticker
}

type BackupConfig struct {
    PeriodicEnabled   bool          `yaml:"periodic_enabled"`
    PeriodicInterval  time.Duration `yaml:"periodic_interval"`
    PreSession        bool          `yaml:"pre_session"`
    PreMigration      bool          `yaml:"pre_migration"`
    RetentionCount    int           `yaml:"retention_count"`
}

func (b *BackupManager) BackupDatabase(dbPath string) (string, error) {
    // Generate backup path
    dbName := filepath.Base(dbPath)
    dbName = strings.TrimSuffix(dbName, ".db")
    timestamp := time.Now().Format("20060102_150405")
    backupPath := filepath.Join(b.backupDir, fmt.Sprintf("%s_%s.db", dbName, timestamp))

    // Use SQLite Online Backup API
    srcDB, err := sql.Open("sqlite3", dbPath)
    if err != nil {
        return "", err
    }
    defer srcDB.Close()

    dstDB, err := sql.Open("sqlite3", backupPath)
    if err != nil {
        return "", err
    }
    defer dstDB.Close()

    // Perform backup using SQLite backup API
    if err := sqliteBackup(srcDB, dstDB); err != nil {
        os.Remove(backupPath)
        return "", err
    }

    // Cleanup old backups
    b.cleanupOldBackups(dbName)

    return backupPath, nil
}

func (b *BackupManager) cleanupOldBackups(dbName string) {
    pattern := filepath.Join(b.backupDir, dbName+"_*.db")
    matches, _ := filepath.Glob(pattern)

    if len(matches) <= b.config.RetentionCount {
        return
    }

    // Sort by modification time (oldest first)
    sort.Slice(matches, func(i, j int) bool {
        fi, _ := os.Stat(matches[i])
        fj, _ := os.Stat(matches[j])
        return fi.ModTime().Before(fj.ModTime())
    })

    // Remove oldest
    toRemove := len(matches) - b.config.RetentionCount
    for i := 0; i < toRemove; i++ {
        os.Remove(matches[i])
    }
}
```

#### Corruption Detection & Recovery

```
┌─────────────────────────────────────────────────────────────────────────────────────┐
│                         CORRUPTION DETECTION & RECOVERY                              │
├─────────────────────────────────────────────────────────────────────────────────────┤
│                                                                                     │
│  Detection:                                                                         │
│  ──────────                                                                         │
│  1. ON STARTUP    │ Full PRAGMA integrity_check                                     │
│  2. PERIODIC      │ Quick check every 5 minutes (configurable)                      │
│  3. ON ERROR      │ Check after unexpected SQL errors                               │
│                                                                                     │
│  On corruption detected:                                                            │
│  ───────────────────────                                                            │
│                                                                                     │
│  ┌─────────────────────────────────────────────────────────────────────────┐        │
│  │ 1. Log corruption details                                               │        │
│  │ 2. Notify user: "Database corruption detected in {db}"                  │        │
│  │ 3. Attempt auto-restore:                                                │        │
│  │                                                                         │        │
│  │    Backup available?                                                    │        │
│  │         │                                                               │        │
│  │    ┌────┴────┐                                                          │        │
│  │   YES       NO                                                          │        │
│  │    │         │                                                          │        │
│  │    ▼         ▼                                                          │        │
│  │  Restore   Rebuildable?                                                 │        │
│  │  from      (index.db, knowledge.db)                                     │        │
│  │  backup         │                                                       │        │
│  │    │       ┌────┴────┐                                                  │        │
│  │    │      YES       NO                                                  │        │
│  │    │       │         │                                                  │        │
│  │    │       ▼         ▼                                                  │        │
│  │    │    Rebuild   Fatal error                                           │        │
│  │    │    from      "Cannot recover {db}"                                 │        │
│  │    │    source                                                          │        │
│  │    │       │                                                            │        │
│  │    └───────┴────────────────────────────────────────────┐               │        │
│  │                                                         │               │        │
│  │                                                         ▼               │        │
│  │                                              Verify restored/rebuilt    │        │
│  │                                              Continue operation         │        │
│  │                                                                         │        │
│  └─────────────────────────────────────────────────────────────────────────┘        │
│                                                                                     │
│  Rebuildable databases:                                                             │
│  ──────────────────────                                                             │
│  • index.db      → Re-index codebase (Librarian)                                    │
│  • knowledge.db  → Rebuild from session logs + re-learn patterns                    │
│                                                                                     │
│  Non-rebuildable (require backup):                                                  │
│  ────────────────────────────────                                                   │
│  • system.db     → Session registry, subscription tracking                          │
│  • session.db    → Session-specific state                                           │
│                                                                                     │
└─────────────────────────────────────────────────────────────────────────────────────┘
```

#### Concurrent Access

```
┌─────────────────────────────────────────────────────────────────────────────────────┐
│                         CONCURRENT DATABASE ACCESS                                   │
├─────────────────────────────────────────────────────────────────────────────────────┤
│                                                                                     │
│  SQLite Configuration:                                                              │
│  ─────────────────────                                                              │
│  PRAGMA journal_mode = WAL;        -- Write-ahead logging                           │
│  PRAGMA busy_timeout = 30000;      -- 30s busy timeout (configurable)               │
│  PRAGMA synchronous = NORMAL;      -- Balance durability/performance                │
│  PRAGMA cache_size = -64000;       -- 64MB cache                                    │
│  PRAGMA foreign_keys = ON;         -- Enforce referential integrity                 │
│                                                                                     │
│  Connection Pooling:                                                                │
│  ──────────────────                                                                 │
│  • Per-database connection pool                                                     │
│  • Read connections: unlimited (WAL allows concurrent reads)                        │
│  • Write connections: 1 per database (SQLite limitation)                            │
│  • Configurable pool size                                                           │
│                                                                                     │
│  Advisory Locks (cross-process):                                                    │
│  ───────────────────────────────                                                    │
│  For critical operations that need exclusive access:                                │
│  • Database migrations                                                              │
│  • HNSW index rebuild                                                               │
│  • XOR filter rebuild                                                               │
│  • Backup operations                                                                │
│                                                                                     │
│  Lock files: {state}/locks/{db_name}.lock                                           │
│                                                                                     │
│  Implementation: flock (Unix) / LockFileEx (Windows)                                │
│                                                                                     │
└─────────────────────────────────────────────────────────────────────────────────────┘
```

```go
type DatabaseManager struct {
    pools     map[string]*ConnectionPool
    lockDir   string
    mu        sync.RWMutex
}

type ConnectionPool struct {
    db        *sql.DB
    path      string
    lockFile  string
    config    PoolConfig
}

type PoolConfig struct {
    MaxOpenConns    int           `yaml:"max_open_conns"`
    MaxIdleConns    int           `yaml:"max_idle_conns"`
    ConnMaxLifetime time.Duration `yaml:"conn_max_lifetime"`
    BusyTimeout     time.Duration `yaml:"busy_timeout"`
}

func (dm *DatabaseManager) Open(dbPath string, config PoolConfig) (*ConnectionPool, error) {
    dm.mu.Lock()
    defer dm.mu.Unlock()

    if pool, ok := dm.pools[dbPath]; ok {
        return pool, nil
    }

    // Open with WAL mode
    dsn := fmt.Sprintf("%s?_journal_mode=WAL&_busy_timeout=%d&_synchronous=NORMAL",
        dbPath, config.BusyTimeout.Milliseconds())

    db, err := sql.Open("sqlite3", dsn)
    if err != nil {
        return nil, err
    }

    // Configure pool
    db.SetMaxOpenConns(config.MaxOpenConns)
    db.SetMaxIdleConns(config.MaxIdleConns)
    db.SetConnMaxLifetime(config.ConnMaxLifetime)

    // Initialize pragmas
    if _, err := db.Exec(`
        PRAGMA foreign_keys = ON;
        PRAGMA cache_size = -64000;
    `); err != nil {
        db.Close()
        return nil, err
    }

    pool := &ConnectionPool{
        db:       db,
        path:     dbPath,
        lockFile: filepath.Join(dm.lockDir, filepath.Base(dbPath)+".lock"),
        config:   config,
    }

    dm.pools[dbPath] = pool
    return pool, nil
}

// AcquireExclusiveLock gets cross-process lock for critical operations
func (p *ConnectionPool) AcquireExclusiveLock(ctx context.Context) (func(), error) {
    f, err := os.OpenFile(p.lockFile, os.O_CREATE|os.O_RDWR, 0600)
    if err != nil {
        return nil, err
    }

    // Try to acquire lock with context timeout
    done := make(chan error, 1)
    go func() {
        done <- syscall.Flock(int(f.Fd()), syscall.LOCK_EX)
    }()

    select {
    case err := <-done:
        if err != nil {
            f.Close()
            return nil, err
        }
    case <-ctx.Done():
        f.Close()
        return nil, ctx.Err()
    }

    // Return release function
    return func() {
        syscall.Flock(int(f.Fd()), syscall.LOCK_UN)
        f.Close()
    }, nil
}

// IntegrityCheck runs periodic integrity verification
func (p *ConnectionPool) IntegrityCheck(ctx context.Context, quick bool) error {
    var query string
    if quick {
        query = "PRAGMA quick_check"
    } else {
        query = "PRAGMA integrity_check"
    }

    rows, err := p.db.QueryContext(ctx, query)
    if err != nil {
        return err
    }
    defer rows.Close()

    var result string
    for rows.Next() {
        if err := rows.Scan(&result); err != nil {
            return err
        }
        if result != "ok" {
            return fmt.Errorf("integrity check failed: %s", result)
        }
    }

    return nil
}
```

### Storage Configuration

```yaml
# ~/.sylk/config.yaml - Storage configuration

storage:
  # Database settings
  database:
    busy_timeout: 30s
    max_open_conns: 10
    max_idle_conns: 5
    conn_max_lifetime: 1h

  # Backup settings
  backup:
    periodic:
      enabled: true
      interval: 24h
    pre_session: true
    pre_migration: true
    retention_count: 10

  # Integrity check settings
  integrity:
    startup_check: true          # Full check on startup
    periodic_check: true         # Quick checks during runtime
    periodic_interval: 5m

  # Cache TTL tiers
  cache:
    hot_ttl: 1h                  # Never auto-evict within TTL
    warm_ttl: 24h                # Evict under pressure
    cold_ttl: 168h               # 7 days, evict first

credentials:
  # Profile to use (can be overridden by SYLK_PROFILE)
  profile: default

  # Fallback encryption (when keychain unavailable)
  encryption:
    algorithm: aes-256-gcm
    key_derivation: argon2id
    argon2_time: 1
    argon2_memory: 65536         # 64MB
    argon2_parallelism: 4
```

---

## Security Model

The security model provides defense-in-depth through layered isolation, permission boundaries, and comprehensive audit logging. Security is designed to be robust without impeding developer workflow.

### Design Philosophy

```
┌─────────────────────────────────────────────────────────────────────────────────────┐
│                           SECURITY DESIGN PRINCIPLES                                 │
├─────────────────────────────────────────────────────────────────────────────────────┤
│                                                                                     │
│  1. DEFENSE IN DEPTH                                                                │
│     ├── Multiple independent layers of protection                                   │
│     ├── Failure of one layer doesn't compromise system                              │
│     └── Each layer assumes others may fail                                          │
│                                                                                     │
│  2. LEAST PRIVILEGE                                                                 │
│     ├── Agents receive minimum permissions needed                                   │
│     ├── Permissions granted just-in-time                                            │
│     └── Escalation requires explicit approval                                       │
│                                                                                     │
│  3. USER SOVEREIGNTY                                                                │
│     ├── User has final authority over all permissions                               │
│     ├── Permissions adjustable at any time                                          │
│     └── Clear visibility into what agents can do                                    │
│                                                                                     │
│  4. FAIL-SECURE                                                                     │
│     ├── Deny by default                                                             │
│     ├── Ambiguous situations → deny + ask                                           │
│     └── Security failures → graceful degradation                                    │
│                                                                                     │
│  5. TRANSPARENCY                                                                    │
│     ├── All security-relevant actions logged                                        │
│     ├── Tamper-evident audit trail                                                  │
│     └── User can query and review at any time                                       │
│                                                                                     │
└─────────────────────────────────────────────────────────────────────────────────────┘
```

### Agent Permission Boundaries

Agents operate within role-based permission boundaries with explicit escalation paths.

#### Role Hierarchy

```
┌─────────────────────────────────────────────────────────────────────────────────────┐
│                              AGENT ROLE HIERARCHY                                    │
├─────────────────────────────────────────────────────────────────────────────────────┤
│                                                                                     │
│  ADMIN (highest)                                                                    │
│  │  ├── Full system access                                                          │
│  │  ├── Credential management                                                       │
│  │  ├── Configuration changes                                                       │
│  │  └── Security policy modification                                                │
│  │                                                                                  │
│  ORCHESTRATOR                                                                       │
│  │  ├── Pipeline creation and management                                            │
│  │  ├── Cross-agent coordination                                                    │
│  │  ├── Resource allocation decisions                                               │
│  │  └── Workflow-level permissions                                                  │
│  │                                                                                  │
│  SUPERVISOR                                                                         │
│  │  ├── Agent spawning and termination                                              │
│  │  ├── Task delegation                                                             │
│  │  ├── Result validation                                                           │
│  │  └── Error escalation                                                            │
│  │                                                                                  │
│  WORKER                                                                             │
│  │  ├── File read/write within scope                                                │
│  │  ├── Process execution (allowlisted)                                             │
│  │  ├── Network requests (allowlisted domains)                                      │
│  │  └── Tool invocation                                                             │
│  │                                                                                  │
│  OBSERVER + KNOWLEDGE                                                               │
│  │  ├── All Observer permissions                                                    │
│  │  ├── Knowledge base write access                                                 │
│  │  └── Cross-session knowledge sharing                                             │
│  │                                                                                  │
│  OBSERVER (lowest)                                                                  │
│     ├── Read-only file access                                                       │
│     ├── Query execution (read-only)                                                 │
│     └── No external side effects                                                    │
│                                                                                     │
└─────────────────────────────────────────────────────────────────────────────────────┘
```

#### Agent Role Assignments

```go
// AgentRole defines the permission level for an agent
type AgentRole string

const (
    RoleObserver           AgentRole = "observer"
    RoleObserverKnowledge  AgentRole = "observer_knowledge"
    RoleWorker             AgentRole = "worker"
    RoleSupervisor         AgentRole = "supervisor"
    RoleOrchestrator       AgentRole = "orchestrator"
    RoleAdmin              AgentRole = "admin"
)

// DefaultAgentRoles maps agent types to their default roles
var DefaultAgentRoles = map[string]AgentRole{
    "guide":       RoleSupervisor,       // Coordinates user interaction
    "architect":   RoleSupervisor,       // Plans workflows
    "academic":    RoleObserver,         // Research only
    "librarian":   RoleObserverKnowledge,// Reads + indexes knowledge
    "archivalist": RoleObserverKnowledge,// Reads + stores knowledge
    "specialist":  RoleWorker,           // Executes tasks
    "critic":      RoleObserver,         // Reviews only
    "scribe":      RoleWorker,           // Writes documentation
}
```

#### Permission Enforcement

```go
// PermissionManager enforces agent permission boundaries
type PermissionManager struct {
    mu sync.RWMutex

    projectPath    string
    projectID      string

    // Per-project persistent allowlists
    commandAllowlist map[string]bool      // command name → allowed (ignores args)
    domainAllowlist  map[string]bool      // domain → allowed
    pathAllowlist    map[string]PathPerm  // path pattern → permission level

    // Safe defaults (always allowed without prompting)
    safeCommands     map[string]bool
    safeDomains      map[string]bool
    safePathPatterns []string

    // Audit logger
    auditLog *AuditLogger
}

type PathPerm struct {
    Read   bool
    Write  bool
    Delete bool
}

// Permission file location: .sylk/local/permissions.yaml (gitignored)
type ProjectPermissions struct {
    Commands []string          `yaml:"commands"`        // Allowed commands
    Domains  []string          `yaml:"domains"`         // Allowed network domains
    Paths    map[string]string `yaml:"paths"`           // path → "read"|"write"|"delete"

    // Special file handling
    AllowModifyEnv       bool `yaml:"allow_modify_env"`       // .env modification
    AllowModifyGitignore bool `yaml:"allow_modify_gitignore"` // .gitignore modification
}

func DefaultSafeCommands() map[string]bool {
    return map[string]bool{
        // Read-only inspection
        "ls": true, "cat": true, "head": true, "tail": true,
        "find": true, "grep": true, "rg": true, "fd": true,
        "wc": true, "file": true, "stat": true, "which": true,

        // Version control (read operations)
        "git status": true, "git diff": true, "git log": true,
        "git show": true, "git branch": true,

        // Language tools (analysis)
        "go vet": true, "go fmt": true, "gofmt": true,
        "npm list": true, "npm outdated": true,
        "python -m py_compile": true,

        // Build tools (typically safe)
        "make": true, "go build": true, "go test": true,
        "npm test": true, "npm run build": true,
        "cargo build": true, "cargo test": true,
    }
}

func DefaultSafeDomains() map[string]bool {
    return map[string]bool{
        // Package registries
        "pkg.go.dev": true, "proxy.golang.org": true,
        "npmjs.com": true, "registry.npmjs.org": true,
        "pypi.org": true, "crates.io": true,

        // Documentation
        "golang.org": true, "go.dev": true,
        "docs.python.org": true, "nodejs.org": true,
        "developer.mozilla.org": true,

        // Version control
        "github.com": true, "gitlab.com": true, "bitbucket.org": true,
    }
}
```

#### Permission Checking Flow

```
┌─────────────────────────────────────────────────────────────────────────────────────┐
│                           PERMISSION CHECK FLOW                                      │
├─────────────────────────────────────────────────────────────────────────────────────┤
│                                                                                     │
│  Agent requests action                                                              │
│         │                                                                           │
│         ▼                                                                           │
│  ┌──────────────────┐                                                               │
│  │ Check agent role │                                                               │
│  │ has capability   │                                                               │
│  └────────┬─────────┘                                                               │
│           │                                                                         │
│           ▼                                                                         │
│     Role allows?                                                                    │
│      │       │                                                                      │
│     No      Yes                                                                     │
│      │       │                                                                      │
│      ▼       ▼                                                                      │
│   DENY   ┌──────────────────┐                                                       │
│          │ Check safe list  │                                                       │
│          │ (Sylk defaults)  │                                                       │
│          └────────┬─────────┘                                                       │
│                   │                                                                 │
│                   ▼                                                                 │
│             In safe list?                                                           │
│              │       │                                                              │
│             No      Yes                                                             │
│              │       │                                                              │
│              ▼       ▼                                                              │
│  ┌─────────────────┐ ALLOW                                                          │
│  │ Check project   │ (log action)                                                   │
│  │ allowlist       │                                                                │
│  └────────┬────────┘                                                                │
│           │                                                                         │
│           ▼                                                                         │
│     In allowlist?                                                                   │
│      │       │                                                                      │
│     No      Yes                                                                     │
│      │       │                                                                      │
│      ▼       ▼                                                                      │
│  ┌────────────┐ ALLOW                                                               │
│  │ Prompt     │ (log action)                                                        │
│  │ user       │                                                                     │
│  └─────┬──────┘                                                                     │
│        │                                                                            │
│        ▼                                                                            │
│   User approves?                                                                    │
│    │        │                                                                       │
│   No       Yes                                                                      │
│    │        │                                                                       │
│    ▼        ▼                                                                       │
│  DENY   ┌──────────────────┐                                                        │
│  (log)  │ Add to project   │                                                        │
│         │ allowlist        │                                                        │
│         └────────┬─────────┘                                                        │
│                  │                                                                  │
│                  ▼                                                                  │
│               ALLOW                                                                 │
│               (log + persist)                                                       │
│                                                                                     │
└─────────────────────────────────────────────────────────────────────────────────────┘
```

```go
// CheckPermission validates if an agent can perform an action
func (pm *PermissionManager) CheckPermission(
    ctx context.Context,
    agentID string,
    agentRole AgentRole,
    action PermissionAction,
) (PermissionResult, error) {
    pm.mu.RLock()
    defer pm.mu.RUnlock()

    // Step 1: Check role capability
    if !roleHasCapability(agentRole, action.Type) {
        pm.auditLog.LogDenied(agentID, action, "role_insufficient")
        return PermissionResult{Allowed: false, Reason: "role does not permit this action"}, nil
    }

    // Step 2: Check safe lists (always allowed)
    if pm.isInSafeList(action) {
        pm.auditLog.LogAllowed(agentID, action, "safe_list")
        return PermissionResult{Allowed: true, Source: "safe_list"}, nil
    }

    // Step 3: Check project allowlist
    if pm.isInProjectAllowlist(action) {
        pm.auditLog.LogAllowed(agentID, action, "project_allowlist")
        return PermissionResult{Allowed: true, Source: "project_allowlist"}, nil
    }

    // Step 4: Requires user approval
    return PermissionResult{
        Allowed:          false,
        RequiresApproval: true,
        ApprovalPrompt:   pm.formatApprovalPrompt(action),
    }, nil
}

// Command matching ignores arguments
func (pm *PermissionManager) isCommandAllowed(cmd string) bool {
    // Extract base command (ignore arguments)
    parts := strings.Fields(cmd)
    if len(parts) == 0 {
        return false
    }
    baseCmd := parts[0]

    // Check safe commands
    if pm.safeCommands[baseCmd] {
        return true
    }

    // Check project allowlist (base command only)
    return pm.commandAllowlist[baseCmd]
}

// ApproveAndPersist adds an action to the project allowlist
func (pm *PermissionManager) ApproveAndPersist(action PermissionAction) error {
    pm.mu.Lock()
    defer pm.mu.Unlock()

    switch action.Type {
    case ActionTypeCommand:
        // Store base command only (not full command with args)
        baseCmd := strings.Fields(action.Target)[0]
        pm.commandAllowlist[baseCmd] = true

    case ActionTypeNetwork:
        // Store domain only
        pm.domainAllowlist[action.Target] = true

    case ActionTypePath:
        pm.pathAllowlist[action.Target] = action.PathPerm
    }

    return pm.persistAllowlists()
}

func (pm *PermissionManager) persistAllowlists() error {
    perms := ProjectPermissions{
        Commands: mapKeys(pm.commandAllowlist),
        Domains:  mapKeys(pm.domainAllowlist),
        Paths:    pm.serializePathPerms(),
    }

    data, err := yaml.Marshal(perms)
    if err != nil {
        return err
    }

    permPath := filepath.Join(pm.projectPath, ".sylk", "local", "permissions.yaml")
    return os.WriteFile(permPath, data, 0600)
}
```

#### Pipeline Permission Inheritance

Pipelines declare required permissions upfront, with runtime escalation for unexpected needs.

```go
// WorkflowPermissions declares permissions needed by a workflow
type WorkflowPermissions struct {
    // Pre-declared permissions (shown to user before execution)
    Commands []string `json:"commands"` // Commands workflow will run
    Domains  []string `json:"domains"`  // Network domains needed
    Paths    []string `json:"paths"`    // Paths to modify

    // Permission inheritance mode
    InheritFromParent bool `json:"inherit_from_parent"` // Inherit calling workflow's perms

    // Allow runtime escalation prompts
    AllowRuntimeEscalation bool `json:"allow_runtime_escalation"`
}

// PipelinePermissionManager manages permissions across pipeline stages
type PipelinePermissionManager struct {
    baseManager *PermissionManager

    // Workflow-granted permissions (valid for workflow duration)
    workflowPerms map[string]*WorkflowPermissions // workflowID → permissions

    // Runtime escalations (require user approval)
    pendingEscalations chan EscalationRequest
}

func (ppm *PipelinePermissionManager) CheckPipelinePermission(
    ctx context.Context,
    workflowID string,
    agentID string,
    action PermissionAction,
) (PermissionResult, error) {
    // Check workflow pre-declared permissions first
    if perms, ok := ppm.workflowPerms[workflowID]; ok {
        if ppm.actionInWorkflowPerms(action, perms) {
            return PermissionResult{Allowed: true, Source: "workflow_declaration"}, nil
        }
    }

    // Fall back to base permission check
    result, err := ppm.baseManager.CheckPermission(ctx, agentID, RoleWorker, action)
    if err != nil {
        return result, err
    }

    // If base check requires approval and workflow allows runtime escalation
    if result.RequiresApproval {
        if perms, ok := ppm.workflowPerms[workflowID]; ok && perms.AllowRuntimeEscalation {
            // Queue escalation request for user
            return ppm.queueEscalation(ctx, workflowID, action)
        }
    }

    return result, nil
}
```

### Secret Management

Protects against secret leakage through detection, redaction, and environment isolation. Uses a layered approach: hooks for enforcement, skills for proactive agent validation.

#### Design Principles

```
┌─────────────────────────────────────────────────────────────────────────────────────┐
│                        SECRET MANAGEMENT PRINCIPLES                                   │
├─────────────────────────────────────────────────────────────────────────────────────┤
│                                                                                     │
│  1. CONTEXT-AWARE HANDLING                                                          │
│     ├── User prompts: REJECT (user is pasting secrets)                              │
│     ├── Codebase scan: REDACT (preserve structure, hide values)                     │
│     ├── Tool output: REDACT (before returning to LLM context)                       │
│     └── Agent-to-agent: REDACT (in transit)                                         │
│                                                                                     │
│  2. ENVIRONMENT ISOLATION                                                           │
│     ├── Tools read env vars from os.Getenv() ONLY                                   │
│     ├── Env vars NEVER accepted as LLM-provided parameters                          │
│     └── Prevents prompt injection of fake credentials                               │
│                                                                                     │
│  3. GRACEFUL DEGRADATION                                                            │
│     ├── Librarian indexing continues with redacted content                          │
│     ├── Sensitive files tracked (existence known, content hidden)                   │
│     └── No arbitrary failures from encountering secrets                             │
│                                                                                     │
│  4. DEFENSE IN DEPTH                                                                │
│     ├── Hooks: Automatic enforcement (can't bypass)                                 │
│     ├── Skills: Proactive agent validation (opt-in)                                 │
│     └── Tool design: Env vars read internally, not parameters                       │
│                                                                                     │
└─────────────────────────────────────────────────────────────────────────────────────┘
```

#### Secret Patterns

```go
// SecretPatterns defines regex patterns for detecting secrets
var SecretPatterns = []*SecretPattern{
    // API Keys (generic)
    {
        Name:     "api_key_generic",
        Pattern:  regexp.MustCompile(`(?i)(api[_-]?key|apikey)['":\s]*[=:]\s*['"]?[a-zA-Z0-9_\-]{20,}`),
        Severity: SeverityHigh,
    },
    // Passwords and secrets
    {
        Name:     "password_generic",
        Pattern:  regexp.MustCompile(`(?i)(secret|password|passwd|pwd)['":\s]*[=:]\s*['"]?[^\s'"]{8,}`),
        Severity: SeverityHigh,
    },
    // Bearer tokens
    {
        Name:     "bearer_token",
        Pattern:  regexp.MustCompile(`(?i)(bearer|token)['":\s]*[=:]\s*['"]?[a-zA-Z0-9_\-\.]{20,}`),
        Severity: SeverityHigh,
    },
    // Private keys
    {
        Name:     "private_key",
        Pattern:  regexp.MustCompile(`(?i)-----BEGIN\s+(RSA|DSA|EC|OPENSSH|PGP)?\s*PRIVATE KEY-----`),
        Severity: SeverityCritical,
    },
    // AWS credentials
    {
        Name:     "aws_credentials",
        Pattern:  regexp.MustCompile(`(?i)(aws_access_key_id|aws_secret)['":\s]*[=:]\s*['"]?[A-Z0-9]{16,}`),
        Severity: SeverityCritical,
    },
    // GitHub PAT
    {
        Name:     "github_pat",
        Pattern:  regexp.MustCompile(`ghp_[a-zA-Z0-9]{36}`),
        Severity: SeverityHigh,
    },
    // OpenAI key
    {
        Name:     "openai_key",
        Pattern:  regexp.MustCompile(`sk-[a-zA-Z0-9]{48}`),
        Severity: SeverityHigh,
    },
    // Anthropic key
    {
        Name:     "anthropic_key",
        Pattern:  regexp.MustCompile(`sk-ant-[a-zA-Z0-9\-]{80,}`),
        Severity: SeverityHigh,
    },
    // Connection strings
    {
        Name:     "connection_string",
        Pattern:  regexp.MustCompile(`(?i)(mongodb|postgres|mysql|redis)(\+srv)?://[^\s]+:[^\s]+@`),
        Severity: SeverityCritical,
    },
    // Generic high-entropy (base64-ish long strings in assignments)
    {
        Name:     "high_entropy_assignment",
        Pattern:  regexp.MustCompile(`(?i)(key|secret|token|credential)['":\s]*[=:]\s*['"]?[A-Za-z0-9+/=]{40,}`),
        Severity: SeverityMedium,
    },
}

// SensitiveFilePatterns defines files to skip or handle specially
var SensitiveFilePatterns = []glob.Glob{
    glob.MustCompile(".env"),
    glob.MustCompile(".env.*"),
    glob.MustCompile("*.pem"),
    glob.MustCompile("*.key"),
    glob.MustCompile("*.p12"),
    glob.MustCompile("*.pfx"),
    glob.MustCompile("*credentials*"),
    glob.MustCompile("*secrets*"),
    glob.MustCompile(".aws/credentials"),
    glob.MustCompile(".ssh/*"),
    glob.MustCompile("**/secrets.yaml"),
    glob.MustCompile("**/secrets.json"),
    glob.MustCompile("**/.npmrc"),
    glob.MustCompile("**/.pypirc"),
}

// EnvVarPattern detects env var references that shouldn't be in LLM parameters
var EnvVarPattern = regexp.MustCompile(`\$\{?([A-Z_][A-Z0-9_]*)\}?`)
```

#### SecretSanitizer Service

```go
// SecretSanitizer provides context-aware secret handling
type SecretSanitizer struct {
    patterns         []*SecretPattern
    sensitiveFiles   []glob.Glob
    redactText       string            // Default: "[REDACTED]"
    redactFileText   string            // Default: "(contents not indexed for security)"

    // Metrics
    mu               sync.RWMutex
    redactionCounts  map[string]int64  // pattern name → count
    skippedFiles     map[string]int64  // file pattern → count
}

type SecretPattern struct {
    Name     string
    Pattern  *regexp.Regexp
    Severity SecretSeverity
}

type SecretSeverity int

const (
    SeverityLow SecretSeverity = iota
    SeverityMedium
    SeverityHigh
    SeverityCritical
)

type SanitizeResult struct {
    Path            string
    Skipped         bool
    SkipReason      string
    RedactionCount  int
    PatternsMatched []string
    OriginalSize    int
    SanitizedSize   int
}

func NewSecretSanitizer() *SecretSanitizer {
    return &SecretSanitizer{
        patterns:        SecretPatterns,
        sensitiveFiles:  SensitiveFilePatterns,
        redactText:      "[REDACTED]",
        redactFileText:  "(contents not indexed for security)",
        redactionCounts: make(map[string]int64),
        skippedFiles:    make(map[string]int64),
    }
}

// SanitizeForIndex prepares content for vector DB storage (Librarian use)
func (s *SecretSanitizer) SanitizeForIndex(path string, content []byte) ([]byte, *SanitizeResult) {
    result := &SanitizeResult{
        Path:         path,
        OriginalSize: len(content),
    }

    // Check if entire file should be skipped
    for _, pattern := range s.sensitiveFiles {
        if pattern.Match(path) {
            result.Skipped = true
            result.SkipReason = "sensitive file pattern"
            s.recordSkip(pattern.String())

            // Return stub that preserves file existence without content
            stub := fmt.Sprintf("# %s\n%s\n\nFile type: %s",
                filepath.Base(path),
                s.redactFileText,
                filepath.Ext(path))
            result.SanitizedSize = len(stub)
            return []byte(stub), result
        }
    }

    // Redact secrets in-place
    sanitized := content
    for _, pattern := range s.patterns {
        matches := pattern.Pattern.FindAllIndex(sanitized, -1)
        if len(matches) > 0 {
            result.RedactionCount += len(matches)
            result.PatternsMatched = append(result.PatternsMatched, pattern.Name)
            s.recordRedaction(pattern.Name, len(matches))
            sanitized = pattern.Pattern.ReplaceAll(sanitized, []byte(s.redactText))
        }
    }

    result.SanitizedSize = len(sanitized)
    return sanitized, result
}

// CheckUserPrompt validates user input for secrets (REJECT mode)
func (s *SecretSanitizer) CheckUserPrompt(prompt string) *SecretDetection {
    detection := &SecretDetection{
        HasSecrets: false,
        Findings:   make([]SecretFinding, 0),
    }

    for _, pattern := range s.patterns {
        matches := pattern.Pattern.FindAllStringIndex(prompt, -1)
        for _, match := range matches {
            detection.HasSecrets = true
            detection.Findings = append(detection.Findings, SecretFinding{
                PatternName: pattern.Name,
                Severity:    pattern.Severity,
                StartPos:    match[0],
                EndPos:      match[1],
                // Don't include the actual secret in the finding!
                Context:     maskContext(prompt, match[0], match[1]),
            })
        }
    }

    return detection
}

type SecretDetection struct {
    HasSecrets bool
    Findings   []SecretFinding
}

type SecretFinding struct {
    PatternName string
    Severity    SecretSeverity
    StartPos    int
    EndPos      int
    Context     string  // Masked context showing location, not value
}

// SanitizeToolOutput redacts secrets from tool output before context
func (s *SecretSanitizer) SanitizeToolOutput(output string) (string, int) {
    redactionCount := 0
    sanitized := output

    for _, pattern := range s.patterns {
        matches := pattern.Pattern.FindAllStringIndex(sanitized, -1)
        if len(matches) > 0 {
            redactionCount += len(matches)
            sanitized = pattern.Pattern.ReplaceAllString(sanitized, s.redactText)
        }
    }

    return sanitized, redactionCount
}

func maskContext(text string, start, end int) string {
    // Show surrounding context with the secret masked
    contextStart := max(0, start-20)
    contextEnd := min(len(text), end+20)

    masked := text[contextStart:start] + "[***]" + text[end:contextEnd]
    if contextStart > 0 {
        masked = "..." + masked
    }
    if contextEnd < len(text) {
        masked = masked + "..."
    }
    return masked
}
```

#### Security Hooks

```go
// PrePromptSecretHook rejects user prompts containing secrets
var PrePromptSecretHook = &Hook{
    Name:     "secret_detection",
    Type:     PrePrompt,
    Priority: HookPriorityFirst,  // Run before any processing
    Handler: func(ctx context.Context, data *PromptHookData) (*PromptHookData, error) {
        sanitizer := GetSecretSanitizer(ctx)
        detection := sanitizer.CheckUserPrompt(data.UserMessage)

        if detection.HasSecrets {
            // Log the detection (not the secrets!)
            GetAuditLogger(ctx).LogSecretDetection(
                "user_prompt",
                len(detection.Findings),
                detection.Findings[0].Severity,
            )

            // Return user-friendly error
            return nil, &SecretDetectedError{
                Message: fmt.Sprintf(
                    "Your message appears to contain %d secret(s) (e.g., %s). "+
                    "Please remove sensitive values before sending. "+
                    "Use environment variables or config files instead.",
                    len(detection.Findings),
                    detection.Findings[0].PatternName,
                ),
                Findings: detection.Findings,
            }
        }

        return data, nil
    },
}

// PostToolSecretHook redacts secrets from tool output
var PostToolSecretHook = &Hook{
    Name:     "secret_redaction_output",
    Type:     PostTool,
    Priority: HookPriorityLast,  // Run after tool completes
    Handler: func(ctx context.Context, data *ToolCallHookData) (*ToolCallHookData, error) {
        if data.Output == "" {
            return data, nil
        }

        sanitizer := GetSecretSanitizer(ctx)
        sanitized, count := sanitizer.SanitizeToolOutput(data.Output)

        if count > 0 {
            GetAuditLogger(ctx).LogSecretRedaction("tool_output", data.ToolName, count)
            data.Output = sanitized
        }

        return data, nil
    },
}

// PreToolEnvVarHook prevents env vars from being passed as parameters
var PreToolEnvVarHook = &Hook{
    Name:     "env_var_isolation",
    Type:     PreTool,
    Priority: HookPriorityFirst,
    Handler: func(ctx context.Context, data *ToolCallHookData) (*ToolCallHookData, error) {
        // Check all string parameters for env var patterns
        for paramName, paramValue := range data.Parameters {
            strValue, ok := paramValue.(string)
            if !ok {
                continue
            }

            // Check for env var references
            if matches := EnvVarPattern.FindStringSubmatch(strValue); len(matches) > 0 {
                envVarName := matches[1]

                // Check if this looks like an attempt to inject an env var value
                // (i.e., the parameter contains what looks like an actual credential)
                for _, pattern := range SecretPatterns {
                    if pattern.Pattern.MatchString(strValue) {
                        GetAuditLogger(ctx).LogEnvVarInjectionAttempt(
                            data.ToolName,
                            paramName,
                            envVarName,
                        )
                        return nil, &EnvVarInjectionError{
                            Message: fmt.Sprintf(
                                "Parameter '%s' contains what appears to be a secret value. "+
                                "Tools read environment variables internally - do not pass them as parameters.",
                                paramName,
                            ),
                            ToolName:  data.ToolName,
                            ParamName: paramName,
                        }
                    }
                }
            }
        }

        return data, nil
    },
}
```

#### Librarian Pre-Index Hook

```go
// LibrarianPreIndexHook sanitizes content before vector DB storage
var LibrarianPreIndexHook = &Hook{
    Name:     "secret_sanitization_index",
    Type:     PreIndex,  // Librarian-specific hook type
    Priority: HookPriorityFirst,
    Handler: func(ctx context.Context, data *IndexHookData) (*IndexHookData, error) {
        sanitizer := GetSecretSanitizer(ctx)

        sanitized, result := sanitizer.SanitizeForIndex(data.FilePath, data.Content)

        if result.Skipped {
            GetAuditLogger(ctx).LogSensitiveFileSkipped(data.FilePath, result.SkipReason)
        } else if result.RedactionCount > 0 {
            GetAuditLogger(ctx).LogSecretRedaction(
                "index",
                data.FilePath,
                result.RedactionCount,
            )
        }

        // Store sanitization metadata for transparency
        data.Content = sanitized
        data.Metadata["sanitized"] = result.RedactionCount > 0 || result.Skipped
        data.Metadata["redaction_count"] = result.RedactionCount
        data.Metadata["skipped"] = result.Skipped

        return data, nil
    },
}
```

#### Agent-to-Agent Sanitization

```go
// InterAgentSecretHook sanitizes messages between agents
var InterAgentSecretHook = &Hook{
    Name:     "secret_sanitization_transit",
    Type:     PreDispatch,  // Before message sent to another agent
    Priority: HookPriorityLast,
    Handler: func(ctx context.Context, data *DispatchHookData) (*DispatchHookData, error) {
        sanitizer := GetSecretSanitizer(ctx)

        // Sanitize the message content
        sanitized, count := sanitizer.SanitizeToolOutput(data.Message)

        if count > 0 {
            GetAuditLogger(ctx).LogSecretRedaction(
                "agent_dispatch",
                fmt.Sprintf("%s->%s", data.SourceAgent, data.TargetAgent),
                count,
            )
            data.Message = sanitized
        }

        return data, nil
    },
}
```

#### Proactive Validation Skills

```go
// validate_content - Proactive secret detection skill
skills.NewSkill("validate_content").
    Description("Check content for secrets before processing").
    Domain("security").
    Keywords("validate", "check", "secrets", "safe", "security").
    StringParam("content", "Content to validate", true).
    Returns(schema.Object{
        "safe":       schema.Boolean("Whether content is safe to process"),
        "findings":   schema.Array("List of detected issues"),
        "suggestion": schema.String("Recommended action"),
    })

// check_file_sensitivity - Check if file should be handled specially
skills.NewSkill("check_file_sensitivity").
    Description("Check if a file path matches sensitive file patterns").
    Domain("security").
    Keywords("sensitive", "file", "check", "secret").
    StringParam("path", "File path to check", true).
    Returns(schema.Object{
        "sensitive":  schema.Boolean("Whether file is sensitive"),
        "pattern":    schema.String("Matching pattern if sensitive"),
        "handling":   schema.String("Recommended handling: skip|redact|normal"),
    })

// sanitize_for_display - Redact secrets for safe display
skills.NewSkill("sanitize_for_display").
    Description("Redact secrets from content for safe display to user").
    Domain("security").
    Keywords("sanitize", "redact", "display", "safe").
    StringParam("content", "Content to sanitize", true).
    Returns(schema.Object{
        "sanitized":       schema.String("Sanitized content"),
        "redaction_count": schema.Integer("Number of redactions made"),
    })
```

#### Tool Design Guidelines

```
┌─────────────────────────────────────────────────────────────────────────────────────┐
│                     TOOL DESIGN: ENVIRONMENT VARIABLE HANDLING                        │
├─────────────────────────────────────────────────────────────────────────────────────┤
│                                                                                     │
│  CORRECT: Tool reads env vars internally                                            │
│  ─────────────────────────────────────────────────────────────────────────────────  │
│                                                                                     │
│  func (t *DatabaseTool) Execute(ctx context.Context, params Params) (Result, error) │
│  {                                                                                  │
│      // Read from environment - never from params                                   │
│      connStr := os.Getenv("DATABASE_URL")                                           │
│      if connStr == "" {                                                             │
│          return Result{}, errors.New("DATABASE_URL not set")                        │
│      }                                                                              │
│      // ... use connStr                                                             │
│  }                                                                                  │
│                                                                                     │
│  INCORRECT: Tool accepts connection string as parameter                             │
│  ─────────────────────────────────────────────────────────────────────────────────  │
│                                                                                     │
│  func (t *DatabaseTool) Execute(ctx context.Context, params Params) (Result, error) │
│  {                                                                                  │
│      // WRONG: LLM could be tricked into passing malicious value                    │
│      connStr := params.ConnectionString  // ❌ NEVER DO THIS                        │
│      // ...                                                                         │
│  }                                                                                  │
│                                                                                     │
│  TOOL PARAMETER RULES:                                                              │
│  ├── NEVER accept: connection strings, API keys, tokens, passwords                  │
│  ├── NEVER accept: paths to credential files                                        │
│  ├── ALLOWED: database names, table names, query parameters                         │
│  ├── ALLOWED: file paths (within project scope)                                     │
│  └── ALLOWED: configuration keys (tool looks up value internally)                   │
│                                                                                     │
│  ENV VAR NAMING CONVENTION:                                                         │
│  ├── SYLK_<SERVICE>_<CREDENTIAL_TYPE>                                               │
│  ├── Example: SYLK_OPENAI_API_KEY                                                   │
│  ├── Example: SYLK_DATABASE_URL                                                     │
│  └── Example: SYLK_GITHUB_TOKEN                                                     │
│                                                                                     │
└─────────────────────────────────────────────────────────────────────────────────────┘
```

#### Secret Detection Flow

```
┌─────────────────────────────────────────────────────────────────────────────────────┐
│                        SECRET HANDLING BY CONTEXT                                     │
├─────────────────────────────────────────────────────────────────────────────────────┤
│                                                                                     │
│  USER PROMPT                                                                        │
│  ───────────                                                                        │
│      │                                                                              │
│      ▼                                                                              │
│  PrePromptSecretHook                                                                │
│      │                                                                              │
│      ├── Secrets detected? ──Yes──► REJECT with helpful message                     │
│      │                              "Remove secrets, use env vars instead"          │
│      │                                                                              │
│      └── No secrets ──────────────► Continue to agent                               │
│                                                                                     │
│  LIBRARIAN INDEXING                                                                 │
│  ─────────────────────                                                              │
│      │                                                                              │
│      ▼                                                                              │
│  LibrarianPreIndexHook                                                              │
│      │                                                                              │
│      ├── Sensitive file pattern? ──Yes──► Return stub (file exists, content hidden) │
│      │                                    "# .env\n(contents not indexed...)"       │
│      │                                                                              │
│      └── Scan content                                                               │
│          │                                                                          │
│          ├── Secrets found? ──Yes──► REDACT in-place, continue indexing             │
│          │                           "api_key = [REDACTED]"                         │
│          │                                                                          │
│          └── No secrets ──────────► Index normally                                  │
│                                                                                     │
│  TOOL OUTPUT                                                                        │
│  ───────────                                                                        │
│      │                                                                              │
│      ▼                                                                              │
│  PostToolSecretHook                                                                 │
│      │                                                                              │
│      └── Always scan and REDACT before returning to LLM context                     │
│                                                                                     │
│  AGENT-TO-AGENT                                                                     │
│  ──────────────                                                                     │
│      │                                                                              │
│      ▼                                                                              │
│  InterAgentSecretHook                                                               │
│      │                                                                              │
│      └── Always scan and REDACT before dispatch                                     │
│                                                                                     │
└─────────────────────────────────────────────────────────────────────────────────────┘
```

### Sandboxing Strategy

Sandboxing provides defense-in-depth for subprocess execution. **Sandboxing is OFF by default** and can be enabled with `/sandbox enable`.

#### Sandbox Architecture

```
┌─────────────────────────────────────────────────────────────────────────────────────┐
│                           SANDBOX ARCHITECTURE                                       │
├─────────────────────────────────────────────────────────────────────────────────────┤
│                                                                                     │
│  LAYER 1: OS-Level Sandbox (bubblewrap/Seatbelt)                                    │
│  ├── Process isolation from host system                                             │
│  ├── Filesystem namespace restrictions                                              │
│  ├── Network namespace isolation                                                    │
│  └── Resource limits (CPU, memory, file descriptors)                                │
│                                                                                     │
│  LAYER 2: Virtual Filesystem (VFS) Layer                                            │
│  ├── Copy-on-write for all file modifications                                       │
│  ├── Working directory boundary enforcement                                         │
│  ├── Path escape prevention (symlink, .., absolute paths)                           │
│  └── User-approved exceptions for specific paths                                    │
│                                                                                     │
│  LAYER 3: Network Proxy                                                             │
│  ├── All subprocess network traffic routed through Sylk                             │
│  ├── Domain allowlist enforcement                                                   │
│  ├── Request/response logging                                                       │
│  └── Bandwidth and connection limits                                                │
│                                                                                     │
│  LAYER 4: Permission System                                                         │
│  ├── Command allowlist (ignores arguments)                                          │
│  ├── Per-project persistent permissions                                             │
│  └── Sylk-specified safe defaults                                                   │
│                                                                                     │
└─────────────────────────────────────────────────────────────────────────────────────┘
```

#### Platform-Native Sandbox Implementation

```go
// SandboxConfig configures sandbox behavior
type SandboxConfig struct {
    Enabled bool `yaml:"enabled"` // Default: false

    // Layer controls
    OSIsolation     bool `yaml:"os_isolation"`      // bubblewrap/Seatbelt
    VFSLayer        bool `yaml:"vfs_layer"`         // Copy-on-write filesystem
    NetworkProxy    bool `yaml:"network_proxy"`     // Domain-filtered proxy

    // Resource limits
    MaxCPUPercent   int           `yaml:"max_cpu_percent"`
    MaxMemoryMB     int           `yaml:"max_memory_mb"`
    MaxOpenFiles    int           `yaml:"max_open_files"`
    MaxProcesses    int           `yaml:"max_processes"`
    NetworkTimeout  time.Duration `yaml:"network_timeout"`

    // Graceful degradation
    FallbackOnError bool `yaml:"fallback_on_error"` // If sandbox fails, run unsandboxed with warning
}

func DefaultSandboxConfig() SandboxConfig {
    return SandboxConfig{
        Enabled:         false, // OFF by default
        OSIsolation:     true,
        VFSLayer:        true,
        NetworkProxy:    true,
        MaxCPUPercent:   80,
        MaxMemoryMB:     1024,
        MaxOpenFiles:    256,
        MaxProcesses:    32,
        NetworkTimeout:  30 * time.Second,
        FallbackOnError: true,
    }
}

// Sandbox provides layered process isolation
type Sandbox struct {
    config      SandboxConfig
    vfs         *VirtualFilesystem
    proxy       *NetworkProxy
    permissions *PermissionManager
    auditLog    *AuditLogger
}

// Execute runs a command within the sandbox
func (s *Sandbox) Execute(ctx context.Context, cmd *exec.Cmd) (*SandboxResult, error) {
    if !s.config.Enabled {
        // Sandbox disabled - run directly with permission checks only
        return s.executeUnsandboxed(ctx, cmd)
    }

    // Build sandboxed command based on platform
    sandboxedCmd, err := s.wrapWithOSSandbox(cmd)
    if err != nil {
        if s.config.FallbackOnError {
            s.auditLog.LogWarning("sandbox_fallback", "OS sandbox unavailable, running unsandboxed")
            return s.executeUnsandboxed(ctx, cmd)
        }
        return nil, fmt.Errorf("sandbox setup failed: %w", err)
    }

    // Set up VFS layer
    if s.config.VFSLayer {
        if err := s.vfs.PrepareForCommand(cmd); err != nil {
            return nil, fmt.Errorf("VFS setup failed: %w", err)
        }
        defer s.vfs.Cleanup(cmd)
    }

    // Set up network proxy
    if s.config.NetworkProxy {
        s.proxy.RouteCommand(sandboxedCmd)
    }

    // Execute
    result, err := s.runWithLimits(ctx, sandboxedCmd)

    // Merge VFS changes if successful
    if err == nil && s.config.VFSLayer {
        if err := s.vfs.MergeChanges(cmd); err != nil {
            return result, fmt.Errorf("VFS merge failed: %w", err)
        }
    }

    return result, err
}
```

#### OS-Level Sandbox (Linux - bubblewrap)

```go
// wrapWithBubblewrap creates a sandboxed command using bubblewrap
func (s *Sandbox) wrapWithBubblewrap(cmd *exec.Cmd) (*exec.Cmd, error) {
    if _, err := exec.LookPath("bwrap"); err != nil {
        return nil, fmt.Errorf("bubblewrap not installed")
    }

    args := []string{
        // New namespaces
        "--unshare-all",
        "--share-net", // We control network via proxy

        // Bind mount working directory read-write
        "--bind", cmd.Dir, cmd.Dir,

        // Read-only system paths
        "--ro-bind", "/usr", "/usr",
        "--ro-bind", "/lib", "/lib",
        "--ro-bind", "/lib64", "/lib64",
        "--ro-bind", "/bin", "/bin",
        "--ro-bind", "/etc/resolv.conf", "/etc/resolv.conf",
        "--ro-bind", "/etc/ssl", "/etc/ssl",

        // Proc and dev
        "--proc", "/proc",
        "--dev", "/dev",

        // Temp directory
        "--tmpfs", "/tmp",

        // Resource limits
        "--die-with-parent",
    }

    // Add command and its arguments
    args = append(args, cmd.Path)
    args = append(args, cmd.Args[1:]...)

    sandboxedCmd := exec.CommandContext(cmd.Context(), "bwrap", args...)
    sandboxedCmd.Dir = cmd.Dir
    sandboxedCmd.Env = cmd.Env
    sandboxedCmd.Stdin = cmd.Stdin
    sandboxedCmd.Stdout = cmd.Stdout
    sandboxedCmd.Stderr = cmd.Stderr

    return sandboxedCmd, nil
}
```

#### OS-Level Sandbox (macOS - Seatbelt)

```go
// wrapWithSeatbelt creates a sandboxed command using macOS sandbox-exec
func (s *Sandbox) wrapWithSeatbelt(cmd *exec.Cmd) (*exec.Cmd, error) {
    // Generate sandbox profile
    profile := s.generateSeatbeltProfile(cmd)

    // Write profile to temp file
    profilePath := filepath.Join(os.TempDir(), fmt.Sprintf("sylk-sandbox-%d.sb", os.Getpid()))
    if err := os.WriteFile(profilePath, []byte(profile), 0600); err != nil {
        return nil, err
    }

    args := []string{"-f", profilePath, cmd.Path}
    args = append(args, cmd.Args[1:]...)

    sandboxedCmd := exec.CommandContext(cmd.Context(), "sandbox-exec", args...)
    sandboxedCmd.Dir = cmd.Dir
    sandboxedCmd.Env = cmd.Env
    sandboxedCmd.Stdin = cmd.Stdin
    sandboxedCmd.Stdout = cmd.Stdout
    sandboxedCmd.Stderr = cmd.Stderr

    return sandboxedCmd, nil
}

func (s *Sandbox) generateSeatbeltProfile(cmd *exec.Cmd) string {
    return fmt.Sprintf(`
(version 1)
(deny default)

; Allow basic process operations
(allow process-exec (literal "%s"))
(allow process-fork)

; Allow reading from system locations
(allow file-read*
    (subpath "/usr")
    (subpath "/bin")
    (subpath "/Library/Frameworks")
    (subpath "/System/Library"))

; Allow read-write to working directory
(allow file-read* (subpath "%s"))
(allow file-write* (subpath "%s"))

; Allow network (controlled by proxy)
(allow network*)

; Allow temp files
(allow file-read* (subpath "/tmp"))
(allow file-write* (subpath "/tmp"))
`, cmd.Path, cmd.Dir, cmd.Dir)
}
```

#### Virtual Filesystem Layer

```go
// VirtualFilesystem provides copy-on-write file operations
type VirtualFilesystem struct {
    mu sync.RWMutex

    workingDir      string
    stagingDir      string
    approvedPaths   map[string]bool // User-approved paths outside working dir

    // Track modifications
    modifications map[string]*FileModification

    auditLog *AuditLogger
}

type FileModification struct {
    OriginalPath string
    StagingPath  string
    Operation    string // "create", "modify", "delete"
    Timestamp    time.Time
}

// PrepareForCommand sets up VFS for a command execution
func (vfs *VirtualFilesystem) PrepareForCommand(cmd *exec.Cmd) error {
    // Create staging directory for this command
    stagingDir, err := os.MkdirTemp(vfs.stagingDir, "cmd-")
    if err != nil {
        return err
    }

    // Set environment to redirect writes to staging
    cmd.Env = append(cmd.Env, fmt.Sprintf("SYLK_VFS_STAGING=%s", stagingDir))

    return nil
}

// ValidatePath checks if a path is within allowed boundaries
func (vfs *VirtualFilesystem) ValidatePath(path string) error {
    // Resolve to absolute path
    absPath, err := filepath.Abs(path)
    if err != nil {
        return err
    }

    // Resolve symlinks to detect escapes
    realPath, err := filepath.EvalSymlinks(absPath)
    if err != nil && !os.IsNotExist(err) {
        return err
    }
    if realPath == "" {
        realPath = absPath
    }

    // Check if within working directory
    if strings.HasPrefix(realPath, vfs.workingDir) {
        return nil
    }

    // Check if in approved paths
    vfs.mu.RLock()
    defer vfs.mu.RUnlock()

    for approvedPath := range vfs.approvedPaths {
        if strings.HasPrefix(realPath, approvedPath) {
            return nil
        }
    }

    return fmt.Errorf("path %q is outside allowed boundaries", path)
}

// MergeChanges applies staged modifications to the real filesystem
func (vfs *VirtualFilesystem) MergeChanges(cmd *exec.Cmd) error {
    stagingDir := vfs.getStagingDir(cmd)
    if stagingDir == "" {
        return nil
    }

    return filepath.Walk(stagingDir, func(path string, info os.FileInfo, err error) error {
        if err != nil {
            return err
        }

        relPath, _ := filepath.Rel(stagingDir, path)
        targetPath := filepath.Join(vfs.workingDir, relPath)

        if info.IsDir() {
            return os.MkdirAll(targetPath, info.Mode())
        }

        // Copy file from staging to working dir
        return vfs.copyFile(path, targetPath, info.Mode())
    })
}
```

#### Network Proxy

```go
// NetworkProxy filters subprocess network traffic
type NetworkProxy struct {
    mu sync.RWMutex

    listener      net.Listener
    allowedDomains map[string]bool

    // Statistics
    requestsAllowed int64
    requestsBlocked int64

    auditLog *AuditLogger
}

// RouteCommand sets up environment for network proxy
func (np *NetworkProxy) RouteCommand(cmd *exec.Cmd) {
    proxyURL := fmt.Sprintf("http://127.0.0.1:%d", np.listener.Addr().(*net.TCPAddr).Port)

    cmd.Env = append(cmd.Env,
        fmt.Sprintf("HTTP_PROXY=%s", proxyURL),
        fmt.Sprintf("HTTPS_PROXY=%s", proxyURL),
        fmt.Sprintf("http_proxy=%s", proxyURL),
        fmt.Sprintf("https_proxy=%s", proxyURL),
    )
}

// handleRequest processes a proxied request
func (np *NetworkProxy) handleRequest(w http.ResponseWriter, r *http.Request) {
    host := r.Host
    if h, _, err := net.SplitHostPort(host); err == nil {
        host = h
    }

    // Check domain allowlist
    np.mu.RLock()
    allowed := np.allowedDomains[host]
    np.mu.RUnlock()

    if !allowed {
        atomic.AddInt64(&np.requestsBlocked, 1)
        np.auditLog.LogNetworkBlocked(host, r.URL.String())
        http.Error(w, "Domain not in allowlist", http.StatusForbidden)
        return
    }

    atomic.AddInt64(&np.requestsAllowed, 1)
    np.auditLog.LogNetworkAllowed(host, r.URL.String())

    // Forward request
    np.forwardRequest(w, r)
}
```

#### Sandbox CLI Commands

```
/sandbox status          # Show sandbox status and configuration
/sandbox enable          # Enable all sandbox layers
/sandbox disable         # Disable sandbox (with warning)
/sandbox enable vfs      # Enable specific layer
/sandbox disable network # Disable specific layer
/sandbox allow path /etc/hosts  # Allow specific path
/sandbox deny path /etc/hosts   # Remove path allowance
```

### Audit Logging

All security-relevant actions are logged in a tamper-evident format.

#### What Gets Logged

```
┌─────────────────────────────────────────────────────────────────────────────────────┐
│                           AUDIT LOG CATEGORIES                                       │
├─────────────────────────────────────────────────────────────────────────────────────┤
│                                                                                     │
│  PERMISSION EVENTS                                                                  │
│  ├── Permission granted (agent, action, source)                                     │
│  ├── Permission denied (agent, action, reason)                                      │
│  ├── Permission escalation requested                                                │
│  ├── Permission escalation approved/denied                                          │
│  └── Allowlist modification                                                         │
│                                                                                     │
│  FILE OPERATIONS                                                                    │
│  ├── File read (path, agent, size)                                                  │
│  ├── File write (path, agent, size, hash)                                           │
│  ├── File delete (path, agent)                                                      │
│  └── Directory operations                                                           │
│                                                                                     │
│  PROCESS EXECUTION                                                                  │
│  ├── Command executed (command, agent, exit_code)                                   │
│  ├── Command blocked (command, reason)                                              │
│  ├── Process killed (pid, reason)                                                   │
│  └── Sandbox events (layer, action, outcome)                                        │
│                                                                                     │
│  NETWORK ACTIVITY                                                                   │
│  ├── Request allowed (domain, url, agent)                                           │
│  ├── Request blocked (domain, url, reason)                                          │
│  └── Connection statistics                                                          │
│                                                                                     │
│  LLM INTERACTIONS                                                                   │
│  ├── API call (provider, model, tokens, cost)                                       │
│  ├── Tool invocation (tool, args, result_summary)                                   │
│  └── Context window events                                                          │
│                                                                                     │
│  SESSION EVENTS                                                                     │
│  ├── Session start/end                                                              │
│  ├── Agent spawn/terminate                                                          │
│  ├── Pipeline stage transitions                                                     │
│  └── Error occurrences                                                              │
│                                                                                     │
│  CONFIGURATION CHANGES                                                              │
│  ├── Permission changes                                                             │
│  ├── Configuration file modifications                                               │
│  └── Credential access (not values, just access events)                             │
│                                                                                     │
└─────────────────────────────────────────────────────────────────────────────────────┘
```

#### Log Format and Storage

```go
// AuditEntry represents a single audit log entry
type AuditEntry struct {
    ID        string          `json:"id"`         // UUID
    Timestamp time.Time       `json:"timestamp"`
    Sequence  uint64          `json:"sequence"`   // Monotonic sequence number

    // Event classification
    Category  AuditCategory   `json:"category"`   // permission, file, process, network, llm, session, config
    EventType string          `json:"event_type"` // Specific event within category
    Severity  AuditSeverity   `json:"severity"`   // info, warning, security, critical

    // Context
    SessionID   string `json:"session_id"`
    ProjectID   string `json:"project_id"`
    AgentID     string `json:"agent_id,omitempty"`
    WorkflowID  string `json:"workflow_id,omitempty"`

    // Event details
    Action    string                 `json:"action"`
    Target    string                 `json:"target,omitempty"`
    Outcome   string                 `json:"outcome"` // allowed, denied, error
    Details   map[string]interface{} `json:"details,omitempty"`

    // Integrity
    PreviousHash string `json:"previous_hash"` // Hash of previous entry (chain)
    EntryHash    string `json:"entry_hash"`    // Hash of this entry
}

type AuditCategory string

const (
    AuditCategoryPermission AuditCategory = "permission"
    AuditCategoryFile       AuditCategory = "file"
    AuditCategoryProcess    AuditCategory = "process"
    AuditCategoryNetwork    AuditCategory = "network"
    AuditCategoryLLM        AuditCategory = "llm"
    AuditCategorySession    AuditCategory = "session"
    AuditCategoryConfig     AuditCategory = "config"
)

type AuditSeverity string

const (
    AuditSeverityInfo     AuditSeverity = "info"
    AuditSeverityWarning  AuditSeverity = "warning"
    AuditSeveritySecurity AuditSeverity = "security"
    AuditSeverityCritical AuditSeverity = "critical"
)
```

#### Tamper-Evident Storage

```go
// AuditLogger provides tamper-evident audit logging
type AuditLogger struct {
    mu sync.Mutex

    logFile      *os.File
    sequence     uint64
    previousHash string

    // Signing key for periodic signatures
    signingKey ed25519.PrivateKey

    // Signature interval
    signatureInterval int // Sign every N entries
    entriesSinceSign  int

    config AuditLogConfig
}

type AuditLogConfig struct {
    LogPath           string        `yaml:"log_path"`           // ~/.sylk/state/logs/audit/
    SignatureInterval int           `yaml:"signature_interval"` // Default: 100 entries
    RotateSize        int64         `yaml:"rotate_size"`        // Rotate at N bytes
    RetentionPolicy   string        `yaml:"retention_policy"`   // "indefinite" until explicit purge
}

func DefaultAuditLogConfig() AuditLogConfig {
    return AuditLogConfig{
        LogPath:           "", // Set by directory manager
        SignatureInterval: 100,
        RotateSize:        100 * 1024 * 1024, // 100MB per file
        RetentionPolicy:   "indefinite",
    }
}

// Log writes a tamper-evident audit entry
func (al *AuditLogger) Log(entry AuditEntry) error {
    al.mu.Lock()
    defer al.mu.Unlock()

    // Set sequence and chain hash
    al.sequence++
    entry.Sequence = al.sequence
    entry.PreviousHash = al.previousHash
    entry.ID = uuid.New().String()
    entry.Timestamp = time.Now().UTC()

    // Compute entry hash (excludes EntryHash field)
    entry.EntryHash = al.computeEntryHash(entry)
    al.previousHash = entry.EntryHash

    // Write to append-only log
    data, err := json.Marshal(entry)
    if err != nil {
        return err
    }

    if _, err := al.logFile.Write(append(data, '\n')); err != nil {
        return err
    }

    // Periodic signature
    al.entriesSinceSign++
    if al.entriesSinceSign >= al.config.SignatureInterval {
        if err := al.writeSignature(); err != nil {
            return err
        }
        al.entriesSinceSign = 0
    }

    return nil
}

func (al *AuditLogger) computeEntryHash(entry AuditEntry) string {
    // Hash everything except EntryHash
    h := sha256.New()
    h.Write([]byte(fmt.Sprintf("%d", entry.Sequence)))
    h.Write([]byte(entry.PreviousHash))
    h.Write([]byte(entry.Timestamp.Format(time.RFC3339Nano)))
    h.Write([]byte(entry.Category))
    h.Write([]byte(entry.EventType))
    h.Write([]byte(entry.SessionID))
    h.Write([]byte(entry.Action))
    h.Write([]byte(entry.Target))
    h.Write([]byte(entry.Outcome))

    if entry.Details != nil {
        detailsJSON, _ := json.Marshal(entry.Details)
        h.Write(detailsJSON)
    }

    return hex.EncodeToString(h.Sum(nil))
}

// writeSignature writes a signature entry for tamper detection
func (al *AuditLogger) writeSignature() error {
    sigEntry := AuditSignature{
        Timestamp:    time.Now().UTC(),
        SequenceFrom: al.sequence - uint64(al.entriesSinceSign) + 1,
        SequenceTo:   al.sequence,
        ChainHash:    al.previousHash,
    }

    // Sign the chain hash
    sigEntry.Signature = ed25519.Sign(al.signingKey, []byte(al.previousHash))

    data, _ := json.Marshal(sigEntry)
    _, err := al.logFile.Write(append([]byte("SIG:"), append(data, '\n')...))
    return err
}

// VerifyIntegrity checks the audit log chain for tampering
func (al *AuditLogger) VerifyIntegrity() (*IntegrityReport, error) {
    al.mu.Lock()
    defer al.mu.Unlock()

    report := &IntegrityReport{
        StartTime: time.Now(),
    }

    // Read and verify each entry
    scanner := bufio.NewScanner(al.logFile)
    var prevHash string
    var lastSeq uint64

    for scanner.Scan() {
        line := scanner.Text()

        // Check for signature line
        if strings.HasPrefix(line, "SIG:") {
            if err := al.verifySignature(line[4:], prevHash); err != nil {
                report.Errors = append(report.Errors, err.Error())
            }
            report.SignaturesVerified++
            continue
        }

        var entry AuditEntry
        if err := json.Unmarshal([]byte(line), &entry); err != nil {
            report.Errors = append(report.Errors, fmt.Sprintf("parse error at seq %d: %v", lastSeq+1, err))
            continue
        }

        // Verify chain
        if entry.PreviousHash != prevHash {
            report.Errors = append(report.Errors, fmt.Sprintf("chain break at seq %d", entry.Sequence))
        }

        // Verify entry hash
        computedHash := al.computeEntryHash(entry)
        if entry.EntryHash != computedHash {
            report.Errors = append(report.Errors, fmt.Sprintf("hash mismatch at seq %d", entry.Sequence))
        }

        // Verify sequence continuity
        if entry.Sequence != lastSeq+1 {
            report.Errors = append(report.Errors, fmt.Sprintf("sequence gap: %d to %d", lastSeq, entry.Sequence))
        }

        prevHash = entry.EntryHash
        lastSeq = entry.Sequence
        report.EntriesVerified++
    }

    report.EndTime = time.Now()
    report.Valid = len(report.Errors) == 0

    return report, nil
}
```

#### Query Interface

```go
// AuditQuery provides CLI querying of audit logs
type AuditQuery struct {
    logger *AuditLogger
}

// QueryFilter defines audit log query parameters
type QueryFilter struct {
    // Time range
    StartTime *time.Time
    EndTime   *time.Time

    // Filters
    Categories []AuditCategory
    Severities []AuditSeverity
    SessionIDs []string
    AgentIDs   []string
    Actions    []string
    Outcomes   []string

    // Text search
    TargetPattern string // Regex pattern for target field

    // Pagination
    Limit  int
    Offset int

    // Output format
    Format string // "table", "json", "csv"
}

// Query searches audit logs with filters
func (aq *AuditQuery) Query(filter QueryFilter) ([]AuditEntry, error) {
    var results []AuditEntry

    // ... implementation reads log file and applies filters ...

    return results, nil
}

// CLI commands:
// sylk audit query --category permission --severity security --since 24h
// sylk audit query --session abc123 --format json
// sylk audit query --action "file_write" --target "*.go"
// sylk audit verify              # Verify integrity of audit logs
// sylk audit export --since 7d   # Export for external analysis
// sylk audit purge --before 90d  # Purge old entries (requires confirmation)
```

### Session Isolation

Sessions are isolated for security while enabling controlled sharing.

#### Session Model

```
┌─────────────────────────────────────────────────────────────────────────────────────┐
│                              SESSION ISOLATION MODEL                                 │
├─────────────────────────────────────────────────────────────────────────────────────┤
│                                                                                     │
│  SINGLE-USER ASSUMPTION                                                             │
│  ├── Sylk is designed for single-user, local execution                              │
│  ├── No OS-level user isolation required                                            │
│  └── All sessions belong to the same user                                           │
│                                                                                     │
│  PROJECT-SCOPED SHARING                                                             │
│  ├── Sessions on same project share:                                                │
│  │   ├── Project knowledge base (VectorGraphDB)                                     │
│  │   ├── Project permissions (command/domain allowlists)                            │
│  │   └── Project configuration                                                      │
│  │                                                                                  │
│  └── Sessions do NOT share:                                                         │
│      ├── Active agent state                                                         │
│      ├── Conversation context                                                       │
│      └── In-flight tool executions                                                  │
│                                                                                     │
│  CROSS-SESSION KNOWLEDGE                                                            │
│  ├── Each session has own session-scoped knowledge                                  │
│  ├── Session knowledge is READ-ONLY to other sessions                               │
│  └── Archivalist consolidates to project knowledge                                  │
│                                                                                     │
│  CREDENTIAL ISOLATION                                                               │
│  ├── Sessions can override credential profile                                       │
│  ├── Sessions can hold temporary credentials                                        │
│  └── Temporary credentials expire with session, NOT shared                          │
│                                                                                     │
└─────────────────────────────────────────────────────────────────────────────────────┘
```

#### Session Credential Management

```go
// SessionCredentialManager handles session-scoped credentials
type SessionCredentialManager struct {
    mu sync.RWMutex

    sessionID string

    // Base credential manager
    baseManager *CredentialManager

    // Session overrides
    profileOverride string                    // Different profile for this session
    tempCredentials map[string]*TempCredential // Session-temporary credentials
}

type TempCredential struct {
    Provider  string
    APIKey    string    // In-memory only, never persisted
    ExpiresAt time.Time
    Source    string    // How obtained (e.g., "oauth_flow", "user_input")
}

// GetAPIKey resolves credentials with session overrides
func (scm *SessionCredentialManager) GetAPIKey(provider string) (string, error) {
    scm.mu.RLock()
    defer scm.mu.RUnlock()

    // 1. Check session temporary credentials first
    if temp, ok := scm.tempCredentials[provider]; ok {
        if time.Now().Before(temp.ExpiresAt) {
            return temp.APIKey, nil
        }
        // Expired - remove it
        delete(scm.tempCredentials, provider)
    }

    // 2. Use profile override if set
    if scm.profileOverride != "" {
        return scm.baseManager.GetAPIKeyForProfile(scm.profileOverride, provider)
    }

    // 3. Fall back to base manager
    return scm.baseManager.GetAPIKey(provider)
}

// SetTemporaryCredential stores a session-scoped credential
func (scm *SessionCredentialManager) SetTemporaryCredential(provider, apiKey string, ttl time.Duration) {
    scm.mu.Lock()
    defer scm.mu.Unlock()

    scm.tempCredentials[provider] = &TempCredential{
        Provider:  provider,
        APIKey:    apiKey,
        ExpiresAt: time.Now().Add(ttl),
        Source:    "session_input",
    }

    // Log credential access (not the value)
    // scm.auditLog.LogCredentialAccess(provider, "temp_set")
}

// SetProfileOverride changes the credential profile for this session only
func (scm *SessionCredentialManager) SetProfileOverride(profile string) error {
    scm.mu.Lock()
    defer scm.mu.Unlock()

    // Verify profile exists
    if !scm.baseManager.ProfileExists(profile) {
        return fmt.Errorf("profile %q does not exist", profile)
    }

    scm.profileOverride = profile
    return nil
}

// ClearSession removes all session-scoped credentials (called on session end)
func (scm *SessionCredentialManager) ClearSession() {
    scm.mu.Lock()
    defer scm.mu.Unlock()

    // Clear temporary credentials from memory
    for provider := range scm.tempCredentials {
        scm.tempCredentials[provider].APIKey = "" // Zero out before delete
        delete(scm.tempCredentials, provider)
    }

    scm.profileOverride = ""
}
```

#### Cross-Session Knowledge Sharing

```go
// SessionKnowledgeManager coordinates knowledge across sessions
type SessionKnowledgeManager struct {
    mu sync.RWMutex

    projectID string
    sessionID string

    // Project-level knowledge (shared, read-write for Archivalist)
    projectKnowledge *VectorGraphDB

    // Session-level knowledge (this session writes, others read-only)
    sessionKnowledge *VectorGraphDB

    // Read-only views of other sessions
    otherSessionViews map[string]*VectorGraphDB
}

// StoreKnowledge stores knowledge with appropriate scoping
func (skm *SessionKnowledgeManager) StoreKnowledge(entry KnowledgeEntry) error {
    skm.mu.Lock()
    defer skm.mu.Unlock()

    switch entry.Scope {
    case ScopeProject:
        // Only Archivalist can write to project knowledge
        return skm.projectKnowledge.Store(entry)

    case ScopeSession:
        // Current session's knowledge
        return skm.sessionKnowledge.Store(entry)

    default:
        return fmt.Errorf("unknown knowledge scope: %s", entry.Scope)
    }
}

// QueryKnowledge searches across accessible knowledge bases
func (skm *SessionKnowledgeManager) QueryKnowledge(query string, opts QueryOpts) ([]KnowledgeEntry, error) {
    skm.mu.RLock()
    defer skm.mu.RUnlock()

    var results []KnowledgeEntry

    // Always search project knowledge
    projectResults, err := skm.projectKnowledge.Query(query, opts)
    if err != nil {
        return nil, err
    }
    results = append(results, projectResults...)

    // Search own session knowledge
    sessionResults, err := skm.sessionKnowledge.Query(query, opts)
    if err != nil {
        return nil, err
    }
    results = append(results, sessionResults...)

    // Optionally search other sessions (read-only)
    if opts.IncludeOtherSessions {
        for _, view := range skm.otherSessionViews {
            otherResults, err := view.Query(query, opts)
            if err != nil {
                continue // Don't fail on other session errors
            }
            results = append(results, otherResults...)
        }
    }

    return skm.deduplicateResults(results), nil
}
```

### Security Configuration

```yaml
# ~/.sylk/config.yaml - Security configuration

security:
  # Permission system
  permissions:
    # Safe defaults (always allowed without prompting)
    use_safe_defaults: true

    # Path to additional safe commands/domains
    custom_safe_list: null  # Optional: path to custom safe list

    # Permission persistence
    persist_permissions: true
    permissions_location: ".sylk/local/permissions.yaml"

  # Sandboxing (OFF by default)
  sandbox:
    enabled: false

    # Layer controls
    os_isolation: true       # bubblewrap/Seatbelt
    vfs_layer: true          # Copy-on-write filesystem
    network_proxy: true      # Domain-filtered proxy

    # Resource limits
    max_cpu_percent: 80
    max_memory_mb: 1024
    max_open_files: 256
    max_processes: 32
    network_timeout: 30s

    # Graceful degradation
    fallback_on_error: true  # If sandbox unavailable, warn and continue

  # Audit logging
  audit:
    enabled: true

    # What to log (all categories enabled by default)
    log_permissions: true
    log_file_ops: true
    log_process_exec: true
    log_network: true
    log_llm: true
    log_session: true
    log_config: true

    # Integrity
    signature_interval: 100  # Sign every N entries

    # Retention
    retention_policy: indefinite  # Keep until explicit purge

    # Rotation
    rotate_size_mb: 100

  # Session isolation
  session:
    # Knowledge sharing
    share_project_knowledge: true
    cross_session_readonly: true

    # Credential handling
    allow_profile_override: true
    allow_temp_credentials: true
    temp_credential_max_ttl: 24h
```

---

## The Architect: Request Decomposition & Workflow Planning

**CRITICAL: Architect receives requests from Guide, decomposes them, gathers context through Guide, and creates actionable workflows. User clarification is a LAST RESORT.**

### Architect Responsibilities

```
┌─────────────────────────────────────────────────────────────────────────────────────┐
│                          ARCHITECT RESPONSIBILITIES                                  │
├─────────────────────────────────────────────────────────────────────────────────────┤
│                                                                                     │
│  1. REQUEST INTAKE                                                                  │
│     ├── Receive user prompt from Guide (implementation requests)                   │
│     ├── Receive Academic "research paper" from Guide (research-informed requests)  │
│     └── Both paths flow THROUGH Guide - Architect never receives direct input      │
│                                                                                     │
│  2. REQUEST DECOMPOSITION                                                           │
│     ├── Break apart request into discrete components                               │
│     ├── Identify explicit requirements                                             │
│     ├── Identify implicit assumptions                                              │
│     ├── Identify ambiguities requiring resolution                                  │
│     ├── Identify unknowns (missing context)                                        │
│     └── Identify gaps (information needed but not provided)                        │
│                                                                                     │
│  3. CONTEXT GATHERING (via Guide)                                                   │
│     ├── Transform gaps into queries                                                │
│     ├── Submit queries to Guide for routing (NOT directly to agents)              │
│     │   ├── Guide routes codebase queries → Librarian                              │
│     │   ├── Guide routes research queries → Academic                               │
│     │   └── Guide routes history queries → Archivalist                             │
│     ├── Receive responses through Guide                                            │
│     ├── Integrate responses into understanding                                     │
│     └── Iterate until all resolvable gaps are filled                               │
│                                                                                     │
│  4. USER CLARIFICATION (LAST RESORT)                                                │
│     ├── ONLY after exhausting Librarian, Academic, and Archivalist                 │
│     ├── ONLY for information that cannot be determined from agents                 │
│     ├── ONLY for genuine ambiguities (not laziness)                                │
│     └── Ask specific, actionable questions                                         │
│                                                                                     │
│  5. CHALLENGE USER (when warranted)                                                 │
│     ├── Raise concerns when design seems flawed                                    │
│     ├── Point out contradictions with codebase patterns                            │
│     ├── Suggest alternatives when approach has known failure patterns              │
│     └── Technical pushback is valuable - not insubordination                       │
│                                                                                     │
│  6. WORKFLOW CREATION                                                               │
│     ├── Create actionable implementation plan                                      │
│     ├── Build DAG with proper execution order                                      │
│     ├── Define success criteria                                                    │
│     ├── Identify risks and mitigation strategies                                   │
│     └── Submit to Orchestrator for execution                                       │
│                                                                                     │
│  7. EXECUTION OVERSIGHT                                                             │
│     ├── Monitor workflow progress (via Orchestrator signals)                       │
│     ├── Handle interruptions and modifications                                     │
│     ├── Receive validation feedback (Inspector/Tester corrections)                 │
│     ├── Create fix workflows when needed                                           │
│     └── Report completion to user                                                  │
│                                                                                     │
└─────────────────────────────────────────────────────────────────────────────────────┘
```

### Architect Request Flow

```
┌─────────────────────────────────────────────────────────────────────────────────────┐
│                           ARCHITECT REQUEST FLOW                                     │
├─────────────────────────────────────────────────────────────────────────────────────┤
│                                                                                     │
│  INPUT SOURCES (both via Guide):                                                    │
│  ┌──────────────────────────────────────────────────────────────────────────────┐   │
│  │  Path A: User Implementation Request                                         │   │
│  │    User → Guide (classifies as implementation) → Architect                   │   │
│  │                                                                              │   │
│  │  Path B: Research-Informed Request                                           │   │
│  │    User → Guide → Academic (produces "research paper")                       │   │
│  │    Academic → Guide → Architect (with research context)                      │   │
│  └──────────────────────────────────────────────────────────────────────────────┘   │
│       │                                                                             │
│       ▼                                                                             │
│  ┌─────────────────────────────────────────────────────────────────────────────┐    │
│  │  DECOMPOSE REQUEST                                                          │    │
│  │                                                                             │    │
│  │  "Add caching to the API"                                                   │    │
│  │       ↓                                                                     │    │
│  │  Components:                                                                │    │
│  │    - What API? (ambiguous)                                                  │    │
│  │    - What caching strategy? (unknown)                                       │    │
│  │    - What existing patterns? (gap - need Librarian)                        │    │
│  │    - Have we tried this before? (gap - need Archivalist)                   │    │
│  │    - Best practices? (gap - need Academic if novel)                        │    │
│  └─────────────────────────────────────────────────────────────────────────────┘    │
│       │                                                                             │
│       ▼                                                                             │
│  ┌─────────────────────────────────────────────────────────────────────────────┐    │
│  │  GATHER CONTEXT (via Guide)                                                 │    │
│  │                                                                             │    │
│  │  Query 1: "What API endpoints exist?" → Guide → Librarian → response       │    │
│  │  Query 2: "What caching patterns do we use?" → Guide → Librarian → response│    │
│  │  Query 3: "Previous caching implementations?" → Guide → Archivalist → resp │    │
│  │                                                                             │    │
│  │  IMPORTANT: Architect submits queries to Guide, NOT directly to agents      │    │
│  └─────────────────────────────────────────────────────────────────────────────┘    │
│       │                                                                             │
│       ▼                                                                             │
│  ┌─────────────────────────────────────────────────────────────────────────────┐    │
│  │  STILL HAVE UNRESOLVED GAPS?                                                │    │
│  │                                                                             │    │
│  │  ├── Can agents resolve? → Query more agents via Guide                      │    │
│  │  ├── All agents exhausted? → Ask user (LAST RESORT)                         │    │
│  │  └── All gaps filled? → Create workflow                                     │    │
│  └─────────────────────────────────────────────────────────────────────────────┘    │
│       │                                                                             │
│       ▼                                                                             │
│  ┌─────────────────────────────────────────────────────────────────────────────┐    │
│  │  CREATE ACTIONABLE WORKFLOW                                                 │    │
│  │                                                                             │    │
│  │  Plan with:                                                                 │    │
│  │    - Specific implementation steps                                          │    │
│  │    - Files to modify (from Librarian context)                              │    │
│  │    - Patterns to follow (from Librarian context)                           │    │
│  │    - Risks to avoid (from Archivalist context)                             │    │
│  │    - Success criteria                                                       │    │
│  └─────────────────────────────────────────────────────────────────────────────┘    │
│       │                                                                             │
│       ▼                                                                             │
│  Submit to Orchestrator for execution                                               │
│                                                                                     │
└─────────────────────────────────────────────────────────────────────────────────────┘
```

### Architect Steps Protocol

**CRITICAL: Architect follows this protocol for ALL implementation requests.**

```
┌─────────────────────────────────────────────────────────────────────────────────────┐
│                          ARCHITECT STEPS PROTOCOL                                    │
├─────────────────────────────────────────────────────────────────────────────────────┤
│                                                                                     │
│  STEP 1: CLASSIFY REQUEST TYPE                                                      │
│  ┌──────────────────────────────────────────────────────────────────────────────┐   │
│  │  TRIVIAL        │ Single-line fix, typo, obvious change                     │   │
│  │                 │ → Execute immediately (minimal decomposition)              │   │
│  ├─────────────────┼────────────────────────────────────────────────────────────┤   │
│  │  EXPLICIT       │ User specified exact approach, no ambiguity               │   │
│  │                 │ → Verify approach is sound, then plan with locked method   │   │
│  ├─────────────────┼────────────────────────────────────────────────────────────┤   │
│  │  EXPLORATORY    │ Research/learning question received from Academic         │   │
│  │                 │ → Translate research into actionable implementation        │   │
│  ├─────────────────┼────────────────────────────────────────────────────────────┤   │
│  │  OPEN_ENDED     │ Ambiguous scope, multiple valid interpretations           │   │
│  │                 │ → Full decomposition, extensive agent consultation         │   │
│  ├─────────────────┼────────────────────────────────────────────────────────────┤   │
│  │  GITHUB_WORK    │ PR, issue, commit mentioned explicitly                    │   │
│  │                 │ → Full cycle expected (investigate → implement → PR)       │   │
│  └─────────────────┴────────────────────────────────────────────────────────────┘   │
│                                                                                     │
│  STEP 2: CHECK FOR AMBIGUITY                                                        │
│  ┌──────────────────────────────────────────────────────────────────────────────┐   │
│  │  ASK if:                                                                     │   │
│  │    - Multiple valid interpretations exist                                   │   │
│  │    - User references something unclear ("that API", "the bug")              │   │
│  │    - Scope is unbounded (no clear stopping point)                           │   │
│  │    - Requirements conflict with each other                                  │   │
│  │                                                                              │   │
│  │  PROCEED if:                                                                 │   │
│  │    - Single reasonable interpretation                                       │   │
│  │    - Context makes intent clear                                             │   │
│  │    - Scope is bounded                                                       │   │
│  │    - Reasonable assumption can be stated and verified                       │   │
│  └──────────────────────────────────────────────────────────────────────────────┘   │
│                                                                                     │
│  STEP 3: VALIDATE BEFORE ACTING                                                     │
│  ┌──────────────────────────────────────────────────────────────────────────────┐   │
│  │  Check assumptions:                                                          │   │
│  │    - Query Librarian: "Does [assumed file/pattern] exist?"                  │   │
│  │    - Query Archivalist: "Have we done [assumed approach] before?"           │   │
│  │                                                                              │   │
│  │  Check scope:                                                                │   │
│  │    - What are the boundaries of this task?                                  │   │
│  │    - What is explicitly OUT of scope?                                       │   │
│  │                                                                              │   │
│  │  Check tools/agents:                                                         │   │
│  │    - What agents will be needed?                                            │   │
│  │    - What skills are required?                                              │   │
│  └──────────────────────────────────────────────────────────────────────────────┘   │
│                                                                                     │
│  STEP 4: CHALLENGE USER (when warranted)                                            │
│  ┌──────────────────────────────────────────────────────────────────────────────┐   │
│  │  RAISE CONCERNS when:                                                        │   │
│  │    - Design contradicts existing codebase patterns                          │   │
│  │    - Archivalist shows similar approach failed before                       │   │
│  │    - Academic research suggests better alternatives                         │   │
│  │    - Requirements seem to have unintended consequences                      │   │
│  │                                                                              │   │
│  │  HOW to challenge:                                                           │   │
│  │    - State the concern with evidence                                        │   │
│  │    - Propose alternative if available                                       │   │
│  │    - Accept user's final decision after presenting facts                    │   │
│  │                                                                              │   │
│  │  Example:                                                                    │   │
│  │    "Archivalist shows a similar global caching approach caused memory       │   │
│  │    issues last quarter. Librarian indicates we now use per-request caching  │   │
│  │    in other modules. Should I use the per-request pattern instead?"         │   │
│  └──────────────────────────────────────────────────────────────────────────────┘   │
│                                                                                     │
└─────────────────────────────────────────────────────────────────────────────────────┘
```

### Architect Query Routing (via Guide)

**CRITICAL: Architect NEVER queries knowledge agents directly. All queries flow through Guide.**

```
┌─────────────────────────────────────────────────────────────────────────────────────┐
│                      ARCHITECT QUERY ROUTING (VIA GUIDE)                             │
├─────────────────────────────────────────────────────────────────────────────────────┤
│                                                                                     │
│  WHY ROUTE THROUGH GUIDE:                                                           │
│  ├── Guide handles routing decisions (may route differently than expected)          │
│  ├── Guide tracks routing for learning/improvement                                  │
│  ├── Guide maintains message correlation                                            │
│  ├── Guide can parallelize queries when appropriate                                 │
│  └── Consistent message flow architecture                                           │
│                                                                                     │
│  QUERY FLOW:                                                                        │
│                                                                                     │
│  Architect                                                                          │
│      │                                                                              │
│      │ Query: "What caching patterns exist in our codebase?"                       │
│      │                                                                              │
│      ▼                                                                              │
│  GUIDE ─── classifies: codebase query ───▶ LIBRARIAN                               │
│      │                                                                              │
│      │◀─── response: [caching patterns found] ◀─── LIBRARIAN                       │
│      │                                                                              │
│      ▼                                                                              │
│  Architect (receives response, integrates into understanding)                       │
│                                                                                     │
│  PARALLEL QUERIES (Guide handles):                                                  │
│                                                                                     │
│  Architect submits:                                                                 │
│    Query 1: "What files handle API requests?"                                      │
│    Query 2: "Any failed caching implementations?"                                  │
│    Query 3: "Best practices for response caching?"                                 │
│                                                                                     │
│  Guide routes (potentially in parallel):                                            │
│    Query 1 → Librarian                                                             │
│    Query 2 → Archivalist                                                           │
│    Query 3 → Academic                                                              │
│                                                                                     │
│  Architect receives all responses through Guide                                     │
│                                                                                     │
└─────────────────────────────────────────────────────────────────────────────────────┘
```

### User Clarification as Last Resort

**CRITICAL: Architect MUST exhaust all agent consultation before asking the user.**

```
┌─────────────────────────────────────────────────────────────────────────────────────┐
│                     USER CLARIFICATION AS LAST RESORT                                │
├─────────────────────────────────────────────────────────────────────────────────────┤
│                                                                                     │
│  ESCALATION ORDER:                                                                  │
│                                                                                     │
│  1. First: Librarian                                                                │
│     "What does the codebase tell us about [unknown]?"                              │
│     - Existing patterns, implementations, naming conventions                        │
│     - If Librarian answers → DO NOT ask user                                       │
│                                                                                     │
│  2. Second: Archivalist                                                             │
│     "What does history tell us about [unknown]?"                                   │
│     - Previous implementations, decisions, failures                                 │
│     - If Archivalist answers → DO NOT ask user                                     │
│                                                                                     │
│  3. Third: Academic                                                                 │
│     "What do best practices say about [unknown]?"                                  │
│     - Standard approaches, patterns, considerations                                 │
│     - If Academic provides actionable guidance → DO NOT ask user                   │
│                                                                                     │
│  4. ONLY THEN: User                                                                 │
│     "I've checked the codebase, history, and best practices. I still need          │
│     clarification on [specific question]."                                         │
│                                                                                     │
│  ANTI-PATTERN (DO NOT DO):                                                          │
│  ┌──────────────────────────────────────────────────────────────────────────────┐   │
│  │  ❌ "What caching strategy would you like?"                                   │   │
│  │     (without first checking what patterns we already use)                    │   │
│  │                                                                              │   │
│  │  ❌ "Where should I put this?"                                                │   │
│  │     (without first checking existing project structure)                      │   │
│  │                                                                              │   │
│  │  ❌ "How should I handle errors?"                                             │   │
│  │     (without first checking existing error handling patterns)                │   │
│  └──────────────────────────────────────────────────────────────────────────────┘   │
│                                                                                     │
│  CORRECT PATTERN:                                                                   │
│  ┌──────────────────────────────────────────────────────────────────────────────┐   │
│  │  ✓ "Librarian shows we use Redis caching in module X and in-memory in Y.    │   │
│  │    Archivalist shows Redis approach had connection issues. Which pattern     │   │
│  │    should I use for the new API caching?"                                    │   │
│  │                                                                              │   │
│  │  ✓ "Codebase has two API directories: /api/v1 and /api/v2. Archivalist      │   │
│  │    shows new features go to v2. Proceeding with v2 unless you specify."     │   │
│  │    (statement with implicit confirmation - user only responds if wrong)      │   │
│  └──────────────────────────────────────────────────────────────────────────────┘   │
│                                                                                     │
└─────────────────────────────────────────────────────────────────────────────────────┘
```

### Architect System Prompt

```go
const ArchitectSystemPrompt = `
You are the Architect agent. You receive implementation requests from Guide (either direct
user requests or research-informed requests from Academic) and create actionable workflows.

REQUEST HANDLING PROTOCOL:

1. DECOMPOSE THE REQUEST
   - Break apart into discrete components
   - Identify explicit requirements
   - Identify implicit assumptions (state them explicitly)
   - Identify ambiguities (things that could mean multiple things)
   - Identify unknowns (missing context)
   - Identify gaps (information needed but not provided)

2. GATHER CONTEXT (via Guide - NEVER directly)
   For each gap/unknown, submit a query to Guide:
   - Codebase questions → Guide will route to Librarian
   - History questions → Guide will route to Archivalist
   - Research questions → Guide will route to Academic

   CRITICAL: You do NOT query agents directly. Submit queries to Guide.

3. EXHAUST AGENTS BEFORE ASKING USER
   - Query Librarian: "What does the codebase tell us?"
   - Query Archivalist: "What does history tell us?"
   - Query Academic: "What do best practices say?"
   - ONLY after all agents provide insufficient answers, ask the user

   User clarification is a LAST RESORT, not a first option.

4. CHALLENGE THE USER (when warranted)
   If the request seems flawed, contradicts codebase patterns, or has known failure
   patterns from Archivalist, RAISE YOUR CONCERNS with evidence:
   - "Archivalist shows [similar approach] failed because [reason]"
   - "Librarian shows the codebase uses [different pattern] for this"
   - "Academic research suggests [alternative] would be more appropriate"

   Present alternatives, but accept the user's final decision.

5. CREATE ACTIONABLE WORKFLOW
   Once you have sufficient context:
   - Create specific implementation steps
   - Reference specific files (from Librarian context)
   - Follow existing patterns (from Librarian context)
   - Avoid known pitfalls (from Archivalist context)
   - Define success criteria
   - Build DAG for Orchestrator

QUERY FORMAT (to Guide):
{
  "type": "CONTEXT_QUERY",
  "source": "architect",
  "target": "guide",  // Always target Guide, never agents directly
  "payload": {
    "query": "What caching patterns exist in our API layer?",
    "context": "Planning API caching implementation",
    "preferred_source": "librarian"  // Hint, but Guide decides
  }
}

PRE-DELEGATION PROTOCOL:
Before delegating ANY task to execution:
1. Confirm Librarian has provided codebase context
2. Confirm Archivalist has been checked for failure patterns
3. If research was needed, confirm Academic alignment with codebase
4. Emit formal pre-delegation declaration (stored in Archivalist)
`
```

---

## Phase-Based User Interaction

### Phase Flow Diagram

```
┌─────────────────────────────────────────────────────────────────────────────────────┐
│                        PHASE-BASED USER INTERACTION                                 │
├─────────────────────────────────────────────────────────────────────────────────────┤
│                                                                                     │
│  PHASE 0: SESSION MANAGEMENT (always available)                                     │
│  ┌──────────────────────────────────────────────────────────────────────────────┐   │
│  │  User ←→ SESSION MANAGER (via Guide)                                         │   │
│  │                                                                              │   │
│  │  User: "/session new rate-limiter-feature"                                   │   │
│  │  User: "/session list"                                                       │   │
│  │  User: "/session switch <id>"                                                │   │
│  │  User: "/session suspend"                                                    │   │
│  │  User: "/session resume <id>"                                                │   │
│  │                                                                              │   │
│  └──────────────────────────────────────────────────────────────────────────────┘   │
│                                                                                     │
│  PHASE 1: RESEARCH (optional, user-triggered)                                       │
│  ┌──────────────────────────────────────────────────────────────────────────────┐   │
│  │  User ←→ ACADEMIC                                                            │   │
│  │                                                                              │   │
│  │  User: "How would I design a distributed rate limiter?"                      │   │
│  │  Academic: [Research paper with recommendations]                             │   │
│  │                                                                              │   │
│  │  User: "I want to implement this" → transitions to PLANNING                  │   │
│  └──────────────────────────────────────────────────────────────────────────────┘   │
│                                     │                                               │
│                                     ▼                                               │
│  PHASE 2: PLANNING                                                                  │
│  ┌──────────────────────────────────────────────────────────────────────────────┐   │
│  │  User ←→ ARCHITECT (primary)                                                 │   │
│  │                                                                              │   │
│  │  Architect presents implementation plan for approval                         │   │
│  │  Plan is session-scoped (stored in session context)                          │   │
│  │                                                                              │   │
│  │  User can:                                                                   │   │
│  │  - Approve → proceed to execution                                            │   │
│  │  - Modify → "Also add per-endpoint config"                                   │   │
│  │  - Query → "Why 3 levels?" (Architect explains)                              │   │
│  │  - Query codebase → "Show me existing middleware" (→ Librarian, returns)     │   │
│  │  - Switch session → work is preserved                                        │   │
│  └──────────────────────────────────────────────────────────────────────────────┘   │
│                                     │                                               │
│                                     ▼                                               │
│  PHASE 3: EXECUTION                                                                 │
│  ┌──────────────────────────────────────────────────────────────────────────────┐   │
│  │  User ←→ ARCHITECT (status updates only)                                     │   │
│  │                                                                              │   │
│  │  Architect provides progress updates                                         │   │
│  │  Progress tracked in session context                                         │   │
│  │                                                                              │   │
│  │  User can INTERRUPT at any time:                                             │   │
│  │  - "Stop" → Architect halts, awaits instructions                             │   │
│  │  - "Actually, also add X" → Architect revises plan                           │   │
│  │  - "What's taking so long?" → Architect explains current state               │   │
│  │  - "/session suspend" → Preserve state, switch to other work                 │   │
│  │                                                                              │   │
│  │  If Engineer needs clarification:                                            │   │
│  │  - Engineer → Orchestrator → ARCHITECT → User                                │   │
│  │  - User responds to Architect (never sees Engineer)                          │   │
│  └──────────────────────────────────────────────────────────────────────────────┘   │
│                                     │                                               │
│                                     ▼                                               │
│  PHASE 4: INSPECTION                                                                │
│  ┌──────────────────────────────────────────────────────────────────────────────┐   │
│  │  User ←→ INSPECTOR (primary for results review)                              │   │
│  │  User ←→ ARCHITECT (for fix approval)                                        │   │
│  │                                                                              │   │
│  │  Inspector presents findings                                                 │   │
│  │  Issues recorded in session context                                          │   │
│  │                                                                              │   │
│  │  User can:                                                                   │   │
│  │  - Ask Inspector for details → "Explain the race condition"                  │   │
│  │  - Override → "Ignore the per-endpoint config for now"                       │   │
│  │  - Approve fixes → "Fix all of these"                                        │   │
│  │                                                                              │   │
│  │  On fix approval:                                                            │   │
│  │  - Inspector → ARCHITECT (with corrections)                                  │   │
│  │  - Architect presents fix plan to user                                       │   │
│  │  - User approves → back to EXECUTION phase                                   │   │
│  └──────────────────────────────────────────────────────────────────────────────┘   │
│                                     │                                               │
│                                     ▼                                               │
│  PHASE 5: TESTING                                                                   │
│  ┌──────────────────────────────────────────────────────────────────────────────┐   │
│  │  User ←→ TESTER (primary for test planning/results)                          │   │
│  │  User ←→ ARCHITECT (for implementation fix approval)                         │   │
│  │                                                                              │   │
│  │  Tester presents test plan for approval                                      │   │
│  │                                                                              │   │
│  │  User can:                                                                   │   │
│  │  - Approve → Tester → Architect (test DAG) → execution                       │   │
│  │  - Modify → "Also add a test for X"                                          │   │
│  │  - Skip tests → "Skip tests for now"                                         │   │
│  │                                                                              │   │
│  │  After test execution:                                                       │   │
│  │  - Tester presents results                                                   │   │
│  │  - If failures, Tester explains if test or implementation issue              │   │
│  │  - Implementation issues → Architect for fix workflow                        │   │
│  └──────────────────────────────────────────────────────────────────────────────┘   │
│                                     │                                               │
│                                     ▼                                               │
│  PHASE 6: COMPLETE                                                                  │
│  ┌──────────────────────────────────────────────────────────────────────────────┐   │
│  │  User ←→ ARCHITECT (final summary)                                           │   │
│  │                                                                              │   │
│  │  Architect presents completion summary:                                      │   │
│  │  - Files created/modified                                                    │   │
│  │  - Tests passed                                                              │   │
│  │  - Validation status                                                         │   │
│  │  - Deferred items (user overrides)                                           │   │
│  │                                                                              │   │
│  │  Session transitions to COMPLETED state                                      │   │
│  │  Patterns/decisions promoted to global Archivalist                           │   │
│  └──────────────────────────────────────────────────────────────────────────────┘   │
│                                                                                     │
└─────────────────────────────────────────────────────────────────────────────────────┘
```

---

## Agent Roles, Skills, Tools, and Hooks

### Complete Agent Summary

| Agent | Role | User Interaction | Primary Responsibility |
|-------|------|------------------|------------------------|
| **Academic** | External knowledge RAG | DIRECT (triggered by research queries) | Research papers, best practices, external references |
| **Architect** | Planning & coordination | PRIMARY (default agent) | Abstract → Concrete, DAG design, user coordination |
| **Orchestrator** | Workflow execution | NONE (invisible) | Execute DAGs, manage Engineers, status propagation |
| **Engineer** | Task execution | NONE (invisible) | Code writing, problem solving |
| **Designer** | UI/UX implementation | PRIMARY (during UI work) | Component design, styling, accessibility, design systems |
| **Librarian** | Local codebase RAG | DIRECT (triggered by codebase queries) | Code context, pattern detection |
| **Archivalist** | Historical RAG | DIRECT (triggered by history queries) | Past decisions, solution patterns |
| **Inspector** | Code validation | PRIMARY (during inspection) | Compliance checking, issue detection |
| **Tester** | Test planning & execution | PRIMARY (during testing) | Test planning, execution, failure analysis |
| **Guide** | Universal router | NONE (invisible) | Intent classification, message routing |

### Agent Communication Style

**CRITICAL: All agents follow this communication discipline.**

```
┌─────────────────────────────────────────────────────────────────────────────────────┐
│                        AGENT COMMUNICATION STYLE                                     │
├─────────────────────────────────────────────────────────────────────────────────────┤
│                                                                                     │
│  DO:                                                                                │
│  ├── Be concise, direct, technical                                                  │
│  ├── Challenge flawed approaches: "This will fail because..."                       │
│  ├── Say "I don't know" when uncertain                                              │
│  ├── Provide evidence for claims (file paths, line numbers, test results)           │
│  ├── Reference specific files/lines when discussing code                            │
│  └── State facts, not feelings                                                      │
│                                                                                     │
│  DON'T:                                                                             │
│  ├── Status acknowledgments: "I'm on it!", "Sure thing!", "Absolutely!"             │
│  ├── Flattery: "Great question!", "Excellent idea!", "That's smart!"                │
│  ├── Hedging: "I think maybe...", "Perhaps we could...", "It might be..."           │
│  ├── Apologies: "Sorry for the confusion", "My apologies"                           │
│  ├── Filler: "Let me help you with that", "I'd be happy to..."                      │
│  └── Emotional language: "excited", "love", "amazing"                               │
│                                                                                     │
│  WHEN USER'S APPROACH IS FLAWED:                                                    │
│  ┌──────────────────────────────────────────────────────────────────────────────┐   │
│  │  CORRECT:                                                                    │   │
│  │  "This approach has issues:                                                  │   │
│  │   1. [Specific problem with evidence]                                        │   │
│  │   2. [Specific problem with evidence]                                        │   │
│  │   Alternative: [Concrete suggestion with reasoning]"                         │   │
│  │                                                                              │   │
│  │  INCORRECT:                                                                  │   │
│  │  "That's an interesting idea! We could maybe try a different approach..."    │   │
│  └──────────────────────────────────────────────────────────────────────────────┘   │
│                                                                                     │
└─────────────────────────────────────────────────────────────────────────────────────┘
```

#### Base System Prompt Fragment (All Agents)

```go
const AgentCommunicationStylePrompt = `
COMMUNICATION STYLE:
- Be concise, direct, technical
- Challenge flawed approaches with evidence: "This will fail because [specific reason]"
- Say "I don't know" when uncertain - never fabricate
- Provide evidence: file paths, line numbers, test results, error messages
- State facts, not feelings

FORBIDDEN:
- Status acknowledgments ("I'm on it!", "Sure thing!")
- Flattery ("Great question!", "Excellent idea!")
- Hedging ("I think maybe...", "Perhaps we could...")
- Apologies ("Sorry for the confusion")
- Filler phrases ("Let me help you with that")

When disagreeing with user's approach:
"This approach has issues: [specific problems with evidence]. Alternative: [concrete suggestion]."
`
```

### Agent Step 0: Role-Aware Skill Decision

**CRITICAL: Every agent performs Step 0 before processing. This is a role-aware decision gate that determines IF skills should be invoked.**

```
┌─────────────────────────────────────────────────────────────────────────────────────┐
│                    AGENT STEP 0: ROLE-AWARE SKILL DECISION                           │
├─────────────────────────────────────────────────────────────────────────────────────┤
│                                                                                     │
│  PRINCIPLE: Before processing ANY information, each agent asks:                     │
│  "Given MY specific role and the information I'm processing, SHOULD I invoke        │
│   any of my skills?"                                                                │
│                                                                                     │
│  This is NOT simple keyword matching. It is:                                        │
│  ├── CONTEXT-AWARE: Considers the information being processed                       │
│  ├── ROLE-AWARE: Considers what this agent is responsible for                       │
│  └── DECISION-FOCUSED: Determines IF skills should be invoked, not just which       │
│                                                                                     │
│  STEP 0 FLOW (All Agents):                                                          │
│  ┌──────────────────────────────────────────────────────────────────────────────┐   │
│  │  1. Receive information (request, message, data)                             │   │
│  │                                                                              │   │
│  │  2. Evaluate against MY role:                                                │   │
│  │     - What am I responsible for?                                             │   │
│  │     - Is this information within my domain?                                  │   │
│  │     - Do I have skills that apply to this?                                   │   │
│  │                                                                              │   │
│  │  3. Decision:                                                                │   │
│  │     ├── YES, invoke skills → Proceed with skill execution                    │   │
│  │     ├── NO, not my domain → Signal to Guide for re-routing                   │   │
│  │     └── PARTIAL, need more info → Query for additional context               │   │
│  └──────────────────────────────────────────────────────────────────────────────┘   │
│                                                                                     │
│  EXAMPLES BY AGENT:                                                                 │
│  ┌──────────────────────────────────────────────────────────────────────────────┐   │
│  │  GUIDE (Router):                                                             │   │
│  │    Input: "/session new feature-branch"                                      │   │
│  │    Step 0: "I'm the router. This is a session command. Should I invoke       │   │
│  │            session skill?" → YES, execute immediately                        │   │
│  │                                                                              │   │
│  │  ARCHITECT (Planner):                                                        │   │
│  │    Input: Academic research paper on caching strategies                      │   │
│  │    Step 0: "I'm the planner. I received research for implementation.         │   │
│  │            Should I invoke decomposition?" → YES, decompose into workflow    │   │
│  │                                                                              │   │
│  │  LIBRARIAN (Codebase Knowledge):                                             │   │
│  │    Input: "What are best practices for rate limiting?"                       │   │
│  │    Step 0: "I'm codebase knowledge. This asks about external best practices. │   │
│  │            Should I invoke search?" → NO, signal re-route to Academic        │   │
│  │                                                                              │   │
│  │  ACADEMIC (External Knowledge):                                              │   │
│  │    Input: "Where is the auth middleware in our codebase?"                    │   │
│  │    Step 0: "I'm external knowledge. This asks about local codebase.          │   │
│  │            Should I invoke research?" → NO, signal re-route to Librarian     │   │
│  │                                                                              │   │
│  │  INSPECTOR (Validator):                                                      │   │
│  │    Input: Completed code from Engineer                                       │   │
│  │    Step 0: "I'm the validator. This is implementation output.                │   │
│  │            Should I invoke validation?" → YES, validate against criteria     │   │
│  │                                                                              │   │
│  │  TESTER (Test Execution):                                                    │   │
│  │    Input: Request to "add caching to API"                                    │   │
│  │    Step 0: "I'm test execution. This is an implementation request.           │   │
│  │            Should I invoke test planning?" → NO, not my phase yet            │   │
│  └──────────────────────────────────────────────────────────────────────────────┘   │
│                                                                                     │
│  RE-ROUTING SIGNAL:                                                                 │
│  When Step 0 determines "not my domain", agent sends to Guide:                      │
│  {                                                                                  │
│    "type": "REROUTE_REQUEST",                                                       │
│    "source": "librarian",                                                           │
│    "reason": "Query asks about external best practices, not codebase",              │
│    "suggested_target": "academic",                                                  │
│    "original_request": {<original message>}                                         │
│  }                                                                                  │
│                                                                                     │
│  REROUTE HANDLING (Guide's responsibility):                                         │
│  ┌──────────────────────────────────────────────────────────────────────────────┐   │
│  │  1. Guide receives REROUTE_REQUEST from agent                                │   │
│  │                                                                              │   │
│  │  2. Guide attempts re-routing:                                               │   │
│  │     - If suggested_target provided → route to suggested target               │   │
│  │     - If no suggestion → re-classify and route                               │   │
│  │                                                                              │   │
│  │  3. Track reroute count for this request:                                    │   │
│  │     - First reroute → normal, proceed                                        │   │
│  │     - Second reroute → warning, likely ambiguous request                     │   │
│  │     - Third reroute → STOP, ask user for clarification                       │   │
│  │                                                                              │   │
│  │  4. User clarification (LAST RESORT - after 2+ reroutes):                    │   │
│  │     "Your request was routed to [agent1] and [agent2], but neither could     │   │
│  │      handle it. [agent1] said: [reason1]. [agent2] said: [reason2].          │   │
│  │      Can you clarify what you're looking for?"                               │   │
│  └──────────────────────────────────────────────────────────────────────────────┘   │
│                                                                                     │
│  REROUTE TRACKING:                                                                  │
│  {                                                                                  │
│    "request_id": "req_abc123",                                                      │
│    "reroute_count": 2,                                                              │
│    "reroute_history": [                                                             │
│      {"from": "librarian", "reason": "external best practices", "to": "academic"},  │
│      {"from": "academic", "reason": "asks about our codebase", "to": "librarian"}   │
│    ],                                                                               │
│    "action": "ASK_USER_CLARIFICATION"  // after 2+ ping-pongs                       │
│  }                                                                                  │
│                                                                                     │
└─────────────────────────────────────────────────────────────────────────────────────┘
```

#### Agent Step 0 Base Prompt Fragment (All Agents)

```go
const AgentStep0Prompt = `
STEP 0: ROLE-AWARE SKILL DECISION (Do this FIRST)

Before processing ANY information, ask yourself:
"Given MY specific role and this information, SHOULD I invoke any of my skills?"

YOUR ROLE: [AGENT_ROLE_DESCRIPTION]
YOUR DOMAIN: [AGENT_DOMAIN]
YOUR SKILLS: [AGENT_SKILL_LIST]

EVALUATE:
1. Is this information within my domain?
2. Do I have skills that apply to this?
3. Am I the right agent for this task?

DECISION:
- YES → Proceed with skill execution
- NO → Send REROUTE_REQUEST to Guide with reason and suggested target
- PARTIAL → Query for additional context before deciding

REROUTE FORMAT:
{
  "type": "REROUTE_REQUEST",
  "source": "[my_agent_id]",
  "reason": "[why this isn't my domain]",
  "suggested_target": "[better_agent]",
  "original_request": {<original>}
}

DO NOT process information outside your domain. Route it correctly.
`
```

---

## The Three Knowledge RAGs

```
┌─────────────────────────────────────────────────────────────────────────────────────┐
│                            THREE KNOWLEDGE DOMAINS                                  │
├─────────────────────────────────────────────────────────────────────────────────────┤
│                                                                                     │
│  ┌────────────────┬─────────────────────┬─────────────────────────────────────────┐ │
│  │ Agent          │ Question Answered   │ Session Behavior                        │ │
│  ├────────────────┼─────────────────────┼─────────────────────────────────────────┤ │
│  │                │                     │                                         │ │
│  │ LIBRARIAN      │ "What EXISTS?"      │ Session-independent (codebase is        │ │
│  │ (Local RAG)    │                     │ shared across all sessions)             │ │
│  │                │ Current codebase    │                                         │ │
│  │                │ state               │ Note: Tracks which files were read      │ │
│  │                │                     │ per-session for deduplication           │ │
│  │                │                     │                                         │ │
│  ├────────────────┼─────────────────────┼─────────────────────────────────────────┤ │
│  │                │                     │                                         │ │
│  │ ARCHIVALIST    │ "What was DONE?"    │ Session-scoped writes, cross-session    │ │
│  │ (Historical    │                     │ reads for promoted entries              │ │
│  │  RAG)          │ Past decisions,     │                                         │ │
│  │                │ solutions, outcomes │ Entry.SessionID tracks origin           │ │
│  │                │                     │ Entry.Promoted=true for global access   │ │
│  │                │                     │                                         │ │
│  ├────────────────┼─────────────────────┼─────────────────────────────────────────┤ │
│  │                │                     │                                         │ │
│  │ ACADEMIC       │ "What CAN be done?" │ Session-independent (research is        │ │
│  │ (External RAG) │                     │ globally applicable)                    │ │
│  │                │ World knowledge,    │                                         │ │
│  │                │ best practices      │ Results cached for cross-session reuse  │ │
│  │                │                     │                                         │ │
│  └────────────────┴─────────────────────┴─────────────────────────────────────────┘ │
│                                                                                     │
└─────────────────────────────────────────────────────────────────────────────────────┘
```

---

## Agent Skills (Progressive Disclosure)

Each agent has skills that are loaded progressively based on context. Skills are loaded lazily to minimize token usage.

### Skill Loading Strategy

```
┌─────────────────────────────────────────────────────────────────────────────────────┐
│                          PROGRESSIVE SKILL DISCLOSURE                                │
├─────────────────────────────────────────────────────────────────────────────────────┤
│                                                                                     │
│  TIER 1: CORE SKILLS (Always Loaded)                                                │
│  ├── Essential for agent's primary function                                         │
│  ├── ~5-10 skills per agent                                                         │
│  └── Loaded at agent startup                                                        │
│                                                                                     │
│  TIER 2: CONTEXTUAL SKILLS (Loaded on Demand)                                       │
│  ├── Triggered by keywords in user input                                            │
│  ├── Triggered by workflow phase                                                    │
│  ├── Triggered by other agent requests                                              │
│  └── Unloaded when context changes                                                  │
│                                                                                     │
│  TIER 3: SPECIALIZED SKILLS (Explicitly Requested)                                  │
│  ├── Advanced capabilities                                                          │
│  ├── Loaded via DSL command or explicit request                                     │
│  └── Higher token cost, used sparingly                                              │
│                                                                                     │
└─────────────────────────────────────────────────────────────────────────────────────┘
```

### Direct Consultation Skills (All Agents)

Each agent has direct consultation skills for token-efficient inter-agent communication. These bypass Guide routing for known targets. See [Direct Consultation Protocol](#direct-consultation-protocol) for details.

```go
// Direct Consultation Skills - Added to each agent's Tier 1 (Core) skills
// Pattern: consult_<target_agent>

// Standard consultation skill template
type ConsultationSkill struct {
    Name        string   // consult_<agent>
    Description string   // "Directly consult <Agent> for <domain> questions"
    Domain      string   // "consultation"
    Target      string   // Target agent type
    Keywords    []string // Trigger keywords
    Priority    int      // Usually 85 (high but below core function)
    Parameters  []Param  // Standard: question, context, priority
}

// Standard parameters for all consultation skills
var consultationParams = []Param{
    {Name: "question", Type: "string", Required: true, Description: "The question to ask"},
    {Name: "context", Type: "object", Required: false, Description: "Relevant context for the consultation"},
    {Name: "priority", Type: "enum", Values: []string{"blocking", "async"}, Required: false, Default: "blocking"},
    {Name: "max_tokens", Type: "int", Required: false, Default: 2000, Description: "Max response tokens"},
}
```

**Consultation Skill Matrix by Agent:**

| Agent | Has Direct Consultation Skills For |
|-------|-----------------------------------|
| **Guide** | All agents (architect, engineer, designer, inspector, tester, librarian, archivalist, academic) |
| **Architect** | engineer, designer, inspector, tester, librarian, archivalist, academic |
| **Engineer** | architect, designer, inspector, tester, librarian, archivalist, academic |
| **Designer** | architect, engineer, inspector, tester, librarian, archivalist, academic |
| **Inspector** | architect, engineer, designer, tester, librarian, archivalist |
| **Tester** | architect, engineer, designer, inspector, librarian, archivalist |
| **Librarian** | archivalist, academic |
| **Archivalist** | librarian, academic |
| **Academic** | librarian, archivalist |

### Guide Skills

```go
// Core Skills (Tier 1)
guide_skills_core := []Skill{
    {
        Name:        "route",
        Description: "Route a message to the appropriate agent",
        Domain:      "routing",
        Keywords:    []string{"route", "send", "forward", "dispatch"},
        Priority:    100,
        Parameters: []Param{
            {Name: "input", Type: "string", Required: true},
            {Name: "target", Type: "string", Required: false},
            {Name: "intent", Type: "enum", Values: []string{"recall", "store", "check", "declare", "complete"}},
        },
    },
    {
        Name:        "classify",
        Description: "Classify user intent without routing",
        Domain:      "routing",
        Keywords:    []string{"classify", "analyze", "understand"},
        Priority:    90,
    },
    {
        Name:        "help",
        Description: "Provide help about available commands and agents",
        Domain:      "system",
        Keywords:    []string{"help", "?", "how", "what"},
        Priority:    100,
    },
    {
        Name:        "status",
        Description: "Get current system status",
        Domain:      "system",
        Keywords:    []string{"status", "health", "agents"},
        Priority:    80,
    },
}

// Contextual Skills (Tier 2)
guide_skills_contextual := []Skill{
    {
        Name:        "session_switch",
        Description: "Switch to a different session",
        Domain:      "session",
        Keywords:    []string{"switch", "session", "change context"},
        LoadTrigger: "session|switch|context",
    },
    {
        Name:        "session_list",
        Description: "List all available sessions",
        Domain:      "session",
        Keywords:    []string{"sessions", "list", "show sessions"},
        LoadTrigger: "session|list|sessions",
    },
    {
        Name:        "broadcast",
        Description: "Broadcast a message to multiple agents",
        Domain:      "routing",
        Keywords:    []string{"broadcast", "notify all", "announce"},
        LoadTrigger: "broadcast|notify|announce",
    },
}

// Direct Consultation Skills (Tier 1 - Core)
// Guide can directly consult ALL agents (it's the universal router)
guide_skills_consultation := []Skill{
    {
        Name:        "consult_architect",
        Description: "Directly consult Architect for workflow planning and task decomposition",
        Domain:      "consultation",
        Target:      "architect",
        Keywords:    []string{"ask architect", "consult architect", "architect help"},
        Priority:    85,
        Parameters:  consultationParams,
    },
    {
        Name:        "consult_engineer",
        Description: "Directly consult Engineer for implementation questions",
        Domain:      "consultation",
        Target:      "engineer",
        Keywords:    []string{"ask engineer", "consult engineer", "implementation help"},
        Priority:    85,
        Parameters:  consultationParams,
    },
    {
        Name:        "consult_designer",
        Description: "Directly consult Designer for UI/UX questions",
        Domain:      "consultation",
        Target:      "designer",
        Keywords:    []string{"ask designer", "consult designer", "ui help", "ux help"},
        Priority:    85,
        Parameters:  consultationParams,
    },
    {
        Name:        "consult_inspector",
        Description: "Directly consult Inspector for code quality questions",
        Domain:      "consultation",
        Target:      "inspector",
        Keywords:    []string{"ask inspector", "consult inspector", "quality help"},
        Priority:    85,
        Parameters:  consultationParams,
    },
    {
        Name:        "consult_tester",
        Description: "Directly consult Tester for testing questions",
        Domain:      "consultation",
        Target:      "tester",
        Keywords:    []string{"ask tester", "consult tester", "testing help"},
        Priority:    85,
        Parameters:  consultationParams,
    },
    {
        Name:        "consult_librarian",
        Description: "Directly consult Librarian for codebase questions",
        Domain:      "consultation",
        Target:      "librarian",
        Keywords:    []string{"ask librarian", "consult librarian", "codebase help"},
        Priority:    85,
        Parameters:  consultationParams,
    },
    {
        Name:        "consult_archivalist",
        Description: "Directly consult Archivalist for historical context",
        Domain:      "consultation",
        Target:      "archivalist",
        Keywords:    []string{"ask archivalist", "consult archivalist", "history help"},
        Priority:    85,
        Parameters:  consultationParams,
    },
    {
        Name:        "consult_academic",
        Description: "Directly consult Academic for research and best practices",
        Domain:      "consultation",
        Target:      "academic",
        Keywords:    []string{"ask academic", "consult academic", "research help"},
        Priority:    85,
        Parameters:  consultationParams,
    },
}

// Session Recording & Replay Skills (Tier 2)
// Guide owns replay since it's the universal entry point for all queries
guide_skills_replay := []Skill{
    {
        Name:        "start_recording",
        Description: "Start recording session queries for replay",
        Domain:      "replay",
        Keywords:    []string{"record", "capture", "save session", "start recording"},
        LoadTrigger: "record|capture|save session",
        Parameters: []Param{
            {Name: "recording_name", Type: "string", Required: false, Description: "Name for this recording"},
            {Name: "include_responses", Type: "bool", Required: false, Default: true},
            {Name: "include_agent_actions", Type: "bool", Required: false, Default: false, Description: "Capture detailed agent actions"},
        },
    },
    {
        Name:        "stop_recording",
        Description: "Stop recording and save to Archivalist",
        Domain:      "replay",
        Keywords:    []string{"stop recording", "save recording", "end recording"},
        LoadTrigger: "stop recording|save recording|end recording",
        Parameters: []Param{
            {Name: "tags", Type: "array", Required: false, Description: "Tags for categorization"},
            {Name: "description", Type: "string", Required: false, Description: "Description of what was recorded"},
        },
    },
    {
        Name:        "replay_session",
        Description: "Replay a recorded session",
        Domain:      "replay",
        Keywords:    []string{"replay", "rerun", "repeat session", "playback"},
        LoadTrigger: "replay|rerun|repeat|playback",
        Parameters: []Param{
            {Name: "recording_id", Type: "string", Required: true, Description: "Recording ID or name"},
            {Name: "mode", Type: "enum", Values: []string{"exact", "interactive", "dry_run"}, Required: false, Default: "interactive"},
            {Name: "skip_failures", Type: "bool", Required: false, Default: false},
            {Name: "start_from", Type: "int", Required: false, Default: 0, Description: "Query index to start from"},
            {Name: "modifications", Type: "object", Required: false, Description: "Query modifications to apply"},
        },
    },
    {
        Name:        "replay_query",
        Description: "Replay a single query from history",
        Domain:      "replay",
        Keywords:    []string{"replay query", "rerun query", "again", "repeat that"},
        LoadTrigger: "again|repeat|rerun query",
        Parameters: []Param{
            {Name: "query_id", Type: "string", Required: false, Description: "Specific query ID"},
            {Name: "offset", Type: "int", Required: false, Default: -1, Description: "Relative offset (-1 = last query)"},
            {Name: "with_modifications", Type: "string", Required: false, Description: "Modified query text"},
        },
    },
    {
        Name:        "list_recordings",
        Description: "List available session recordings",
        Domain:      "replay",
        Keywords:    []string{"recordings", "list recordings", "saved sessions", "show recordings"},
        LoadTrigger: "recordings|list recordings|saved sessions",
        Parameters: []Param{
            {Name: "tags", Type: "array", Required: false},
            {Name: "session_id", Type: "string", Required: false},
            {Name: "limit", Type: "int", Required: false, Default: 20},
        },
    },
    {
        Name:        "get_recording_info",
        Description: "Get details about a specific recording",
        Domain:      "replay",
        Keywords:    []string{"recording info", "recording details", "show recording"},
        Parameters: []Param{
            {Name: "recording_id", Type: "string", Required: true},
            {Name: "include_queries", Type: "bool", Required: false, Default: false},
        },
    },
}
```

#### Session Recording & Replay Protocol

**Guide records all queries at the routing level and can replay them on demand.**

```
┌─────────────────────────────────────────────────────────────────────────────────────┐
│                      SESSION RECORDING & REPLAY PROTOCOL                             │
├─────────────────────────────────────────────────────────────────────────────────────┤
│                                                                                     │
│  RECORDING FLOW:                                                                    │
│  ┌──────────────────────────────────────────────────────────────────────────────┐   │
│  │  1. User: "start recording auth-setup"                                       │   │
│  │  2. Guide begins capturing all queries                                       │   │
│  │  3. Each query captured with:                                                │   │
│  │     - Raw query text                                                         │   │
│  │     - Classified intent                                                      │   │
│  │     - Routed-to agent                                                        │   │
│  │     - Response (optional)                                                    │   │
│  │     - Agent actions (optional, detailed)                                     │   │
│  │     - Timestamp and duration                                                 │   │
│  │  4. User: "stop recording" with optional tags                                │   │
│  │  5. Recording stored in Archivalist                                          │   │
│  └──────────────────────────────────────────────────────────────────────────────┘   │
│                                                                                     │
│  REPLAY MODES:                                                                      │
│  ┌──────────────────────────────────────────────────────────────────────────────┐   │
│  │  EXACT:                                                                      │   │
│  │  - Re-execute all queries without pauses                                     │   │
│  │  - No user intervention                                                      │   │
│  │  - Useful for automation/scripting                                           │   │
│  │                                                                              │   │
│  │  INTERACTIVE (default):                                                      │   │
│  │  - Pause between queries                                                     │   │
│  │  - User can: continue, skip, modify, abort                                   │   │
│  │  - Useful for learning/teaching                                              │   │
│  │                                                                              │   │
│  │  DRY_RUN:                                                                    │   │
│  │  - Show what would execute without executing                                 │   │
│  │  - Preview the recording                                                     │   │
│  │  - Useful for verification                                                   │   │
│  └──────────────────────────────────────────────────────────────────────────────┘   │
│                                                                                     │
│  RECORDING DATA MODEL:                                                              │
│  {                                                                                  │
│    "id": "rec_abc123",                                                              │
│    "name": "auth-setup",                                                            │
│    "session_id": "sess_xyz",                                                        │
│    "created_at": "2024-01-15T10:30:00Z",                                            │
│    "duration": "5m32s",                                                             │
│    "query_count": 12,                                                               │
│    "tags": ["setup", "auth", "tutorial"],                                           │
│    "queries": [                                                                     │
│      {                                                                              │
│        "index": 0,                                                                  │
│        "timestamp": "2024-01-15T10:30:00Z",                                         │
│        "raw_query": "show me the auth module structure",                            │
│        "classified_intent": "EXPLORATORY",                                          │
│        "routed_to": "librarian",                                                    │
│        "response": "...",                                                           │
│        "success": true,                                                             │
│        "duration": "2.3s"                                                           │
│      },                                                                             │
│      ...                                                                            │
│    ]                                                                                │
│  }                                                                                  │
│                                                                                     │
│  USE CASES:                                                                         │
│  ├── "Record this setup for new team members" → Tutorial creation                  │
│  ├── "Replay yesterday's debugging session" → Context restoration                  │
│  ├── "Run that auth fix again on this branch" → Repeatable workflows              │
│  ├── "Replay but change the config path" → Modified replay                         │
│  └── "Show me what I did last week" → Session archaeology                          │
│                                                                                     │
└─────────────────────────────────────────────────────────────────────────────────────┘
```

#### Guide Session Replay System Prompt Addition

```go
const GuideSessionReplayPrompt = `
SESSION RECORDING & REPLAY:
You can record and replay sessions for automation, teaching, and context restoration.

RECORDING:
- "start recording [name]" - Begin capturing all queries
- "stop recording" - Save recording to Archivalist
- Recording captures: query, intent, routing, response, timing

REPLAY:
- "replay [recording]" - Replay a recorded session
- "replay [recording] --mode=dry_run" - Preview without executing
- "replay query" or "again" - Repeat the last query
- "replay query -2" - Repeat the query before last

MODES:
- exact: No pauses, full automation
- interactive: Pause between queries, allow modifications
- dry_run: Preview only, no execution

MODIFICATIONS:
During interactive replay, you can:
- continue: Execute next query as-is
- skip: Skip this query
- modify: Change the query before executing
- abort: Stop replay
`
```

### Archivalist Skills

```go
// Core Skills (Tier 1)
archivalist_skills_core := []Skill{
    {
        Name:        "store",
        Description: "Store information in the chronicle",
        Domain:      "chronicle",
        Keywords:    []string{"store", "save", "record", "remember", "log"},
        Priority:    100,
        Parameters: []Param{
            {Name: "content", Type: "string", Required: true},
            {Name: "category", Type: "enum", Values: []string{"decision", "insight", "pattern", "failure", "task_state", "timeline", "user_voice", "hypothesis", "open_thread", "general"}, Required: true},
            {Name: "title", Type: "string", Required: false},
        },
    },
    {
        Name:        "query",
        Description: "Query the chronicle for stored information",
        Domain:      "chronicle",
        Keywords:    []string{"query", "find", "search", "recall", "retrieve", "what", "when", "how"},
        Priority:    100,
        Parameters: []Param{
            {Name: "search", Type: "string", Required: false},
            {Name: "category", Type: "enum", Values: []string{"all", "decision", "insight", "pattern", "failure"}, Required: false},
            {Name: "limit", Type: "int", Required: false},
            {Name: "cross_session", Type: "bool", Required: false, Description: "Query across all sessions"},
        },
    },
    {
        Name:        "briefing",
        Description: "Get current session state briefing",
        Domain:      "memory",
        Keywords:    []string{"briefing", "status", "context", "state", "progress", "current"},
        Priority:    90,
        Parameters: []Param{
            {Name: "tier", Type: "enum", Values: []string{"micro", "standard", "full"}, Required: false},
        },
    },
}

// Contextual Skills (Tier 2)
archivalist_skills_contextual := []Skill{
    {
        Name:        "cross_session_query",
        Description: "Query historical data from other sessions",
        Domain:      "chronicle",
        Keywords:    []string{"other sessions", "past work", "historical", "before"},
        LoadTrigger: "other session|past|historical|before|previous",
        Parameters: []Param{
            {Name: "session_ids", Type: "array", Required: false},
            {Name: "exclude_current", Type: "bool", Required: false},
        },
    },
    {
        Name:        "promote_pattern",
        Description: "Promote a session-local pattern to global",
        Domain:      "chronicle",
        Keywords:    []string{"promote", "global", "share"},
        LoadTrigger: "promote|global|share pattern",
    },
    {
        Name:        "session_summary",
        Description: "Generate summary of a session's work",
        Domain:      "chronicle",
        Keywords:    []string{"summarize session", "session summary"},
        LoadTrigger: "summarize|summary",
    },
}

// Specialized Skills (Tier 3)
archivalist_skills_specialized := []Skill{
    {
        Name:        "fact_extraction",
        Description: "Extract structured facts from entries",
        Domain:      "analysis",
        Keywords:    []string{"extract facts", "analyze entries"},
        RequiresExplicit: true,
    },
    {
        Name:        "conflict_resolution",
        Description: "Resolve conflicts between concurrent writes",
        Domain:      "concurrency",
        Keywords:    []string{"conflict", "resolve", "merge"},
        RequiresExplicit: true,
    },
}

// Direct Consultation Skills (Tier 1 - Core)
// Archivalist can directly consult knowledge agents
archivalist_skills_consultation := []Skill{
    {
        Name:        "consult_librarian",
        Description: "Directly consult Librarian for current codebase state",
        Domain:      "consultation",
        Target:      "librarian",
        Keywords:    []string{"ask librarian", "current code", "codebase state"},
        Priority:    85,
        Parameters:  consultationParams,
    },
    {
        Name:        "consult_academic",
        Description: "Directly consult Academic for research context",
        Domain:      "consultation",
        Target:      "academic",
        Keywords:    []string{"ask academic", "research context", "best practices"},
        Priority:    85,
        Parameters:  consultationParams,
    },
}

// Failure Pattern Memory Skills (Tier 2 - loaded for implementation/debugging)
archivalist_skills_failure := []Skill{
    {
        Name:        "query_failure_patterns",
        Description: "Query for similar past failures and their resolutions",
        Domain:      "failure_memory",
        Keywords:    []string{"failure", "failed", "error", "similar", "tried before"},
        LoadTrigger: "implement|build|error|fail|debug|fix",
        Parameters: []Param{
            {Name: "approach", Type: "string", Required: true, Description: "The approach being considered"},
            {Name: "context", Type: "string", Required: false, Description: "Current task context"},
        },
    },
    {
        Name:        "record_failure",
        Description: "Record a failure pattern for future reference",
        Domain:      "failure_memory",
        Keywords:    []string{"record", "failure", "failed", "error"},
        Parameters: []Param{
            {Name: "approach_signature", Type: "string", Required: true, Description: "Normalized description of what was tried"},
            {Name: "error_pattern", Type: "string", Required: true, Description: "What went wrong"},
            {Name: "context", Type: "string", Required: true, Description: "Task context"},
            {Name: "resolution", Type: "string", Required: false, Description: "What fixed it (if known)"},
        },
    },
    {
        Name:        "get_approach_statistics",
        Description: "Get success/failure statistics for similar approaches",
        Domain:      "failure_memory",
        Keywords:    []string{"statistics", "success rate", "how often"},
        Parameters: []Param{
            {Name: "approach_type", Type: "string", Required: true, Description: "Type of approach to check"},
        },
    },
    {
        Name:        "mark_decision_outcome",
        Description: "Mark a past decision as succeeded or failed",
        Domain:      "failure_memory",
        Keywords:    []string{"outcome", "succeeded", "failed", "result"},
        Parameters: []Param{
            {Name: "decision_id", Type: "string", Required: true},
            {Name: "outcome", Type: "enum", Values: []string{"success", "failure", "partial"}, Required: true},
            {Name: "notes", Type: "string", Required: false},
        },
    },
}
```

#### Failure Pattern Memory Protocol

**CRITICAL: Archivalist maintains institutional memory of failures to prevent repetition.**

```
┌─────────────────────────────────────────────────────────────────────────────────────┐
│                        FAILURE PATTERN MEMORY PROTOCOL                               │
├─────────────────────────────────────────────────────────────────────────────────────┤
│                                                                                     │
│  FAILURE ENTRY STRUCTURE:                                                           │
│  {                                                                                  │
│    "id": "fail_abc123",                                                             │
│    "approach_signature": "normalized description of what was tried",                │
│    "error_pattern": "what went wrong",                                              │
│    "context": "task context when failure occurred",                                 │
│    "resolution": "what fixed it (if known)",                                        │
│    "recurrence_count": 3,                                                           │
│    "last_occurrence": "2024-01-15T10:30:00Z",                                       │
│    "session_ids": ["sess_1", "sess_2", "sess_3"]                                    │
│  }                                                                                  │
│                                                                                     │
│  WHEN TO CHECK FAILURES:                                                            │
│  ├── Before any implementation approach is chosen                                   │
│  ├── When debugging an error (similar errors may have known fixes)                  │
│  ├── When Architect queries for past patterns                                       │
│  └── When Engineer escalates with TASK_HELP                                         │
│                                                                                     │
│  ALERT THRESHOLDS:                                                                  │
│  ├── recurrence_count >= 2: "SIMILAR FAILURE DETECTED" warning                      │
│  ├── recurrence_count >= 5: "RECURRING FAILURE" - suggest different approach        │
│  └── Same session failure: "REPEATED FAILURE" - trigger escalation                  │
│                                                                                     │
│  RESPONSE FORMAT (when similar failure found):                                      │
│  "⚠️ SIMILAR FAILURE DETECTED:                                                       │
│   Approach: [approach_signature]                                                    │
│   Failed: [recurrence_count] times                                                  │
│   Error: [error_pattern]                                                            │
│   Resolution: [resolution or 'No known resolution']                                 │
│   Recommendation: [alternative approach or 'Proceed with caution']"                 │
│                                                                                     │
└─────────────────────────────────────────────────────────────────────────────────────┘
```

#### Archivalist Failure Memory System Prompt Addition

```go
const ArchivalistFailureMemoryPrompt = `
FAILURE PATTERN MEMORY:
You maintain institutional memory of failures to prevent repetition.

1. WHEN STORING DECISIONS:
   - Track outcome field: success/failure/partial
   - If failure, extract approach_signature and error_pattern
   - Increment recurrence_count for similar failures

2. WHEN QUERIED ABOUT APPROACHES:
   - ALWAYS check failure_patterns first
   - If similar failure exists (similarity > 0.7), include warning
   - Format: "⚠️ SIMILAR FAILURE DETECTED: [approach] failed [N] times. [error]. Resolution: [fix or 'unknown']"

3. SIMILARITY MATCHING:
   - Normalize approach descriptions (lowercase, remove specifics)
   - Match on: technology, pattern type, error category
   - Example: "add caching to API" matches "implement cache layer for endpoints"

4. CROSS-SESSION LEARNING:
   - Failure patterns are ALWAYS cross-session readable
   - A failure in session A should warn session B
   - Resolutions discovered later update all related failures

5. DECISION OUTCOME TRACKING:
   - When tasks complete, record outcome
   - Query: "How often does [approach] succeed?" → return statistics
   - Low success rate (< 50%) → recommend alternative
`
```

#### Retrieval Accuracy Tracking Protocol

**CRITICAL: Archivalist tracks its own retrieval accuracy to ensure stored information remains useful.**

```
┌─────────────────────────────────────────────────────────────────────────────────────┐
│                      RETRIEVAL ACCURACY TRACKING PROTOCOL                            │
├─────────────────────────────────────────────────────────────────────────────────────┤
│                                                                                     │
│  PRINCIPLE: Retrieved information must be accurate and actionable.                  │
│                                                                                     │
│  ACCURACY SIGNALS:                                                                  │
│  ┌──────────────────────────────────────────────────────────────────────────────┐   │
│  │  1. STALE_RETRIEVAL: Retrieved info was outdated (codebase changed)          │   │
│  │     → Stored pattern/decision no longer applies                              │   │
│  │                                                                              │   │
│  │  2. IRRELEVANT_RETRIEVAL: Retrieved info didn't match query intent           │   │
│  │     → Similarity matching produced false positive                            │   │
│  │                                                                              │   │
│  │  3. INCOMPLETE_RETRIEVAL: Retrieved info was partial                         │   │
│  │     → Related entries weren't linked/surfaced together                       │   │
│  │                                                                              │   │
│  │  4. WRONG_RESOLUTION: Stored resolution didn't fix the problem               │   │
│  │     → Resolution was context-specific, not generalizable                     │   │
│  │                                                                              │   │
│  │  5. CONFLICTING_ENTRIES: Multiple entries gave contradictory guidance        │   │
│  │     → Entries from different contexts need reconciliation                    │   │
│  └──────────────────────────────────────────────────────────────────────────────┘   │
│                                                                                     │
│  ACCURACY ENTRY STRUCTURE:                                                          │
│  {                                                                                  │
│    "id": "retrieval_acc_abc123",                                                    │
│    "query_id": "original query",                                                    │
│    "entries_returned": ["entry_1", "entry_2"],                                      │
│    "accuracy_issue": "STALE_RETRIEVAL",                                             │
│    "details": "Pattern from 3 months ago, codebase now uses different approach",   │
│    "correction": "Updated pattern entry with current approach",                     │
│    "timestamp": "2024-01-15T10:30:00Z"                                              │
│  }                                                                                  │
│                                                                                     │
│  SELF-HEALING ACTIONS:                                                              │
│  ├── STALE: Mark entry as "needs_review", add staleness warning to retrievals      │
│  ├── IRRELEVANT: Adjust similarity thresholds, add negative example                │
│  ├── INCOMPLETE: Create links between related entries                              │
│  ├── WRONG_RESOLUTION: Mark resolution as "context_specific", add failure note     │
│  └── CONFLICTING: Add reconciliation note, surface both with comparison            │
│                                                                                     │
│  STORAGE VERIFICATION:                                                              │
│  ├── Verify entry stored correctly (read-after-write)                              │
│  ├── Verify entry retrievable by expected queries                                  │
│  ├── Verify cross-references are bidirectional                                     │
│  └── Periodic integrity checks on high-value entries                               │
│                                                                                     │
└─────────────────────────────────────────────────────────────────────────────────────┘
```

#### Archivalist Retrieval Accuracy Skills

```go
// Retrieval Accuracy Tracking Skills
archivalist_skills_accuracy := []Skill{
    {
        Name:        "report_retrieval_issue",
        Description: "Report an issue with retrieved information",
        Domain:      "accuracy",
        Keywords:    []string{"wrong", "stale", "outdated", "irrelevant"},
        Parameters: []Param{
            {Name: "query_id", Type: "string", Required: true},
            {Name: "entry_ids", Type: "array", Required: true},
            {Name: "issue_type", Type: "enum", Values: []string{
                "STALE_RETRIEVAL", "IRRELEVANT_RETRIEVAL", "INCOMPLETE_RETRIEVAL",
                "WRONG_RESOLUTION", "CONFLICTING_ENTRIES",
            }, Required: true},
            {Name: "details", Type: "string", Required: true},
            {Name: "correction", Type: "string", Required: false},
        },
    },
    {
        Name:        "mark_entry_stale",
        Description: "Mark an entry as potentially stale",
        Domain:      "accuracy",
        Keywords:    []string{"stale", "outdated", "old"},
        Parameters: []Param{
            {Name: "entry_id", Type: "string", Required: true},
            {Name: "reason", Type: "string", Required: true},
        },
    },
    {
        Name:        "verify_storage",
        Description: "Verify an entry was stored and is retrievable",
        Domain:      "accuracy",
        Keywords:    []string{"verify", "check", "confirm"},
        Parameters: []Param{
            {Name: "entry_id", Type: "string", Required: true},
            {Name: "expected_queries", Type: "array", Required: false},
        },
    },
    {
        Name:        "get_retrieval_accuracy_metrics",
        Description: "Get retrieval accuracy metrics",
        Domain:      "accuracy",
        Keywords:    []string{"metrics", "accuracy", "quality"},
    },
}

// Session Recording Storage Skills (Tier 2)
// Archivalist stores recordings captured by Guide
archivalist_skills_recordings := []Skill{
    {
        Name:        "store_recording",
        Description: "Store a session recording from Guide",
        Domain:      "recordings",
        Keywords:    []string{"store recording", "save recording"},
        Parameters: []Param{
            {Name: "recording", Type: "object", Required: true, Description: "SessionRecording object"},
            {Name: "tags", Type: "array", Required: false},
        },
    },
    {
        Name:        "get_recording",
        Description: "Retrieve a session recording by ID or name",
        Domain:      "recordings",
        Keywords:    []string{"get recording", "retrieve recording"},
        Parameters: []Param{
            {Name: "recording_id", Type: "string", Required: false},
            {Name: "recording_name", Type: "string", Required: false},
        },
    },
    {
        Name:        "query_recordings",
        Description: "Query session recordings with filters",
        Domain:      "recordings",
        Keywords:    []string{"query recordings", "search recordings", "find recordings"},
        Parameters: []Param{
            {Name: "tags", Type: "array", Required: false},
            {Name: "session_id", Type: "string", Required: false},
            {Name: "date_range", Type: "object", Required: false, Description: "{start, end}"},
            {Name: "contains_query", Type: "string", Required: false, Description: "Search within recorded queries"},
            {Name: "limit", Type: "int", Required: false, Default: 20},
        },
    },
    {
        Name:        "delete_recording",
        Description: "Delete a session recording",
        Domain:      "recordings",
        Keywords:    []string{"delete recording", "remove recording"},
        Parameters: []Param{
            {Name: "recording_id", Type: "string", Required: true},
        },
    },
    {
        Name:        "update_recording_metadata",
        Description: "Update recording metadata (name, tags, description)",
        Domain:      "recordings",
        Keywords:    []string{"update recording", "tag recording", "rename recording"},
        Parameters: []Param{
            {Name: "recording_id", Type: "string", Required: true},
            {Name: "name", Type: "string", Required: false},
            {Name: "tags", Type: "array", Required: false},
            {Name: "description", Type: "string", Required: false},
        },
    },
}
```

#### Session Recording Data Model

```go
// core/recordings/types.go

type RecordingID string

type SessionRecording struct {
    ID            RecordingID      `json:"id"`
    Name          string           `json:"name,omitempty"`
    Description   string           `json:"description,omitempty"`
    SessionID     string           `json:"session_id"`
    CreatedAt     time.Time        `json:"created_at"`
    Duration      time.Duration    `json:"duration"`
    QueryCount    int              `json:"query_count"`
    Tags          []string         `json:"tags,omitempty"`

    // Recording options used
    IncludeResponses    bool       `json:"include_responses"`
    IncludeAgentActions bool       `json:"include_agent_actions"`

    // The recorded queries
    Queries       []RecordedQuery  `json:"queries"`
}

type RecordedQuery struct {
    Index             int              `json:"index"`
    Timestamp         time.Time        `json:"timestamp"`
    RawQuery          string           `json:"raw_query"`
    ClassifiedIntent  string           `json:"classified_intent"`
    RoutedTo          string           `json:"routed_to"`
    Response          string           `json:"response,omitempty"`
    Success           bool             `json:"success"`
    Duration          time.Duration    `json:"duration"`
    AgentActions      []AgentAction    `json:"agent_actions,omitempty"`
}

type AgentAction struct {
    AgentID    string      `json:"agent_id"`
    ActionType string      `json:"action_type"` // "skill_call", "tool_use", "message"
    Detail     string      `json:"detail"`
    Timestamp  time.Time   `json:"timestamp"`
}

type ReplayState struct {
    RecordingID   RecordingID  `json:"recording_id"`
    CurrentIndex  int          `json:"current_index"`
    Mode          string       `json:"mode"` // exact, interactive, dry_run
    Modifications map[int]string `json:"modifications,omitempty"`
    SkippedIndices []int       `json:"skipped_indices,omitempty"`
    StartedAt     time.Time    `json:"started_at"`
}
```

#### Archivalist Retrieval Accuracy System Prompt Addition

```go
const ArchivalistRetrievalAccuracyPrompt = `
RETRIEVAL ACCURACY TRACKING:
Your value depends on retrieval accuracy. Track and improve it.

ACCURACY ISSUES TO DETECT:
1. STALE_RETRIEVAL: Info was outdated (codebase/context changed)
2. IRRELEVANT_RETRIEVAL: Info didn't match query intent
3. INCOMPLETE_RETRIEVAL: Related info wasn't surfaced together
4. WRONG_RESOLUTION: Stored fix didn't work in new context
5. CONFLICTING_ENTRIES: Multiple entries gave contradictory guidance

WHEN ISSUE REPORTED:
1. Log in accuracy tracking
2. Take self-healing action:
   - STALE: Mark "needs_review", add staleness warning
   - IRRELEVANT: Add as negative example for similarity
   - INCOMPLETE: Create cross-reference links
   - WRONG_RESOLUTION: Add "context_specific" flag
   - CONFLICTING: Add reconciliation note

STORAGE VERIFICATION:
After every store operation:
1. Read-after-write verification
2. Test retrieval with expected query patterns
3. Verify cross-references work both directions

PROACTIVE STALENESS DETECTION:
- Track entry age vs. related file modification times
- If stored pattern references files that changed, flag for review
- Periodic integrity checks on frequently-accessed entries
`
```

### Architect Skills

```go
// Core Skills (Tier 1)
architect_skills_core := []Skill{
    {
        Name:        "plan",
        Description: "Create an implementation plan from requirements",
        Domain:      "planning",
        Keywords:    []string{"plan", "implement", "build", "create", "add"},
        Priority:    100,
        Parameters: []Param{
            {Name: "request", Type: "string", Required: true},
            {Name: "constraints", Type: "array", Required: false},
        },
    },
    {
        Name:        "clarify",
        Description: "Ask user for clarification on requirements",
        Domain:      "coordination",
        Keywords:    []string{"clarify", "unclear", "question"},
        Priority:    90,
    },
    {
        Name:        "status_update",
        Description: "Provide execution status to user",
        Domain:      "coordination",
        Keywords:    []string{"status", "progress", "update"},
        Priority:    80,
    },
    {
        Name:        "approve_plan",
        Description: "Present plan for user approval",
        Domain:      "coordination",
        Keywords:    []string{"approve", "review", "confirm"},
        Priority:    90,
    },
}

// Contextual Skills (Tier 2)
architect_skills_contextual := []Skill{
    {
        Name:        "modify_plan",
        Description: "Modify existing plan based on feedback",
        Domain:      "planning",
        Keywords:    []string{"change", "modify", "update plan"},
        LoadTrigger: "change|modify|different|instead",
    },
    {
        Name:        "create_fix_dag",
        Description: "Create fix workflow from Inspector/Tester corrections",
        Domain:      "planning",
        Keywords:    []string{"fix", "correct", "repair"},
        LoadTrigger: "fix|correct|repair|issue|fail",
    },
    {
        Name:        "interrupt_handler",
        Description: "Handle user interruption during execution",
        Domain:      "coordination",
        Keywords:    []string{"stop", "wait", "pause", "interrupt"},
        LoadTrigger: "stop|wait|pause|hold",
    },
}

// Direct Consultation Skills (Tier 1 - Core, bypasses Guide for token efficiency)
// See Direct Consultation Protocol for details
architect_skills_consultation := []Skill{
    {
        Name:        "consult_engineer",
        Description: "Directly consult Engineer for implementation feasibility",
        Domain:      "consultation",
        Target:      "engineer",
        Keywords:    []string{"can we implement", "feasibility", "implementation approach"},
        Priority:    85,
        Parameters:  consultationParams,
    },
    {
        Name:        "consult_designer",
        Description: "Directly consult Designer for UI/UX approach",
        Domain:      "consultation",
        Target:      "designer",
        Keywords:    []string{"ui approach", "ux design", "component design"},
        Priority:    85,
        Parameters:  consultationParams,
    },
    {
        Name:        "consult_inspector",
        Description: "Directly consult Inspector for quality considerations",
        Domain:      "consultation",
        Target:      "inspector",
        Keywords:    []string{"quality check", "code review", "validation approach"},
        Priority:    85,
        Parameters:  consultationParams,
    },
    {
        Name:        "consult_tester",
        Description: "Directly consult Tester for testing strategy",
        Domain:      "consultation",
        Target:      "tester",
        Keywords:    []string{"testing approach", "test coverage", "test strategy"},
        Priority:    85,
        Parameters:  consultationParams,
    },
    {
        Name:        "consult_librarian",
        Description: "Directly consult Librarian for codebase context (bypasses Guide)",
        Domain:      "consultation",
        Target:      "librarian",
        Keywords:    []string{"codebase", "pattern", "where", "how", "existing", "find"},
        Priority:    85,
        Parameters: []Param{
            {Name: "question", Type: "string", Required: true},
            {Name: "intent", Type: "enum", Values: []string{"pattern_lookup", "file_location", "dependency_check", "health_assessment"}, Required: false},
            {Name: "subject", Type: "string", Required: false, Description: "What we're asking about"},
            {Name: "specificity", Type: "enum", Values: []string{"file_level", "function_level", "line_level"}, Required: false, Default: "file_level"},
        },
    },
    {
        Name:        "consult_archivalist",
        Description: "Directly consult Archivalist for historical context (bypasses Guide)",
        Domain:      "consultation",
        Target:      "archivalist",
        Keywords:    []string{"before", "last time", "previously", "history", "failed", "similar"},
        Priority:    85,
        Parameters: []Param{
            {Name: "question", Type: "string", Required: true},
            {Name: "intent", Type: "enum", Values: []string{"decision_lookup", "failure_check", "session_history", "approach_statistics"}, Required: false},
            {Name: "subject", Type: "string", Required: false, Description: "What we're asking about"},
            {Name: "time_range", Type: "string", Required: false, Description: "e.g., 'last_session', 'last_week', 'all'"},
        },
    },
    {
        Name:        "consult_academic",
        Description: "Directly consult Academic for research and best practices (bypasses Guide)",
        Domain:      "consultation",
        Target:      "academic",
        Keywords:    []string{"best practice", "how should", "recommended", "standard", "research"},
        Priority:    85,
        Parameters: []Param{
            {Name: "question", Type: "string", Required: true},
            {Name: "intent", Type: "enum", Values: []string{"best_practice", "pattern_research", "technology_comparison", "implementation_guidance"}, Required: false},
            {Name: "subject", Type: "string", Required: false, Description: "Topic to research"},
            {Name: "require_codebase_validation", Type: "bool", Required: false, Default: true},
        },
    },
}

// Pre-Delegation Planning Skills (Tier 1 - MANDATORY for task dispatch)
architect_skills_predelegation := []Skill{
    {
        Name:        "pre_delegation_declare",
        Description: "Formal 4-part declaration before delegating any task (MANDATORY)",
        Domain:      "delegation",
        Keywords:    []string{"delegate", "dispatch", "assign", "task"},
        Priority:    100,
        Required:    true, // Cannot delegate without this
        Parameters: []Param{
            {Name: "target_agent", Type: "enum", Values: []string{"engineer", "designer"}, Required: true},
            {Name: "reasoning", Type: "string", Required: true, Description: "Why this agent? Why this approach?"},
            {Name: "required_skills", Type: "array", Required: true, Description: "Skills needed for this task"},
            {Name: "expected_outcome", Type: "string", Required: true, Description: "What constitutes success"},
            {Name: "failure_criteria", Type: "string", Required: true, Description: "What indicates wrong approach"},
            {Name: "librarian_health", Type: "object", Required: true, Description: "Codebase health from Librarian"},
            {Name: "archivalist_check", Type: "object", Required: true, Description: "Failure patterns from Archivalist"},
        },
    },
    {
        Name:        "consult_before_planning",
        Description: "Mandatory consultation with knowledge agents before creating plans",
        Domain:      "consultation",
        Keywords:    []string{"consult", "context", "before planning"},
        Priority:    100,
        Required:    true, // Cannot plan without this
    },
}
```

#### Pre-Delegation Planning Protocol

**CRITICAL: Architect MUST consult knowledge agents and emit formal declaration before delegating ANY task.**

```
┌─────────────────────────────────────────────────────────────────────────────────────┐
│                      PRE-DELEGATION PLANNING PROTOCOL                                │
├─────────────────────────────────────────────────────────────────────────────────────┤
│                                                                                     │
│  MANDATORY CONSULTATIONS (Before Planning):                                         │
│  ┌──────────────────────────────────────────────────────────────────────────────┐   │
│  │  1. LIBRARIAN: "What is the codebase health for [target area]?"              │   │
│  │     → Returns: maturity, pattern_confidence, test_coverage, debt_markers     │   │
│  │                                                                              │   │
│  │  2. ARCHIVALIST: "Have similar approaches failed before?"                    │   │
│  │     → Returns: failure_patterns, success_statistics, warnings                │   │
│  │                                                                              │   │
│  │  3. ACADEMIC (if research needed): "Best practice for [approach]?"           │   │
│  │     → Returns: recommendation with codebase_alignment score                  │   │
│  └──────────────────────────────────────────────────────────────────────────────┘   │
│                                                                                     │
│  PLANNING WITHOUT CONSULTATION IS FORBIDDEN.                                        │
│                                                                                     │
│  PRE-DELEGATION DECLARATION (4-Part Format):                                        │
│  ┌──────────────────────────────────────────────────────────────────────────────┐   │
│  │  {                                                                           │   │
│  │    "delegation_id": "del_abc123",                                            │   │
│  │    "target_agent": "engineer",                                               │   │
│  │    "reasoning": "Engineer needed for backend API work. Approach uses         │   │
│  │                  existing middleware pattern per Librarian.",                │   │
│  │    "required_skills": ["write_file", "run_command", "consult_librarian"],    │   │
│  │    "expected_outcome": "New endpoint /api/users returns user list",          │   │
│  │    "failure_criteria": "Type errors, test failures, pattern mismatch",       │   │
│  │    "librarian_health": {                                                     │   │
│  │      "maturity": "DISCIPLINED",                                              │   │
│  │      "pattern_confidence": 0.89,                                             │   │
│  │      "recommendation": "Follow middleware.go pattern"                        │   │
│  │    },                                                                        │   │
│  │    "archivalist_check": {                                                    │   │
│  │      "similar_failures": 0,                                                  │   │
│  │      "approach_success_rate": 0.85,                                          │   │
│  │      "warnings": []                                                          │   │
│  │    }                                                                         │   │
│  │  }                                                                           │   │
│  └──────────────────────────────────────────────────────────────────────────────┘   │
│                                                                                     │
│  DECLARATION IS STORED IN ARCHIVALIST for:                                          │
│  ├── Traceability (why was this approach chosen?)                                   │
│  ├── Post-mortem analysis (did the reasoning hold?)                                 │
│  └── Learning (what patterns lead to success/failure?)                              │
│                                                                                     │
└─────────────────────────────────────────────────────────────────────────────────────┘
```

#### Architect Pre-Delegation System Prompt Addition

```go
const ArchitectPreDelegationPrompt = `
PRE-DELEGATION PLANNING PROTOCOL:
You MUST consult knowledge agents (via Guide) and emit formal declaration before delegating ANY task.

CRITICAL: ALL QUERIES GO THROUGH GUIDE
You do NOT query agents directly. Submit queries to Guide, which routes to the appropriate agent.
This ensures proper message correlation, routing tracking, and consistent architecture.

MANDATORY CONSULTATIONS (Before Planning):
1. Submit to Guide: "What is the codebase health for [target area]?"
   → Guide routes to Librarian
   → Returns: maturity, pattern_confidence, test_coverage, debt_markers

2. Submit to Guide: "Have similar approaches failed before?"
   → Guide routes to Archivalist
   → Returns: failure_patterns, success_statistics, warnings

3. If research needed, submit to Guide: "Best practice for [approach]?"
   → Guide routes to Academic
   → Returns: recommendation with codebase_alignment score

PLANNING WITHOUT CONSULTATION IS FORBIDDEN.

USER CLARIFICATION IS LAST RESORT:
- ONLY ask user after exhausting Librarian, Archivalist, and Academic
- If an agent can answer the question, DO NOT ask the user
- When you must ask the user, explain what you already checked:
  "I've checked the codebase (Librarian) and history (Archivalist). I still need..."

CHALLENGE USER WHEN WARRANTED:
If the request seems flawed, contradicts patterns, or has known failure history:
- State your concern with evidence from agents
- Propose an alternative approach
- Accept user's final decision after presenting facts

PRE-DELEGATION DECLARATION (Mandatory):
Before dispatching ANY task, emit a structured declaration:
{
  "delegation_id": "<unique>",
  "target_agent": "engineer|designer",
  "reasoning": "Why this agent? Why this approach?",
  "required_skills": ["skill_a", "skill_b"],
  "expected_outcome": "What constitutes success",
  "failure_criteria": "What indicates wrong approach",
  "librarian_health": {<codebase health response>},
  "archivalist_check": {<failure patterns response>},
  "user_clarification_needed": false,  // true only if agents couldn't answer
  "challenges_raised": []  // any concerns raised to user
}

This declaration:
- Is stored in Archivalist for traceability
- Is validated by Orchestrator before dispatch
- Enables post-mortem analysis and learning
`
```

// Plan Mode & Todo Management Skills (Tier 2)
// Inspired by anomalyco/opencode plan mode tools and todo management
architect_skills_planning := []Skill{
    {
        Name:        "enter_plan_mode",
        Description: "Enter plan mode for complex tasks requiring user approval",
        Domain:      "planning",
        Keywords:    []string{"plan", "complex", "design", "architecture"},
        LoadTrigger: "complex|multi-step|design|architecture|refactor",
        Parameters: []Param{
            {Name: "task_description", Type: "string", Required: true},
            {Name: "plan_file", Type: "string", Required: false, Description: "Path for plan file"},
        },
    },
    {
        Name:        "exit_plan_mode",
        Description: "Exit plan mode with approval request",
        Domain:      "planning",
        Keywords:    []string{"approve", "ready", "proceed"},
        Parameters: []Param{
            {Name: "allowed_prompts", Type: "array", Required: false, Description: "Bash permissions needed"},
        },
    },
    {
        Name:        "update_plan_file",
        Description: "Update the plan file with new content",
        Domain:      "planning",
        Keywords:    []string{"update plan", "revise", "modify plan"},
        Parameters: []Param{
            {Name: "content", Type: "string", Required: true},
            {Name: "append", Type: "bool", Required: false, Default: false},
        },
    },
    {
        Name:        "todo_write",
        Description: "Manage task todo list for tracking progress",
        Domain:      "tracking",
        Keywords:    []string{"todo", "task", "track", "progress"},
        Parameters: []Param{
            {Name: "todos", Type: "array", Required: true, Description: "Array of {content, status, activeForm}"},
        },
    },
    {
        Name:        "todo_mark_complete",
        Description: "Mark a todo item as complete",
        Domain:      "tracking",
        Keywords:    []string{"done", "complete", "finished"},
        Parameters: []Param{
            {Name: "index", Type: "int", Required: true},
        },
    },
    {
        Name:        "ask_user_question",
        Description: "Ask user for clarification or decision",
        Domain:      "coordination",
        Keywords:    []string{"ask", "clarify", "question", "decide"},
        Parameters: []Param{
            {Name: "questions", Type: "array", Required: true, Description: "Questions with options"},
        },
    },
}
```

#### Plan Mode Protocol

**Architect uses plan mode for complex tasks requiring user approval before implementation.**

```
┌─────────────────────────────────────────────────────────────────────────────────────┐
│                           PLAN MODE PROTOCOL                                         │
├─────────────────────────────────────────────────────────────────────────────────────┤
│                                                                                     │
│  WHEN TO ENTER PLAN MODE:                                                           │
│  ├── New feature implementation (non-trivial)                                       │
│  ├── Multiple valid approaches exist                                                │
│  ├── Architectural decisions required                                               │
│  ├── Multi-file changes expected                                                    │
│  └── Unclear requirements need exploration                                          │
│                                                                                     │
│  PLAN MODE WORKFLOW:                                                                │
│  1. enter_plan_mode with task description                                           │
│  2. Explore codebase (Librarian queries, Glob, Grep, Read)                          │
│  3. Consult knowledge agents (Academic, Archivalist)                                │
│  4. Write plan to plan file (approach, steps, considerations)                       │
│  5. Use ask_user_question if clarifications needed                                  │
│  6. exit_plan_mode to request user approval                                         │
│  7. Wait for approval before implementing                                           │
│                                                                                     │
│  TODO MANAGEMENT:                                                                   │
│  ├── Create todos for multi-step tasks (3+ steps)                                   │
│  ├── Update status as work progresses                                               │
│  ├── Mark complete IMMEDIATELY after finishing                                      │
│  ├── Only ONE todo should be in_progress at a time                                  │
│  └── Gives user visibility into progress                                            │
│                                                                                     │
│  PLAN FILE FORMAT:                                                                  │
│  ```markdown                                                                        │
│  # Plan: [Task Name]                                                                │
│                                                                                     │
│  ## Overview                                                                        │
│  [Brief description of the approach]                                                │
│                                                                                     │
│  ## Files to Modify                                                                 │
│  - path/to/file1.go: [what changes]                                                 │
│  - path/to/file2.go: [what changes]                                                 │
│                                                                                     │
│  ## Implementation Steps                                                            │
│  1. [Step 1]                                                                        │
│  2. [Step 2]                                                                        │
│                                                                                     │
│  ## Considerations                                                                  │
│  - [Trade-off or risk]                                                              │
│  ```                                                                                │
│                                                                                     │
└─────────────────────────────────────────────────────────────────────────────────────┘
```

### Engineer System Prompt

```go
const EngineerSystemPrompt = `
You are the Engineer agent. You implement code changes based on tasks from Architect.

## Knowledge Agent Consultation (via Guide)

You can consult knowledge agents for implementation context. ALL consultations go through Guide:

CONTEXT REQUESTS (to Librarian):
- "What patterns exist for error handling in this codebase?"
- "Show me existing implementations of similar functionality"
- "Where are the utility functions for X?"
- "What's the project structure for this module?"

HISTORY REQUESTS (to Archivalist):
- "Have we implemented something similar before?"
- "What issues occurred with this approach last time?"
- "What was the resolution for similar bugs?"
- "Success rate for this type of change?"

RESEARCH REQUESTS (to Academic):
- "Best practices for implementing rate limiting?"
- "Security considerations for this approach?"
- "Performance implications of this pattern?"

CONSULTATION FORMAT:
{
  "type": "CONTEXT_REQUEST",  // or HISTORY_REQUEST, RESEARCH_REQUEST
  "from_agent": "engineer",
  "query": {
    "intent": "pattern_lookup",
    "subject": "middleware authentication",
    "context": "implementing new auth endpoint"
  }
}

Guide routes to appropriate knowledge agent and returns response.

WHEN TO CONSULT:
- BEFORE implementing (get existing patterns from Librarian)
- WHEN stuck (check Archivalist for similar past issues)
- WHEN uncertain about approach (consult Academic for best practices)
- AFTER failures (query Archivalist for resolution patterns)

IMPLEMENTATION PROTOCOL:
1. Query Librarian for existing patterns in target area
2. Query Archivalist for past issues with similar changes
3. Implement following existing patterns
4. If stuck, consult knowledge agents before asking user
5. Report progress and any blockers

CRITICAL: Exhaust knowledge agents BEFORE requesting help from user.
`
```

### Engineer Skills

```go
// Core Skills (Tier 1)
engineer_skills_core := []Skill{
    {
        Name:        "execute_task",
        Description: "Execute an assigned task",
        Domain:      "execution",
        Keywords:    []string{"execute", "do", "implement", "code"},
        Priority:    100,
    },
    {
        Name:        "read_file",
        Description: "Read a file's contents",
        Domain:      "files",
        Keywords:    []string{"read", "view", "show", "cat"},
        Priority:    100,
    },
    {
        Name:        "write_file",
        Description: "Write content to a file",
        Domain:      "files",
        Keywords:    []string{"write", "create", "save"},
        Priority:    100,
    },
    {
        Name:        "edit_file",
        Description: "Edit an existing file",
        Domain:      "files",
        Keywords:    []string{"edit", "modify", "change", "update"},
        Priority:    100,
    },
    {
        Name:        "run_command",
        Description: "Execute a shell command",
        Domain:      "execution",
        Keywords:    []string{"run", "execute", "shell", "bash"},
        Priority:    90,
    },
    {
        Name:        "request_help",
        Description: "Request clarification via Orchestrator",
        Domain:      "coordination",
        Keywords:    []string{"help", "unclear", "confused", "question"},
        Priority:    80,
    },
}

// Direct Consultation Skills (Tier 1 - Core, bypasses Guide for token efficiency)
// See Direct Consultation Protocol for details
engineer_skills_consultation := []Skill{
    {
        Name:        "consult_architect",
        Description: "Directly consult Architect for task clarification or workflow questions",
        Domain:      "consultation",
        Target:      "architect",
        Keywords:    []string{"task unclear", "workflow question", "planning help"},
        Priority:    85,
        Parameters:  consultationParams,
    },
    {
        Name:        "consult_designer",
        Description: "Directly consult Designer for UI/UX implementation guidance",
        Domain:      "consultation",
        Target:      "designer",
        Keywords:    []string{"ui implementation", "component styling", "design tokens"},
        Priority:    85,
        Parameters:  consultationParams,
    },
    {
        Name:        "consult_inspector",
        Description: "Directly consult Inspector for code quality questions",
        Domain:      "consultation",
        Target:      "inspector",
        Keywords:    []string{"quality check", "best practice", "code review"},
        Priority:    85,
        Parameters:  consultationParams,
    },
    {
        Name:        "consult_tester",
        Description: "Directly consult Tester for testing approach questions",
        Domain:      "consultation",
        Target:      "tester",
        Keywords:    []string{"testing approach", "test coverage", "test strategy"},
        Priority:    85,
        Parameters:  consultationParams,
    },
    {
        Name:        "consult_librarian",
        Description: "Directly consult Librarian for codebase context (bypasses Guide)",
        Domain:      "consultation",
        Target:      "librarian",
        Keywords:    []string{"codebase", "pattern", "where", "how", "existing", "find"},
        Priority:    85,
        Parameters: []Param{
            {Name: "question", Type: "string", Required: true},
            {Name: "intent", Type: "enum", Values: []string{"pattern_lookup", "file_location", "dependency_check", "health_assessment"}, Required: false},
            {Name: "subject", Type: "string", Required: false, Description: "What we're asking about"},
            {Name: "specificity", Type: "enum", Values: []string{"file_level", "function_level", "line_level"}, Required: false, Default: "file_level"},
        },
    },
    {
        Name:        "consult_archivalist",
        Description: "Directly consult Archivalist for historical context (bypasses Guide)",
        Domain:      "consultation",
        Target:      "archivalist",
        Keywords:    []string{"before", "last time", "previously", "history", "failed", "similar"},
        Priority:    85,
        Parameters: []Param{
            {Name: "question", Type: "string", Required: true},
            {Name: "intent", Type: "enum", Values: []string{"decision_lookup", "failure_check", "session_history", "approach_statistics"}, Required: false},
            {Name: "subject", Type: "string", Required: false, Description: "What we're asking about"},
            {Name: "time_range", Type: "string", Required: false, Description: "e.g., 'last_session', 'last_week', 'all'"},
        },
    },
    {
        Name:        "consult_academic",
        Description: "Directly consult Academic for research and best practices (bypasses Guide)",
        Domain:      "consultation",
        Target:      "academic",
        Keywords:    []string{"best practice", "how should", "recommended", "standard", "research"},
        Priority:    85,
        Parameters: []Param{
            {Name: "question", Type: "string", Required: true},
            {Name: "intent", Type: "enum", Values: []string{"best_practice", "pattern_research", "technology_comparison", "implementation_guidance"}, Required: false},
            {Name: "subject", Type: "string", Required: false, Description: "Topic to research"},
            {Name: "require_codebase_validation", Type: "bool", Required: false, Default: true},
        },
    },
}

// Multi-Edit & Structural Refactoring Skills (Tier 2)
// Inspired by anomalyco/opencode multiedit/patch and oh-my-opencode ast_grep_replace
engineer_skills_multiedit := []Skill{
    {
        Name:        "multi_edit",
        Description: "Apply multiple edits to a file atomically",
        Domain:      "files",
        Keywords:    []string{"multi", "batch", "edits", "atomic"},
        LoadTrigger: "multiple|several|batch|all",
        Parameters: []Param{
            {Name: "file", Type: "string", Required: true},
            {Name: "edits", Type: "array", Required: true, Description: "Array of {old_string, new_string} pairs"},
            {Name: "dry_run", Type: "bool", Required: false, Default: false},
        },
    },
    {
        Name:        "patch_file",
        Description: "Apply unified diff patch to file",
        Domain:      "files",
        Keywords:    []string{"patch", "diff", "apply"},
        LoadTrigger: "patch|diff|apply",
        Parameters: []Param{
            {Name: "file", Type: "string", Required: true},
            {Name: "patch", Type: "string", Required: true, Description: "Unified diff format"},
            {Name: "reverse", Type: "bool", Required: false, Default: false},
        },
    },
    {
        Name:        "ast_grep_replace",
        Description: "Structural find-and-replace using AST patterns",
        Domain:      "refactoring",
        Keywords:    []string{"refactor", "rename", "replace", "structural"},
        LoadTrigger: "refactor|rename|replace all|structural",
        Parameters: []Param{
            {Name: "pattern", Type: "string", Required: true, Description: "AST pattern to match"},
            {Name: "replacement", Type: "string", Required: true, Description: "AST replacement pattern"},
            {Name: "language", Type: "enum", Values: []string{"go", "typescript", "python", "rust", "java"}, Required: true},
            {Name: "path", Type: "string", Required: false, Description: "Scope to specific path"},
            {Name: "dry_run", Type: "bool", Required: false, Default: true},
        },
    },
    {
        Name:        "batch_write",
        Description: "Write multiple files atomically",
        Domain:      "files",
        Keywords:    []string{"batch", "multiple files", "atomic write"},
        LoadTrigger: "multiple files|batch|together",
        Parameters: []Param{
            {Name: "files", Type: "array", Required: true, Description: "Array of {path, content} pairs"},
            {Name: "create_dirs", Type: "bool", Required: false, Default: true},
        },
    },
    {
        Name:        "run_background",
        Description: "Run command in background with monitoring",
        Domain:      "execution",
        Keywords:    []string{"background", "async", "long"},
        LoadTrigger: "background|async|long running",
        Parameters: []Param{
            {Name: "command", Type: "string", Required: true},
            {Name: "working_dir", Type: "string", Required: false},
            {Name: "timeout", Type: "duration", Required: false},
        },
    },
}
```

#### Multi-Edit Protocol

**CRITICAL: Use multi-edit for batch changes to ensure atomicity and reduce round trips.**

```
┌─────────────────────────────────────────────────────────────────────────────────────┐
│                           MULTI-EDIT PROTOCOL                                        │
├─────────────────────────────────────────────────────────────────────────────────────┤
│                                                                                     │
│  WHEN TO USE MULTI-EDIT:                                                            │
│  ├── Multiple related changes in same file                                          │
│  ├── Renaming a variable/function (multiple occurrences)                            │
│  ├── Applying a consistent pattern change                                           │
│  └── Any batch of changes that should succeed or fail together                      │
│                                                                                     │
│  WHEN TO USE AST_GREP_REPLACE:                                                      │
│  ├── Structural refactoring (rename function across files)                          │
│  ├── Code pattern migration (old API → new API)                                     │
│  ├── Consistent code transformations                                                │
│  └── When regex would miss structural boundaries                                    │
│                                                                                     │
│  WHEN TO USE BATCH_WRITE:                                                           │
│  ├── Creating multiple related files (e.g., interface + implementation + test)      │
│  ├── Scaffolding new module structure                                               │
│  └── Changes that span multiple files and must be atomic                            │
│                                                                                     │
│  ATOMICITY GUARANTEE:                                                               │
│  ├── All edits in multi_edit apply together or none apply                           │
│  ├── batch_write creates all files or rolls back on failure                         │
│  └── ast_grep_replace with dry_run=true shows preview before applying              │
│                                                                                     │
│  AST PATTERN EXAMPLES:                                                              │
│  ├── Rename function: "func oldName($$$)" → "func newName($$$)"                    │
│  ├── Add error check: "x, _ := foo()" → "x, err := foo(); if err != nil { ... }"   │
│  └── Update import: 'import "old/pkg"' → 'import "new/pkg"'                        │
│                                                                                     │
└─────────────────────────────────────────────────────────────────────────────────────┘
```

### Designer Skills

**Model**: Gemini 3 Pro (via go-genai)

**Pipeline Architecture**: Single-Worker Pipeline - Designer is the sole worker in its pipeline, with Inspector and Tester in UI mode.

```go
// Core Skills (Tier 1) - Always loaded
designer_skills_core := []Skill{
    {
        Name:        "search_components",
        Description: "Search existing component library (MUST call before creating)",
        Domain:      "ui",
        Keywords:    []string{"find", "search", "existing", "component", "reuse"},
        Priority:    100,
        Required:    true, // Hook enforces this is called before create_component
        Parameters: []Param{
            {Name: "query", Type: "string", Required: true},
            {Name: "category", Type: "string", Required: false},
            {Name: "framework", Type: "enum", Values: []string{"react", "vue", "svelte", "solid", "all"}, Required: false},
        },
    },
    {
        Name:        "create_component",
        Description: "Create a new UI component (only after search confirms no match)",
        Domain:      "ui",
        Keywords:    []string{"component", "create", "new", "ui"},
        Priority:    100,
        Parameters: []Param{
            {Name: "name", Type: "string", Required: true},
            {Name: "category", Type: "string", Required: true},
            {Name: "props", Type: "array", Required: true},
            {Name: "design_tokens", Type: "array", Required: true}, // NEVER hardcode values
            {Name: "accessibility", Type: "object", Required: true}, // WCAG 2.1 AA required
        },
    },
    {
        Name:        "extend_component",
        Description: "Extend or compose existing components",
        Domain:      "ui",
        Keywords:    []string{"extend", "compose", "variant", "inherit"},
        Priority:    100,
        Parameters: []Param{
            {Name: "base_component", Type: "string", Required: true},
            {Name: "variant_name", Type: "string", Required: true},
            {Name: "modifications", Type: "object", Required: true},
        },
    },
    {
        Name:        "get_design_tokens",
        Description: "Retrieve design system tokens (always use tokens, never hardcode)",
        Domain:      "design_system",
        Keywords:    []string{"token", "color", "spacing", "typography", "theme"},
        Priority:    100,
        Parameters: []Param{
            {Name: "category", Type: "enum", Values: []string{"colors", "spacing", "typography", "shadows", "borders", "motion", "all"}, Required: true},
            {Name: "variant", Type: "string", Required: false},
        },
    },
    {
        Name:        "check_accessibility",
        Description: "Check component accessibility compliance (WCAG 2.1 AA minimum)",
        Domain:      "accessibility",
        Keywords:    []string{"a11y", "accessibility", "aria", "wcag", "contrast"},
        Priority:    100,
        Required:    true, // Hook warns if not called before completion
        Parameters: []Param{
            {Name: "target", Type: "string", Required: true},
            {Name: "wcag_level", Type: "enum", Values: []string{"A", "AA", "AAA"}, Required: false, Default: "AA"},
        },
    },
    {
        Name:        "preview_component",
        Description: "Preview UI component in isolation",
        Domain:      "preview",
        Keywords:    []string{"preview", "render", "show", "display", "storybook"},
        Priority:    90,
        Parameters: []Param{
            {Name: "component", Type: "string", Required: true},
            {Name: "viewport", Type: "enum", Values: []string{"mobile", "tablet", "desktop"}, Required: false},
            {Name: "theme", Type: "enum", Values: []string{"light", "dark"}, Required: false},
        },
    },
}

// Contextual Skills (Tier 2) - Loaded based on task triggers
designer_skills_contextual := []Skill{
    {
        Name:        "create_layout",
        Description: "Create responsive layout using design system grid",
        Domain:      "layout",
        Keywords:    []string{"layout", "grid", "flex", "responsive", "breakpoint"},
        LoadTrigger: "layout|grid|flex|responsive|breakpoint|page",
        Parameters: []Param{
            {Name: "type", Type: "enum", Values: []string{"grid", "flex", "stack", "split"}, Required: true},
            {Name: "breakpoints", Type: "object", Required: true},
            {Name: "regions", Type: "array", Required: true},
        },
    },
    {
        Name:        "create_animation",
        Description: "Create animation using motion tokens (with reduced-motion fallback)",
        Domain:      "animation",
        Keywords:    []string{"animate", "transition", "motion", "spring"},
        LoadTrigger: "animate|transition|motion|effect|spring",
        Parameters: []Param{
            {Name: "name", Type: "string", Required: true},
            {Name: "type", Type: "enum", Values: []string{"transition", "keyframe", "spring"}, Required: true},
            {Name: "duration", Type: "string", Required: true}, // Must be token
            {Name: "reduced_motion", Type: "object", Required: true}, // Required fallback
        },
    },
    {
        Name:        "run_visual_diff",
        Description: "Compare component against baseline for visual regressions",
        Domain:      "testing",
        Keywords:    []string{"diff", "compare", "regression", "baseline"},
        LoadTrigger: "diff|compare|regression|baseline|visual",
    },
    {
        Name:        "validate_tokens",
        Description: "Validate component uses only approved design tokens",
        Domain:      "design_system",
        Keywords:    []string{"validate", "check", "tokens", "hardcode"},
        LoadTrigger: "validate|check|token|hardcode|audit",
    },
    {
        Name:        "check_color_contrast",
        Description: "Verify color contrast meets WCAG requirements",
        Domain:      "accessibility",
        Keywords:    []string{"contrast", "color", "wcag", "ratio"},
        LoadTrigger: "contrast|color ratio|wcag|readable",
    },
}

// Direct Consultation Skills (Tier 1 - Core, bypasses Guide for token efficiency)
// See Direct Consultation Protocol for details
designer_skills_consultation := []Skill{
    {
        Name:        "consult_architect",
        Description: "Directly consult Architect for design requirements or workflow questions",
        Domain:      "consultation",
        Target:      "architect",
        Keywords:    []string{"design requirements", "workflow question", "planning help"},
        Priority:    85,
        Parameters:  consultationParams,
    },
    {
        Name:        "consult_engineer",
        Description: "Directly consult Engineer for implementation feasibility",
        Domain:      "consultation",
        Target:      "engineer",
        Keywords:    []string{"can implement", "technical feasibility", "state management"},
        Priority:    85,
        Parameters:  consultationParams,
    },
    {
        Name:        "consult_inspector",
        Description: "Directly consult Inspector for UI quality questions",
        Domain:      "consultation",
        Target:      "inspector",
        Keywords:    []string{"ui review", "component quality", "token validation"},
        Priority:    85,
        Parameters:  consultationParams,
    },
    {
        Name:        "consult_tester",
        Description: "Directly consult Tester for visual/a11y testing questions",
        Domain:      "consultation",
        Target:      "tester",
        Keywords:    []string{"visual testing", "a11y testing", "responsive testing"},
        Priority:    85,
        Parameters:  consultationParams,
    },
    {
        Name:        "consult_librarian",
        Description: "Directly consult Librarian for codebase/component search (bypasses Guide)",
        Domain:      "consultation",
        Target:      "librarian",
        Keywords:    []string{"codebase", "pattern", "where", "how", "existing", "find", "component"},
        Priority:    85,
        Parameters: []Param{
            {Name: "question", Type: "string", Required: true},
            {Name: "intent", Type: "enum", Values: []string{"pattern_lookup", "file_location", "dependency_check", "component_search"}, Required: false},
            {Name: "subject", Type: "string", Required: false, Description: "What we're asking about"},
            {Name: "specificity", Type: "enum", Values: []string{"file_level", "function_level", "line_level"}, Required: false, Default: "file_level"},
        },
    },
    {
        Name:        "consult_archivalist",
        Description: "Directly consult Archivalist for design history (bypasses Guide)",
        Domain:      "consultation",
        Target:      "archivalist",
        Keywords:    []string{"before", "last time", "previously", "history", "failed", "similar", "design"},
        Priority:    85,
        Parameters: []Param{
            {Name: "question", Type: "string", Required: true},
            {Name: "intent", Type: "enum", Values: []string{"decision_lookup", "failure_check", "session_history", "design_history"}, Required: false},
            {Name: "subject", Type: "string", Required: false, Description: "What we're asking about"},
            {Name: "time_range", Type: "string", Required: false, Description: "e.g., 'last_session', 'last_week', 'all'"},
        },
    },
    {
        Name:        "consult_academic",
        Description: "Directly consult Academic for UX research and best practices (bypasses Guide)",
        Domain:      "consultation",
        Target:      "academic",
        Keywords:    []string{"best practice", "how should", "recommended", "standard", "research", "ux", "accessibility"},
        Priority:    85,
        Parameters: []Param{
            {Name: "question", Type: "string", Required: true},
            {Name: "intent", Type: "enum", Values: []string{"best_practice", "pattern_research", "ux_research", "accessibility_guidance"}, Required: false},
            {Name: "subject", Type: "string", Required: false, Description: "Topic to research"},
            {Name: "require_codebase_validation", Type: "bool", Required: false, Default: true},
        },
    },
}

// Specialized Skills (Tier 3) - Loaded only on explicit request
designer_skills_specialized := []Skill{
    {
        Name:        "design_system_scaffold",
        Description: "Scaffold a complete design system from scratch",
        Domain:      "design_system",
        Keywords:    []string{"scaffold", "design system", "foundation", "setup"},
        LoadTrigger: "explicit_request",
    },
    {
        Name:        "theme_migration",
        Description: "Migrate between design systems or themes",
        Domain:      "design_system",
        Keywords:    []string{"migrate", "theme", "redesign", "rebrand"},
        LoadTrigger: "explicit_request",
    },
    {
        Name:        "full_responsive_audit",
        Description: "Full responsive design audit across all breakpoints",
        Domain:      "responsive",
        Keywords:    []string{"audit", "responsive", "mobile", "tablet", "breakpoint"},
        LoadTrigger: "explicit_request",
    },
    {
        Name:        "full_accessibility_audit",
        Description: "Comprehensive accessibility audit (WCAG 2.1 AAA)",
        Domain:      "accessibility",
        Keywords:    []string{"audit", "accessibility", "wcag", "aaa"},
        LoadTrigger: "explicit_request",
    },
}

// Pipeline Skills (Tier 4) - For inter-pipeline coordination
// NOTE: Designer does NOT directly communicate with Engineer.
// These skills manage artifacts for Architect-coordinated pipeline sequencing.
designer_skills_pipeline := []Skill{
    {
        Name:        "read_pipeline_context",
        Description: "Read artifacts from completed predecessor pipeline (e.g., Engineer's types/hooks)",
        Domain:      "pipeline",
        Keywords:    []string{"context", "artifacts", "predecessor", "input"},
        LoadTrigger: "auto", // Loaded when pipeline has predecessors
        Parameters: []Param{
            {Name: "pipeline_id", Type: "string", Required: true},
            {Name: "artifact_type", Type: "enum", Values: []string{"types", "hooks", "api_schema", "all"}, Required: false},
        },
    },
    {
        Name:        "emit_artifact",
        Description: "Emit artifact for potential downstream pipelines",
        Domain:      "pipeline",
        Keywords:    []string{"emit", "artifact", "output", "export"},
        LoadTrigger: "auto", // Called automatically by hooks
        Parameters: []Param{
            {Name: "artifact_type", Type: "enum", Values: []string{"component", "layout", "tokens", "styles"}, Required: true},
            {Name: "artifact_data", Type: "object", Required: true},
        },
    },
    {
        Name:        "signal_dependency",
        Description: "Signal Architect that follow-up pipeline is needed (does NOT create it)",
        Domain:      "pipeline",
        Keywords:    []string{"need", "require", "dependency", "follow-up"},
        LoadTrigger: "auto",
        Parameters: []Param{
            {Name: "dependency_type", Type: "enum", Values: []string{"engineering", "testing", "review"}, Required: true},
            {Name: "description", Type: "string", Required: true},
            {Name: "blocking", Type: "boolean", Required: true},
        },
    },
    {
        Name:        "signal_complete",
        Description: "Signal task completion with summary and artifacts",
        Domain:      "pipeline",
        Keywords:    []string{"done", "complete", "finish", "success"},
        LoadTrigger: "auto",
        Parameters: []Param{
            {Name: "result", Type: "string", Required: true},
            {Name: "files_modified", Type: "array", Required: true},
            {Name: "accessibility_passed", Type: "boolean", Required: true},
        },
    },
}

// Vision & Multimodal Skills (Tier 2 - Visual Analysis)
// Inspired by oh-my-opencode look_at multimodal capability
designer_skills_vision := []Skill{
    {
        Name:        "look_at",
        Description: "Analyze image/screenshot for UI review",
        Domain:      "vision",
        Keywords:    []string{"screenshot", "image", "visual", "look", "see"},
        LoadTrigger: "screenshot|image|mockup|design|visual|look at",
        Parameters: []Param{
            {Name: "image_path", Type: "string", Required: true, Description: "Path to image file or URL"},
            {Name: "prompt", Type: "string", Required: true, Description: "What to analyze in the image"},
            {Name: "mode", Type: "enum", Values: []string{"describe", "compare", "accessibility", "spacing", "consistency"}, Required: false, Default: "describe"},
        },
    },
    {
        Name:        "compare_screenshots",
        Description: "Compare two screenshots for visual diff",
        Domain:      "vision",
        Keywords:    []string{"compare", "diff", "before", "after", "regression"},
        LoadTrigger: "compare|diff|before after|regression",
        Parameters: []Param{
            {Name: "baseline", Type: "string", Required: true, Description: "Path to baseline image"},
            {Name: "current", Type: "string", Required: true, Description: "Path to current image"},
            {Name: "threshold", Type: "float", Required: false, Default: 0.01, Description: "Diff threshold (0-1)"},
            {Name: "highlight_diff", Type: "bool", Required: false, Default: true},
        },
    },
    {
        Name:        "analyze_design_mockup",
        Description: "Analyze design mockup image for implementation",
        Domain:      "vision",
        Keywords:    []string{"mockup", "figma", "design", "implement", "match"},
        LoadTrigger: "mockup|figma|design spec|match design",
        Parameters: []Param{
            {Name: "mockup_path", Type: "string", Required: true},
            {Name: "extract", Type: "enum", Values: []string{"components", "spacing", "colors", "typography", "all"}, Required: false, Default: "all"},
            {Name: "target_framework", Type: "enum", Values: []string{"react", "vue", "svelte", "css"}, Required: false},
        },
    },
    {
        Name:        "visual_accessibility_check",
        Description: "Check visual accessibility of screenshot/component",
        Domain:      "vision",
        Keywords:    []string{"visual a11y", "contrast", "touch target", "font size"},
        LoadTrigger: "visual accessibility|contrast check|touch target",
        Parameters: []Param{
            {Name: "image_path", Type: "string", Required: true},
            {Name: "checks", Type: "array", Required: false, Default: []string{"contrast", "touch_targets", "font_size", "focus_indicators"}},
        },
    },
}
```

#### Vision & Multimodal Protocol

**Designer leverages vision capabilities for UI review and implementation accuracy.**

```
┌─────────────────────────────────────────────────────────────────────────────────────┐
│                      VISION & MULTIMODAL PROTOCOL                                    │
├─────────────────────────────────────────────────────────────────────────────────────┤
│                                                                                     │
│  USE CASES:                                                                         │
│  ┌──────────────────────────────────────────────────────────────────────────────┐   │
│  │  1. DESIGN IMPLEMENTATION                                                    │   │
│  │     - User provides mockup/figma screenshot                                  │   │
│  │     - analyze_design_mockup extracts components, spacing, colors             │   │
│  │     - Designer implements matching the visual spec                           │   │
│  │                                                                              │   │
│  │  2. VISUAL REVIEW                                                            │   │
│  │     - After implementation, capture screenshot                               │   │
│  │     - look_at verifies implementation matches intent                         │   │
│  │     - Catches spacing, alignment, color issues                               │   │
│  │                                                                              │   │
│  │  3. REGRESSION DETECTION                                                     │   │
│  │     - compare_screenshots finds unintended visual changes                    │   │
│  │     - Integrates with visual testing pipeline                                │   │
│  │     - Threshold-based diff detection                                         │   │
│  │                                                                              │   │
│  │  4. ACCESSIBILITY VERIFICATION                                               │   │
│  │     - visual_accessibility_check validates contrast ratios                   │   │
│  │     - Checks touch target sizes (44x44px minimum)                            │   │
│  │     - Validates font sizes meet WCAG requirements                            │   │
│  └──────────────────────────────────────────────────────────────────────────────┘   │
│                                                                                     │
│  WORKFLOW INTEGRATION:                                                              │
│  ├── User provides mockup → analyze_design_mockup → implementation plan            │
│  ├── After implementation → look_at screenshot → verify match                      │
│  ├── Before merge → compare_screenshots → detect regressions                       │
│  └── Final review → visual_accessibility_check → confirm WCAG compliance           │
│                                                                                     │
│  NOTE: Requires multimodal model (Gemini 3 Pro supports vision)                     │
│                                                                                     │
└─────────────────────────────────────────────────────────────────────────────────────┘
```

**Designer Skill Routing (DSL Shortcuts)**:
```
/ui <description>        → Route to Designer pipeline
/component <name>        → designer_search_components + designer_create_component
/style <component>       → designer_get_design_tokens + edit
/a11y <component>        → designer_check_accessibility
/preview <component>     → designer_preview_component
/tokens [category]       → designer_get_design_tokens
```

### Librarian Skills

```go
// Core Skills (Tier 1)
librarian_skills_core := []Skill{
    {
        Name:        "search_code",
        Description: "Search codebase for patterns or symbols",
        Domain:      "code",
        Keywords:    []string{"search", "find", "where", "grep"},
        Priority:    100,
        Parameters: []Param{
            {Name: "query", Type: "string", Required: true},
            {Name: "file_pattern", Type: "string", Required: false},
            {Name: "symbol_type", Type: "enum", Values: []string{"function", "type", "const", "var", "all"}, Required: false},
        },
    },
    {
        Name:        "get_context",
        Description: "Get contextual information about code",
        Domain:      "code",
        Keywords:    []string{"context", "explain", "what is", "how does"},
        Priority:    100,
    },
    {
        Name:        "list_files",
        Description: "List files matching pattern",
        Domain:      "files",
        Keywords:    []string{"list", "show files", "ls"},
        Priority:    90,
    },
    {
        Name:        "get_structure",
        Description: "Get codebase structure overview",
        Domain:      "code",
        Keywords:    []string{"structure", "overview", "organization"},
        Priority:    80,
    },
}

// Contextual Skills (Tier 2)
librarian_skills_contextual := []Skill{
    {
        Name:        "analyze_dependencies",
        Description: "Analyze dependency relationships",
        Domain:      "code",
        Keywords:    []string{"dependencies", "imports", "uses"},
        LoadTrigger: "depend|import|uses|requires",
    },
    {
        Name:        "detect_patterns",
        Description: "Detect coding patterns in codebase",
        Domain:      "code",
        Keywords:    []string{"patterns", "conventions", "style"},
        LoadTrigger: "pattern|convention|style|how do we",
    },
    // Codebase Health Assessment (Tier 2 - loaded for implementation queries)
    {
        Name:        "assess_codebase_health",
        Description: "Assess codebase maturity, patterns, and health indicators",
        Domain:      "assessment",
        Keywords:    []string{"health", "maturity", "debt", "quality"},
        LoadTrigger: "implement|build|create|add|change|modify",
        Parameters: []Param{
            {Name: "scope", Type: "string", Required: false, Description: "Path or 'full' for entire codebase"},
        },
    },
    {
        Name:        "get_test_coverage",
        Description: "Get test coverage information for a path",
        Domain:      "assessment",
        Keywords:    []string{"coverage", "tests", "tested"},
        LoadTrigger: "coverage|test|tested",
    },
    {
        Name:        "detect_technical_debt",
        Description: "Detect technical debt indicators in code area",
        Domain:      "assessment",
        Keywords:    []string{"debt", "todo", "fixme", "hack", "workaround"},
        LoadTrigger: "debt|todo|fixme|hack|cleanup",
    },
}

// Enhanced Search & AST Skills (Tier 2 - Structural Code Analysis)
// Inspired by oh-my-opencode ast_grep and anomalyco/opencode codesearch
librarian_skills_ast := []Skill{
    {
        Name:        "ast_grep_search",
        Description: "Search code using AST patterns for structural matching",
        Domain:      "ast",
        Keywords:    []string{"ast", "structure", "pattern", "syntax", "semantic"},
        LoadTrigger: "refactor|rename|structure|pattern|all occurrences",
        Parameters: []Param{
            {Name: "pattern", Type: "string", Required: true, Description: "AST pattern to match (e.g., 'func $NAME($ARGS) $RET { $$$BODY }')"},
            {Name: "language", Type: "enum", Values: []string{"go", "typescript", "python", "rust", "java", "all"}, Required: false},
            {Name: "path", Type: "string", Required: false, Description: "Path to search within"},
        },
    },
    {
        Name:        "codesearch",
        Description: "Semantic code search with understanding of code structure",
        Domain:      "search",
        Keywords:    []string{"semantic", "intelligent", "code search", "meaning"},
        LoadTrigger: "how is|where is|find all|usage of",
        Parameters: []Param{
            {Name: "query", Type: "string", Required: true, Description: "Natural language or code query"},
            {Name: "include_tests", Type: "bool", Required: false, Default: false},
            {Name: "scope", Type: "enum", Values: []string{"project", "package", "file"}, Required: false},
        },
    },
}

// LSP Integration Skills (Tier 2 - Language Server Protocol)
// Inspired by oh-my-opencode LSP tools
librarian_skills_lsp := []Skill{
    {
        Name:        "lsp_go_to_definition",
        Description: "Navigate to symbol definition using LSP",
        Domain:      "lsp",
        Keywords:    []string{"definition", "go to", "where defined", "source"},
        LoadTrigger: "definition|where is|go to|source of",
        Parameters: []Param{
            {Name: "file", Type: "string", Required: true},
            {Name: "line", Type: "int", Required: true},
            {Name: "column", Type: "int", Required: true},
        },
    },
    {
        Name:        "lsp_find_references",
        Description: "Find all references to a symbol using LSP",
        Domain:      "lsp",
        Keywords:    []string{"references", "usages", "callers", "used by"},
        LoadTrigger: "references|usages|callers|who uses|used by",
        Parameters: []Param{
            {Name: "file", Type: "string", Required: true},
            {Name: "line", Type: "int", Required: true},
            {Name: "column", Type: "int", Required: true},
            {Name: "include_declaration", Type: "bool", Required: false, Default: false},
        },
    },
    {
        Name:        "lsp_hover",
        Description: "Get type information and documentation for symbol",
        Domain:      "lsp",
        Keywords:    []string{"type", "docs", "signature", "info"},
        LoadTrigger: "what type|signature|docs for",
        Parameters: []Param{
            {Name: "file", Type: "string", Required: true},
            {Name: "line", Type: "int", Required: true},
            {Name: "column", Type: "int", Required: true},
        },
    },
    {
        Name:        "lsp_symbols",
        Description: "Get all symbols in file or workspace",
        Domain:      "lsp",
        Keywords:    []string{"symbols", "outline", "structure"},
        LoadTrigger: "outline|symbols|structure",
        Parameters: []Param{
            {Name: "scope", Type: "enum", Values: []string{"file", "workspace"}, Required: true},
            {Name: "file", Type: "string", Required: false, Description: "Required if scope is 'file'"},
            {Name: "query", Type: "string", Required: false, Description: "Filter symbols by name"},
        },
    },
    {
        Name:        "lsp_call_hierarchy",
        Description: "Get call hierarchy (incoming/outgoing calls) for a function",
        Domain:      "lsp",
        Keywords:    []string{"calls", "hierarchy", "callers", "callees"},
        LoadTrigger: "who calls|calls what|call hierarchy|call graph",
        Parameters: []Param{
            {Name: "file", Type: "string", Required: true},
            {Name: "line", Type: "int", Required: true},
            {Name: "column", Type: "int", Required: true},
            {Name: "direction", Type: "enum", Values: []string{"incoming", "outgoing", "both"}, Required: true},
            {Name: "depth", Type: "int", Required: false, Default: 2},
        },
    },
}
```

#### Enhanced Search & AST Protocol

**Librarian uses AST-based search for structural queries and LSP for precise navigation.**

```
┌─────────────────────────────────────────────────────────────────────────────────────┐
│                      ENHANCED SEARCH STRATEGY                                        │
├─────────────────────────────────────────────────────────────────────────────────────┤
│                                                                                     │
│  SEARCH TYPE SELECTION:                                                             │
│  ┌──────────────────────────────────────────────────────────────────────────────┐   │
│  │  Query Type              │ Recommended Tool                                  │   │
│  ├──────────────────────────┼─────────────────────────────────────────────────┤   │
│  │  "where is X defined"    │ lsp_go_to_definition (if position known)         │   │
│  │                          │ OR search_code with symbol_type                  │   │
│  ├──────────────────────────┼─────────────────────────────────────────────────┤   │
│  │  "who uses X"            │ lsp_find_references (precise)                    │   │
│  │                          │ OR codesearch for broader context                │   │
│  ├──────────────────────────┼─────────────────────────────────────────────────┤   │
│  │  "all functions that..." │ ast_grep_search with pattern                     │   │
│  ├──────────────────────────┼─────────────────────────────────────────────────┤   │
│  │  "how does X work"       │ codesearch + lsp_call_hierarchy                  │   │
│  ├──────────────────────────┼─────────────────────────────────────────────────┤   │
│  │  "rename X to Y"         │ lsp_find_references (to find all occurrences)    │   │
│  │                          │ + ast_grep_search (to verify structural match)  │   │
│  └──────────────────────────┴─────────────────────────────────────────────────┘   │
│                                                                                     │
│  AST PATTERN EXAMPLES:                                                              │
│  ├── Go: "func $NAME($ARGS) error { $$$ }" - Find all error-returning functions   │
│  ├── Go: "if err != nil { return err }" - Find basic error handling              │
│  ├── TS: "async function $NAME($$$) { $$$ }" - Find async functions               │
│  └── Python: "def $NAME(self, $$$): $$$" - Find instance methods                  │
│                                                                                     │
│  LSP FALLBACK:                                                                      │
│  If LSP not available for language, fall back to:                                  │
│  1. ast_grep_search for structural queries                                         │
│  2. search_code with regex for text queries                                        │
│  3. codesearch for semantic queries                                                │
│                                                                                     │
└─────────────────────────────────────────────────────────────────────────────────────┘
```

#### Codebase Health Assessment Protocol

**CRITICAL: Librarian assesses codebase health before providing context for implementation tasks.**

```
┌─────────────────────────────────────────────────────────────────────────────────────┐
│                      CODEBASE HEALTH ASSESSMENT PROTOCOL                             │
├─────────────────────────────────────────────────────────────────────────────────────┤
│                                                                                     │
│  MATURITY LEVELS:                                                                   │
│  ┌──────────────────────────────────────────────────────────────────────────────┐   │
│  │  DISCIPLINED   │ Consistent patterns, high test coverage, types enforced    │   │
│  │                │ → Follow existing patterns strictly                         │   │
│  ├────────────────┼─────────────────────────────────────────────────────────────┤   │
│  │  TRANSITIONAL  │ Mixed patterns, partial coverage, some legacy              │   │
│  │                │ → Follow newer patterns, don't propagate old patterns       │   │
│  ├────────────────┼─────────────────────────────────────────────────────────────┤   │
│  │  LEGACY        │ Inconsistent, minimal tests, tech debt                      │   │
│  │                │ → Isolate changes, avoid spreading patterns                 │   │
│  ├────────────────┼─────────────────────────────────────────────────────────────┤   │
│  │  GREENFIELD    │ New/empty, no established patterns                          │   │
│  │                │ → Establish patterns with user approval                     │   │
│  └────────────────┴─────────────────────────────────────────────────────────────┘   │
│                                                                                     │
│  HEALTH INDICATORS:                                                                 │
│  ├── Pattern Consistency Score: 0.0-1.0 (how consistent are patterns?)              │
│  ├── Test Coverage: percentage and distribution                                     │
│  ├── Technical Debt Markers: TODO/FIXME/HACK count                                  │
│  ├── Type Safety: strict types vs any/unknown usage                                 │
│  └── Documentation: inline docs, README presence                                    │
│                                                                                     │
│  RESPONSE FORMAT (included in implementation context):                              │
│  {                                                                                  │
│    "area": "path/to/code",                                                          │
│    "maturity": "DISCIPLINED|TRANSITIONAL|LEGACY|GREENFIELD",                        │
│    "pattern_confidence": 0.85,                                                      │
│    "test_coverage": "78%",                                                          │
│    "debt_markers": 3,                                                               │
│    "recommendation": "Follow existing middleware pattern in auth.go"               │
│  }                                                                                  │
│                                                                                     │
└─────────────────────────────────────────────────────────────────────────────────────┘
```

#### Librarian Health Assessment System Prompt Addition

```go
const LibrarianHealthAssessmentPrompt = `
CODEBASE HEALTH ASSESSMENT:
When providing context for IMPLEMENTATION tasks (create, add, modify, change):

1. ASSESS MATURITY of the target area:
   - DISCIPLINED: Consistent patterns, tests, types → "Follow existing patterns strictly"
   - TRANSITIONAL: Mixed quality → "Follow newer patterns, don't propagate legacy"
   - LEGACY: Tech debt, inconsistent → "Isolate changes, minimize pattern spread"
   - GREENFIELD: New code → "Establish patterns with user approval"

2. MEASURE HEALTH INDICATORS:
   - Pattern consistency (0.0-1.0): How uniform are patterns?
   - Test coverage: What percentage? Unit vs integration?
   - Debt markers: Count of TODO/FIXME/HACK comments
   - Type safety: Strict types or permissive?

3. INCLUDE IN RESPONSE:
   Always append health assessment to implementation context:
   "CODEBASE HEALTH [area]: [MATURITY] | Patterns: [score] | Coverage: [%] | Debt: [count]
    Recommendation: [specific guidance based on maturity]"

4. FLAG RISKS:
   If maturity is LEGACY or pattern_confidence < 0.5:
   "WARNING: Low pattern confidence. Recommend explicit pattern review before implementation."
`
```

#### Context Quality Feedback Protocol

**CRITICAL: Librarian receives feedback when its context leads to failures, enabling continuous improvement.**

```
┌─────────────────────────────────────────────────────────────────────────────────────┐
│                      CONTEXT QUALITY FEEDBACK PROTOCOL                               │
├─────────────────────────────────────────────────────────────────────────────────────┤
│                                                                                     │
│  PRINCIPLE: Context quality is measured by implementation success.                  │
│                                                                                     │
│  FEEDBACK SIGNALS:                                                                  │
│  ┌──────────────────────────────────────────────────────────────────────────────┐   │
│  │  1. ENGINEER_FAILURE: Engineer fails using Librarian's context               │   │
│  │     → Archivalist records failure with context_source: "librarian"           │   │
│  │                                                                              │   │
│  │  2. PATTERN_MISMATCH: Inspector finds code doesn't match stated patterns     │   │
│  │     → Librarian's pattern detection was inaccurate                           │   │
│  │                                                                              │   │
│  │  3. OUTDATED_CONTEXT: Context referenced files that changed                  │   │
│  │     → Cache invalidation wasn't triggered properly                           │   │
│  │                                                                              │   │
│  │  4. MISSING_CONTEXT: Engineer needed info Librarian didn't provide           │   │
│  │     → Query didn't surface relevant code                                     │   │
│  │                                                                              │   │
│  │  5. WRONG_RECOMMENDATION: Health assessment led to wrong approach            │   │
│  │     → Maturity classification was incorrect                                  │   │
│  └──────────────────────────────────────────────────────────────────────────────┘   │
│                                                                                     │
│  FEEDBACK ENTRY STRUCTURE:                                                          │
│  {                                                                                  │
│    "id": "ctx_feedback_abc123",                                                     │
│    "query_id": "original query that produced the context",                          │
│    "context_provided": "summary of what Librarian returned",                        │
│    "failure_type": "PATTERN_MISMATCH",                                              │
│    "actual_outcome": "Engineer used wrong pattern, Inspector caught it",            │
│    "correct_context": "should have mentioned X pattern in Y file",                  │
│    "timestamp": "2024-01-15T10:30:00Z"                                              │
│  }                                                                                  │
│                                                                                     │
│  LEARNING INTEGRATION:                                                              │
│  ├── Store feedback in Archivalist category "librarian_context_feedback"           │
│  ├── Query feedback when similar queries arrive                                    │
│  ├── Adjust search/synthesis strategy based on past failures                       │
│  ├── Invalidate cache entries that led to failures                                 │
│  └── Refine health assessment heuristics based on wrong classifications            │
│                                                                                     │
│  QUALITY METRICS:                                                                   │
│  ├── Context success rate (contexts that led to successful implementations)        │
│  ├── Pattern accuracy (stated patterns vs. Inspector findings)                     │
│  ├── Health assessment accuracy (maturity vs. actual code quality)                 │
│  └── Cache hit quality (cache hits that were still accurate)                       │
│                                                                                     │
└─────────────────────────────────────────────────────────────────────────────────────┘
```

#### Librarian Context Quality Skills

```go
// Context Quality Feedback Skills
librarian_skills_feedback := []Skill{
    {
        Name:        "receive_context_feedback",
        Description: "Receive feedback about context quality from downstream agents",
        Domain:      "quality",
        Keywords:    []string{"feedback", "quality", "failed", "wrong"},
        Parameters: []Param{
            {Name: "query_id", Type: "string", Required: true},
            {Name: "failure_type", Type: "enum", Values: []string{
                "ENGINEER_FAILURE", "PATTERN_MISMATCH", "OUTDATED_CONTEXT",
                "MISSING_CONTEXT", "WRONG_RECOMMENDATION",
            }, Required: true},
            {Name: "actual_outcome", Type: "string", Required: true},
            {Name: "correct_context", Type: "string", Required: false},
        },
    },
    {
        Name:        "query_context_feedback",
        Description: "Query past context feedback for similar queries",
        Domain:      "quality",
        Keywords:    []string{"past feedback", "similar queries"},
        Parameters: []Param{
            {Name: "query_pattern", Type: "string", Required: true},
        },
    },
    {
        Name:        "get_context_quality_metrics",
        Description: "Get context quality metrics",
        Domain:      "quality",
        Keywords:    []string{"metrics", "accuracy", "success rate"},
    },
}

// Direct Consultation Skills (Tier 1 - Core, bypasses Guide for token efficiency)
// Librarian only consults other knowledge agents (limited scope)
// See Direct Consultation Protocol for details
librarian_skills_consultation := []Skill{
    {
        Name:        "consult_archivalist",
        Description: "Directly consult Archivalist for historical code context",
        Domain:      "consultation",
        Target:      "archivalist",
        Keywords:    []string{"history", "past changes", "previous version", "when changed"},
        Priority:    85,
        Parameters:  consultationParams,
    },
    {
        Name:        "consult_academic",
        Description: "Directly consult Academic for research on code patterns",
        Domain:      "consultation",
        Target:      "academic",
        Keywords:    []string{"best practice", "pattern research", "recommended approach"},
        Priority:    85,
        Parameters:  consultationParams,
    },
}
```

#### Librarian Context Quality System Prompt Addition

```go
const LibrarianContextQualityPrompt = `
CONTEXT QUALITY FEEDBACK:
Your context quality is measured by implementation success.

FEEDBACK SIGNALS:
1. ENGINEER_FAILURE: Your context led to implementation failure
2. PATTERN_MISMATCH: Inspector found code doesn't match your stated patterns
3. OUTDATED_CONTEXT: Your context referenced stale information
4. MISSING_CONTEXT: Engineer needed info you didn't provide
5. WRONG_RECOMMENDATION: Your health assessment led to wrong approach

WHEN FEEDBACK RECEIVED:
1. Store in Archivalist category "librarian_context_feedback"
2. Invalidate related cache entries
3. Update internal heuristics

DURING QUERY PROCESSING:
1. Check for past feedback on similar queries
2. If similar query had feedback, adjust response:
   - Include additional context that was missing
   - Correct pattern descriptions that were wrong
   - Update health assessment if it was inaccurate
3. Add confidence note if query type has history of failures

PROACTIVE QUALITY:
- Track which query types have high failure rates
- For high-failure query types, provide more comprehensive context
- Include "CONFIDENCE: LOW" warning if similar queries failed before
`
```

#### Tool Discovery Skills (Tier 2 - Project Tooling Detection)

**CRITICAL: Librarian owns all tool discovery. Inspector and Tester MUST consult Librarian before executing formatting, linting, or testing.**

```go
// Tool Discovery Skills - Librarian provides tooling intelligence to Inspector/Tester
librarian_skills_tool_discovery := []Skill{
    // Formatter Discovery
    {
        Name:        "detect_formatter",
        Description: "Detect which formatter should be used for a file or project",
        Domain:      "tooling",
        Keywords:    []string{"formatter", "format", "which formatter", "detect format"},
        LoadTrigger: "format|prettier|gofmt|black|rustfmt",
        Parameters: []Param{
            {Name: "file", Type: "string", Required: false, Description: "Specific file to check"},
            {Name: "extension", Type: "string", Required: false, Description: "File extension to check"},
        },
    },
    {
        Name:        "list_formatters",
        Description: "List all available formatters for the project",
        Domain:      "tooling",
        Keywords:    []string{"formatters", "available formatters", "all formatters"},
        LoadTrigger: "format|formatters|style",
        Parameters: []Param{
            {Name: "extension", Type: "string", Required: false, Description: "Filter by extension"},
            {Name: "enabled_only", Type: "bool", Required: false, Default: true},
        },
    },

    // Linter/LSP Discovery
    {
        Name:        "detect_linters",
        Description: "Detect which linters/LSP servers are available for a file",
        Domain:      "tooling",
        Keywords:    []string{"linter", "lint", "which linter", "detect linter", "lsp"},
        LoadTrigger: "lint|linter|check|analyze|lsp",
        Parameters: []Param{
            {Name: "file", Type: "string", Required: false},
            {Name: "extension", Type: "string", Required: false},
        },
    },
    {
        Name:        "list_lsp_servers",
        Description: "List all available LSP servers for the project",
        Domain:      "tooling",
        Keywords:    []string{"lsp", "language server", "servers"},
        LoadTrigger: "lsp|language server",
        Parameters: []Param{
            {Name: "language", Type: "string", Required: false},
            {Name: "enabled_only", Type: "bool", Required: false, Default: true},
        },
    },

    // Test Framework Discovery
    {
        Name:        "detect_test_framework",
        Description: "Detect which test framework is used in the project",
        Domain:      "tooling",
        Keywords:    []string{"test framework", "which test", "detect test", "test runner"},
        LoadTrigger: "test|testing|spec|jest|pytest|go test",
        Parameters: []Param{
            {Name: "path", Type: "string", Required: false, Description: "Specific path to check"},
            {Name: "language", Type: "string", Required: false, Description: "Filter by language"},
        },
    },
    {
        Name:        "list_test_frameworks",
        Description: "List all available test frameworks for the project",
        Domain:      "tooling",
        Keywords:    []string{"test frameworks", "available tests", "all test runners"},
        LoadTrigger: "test|testing|frameworks",
        Parameters: []Param{
            {Name: "language", Type: "string", Required: false},
            {Name: "enabled_only", Type: "bool", Required: false, Default: true},
        },
    },

    // Unified Tool Query
    {
        Name:        "get_project_tools",
        Description: "Get all detected tools for the project (formatters, linters, test frameworks)",
        Domain:      "tooling",
        Keywords:    []string{"tools", "project tools", "tooling", "setup"},
        LoadTrigger: "tools|setup|project|environment",
        Parameters: []Param{
            {Name: "categories", Type: "array", Required: false, Description: "Filter: ['formatters', 'linters', 'test_frameworks']"},
        },
    },
}
```

#### Tool Discovery Protocol

**Librarian is the single source of truth for project tooling.**

```
┌─────────────────────────────────────────────────────────────────────────────────────┐
│                      TOOL DISCOVERY PROTOCOL                                         │
├─────────────────────────────────────────────────────────────────────────────────────┤
│                                                                                     │
│  RESPONSIBILITY SPLIT:                                                              │
│  ┌──────────────────────────────────────────────────────────────────────────────┐   │
│  │  LIBRARIAN (Discovery)        │  INSPECTOR/TESTER (Execution)               │   │
│  ├───────────────────────────────┼─────────────────────────────────────────────┤   │
│  │  detect_formatter             │  format_file, format_files                  │   │
│  │  list_formatters              │  (receives FormatterDefinition)             │   │
│  ├───────────────────────────────┼─────────────────────────────────────────────┤   │
│  │  detect_linters               │  lint_file, lint_files, get_diagnostics    │   │
│  │  list_lsp_servers             │  (receives LSPServerDefinition)             │   │
│  ├───────────────────────────────┼─────────────────────────────────────────────┤   │
│  │  detect_test_framework        │  run_tests_smart, run_tests_with_coverage  │   │
│  │  list_test_frameworks         │  watch_tests, run_changed_tests            │   │
│  │                               │  (receives TestFrameworkDefinition)         │   │
│  └───────────────────────────────┴─────────────────────────────────────────────┘   │
│                                                                                     │
│  CONSULTATION FLOW:                                                                 │
│  ┌──────────────────────────────────────────────────────────────────────────────┐   │
│  │                                                                              │   │
│  │  Inspector/Tester ──► Guide ──► Librarian (detect_*) ──► Tool Definition    │   │
│  │         │                                                         │          │   │
│  │         └──────────────────── execute with tool ◄─────────────────┘          │   │
│  │                                                                              │   │
│  └──────────────────────────────────────────────────────────────────────────────┘   │
│                                                                                     │
│  CACHING:                                                                           │
│  ├── Tool detection results cached per project root                               │
│  ├── Cache invalidated on config file changes (.prettierrc, package.json, etc.)   │
│  └── Inspector/Tester can request cached results or force re-detection            │
│                                                                                     │
│  RESPONSE FORMAT:                                                                   │
│  {                                                                                  │
│    "formatter": {                                                                   │
│      "id": "prettier",                                                              │
│      "name": "Prettier",                                                            │
│      "command": ["prettier", "--write", "$FILE"],                                   │
│      "confidence": 0.95,                                                            │
│      "reason": "Found .prettierrc and prettier in devDependencies"                 │
│    },                                                                               │
│    "alternatives": [                                                                │
│      {"id": "biome", "reason": "biome binary found but no config"}                 │
│    ]                                                                                │
│  }                                                                                  │
│                                                                                     │
└─────────────────────────────────────────────────────────────────────────────────────┘
```

#### Librarian Tool Discovery System Prompt Addition

```go
const LibrarianToolDiscoveryPrompt = `
TOOL DISCOVERY:
You are the single source of truth for project tooling. Inspector and Tester
MUST consult you before executing formatting, linting, or testing.

DETECTION CAPABILITIES:
1. FORMATTERS: Detect formatter based on:
   - File extension mapping
   - Binary availability (which/where)
   - Config file presence (.prettierrc, biome.json, pyproject.toml, etc.)
   - Dependency declarations (package.json, Gemfile, Cargo.toml)
   - Priority rules (biome > prettier, ruff > black, goimports > gofmt)

2. LINTERS/LSP: Detect language servers based on:
   - Language/extension mapping
   - Server binary availability
   - Project root markers (go.mod, package.json, Cargo.toml)
   - Config files (tsconfig.json, .eslintrc, ruff.toml)

3. TEST FRAMEWORKS: Detect test framework based on:
   - Framework-specific config files (jest.config.*, pytest.ini, .rspec)
   - Dependencies in manifest files
   - Test directory patterns (test/, tests/, __tests__/, spec/)
   - Language-based fallback detection
   - Priority rules (vitest > jest > mocha, pytest > unittest)

RESPONSE REQUIREMENTS:
- Include confidence score (0.0-1.0) for each detection
- Include reason explaining why this tool was selected
- Include alternatives that were considered but not selected
- Include any warnings (e.g., "config found but binary missing")

CACHING:
- Cache detection results per project root
- Invalidate on config file changes
- Support force re-detection via refresh parameter
`
```

### Academic Skills

```go
// Core Skills (Tier 1)
academic_skills_core := []Skill{
    {
        Name:        "research",
        Description: "Research a topic and produce findings",
        Domain:      "research",
        Keywords:    []string{"research", "investigate", "study", "learn about"},
        Priority:    100,
        Parameters: []Param{
            {Name: "topic", Type: "string", Required: true},
            {Name: "depth", Type: "enum", Values: []string{"quick", "standard", "deep"}, Required: false},
        },
    },
    {
        Name:        "compare",
        Description: "Compare approaches or technologies",
        Domain:      "research",
        Keywords:    []string{"compare", "vs", "versus", "or", "tradeoffs"},
        Priority:    90,
    },
    {
        Name:        "best_practices",
        Description: "Find best practices for a topic",
        Domain:      "research",
        Keywords:    []string{"best practice", "recommended", "standard", "how should"},
        Priority:    90,
    },
}

// Contextual Skills (Tier 2)
academic_skills_contextual := []Skill{
    {
        Name:        "ingest_source",
        Description: "Ingest external source for research",
        Domain:      "research",
        Keywords:    []string{"github", "paper", "article", "rfc"},
        LoadTrigger: "github|paper|article|rfc|spec",
    },
    {
        Name:        "design_proposal",
        Description: "Create a design proposal document",
        Domain:      "research",
        Keywords:    []string{"design", "architecture", "proposal"},
        LoadTrigger: "design|architect|proposal",
    },
}

// Web Research Skills (Tier 2 - External Knowledge Acquisition)
// Inspired by anomalyco/opencode websearch/webfetch and oh-my-opencode MCP integrations
academic_skills_web := []Skill{
    {
        Name:        "web_search",
        Description: "Search the web for research, documentation, and best practices",
        Domain:      "web",
        Keywords:    []string{"search", "google", "web", "online", "latest"},
        LoadTrigger: "latest|current|2024|2025|recent|online|web",
        Parameters: []Param{
            {Name: "query", Type: "string", Required: true, Description: "Search query"},
            {Name: "domain_filter", Type: "array", Required: false, Description: "Limit to specific domains (e.g., ['docs.go.dev', 'pkg.go.dev'])"},
            {Name: "max_results", Type: "int", Required: false, Default: 10},
        },
    },
    {
        Name:        "web_fetch",
        Description: "Fetch and extract content from a URL",
        Domain:      "web",
        Keywords:    []string{"fetch", "url", "page", "read"},
        LoadTrigger: "http|https|url|link|page",
        Parameters: []Param{
            {Name: "url", Type: "string", Required: true},
            {Name: "extract", Type: "enum", Values: []string{"full", "main_content", "code_blocks", "headings"}, Required: false, Default: "main_content"},
        },
    },
    {
        Name:        "fetch_documentation",
        Description: "Fetch official documentation for a library or framework",
        Domain:      "web",
        Keywords:    []string{"docs", "documentation", "official", "api"},
        LoadTrigger: "docs|documentation|api|reference",
        Parameters: []Param{
            {Name: "package", Type: "string", Required: true, Description: "Package or library name"},
            {Name: "version", Type: "string", Required: false, Description: "Specific version (default: latest)"},
            {Name: "section", Type: "string", Required: false, Description: "Specific documentation section"},
        },
    },
    {
        Name:        "search_packages",
        Description: "Search package registries for libraries",
        Domain:      "web",
        Keywords:    []string{"package", "library", "npm", "go", "pypi", "crates"},
        LoadTrigger: "library|package|npm|go get|pip|cargo",
        Parameters: []Param{
            {Name: "query", Type: "string", Required: true},
            {Name: "registry", Type: "enum", Values: []string{"npm", "go", "pypi", "crates", "all"}, Required: false, Default: "all"},
            {Name: "sort_by", Type: "enum", Values: []string{"relevance", "downloads", "recent", "stars"}, Required: false},
        },
    },
    {
        Name:        "fetch_github_context",
        Description: "Fetch context from a GitHub repository (README, structure, issues)",
        Domain:      "web",
        Keywords:    []string{"github", "repo", "repository", "project"},
        LoadTrigger: "github|repo|repository|open source",
        Parameters: []Param{
            {Name: "repo", Type: "string", Required: true, Description: "owner/repo format"},
            {Name: "content", Type: "enum", Values: []string{"readme", "structure", "issues", "releases", "all"}, Required: false, Default: "readme"},
        },
    },
}

// Research Discipline Skills (Tier 2 - loaded for implementation research)
academic_skills_discipline := []Skill{
    {
        Name:        "validate_against_codebase",
        Description: "Validate research recommendations against codebase patterns",
        Domain:      "validation",
        Keywords:    []string{"validate", "check", "applicable", "reality"},
        LoadTrigger: "implement|build|create|apply",
        Parameters: []Param{
            {Name: "recommendations", Type: "array", Required: true, Description: "Research recommendations to validate"},
            {Name: "codebase_context", Type: "string", Required: true, Description: "Librarian's context response"},
        },
    },
    {
        Name:        "assess_applicability",
        Description: "Assess if research applies to this codebase's maturity level",
        Domain:      "validation",
        Keywords:    []string{"applicable", "fits", "works for"},
        Parameters: []Param{
            {Name: "research_output", Type: "object", Required: true},
            {Name: "codebase_maturity", Type: "enum", Values: []string{"DISCIPLINED", "TRANSITIONAL", "LEGACY", "GREENFIELD"}, Required: true},
        },
    },
    {
        Name:        "identify_adaptation_needs",
        Description: "Identify what adaptations are needed to apply research to codebase",
        Domain:      "validation",
        Keywords:    []string{"adapt", "modify", "adjust", "change"},
        Parameters: []Param{
            {Name: "research_pattern", Type: "string", Required: true},
            {Name: "existing_pattern", Type: "string", Required: true},
        },
    },
}

// Direct Consultation Skills (Tier 1 - Core, bypasses Guide for token efficiency)
// Academic only consults other knowledge agents (limited scope)
// See Direct Consultation Protocol for details
academic_skills_consultation := []Skill{
    {
        Name:        "consult_librarian",
        Description: "Directly consult Librarian for codebase patterns to validate research",
        Domain:      "consultation",
        Target:      "librarian",
        Keywords:    []string{"codebase patterns", "existing code", "reality check"},
        Priority:    85,
        Parameters:  consultationParams,
    },
    {
        Name:        "consult_archivalist",
        Description: "Directly consult Archivalist for historical research outcomes",
        Domain:      "consultation",
        Target:      "archivalist",
        Keywords:    []string{"past research", "previous recommendations", "outcome history"},
        Priority:    85,
        Parameters:  consultationParams,
    },
}
```

#### Research Discipline Protocol

**CRITICAL: Academic validates all recommendations against codebase reality before presenting.**

```
┌─────────────────────────────────────────────────────────────────────────────────────┐
│                        RESEARCH DISCIPLINE PROTOCOL                                  │
├─────────────────────────────────────────────────────────────────────────────────────┤
│                                                                                     │
│  PRINCIPLE: Theory must be validated against codebase reality.                      │
│                                                                                     │
│  BEFORE FINALIZING ANY RECOMMENDATION:                                              │
│  ┌──────────────────────────────────────────────────────────────────────────────┐   │
│  │  1. Request Librarian context for the target area                            │   │
│  │  2. Compare research patterns with existing codebase patterns                │   │
│  │  3. Check codebase maturity level                                            │   │
│  │  4. Flag theory-reality gaps                                                 │   │
│  └──────────────────────────────────────────────────────────────────────────────┘   │
│                                                                                     │
│  APPLICABILITY CLASSIFICATION:                                                      │
│  ├── DIRECT: Research aligns with codebase patterns → Confidence: HIGH             │
│  ├── ADAPTABLE: Research requires modification → Confidence: MEDIUM                │
│  │   → Include specific adaptation requirements                                     │
│  └── INCOMPATIBLE: Research conflicts with established patterns → Confidence: LOW  │
│      → Include "ADAPTATION NEEDED: codebase uses X, research suggests Y"           │
│                                                                                     │
│  MATURITY CONSIDERATIONS:                                                           │
│  ├── DISCIPLINED codebase: Only recommend patterns that fit existing conventions   │
│  ├── TRANSITIONAL: Can recommend improvements but flag migration needs             │
│  ├── LEGACY: Focus on isolated, safe approaches - avoid big refactors             │
│  └── GREENFIELD: Full flexibility but require user approval for new patterns      │
│                                                                                     │
│  RESPONSE FORMAT:                                                                   │
│  {                                                                                  │
│    "recommendation": "...",                                                         │
│    "applicability": "DIRECT|ADAPTABLE|INCOMPATIBLE",                               │
│    "confidence": "HIGH|MEDIUM|LOW",                                                 │
│    "codebase_alignment": "description of how it fits",                             │
│    "adaptation_needed": ["list of changes needed"] or null,                        │
│    "theory_reality_gap": "description" or null                                     │
│  }                                                                                  │
│                                                                                     │
└─────────────────────────────────────────────────────────────────────────────────────┘
```

#### Academic Research Discipline System Prompt Addition

```go
const AcademicResearchDisciplinePrompt = `
RESEARCH DISCIPLINE PROTOCOL:
You must validate all recommendations against codebase reality.

1. BEFORE FINALIZING RECOMMENDATIONS:
   - Request Librarian context: "What patterns exist in [target area]?"
   - Compare external best practices with existing codebase patterns
   - Check codebase maturity level from Librarian's health assessment

2. APPLICABILITY SCORING:
   - DIRECT (HIGH confidence): Research aligns with existing patterns
   - ADAPTABLE (MEDIUM confidence): Research needs modification
   - INCOMPATIBLE (LOW confidence): Research conflicts with codebase

3. THEORY-REALITY GAP DETECTION:
   If research suggests pattern X but codebase uses pattern Y:
   "ADAPTATION NEEDED: Codebase uses [Y pattern]. Research suggests [X pattern].
    To apply: [specific adaptation steps]"

4. MATURITY-AWARE RECOMMENDATIONS:
   - DISCIPLINED: Only recommend patterns fitting existing conventions
   - TRANSITIONAL: Can suggest improvements, flag migration path
   - LEGACY: Focus on safe, isolated changes - no big refactors
   - GREENFIELD: Full flexibility, but require user pattern approval

5. NEVER RECOMMEND:
   - Patterns that conflict with codebase maturity without flagging
   - Complete rewrites when codebase is LEGACY
   - External patterns without checking existing implementations first

6. ALWAYS INCLUDE in implementation research:
   "Codebase Alignment: [DIRECT/ADAPTABLE/INCOMPATIBLE]
    Confidence: [HIGH/MEDIUM/LOW]
    Adaptation: [required changes or 'None']"
`
```

#### Recommendation Outcome Tracking Protocol

**CRITICAL: Academic tracks whether its recommendations succeed when implemented, enabling continuous improvement.**

```
┌─────────────────────────────────────────────────────────────────────────────────────┐
│                    RECOMMENDATION OUTCOME TRACKING PROTOCOL                          │
├─────────────────────────────────────────────────────────────────────────────────────┤
│                                                                                     │
│  PRINCIPLE: Research value is measured by implementation success.                   │
│                                                                                     │
│  OUTCOME SIGNALS:                                                                   │
│  ┌──────────────────────────────────────────────────────────────────────────────┐   │
│  │  1. IMPLEMENTATION_SUCCESS: Recommendation worked as expected                │   │
│  │     → Record as positive example for similar future queries                  │   │
│  │                                                                              │   │
│  │  2. IMPLEMENTATION_FAILURE: Recommendation led to failure                    │   │
│  │     → Analyze why: was it the recommendation or the adaptation?              │   │
│  │                                                                              │   │
│  │  3. ADAPTATION_REQUIRED: Recommendation needed significant changes           │   │
│  │     → Update applicability scoring for this codebase pattern                 │   │
│  │                                                                              │   │
│  │  4. BETTER_ALTERNATIVE_FOUND: Different approach worked better               │   │
│  │     → Record alternative for future similar queries                          │   │
│  │                                                                              │   │
│  │  5. PARTIAL_SUCCESS: Some aspects worked, others didn't                      │   │
│  │     → Extract what worked for future recommendations                         │   │
│  └──────────────────────────────────────────────────────────────────────────────┘   │
│                                                                                     │
│  OUTCOME ENTRY STRUCTURE:                                                           │
│  {                                                                                  │
│    "id": "rec_outcome_abc123",                                                      │
│    "recommendation_id": "original recommendation",                                  │
│    "query": "what the user asked",                                                  │
│    "recommendation_summary": "what Academic recommended",                           │
│    "codebase_context": "maturity, patterns at time of recommendation",             │
│    "outcome": "IMPLEMENTATION_SUCCESS",                                             │
│    "outcome_details": "Approach worked, tests pass, patterns match",               │
│    "lessons_learned": "For DISCIPLINED codebases, this pattern works well",        │
│    "timestamp": "2024-01-15T10:30:00Z"                                              │
│  }                                                                                  │
│                                                                                     │
│  LEARNING INTEGRATION:                                                              │
│  ├── Store outcomes in Archivalist category "academic_recommendation_outcome"      │
│  ├── Query outcomes when similar research queries arrive                           │
│  ├── Adjust confidence scoring based on historical success rates                   │
│  ├── Track which recommendation types work for which codebase maturities           │
│  └── Surface "worked before" or "failed before" notes in recommendations           │
│                                                                                     │
│  SUCCESS RATE TRACKING:                                                             │
│  ├── By recommendation type (pattern, library, architecture)                       │
│  ├── By codebase maturity (DISCIPLINED vs LEGACY success rates)                    │
│  ├── By applicability classification (DIRECT vs ADAPTABLE success)                 │
│  └── By topic area (auth, caching, testing, etc.)                                  │
│                                                                                     │
└─────────────────────────────────────────────────────────────────────────────────────┘
```

#### Academic Recommendation Outcome Skills

```go
// Recommendation Outcome Tracking Skills
academic_skills_outcome := []Skill{
    {
        Name:        "record_recommendation_outcome",
        Description: "Record the outcome of a recommendation after implementation",
        Domain:      "outcome_tracking",
        Keywords:    []string{"outcome", "result", "worked", "failed"},
        Parameters: []Param{
            {Name: "recommendation_id", Type: "string", Required: true},
            {Name: "outcome", Type: "enum", Values: []string{
                "IMPLEMENTATION_SUCCESS", "IMPLEMENTATION_FAILURE",
                "ADAPTATION_REQUIRED", "BETTER_ALTERNATIVE_FOUND", "PARTIAL_SUCCESS",
            }, Required: true},
            {Name: "outcome_details", Type: "string", Required: true},
            {Name: "lessons_learned", Type: "string", Required: false},
        },
    },
    {
        Name:        "query_recommendation_outcomes",
        Description: "Query past outcomes for similar recommendations",
        Domain:      "outcome_tracking",
        Keywords:    []string{"past outcomes", "history", "success rate"},
        Parameters: []Param{
            {Name: "topic", Type: "string", Required: true},
            {Name: "codebase_maturity", Type: "string", Required: false},
        },
    },
    {
        Name:        "get_recommendation_success_rates",
        Description: "Get success rates by recommendation type and context",
        Domain:      "outcome_tracking",
        Keywords:    []string{"success rate", "metrics", "statistics"},
        Parameters: []Param{
            {Name: "group_by", Type: "enum", Values: []string{
                "recommendation_type", "codebase_maturity", "applicability", "topic",
            }, Required: false},
        },
    },
}
```

#### Academic Recommendation Outcome System Prompt Addition

```go
const AcademicRecommendationOutcomePrompt = `
RECOMMENDATION OUTCOME TRACKING:
Your research value is measured by implementation success.

OUTCOME SIGNALS:
1. IMPLEMENTATION_SUCCESS: Recommendation worked as expected
2. IMPLEMENTATION_FAILURE: Recommendation led to failure
3. ADAPTATION_REQUIRED: Significant changes needed beyond prediction
4. BETTER_ALTERNATIVE_FOUND: Different approach worked better
5. PARTIAL_SUCCESS: Some aspects worked, others didn't

WHEN OUTCOME REPORTED:
1. Store in Archivalist category "academic_recommendation_outcome"
2. Extract lessons learned
3. Update internal success rate tracking

DURING RESEARCH:
1. Query past outcomes for similar topics/contexts
2. If similar recommendation failed before:
   - Include "⚠️ PAST FAILURE: Similar recommendation failed in [context]. Reason: [reason]"
   - Suggest alternative that worked
3. If similar recommendation succeeded:
   - Include "✓ VALIDATED: Similar recommendation succeeded in [context]"
   - Note any adaptation requirements
4. Adjust confidence based on historical success:
   - >80% success rate: HIGH confidence
   - 50-80% success rate: MEDIUM confidence + note variability
   - <50% success rate: LOW confidence + recommend alternatives

FEEDBACK LOOP:
When Architect or Engineer reports outcome:
- Match to original recommendation
- Record outcome with full context
- If FAILURE: analyze if issue was recommendation or implementation
`
```

### Inspector System Prompt

```go
const InspectorSystemPrompt = `
You are the Inspector agent. You validate code quality, patterns, and correctness.

## Knowledge Agent Consultation (via Guide)

You can consult knowledge agents for validation context. ALL consultations go through Guide:

CONTEXT REQUESTS (to Librarian):
- "What patterns does this codebase use for error handling?"
- "What style conventions exist for this area?"
- "Does this change match existing patterns?"

HISTORY REQUESTS (to Archivalist):
- "Have we seen this type of bug before?"
- "What validation failures have occurred in similar code?"
- "Success/failure rate for this pattern of change?"

RESEARCH REQUESTS (to Academic):
- "Security best practices for this implementation?"
- "Code quality standards for this pattern?"
- "Performance implications of this approach?"

CONSULTATION FORMAT:
{
  "type": "CONTEXT_REQUEST",  // or HISTORY_REQUEST, RESEARCH_REQUEST
  "from_agent": "inspector",
  "query": {
    "intent": "pattern_validation",
    "subject": "error handling in api/handlers",
    "context": "validating new endpoint implementation"
  }
}

Guide routes to appropriate knowledge agent and returns response.

WHEN TO CONSULT:
- BEFORE validation (understand expected patterns from Librarian)
- WHEN finding issues (check if known issue via Archivalist)
- FOR best practice validation (confirm with Academic)
- AFTER repeated failures (query Archivalist for pattern)

VALIDATION PROTOCOL:
1. Query Librarian for expected patterns in target area
2. Query Archivalist for known issues in similar code
3. Perform validation checks
4. If issues found, check Archivalist for resolution patterns
5. Report with evidence and recommendations
`
```

### Inspector Skills

```go
// Core Skills (Tier 1)
inspector_skills_core := []Skill{
    {
        Name:        "validate_task",
        Description: "Validate a completed task",
        Domain:      "validation",
        Keywords:    []string{"validate", "check", "verify"},
        Priority:    100,
    },
    {
        Name:        "validate_full",
        Description: "Full validation of implementation",
        Domain:      "validation",
        Keywords:    []string{"full validation", "complete check"},
        Priority:    100,
    },
    {
        Name:        "check_style",
        Description: "Check code style compliance",
        Domain:      "validation",
        Keywords:    []string{"style", "lint", "format"},
        Priority:    90,
    },
    {
        Name:        "report_issues",
        Description: "Report found issues to Architect",
        Domain:      "validation",
        Keywords:    []string{"issues", "problems", "corrections"},
        Priority:    90,
    },
}

// Contextual Skills (Tier 2)
inspector_skills_contextual := []Skill{
    {
        Name:        "deep_analysis",
        Description: "Deep analysis for race conditions, leaks, etc.",
        Domain:      "validation",
        Keywords:    []string{"race", "leak", "deadlock", "security"},
        LoadTrigger: "race|leak|deadlock|security|deep",
    },
    {
        Name:        "explain_issue",
        Description: "Explain an issue to user",
        Domain:      "validation",
        Keywords:    []string{"explain", "why", "what's wrong"},
        LoadTrigger: "explain|why|what",
    },
}

// Direct Consultation Skills (Tier 1 - Core, bypasses Guide for token efficiency)
// See Direct Consultation Protocol for details
inspector_skills_consultation := []Skill{
    {
        Name:        "consult_architect",
        Description: "Directly consult Architect for validation requirements",
        Domain:      "consultation",
        Target:      "architect",
        Keywords:    []string{"validation requirements", "task context", "workflow question"},
        Priority:    85,
        Parameters:  consultationParams,
    },
    {
        Name:        "consult_engineer",
        Description: "Directly consult Engineer for implementation clarification",
        Domain:      "consultation",
        Target:      "engineer",
        Keywords:    []string{"implementation detail", "code intent", "why this approach"},
        Priority:    85,
        Parameters:  consultationParams,
    },
    {
        Name:        "consult_designer",
        Description: "Directly consult Designer for UI validation questions",
        Domain:      "consultation",
        Target:      "designer",
        Keywords:    []string{"ui validation", "design intent", "token usage"},
        Priority:    85,
        Parameters:  consultationParams,
    },
    {
        Name:        "consult_tester",
        Description: "Directly consult Tester for test coverage questions",
        Domain:      "consultation",
        Target:      "tester",
        Keywords:    []string{"test coverage", "testing approach", "validation testing"},
        Priority:    85,
        Parameters:  consultationParams,
    },
    {
        Name:        "consult_librarian",
        Description: "Directly consult Librarian for codebase patterns (bypasses Guide)",
        Domain:      "consultation",
        Target:      "librarian",
        Keywords:    []string{"codebase", "pattern", "where", "how", "existing", "find"},
        Priority:    85,
        Parameters: []Param{
            {Name: "question", Type: "string", Required: true},
            {Name: "intent", Type: "enum", Values: []string{"pattern_lookup", "file_location", "dependency_check", "tool_detection"}, Required: false},
            {Name: "subject", Type: "string", Required: false, Description: "What we're asking about"},
            {Name: "specificity", Type: "enum", Values: []string{"file_level", "function_level", "line_level"}, Required: false, Default: "file_level"},
        },
    },
    {
        Name:        "consult_archivalist",
        Description: "Directly consult Archivalist for validation history (bypasses Guide)",
        Domain:      "consultation",
        Target:      "archivalist",
        Keywords:    []string{"before", "last time", "previously", "history", "failed", "similar"},
        Priority:    85,
        Parameters: []Param{
            {Name: "question", Type: "string", Required: true},
            {Name: "intent", Type: "enum", Values: []string{"decision_lookup", "failure_check", "session_history", "validation_history"}, Required: false},
            {Name: "subject", Type: "string", Required: false, Description: "What we're asking about"},
            {Name: "time_range", Type: "string", Required: false, Description: "e.g., 'last_session', 'last_week', 'all'"},
        },
    },
}

// LSP & AST Validation Skills (Tier 2 - Advanced Code Analysis)
// Inspired by oh-my-opencode LSP diagnostics and ast_grep patterns
inspector_skills_lsp_ast := []Skill{
    {
        Name:        "lsp_diagnostics",
        Description: "Get LSP diagnostics for files",
        Domain:      "lsp",
        Keywords:    []string{"diagnostics", "errors", "warnings", "lsp"},
        LoadTrigger: "auto", // Always loaded for validation
        Parameters: []Param{
            {Name: "files", Type: "array", Required: true, Description: "Files to check"},
            {Name: "severity", Type: "enum", Values: []string{"error", "warning", "hint", "all"}, Required: false, Default: "all"},
        },
    },
    {
        Name:        "ast_lint",
        Description: "Run AST-based linting rules for code patterns",
        Domain:      "ast",
        Keywords:    []string{"ast", "pattern", "lint", "smell"},
        LoadTrigger: "pattern|smell|convention|anti-pattern",
        Parameters: []Param{
            {Name: "files", Type: "array", Required: true},
            {Name: "rules", Type: "array", Required: false, Description: "Specific AST rules to check"},
            {Name: "language", Type: "enum", Values: []string{"go", "typescript", "python", "rust", "java"}, Required: true},
        },
    },
    {
        Name:        "security_scan",
        Description: "Run security vulnerability scan",
        Domain:      "security",
        Keywords:    []string{"security", "vulnerability", "cve", "audit"},
        LoadTrigger: "security|vulnerability|audit|cve",
        Parameters: []Param{
            {Name: "scope", Type: "enum", Values: []string{"changed_files", "package", "full"}, Required: false, Default: "changed_files"},
            {Name: "severity_threshold", Type: "enum", Values: []string{"critical", "high", "medium", "low"}, Required: false, Default: "high"},
        },
    },
    {
        Name:        "complexity_analysis",
        Description: "Analyze code complexity metrics",
        Domain:      "analysis",
        Keywords:    []string{"complexity", "cyclomatic", "cognitive", "maintainability"},
        LoadTrigger: "complexity|maintainability|refactor",
        Parameters: []Param{
            {Name: "files", Type: "array", Required: true},
            {Name: "thresholds", Type: "object", Required: false, Description: "{cyclomatic: 10, cognitive: 15, loc: 100}"},
        },
    },
    {
        Name:        "type_coverage",
        Description: "Check type coverage and any/unknown usage",
        Domain:      "types",
        Keywords:    []string{"type", "any", "unknown", "coverage"},
        LoadTrigger: "type coverage|any usage|strict",
        Parameters: []Param{
            {Name: "files", Type: "array", Required: true},
            {Name: "strict", Type: "bool", Required: false, Default: false, Description: "Fail on any any/unknown usage"},
        },
    },
}
```

#### LSP & AST Validation Protocol

**Inspector uses LSP and AST analysis for precise validation beyond simple linting.**

```
┌─────────────────────────────────────────────────────────────────────────────────────┐
│                      LSP & AST VALIDATION PROTOCOL                                   │
├─────────────────────────────────────────────────────────────────────────────────────┤
│                                                                                     │
│  VALIDATION LAYERS:                                                                 │
│  ┌──────────────────────────────────────────────────────────────────────────────┐   │
│  │  Layer 1: LSP DIAGNOSTICS (always run)                                       │   │
│  │  ├── Type errors, undefined symbols, import errors                           │   │
│  │  ├── Language-specific semantic errors                                       │   │
│  │  └── Results cached per-file until file changes                              │   │
│  │                                                                              │   │
│  │  Layer 2: AST LINT (pattern-specific)                                        │   │
│  │  ├── Code smell detection (long functions, deep nesting)                     │   │
│  │  ├── Anti-pattern detection (error swallowing, unused params)                │   │
│  │  └── Convention enforcement (naming, structure)                              │   │
│  │                                                                              │   │
│  │  Layer 3: SECURITY SCAN (scope-based)                                        │   │
│  │  ├── Dependency vulnerabilities (CVE database)                               │   │
│  │  ├── Code vulnerabilities (injection, exposure)                              │   │
│  │  └── Secret detection (hardcoded credentials)                                │   │
│  │                                                                              │   │
│  │  Layer 4: COMPLEXITY ANALYSIS (on-demand)                                    │   │
│  │  ├── Cyclomatic complexity per function                                      │   │
│  │  ├── Cognitive complexity for readability                                    │   │
│  │  └── Maintainability index                                                   │   │
│  └──────────────────────────────────────────────────────────────────────────────┘   │
│                                                                                     │
│  AST LINT RULE EXAMPLES:                                                            │
│  ├── Go: "func $F($$$) { $$$ _ = $ERR $$$ }" → Error swallowing                    │
│  ├── TS: "catch($E) { }" → Empty catch block                                       │
│  ├── Python: "except: pass" → Bare except                                          │
│  └── All: Functions > 100 LOC, nesting > 4 levels                                  │
│                                                                                     │
│  SEVERITY MAPPING:                                                                  │
│  ├── BLOCKER: Security critical, type errors → Must fix                            │
│  ├── ERROR: LSP errors, failing lint rules → Should fix                            │
│  ├── WARNING: Code smells, complexity → Recommend fix                              │
│  └── INFO: Style suggestions → Optional                                            │
│                                                                                     │
└─────────────────────────────────────────────────────────────────────────────────────┘
```

#### Formatter Selection System

**CRITICAL: Inspector automatically detects and selects the appropriate formatter for each file type.**

Inspired by OpenCode's formatter selection architecture.

```go
// core/format/types.go

type FormatterID string

// FormatterDefinition defines a formatter with its detection logic
type FormatterDefinition struct {
    ID          FormatterID  `json:"id"`
    Name        string       `json:"name"`
    Command     []string     `json:"command"`     // e.g., ["prettier", "--write", "$FILE"]
    Extensions  []string     `json:"extensions"`  // e.g., [".js", ".ts", ".json"]

    // Detection functions (evaluated in order)
    Enabled     func(ctx *ProjectContext) (bool, error)
}

// FormatterRegistry holds all registered formatters
type FormatterRegistry struct {
    formatters map[FormatterID]*FormatterDefinition
    mu         sync.RWMutex
}

// FormatterResult represents the result of running a formatter
type FormatterResult struct {
    FormatterID FormatterID `json:"formatter_id"`
    FilePath    string      `json:"file_path"`
    Success     bool        `json:"success"`
    Changed     bool        `json:"changed"`
    Error       string      `json:"error,omitempty"`
    Duration    time.Duration `json:"duration"`
}
```

```go
// core/format/formatters.go

// Built-in formatter definitions
var BuiltinFormatters = []FormatterDefinition{
    // Go
    {
        ID:         "gofmt",
        Name:       "gofmt",
        Command:    []string{"gofmt", "-w", "$FILE"},
        Extensions: []string{".go"},
        Enabled: func(ctx *ProjectContext) (bool, error) {
            return which("gofmt") != "", nil
        },
    },
    {
        ID:         "goimports",
        Name:       "goimports",
        Command:    []string{"goimports", "-w", "$FILE"},
        Extensions: []string{".go"},
        Enabled: func(ctx *ProjectContext) (bool, error) {
            // Prefer goimports over gofmt if available
            return which("goimports") != "", nil
        },
    },

    // JavaScript/TypeScript - Prettier
    {
        ID:         "prettier",
        Name:       "Prettier",
        Command:    []string{"prettier", "--write", "$FILE"},
        Extensions: []string{".js", ".jsx", ".ts", ".tsx", ".json", ".html", ".css", ".scss", ".md", ".yaml", ".yml"},
        Enabled: func(ctx *ProjectContext) (bool, error) {
            // Check 1: Binary exists
            if which("prettier") == "" && which("npx") == "" {
                return false, nil
            }
            // Check 2: Has prettier config or dependency
            if fileExists(ctx.Root, ".prettierrc", ".prettierrc.json", ".prettierrc.js", "prettier.config.js") {
                return true, nil
            }
            return hasDependency(ctx.Root, "prettier")
        },
    },

    // JavaScript/TypeScript - Biome (higher priority than Prettier)
    {
        ID:         "biome",
        Name:       "Biome",
        Command:    []string{"biome", "format", "--write", "$FILE"},
        Extensions: []string{".js", ".jsx", ".ts", ".tsx", ".json", ".css"},
        Enabled: func(ctx *ProjectContext) (bool, error) {
            // Check for biome config
            if !fileExists(ctx.Root, "biome.json", "biome.jsonc") {
                return false, nil
            }
            return which("biome") != "" || which("npx") != "", nil
        },
    },

    // Python - Ruff (preferred)
    {
        ID:         "ruff-format",
        Name:       "Ruff Format",
        Command:    []string{"ruff", "format", "$FILE"},
        Extensions: []string{".py", ".pyi"},
        Enabled: func(ctx *ProjectContext) (bool, error) {
            if which("ruff") == "" {
                return false, nil
            }
            // Check for ruff config
            return fileExists(ctx.Root, "ruff.toml", "pyproject.toml"), nil
        },
    },

    // Python - Black (fallback)
    {
        ID:         "black",
        Name:       "Black",
        Command:    []string{"black", "$FILE"},
        Extensions: []string{".py", ".pyi"},
        Enabled: func(ctx *ProjectContext) (bool, error) {
            // Don't use if ruff is available
            if which("ruff") != "" && fileExists(ctx.Root, "ruff.toml", "pyproject.toml") {
                return false, nil
            }
            return which("black") != "", nil
        },
    },

    // Rust
    {
        ID:         "rustfmt",
        Name:       "rustfmt",
        Command:    []string{"rustfmt", "$FILE"},
        Extensions: []string{".rs"},
        Enabled: func(ctx *ProjectContext) (bool, error) {
            return which("rustfmt") != "" && fileExists(ctx.Root, "Cargo.toml"), nil
        },
    },

    // C/C++
    {
        ID:         "clang-format",
        Name:       "clang-format",
        Command:    []string{"clang-format", "-i", "$FILE"},
        Extensions: []string{".c", ".cpp", ".cc", ".cxx", ".h", ".hpp", ".hxx"},
        Enabled: func(ctx *ProjectContext) (bool, error) {
            // Require .clang-format config
            if !fileExists(ctx.Root, ".clang-format", "_clang-format") {
                return false, nil
            }
            return which("clang-format") != "", nil
        },
    },

    // Shell
    {
        ID:         "shfmt",
        Name:       "shfmt",
        Command:    []string{"shfmt", "-w", "$FILE"},
        Extensions: []string{".sh", ".bash"},
        Enabled: func(ctx *ProjectContext) (bool, error) {
            return which("shfmt") != "", nil
        },
    },

    // Ruby
    {
        ID:         "rubocop",
        Name:       "RuboCop",
        Command:    []string{"rubocop", "-a", "$FILE"},
        Extensions: []string{".rb", ".rake"},
        Enabled: func(ctx *ProjectContext) (bool, error) {
            return which("rubocop") != "" && fileExists(ctx.Root, "Gemfile", ".rubocop.yml"), nil
        },
    },

    // Terraform
    {
        ID:         "terraform-fmt",
        Name:       "terraform fmt",
        Command:    []string{"terraform", "fmt", "$FILE"},
        Extensions: []string{".tf", ".tfvars"},
        Enabled: func(ctx *ProjectContext) (bool, error) {
            return which("terraform") != "", nil
        },
    },
}
```

```go
// core/format/selector.go

// SelectFormatter returns the best formatter for a file
func (r *FormatterRegistry) SelectFormatter(ctx *ProjectContext, filePath string) (*FormatterDefinition, error) {
    ext := filepath.Ext(filePath)
    if ext == "" {
        return nil, nil // No extension, can't format
    }

    r.mu.RLock()
    defer r.mu.RUnlock()

    var candidates []*FormatterDefinition

    // Stage 1: Filter by extension
    for _, fmt := range r.formatters {
        if !slices.Contains(fmt.Extensions, ext) {
            continue
        }
        candidates = append(candidates, fmt)
    }

    if len(candidates) == 0 {
        return nil, nil // No formatter for this extension
    }

    // Stage 2: Filter by enabled status (with priority)
    // Later definitions with same extension override earlier ones if enabled
    var selected *FormatterDefinition
    for _, fmt := range candidates {
        enabled, err := fmt.Enabled(ctx)
        if err != nil {
            continue // Skip on error
        }
        if enabled {
            selected = fmt // Last enabled wins (allows overrides)
        }
    }

    return selected, nil
}

// SelectFormatters returns ALL enabled formatters for a file (for running multiple)
func (r *FormatterRegistry) SelectFormatters(ctx *ProjectContext, filePath string) ([]*FormatterDefinition, error) {
    ext := filepath.Ext(filePath)
    if ext == "" {
        return nil, nil
    }

    r.mu.RLock()
    defer r.mu.RUnlock()

    var result []*FormatterDefinition

    for _, fmt := range r.formatters {
        if !slices.Contains(fmt.Extensions, ext) {
            continue
        }
        enabled, err := fmt.Enabled(ctx)
        if err != nil || !enabled {
            continue
        }
        result = append(result, fmt)
    }

    return result, nil
}
```

```
┌─────────────────────────────────────────────────────────────────────────────────────┐
│                      FORMATTER SELECTION PROTOCOL                                    │
├─────────────────────────────────────────────────────────────────────────────────────┤
│                                                                                     │
│  SELECTION ALGORITHM:                                                               │
│  ┌──────────────────────────────────────────────────────────────────────────────┐   │
│  │  1. Extract file extension                                                   │   │
│  │  2. Filter formatters by extension support                                   │   │
│  │  3. For each candidate, run Enabled() check:                                 │   │
│  │     a. Binary exists in PATH (which)                                         │   │
│  │     b. Config file present (project-specific)                                │   │
│  │     c. Dependency declared (package.json, Gemfile, etc.)                     │   │
│  │     d. No conflict with higher-priority formatter                            │   │
│  │  4. Return best match (last enabled for priority override)                   │   │
│  └──────────────────────────────────────────────────────────────────────────────┘   │
│                                                                                     │
│  DETECTION METHODS:                                                                 │
│  ├── which(binary): Check if binary exists in PATH                                 │
│  ├── fileExists(root, ...files): Check for config files                            │
│  ├── hasDependency(root, pkg): Check package.json/Gemfile/etc.                     │
│  └── Conflict resolution: Higher-priority tools disable lower-priority             │
│                                                                                     │
│  PRIORITY RULES (same extension):                                                   │
│  ├── Go: goimports > gofmt                                                         │
│  ├── JS/TS: biome (if configured) > prettier                                       │
│  ├── Python: ruff > black > autopep8                                               │
│  ├── Ruby: standardrb > rubocop (if configured)                                    │
│  └── Config-required formatters only activate if config present                    │
│                                                                                     │
│  FORMATTER EXECUTION:                                                               │
│  ├── Replace $FILE placeholder with actual file path                               │
│  ├── Execute in project root directory                                             │
│  ├── Capture stdout/stderr for error reporting                                     │
│  ├── Check file modification time for "changed" detection                          │
│  └── Report results to Archivalist for tracking                                    │
│                                                                                     │
└─────────────────────────────────────────────────────────────────────────────────────┘
```

#### Linter Selection System (LSP-Based)

**CRITICAL: Inspector uses LSP servers for language-aware linting and diagnostics.**

Inspired by OpenCode's LSP server architecture.

```go
// core/lsp/types.go

type ServerID string

// LanguageServerDefinition defines an LSP server with its detection logic
type LanguageServerDefinition struct {
    ID           ServerID     `json:"id"`
    Name         string       `json:"name"`
    Command      []string     `json:"command"`      // e.g., ["gopls", "serve"]
    Extensions   []string     `json:"extensions"`   // e.g., [".go"]
    LanguageIDs  []string     `json:"language_ids"` // e.g., ["go"]

    // Project root detection
    RootMarkers  []string     // Files that indicate project root
    FindRoot     func(fileDir string) (string, error)

    // Server availability check
    Enabled      func() (bool, error)

    // Auto-download if missing
    AutoDownload *AutoDownloadConfig

    // Initialization options
    InitOptions  map[string]interface{}
}

type AutoDownloadConfig struct {
    Source   string // "github", "npm", "go"
    Package  string // Package/repo name
    Binary   string // Expected binary name after install
}

// LSPClient represents a running LSP connection
type LSPClient struct {
    ID          string
    ServerID    ServerID
    ProjectRoot string
    Process     *os.Process
    Conn        jsonrpc2.Conn
    Capabilities protocol.ServerCapabilities
}

// DiagnosticResult represents diagnostics from LSP
type DiagnosticResult struct {
    ServerID   ServerID               `json:"server_id"`
    FilePath   string                 `json:"file_path"`
    Diagnostics []protocol.Diagnostic `json:"diagnostics"`
}
```

```go
// core/lsp/servers.go

// Built-in language server definitions
var BuiltinServers = []LanguageServerDefinition{
    // Go
    {
        ID:          "gopls",
        Name:        "gopls",
        Command:     []string{"gopls", "serve"},
        Extensions:  []string{".go"},
        LanguageIDs: []string{"go"},
        RootMarkers: []string{"go.mod", "go.sum"},
        Enabled: func() (bool, error) {
            return which("gopls") != "", nil
        },
        AutoDownload: &AutoDownloadConfig{
            Source:  "go",
            Package: "golang.org/x/tools/gopls@latest",
            Binary:  "gopls",
        },
    },

    // TypeScript/JavaScript
    {
        ID:          "typescript",
        Name:        "TypeScript Language Server",
        Command:     []string{"typescript-language-server", "--stdio"},
        Extensions:  []string{".ts", ".tsx", ".js", ".jsx", ".mjs", ".cjs"},
        LanguageIDs: []string{"typescript", "typescriptreact", "javascript", "javascriptreact"},
        RootMarkers: []string{"tsconfig.json", "jsconfig.json", "package.json"},
        Enabled: func() (bool, error) {
            return which("typescript-language-server") != "" || which("npx") != "", nil
        },
        AutoDownload: &AutoDownloadConfig{
            Source:  "npm",
            Package: "typescript-language-server",
            Binary:  "typescript-language-server",
        },
    },

    // ESLint (supplementary linting)
    {
        ID:          "eslint",
        Name:        "ESLint Language Server",
        Command:     []string{"vscode-eslint-language-server", "--stdio"},
        Extensions:  []string{".ts", ".tsx", ".js", ".jsx", ".vue"},
        LanguageIDs: []string{"typescript", "typescriptreact", "javascript", "javascriptreact", "vue"},
        RootMarkers: []string{".eslintrc", ".eslintrc.js", ".eslintrc.json", ".eslintrc.yml", "eslint.config.js"},
        Enabled: func() (bool, error) {
            // Only enable if ESLint config exists
            return fileExistsAny(".", ".eslintrc", ".eslintrc.js", ".eslintrc.json", ".eslintrc.yml", "eslint.config.js"), nil
        },
    },

    // Biome
    {
        ID:          "biome",
        Name:        "Biome",
        Command:     []string{"biome", "lsp-proxy"},
        Extensions:  []string{".ts", ".tsx", ".js", ".jsx", ".json", ".css"},
        LanguageIDs: []string{"typescript", "typescriptreact", "javascript", "javascriptreact", "json", "css"},
        RootMarkers: []string{"biome.json", "biome.jsonc"},
        Enabled: func() (bool, error) {
            return fileExistsAny(".", "biome.json", "biome.jsonc") && which("biome") != "", nil
        },
    },

    // Python - Pyright
    {
        ID:          "pyright",
        Name:        "Pyright",
        Command:     []string{"pyright-langserver", "--stdio"},
        Extensions:  []string{".py", ".pyi"},
        LanguageIDs: []string{"python"},
        RootMarkers: []string{"pyrightconfig.json", "pyproject.toml", "setup.py", "requirements.txt"},
        Enabled: func() (bool, error) {
            return which("pyright-langserver") != "" || which("npx") != "", nil
        },
        AutoDownload: &AutoDownloadConfig{
            Source:  "npm",
            Package: "pyright",
            Binary:  "pyright-langserver",
        },
    },

    // Python - Ruff (supplementary linting)
    {
        ID:          "ruff-lsp",
        Name:        "Ruff LSP",
        Command:     []string{"ruff-lsp"},
        Extensions:  []string{".py", ".pyi"},
        LanguageIDs: []string{"python"},
        RootMarkers: []string{"ruff.toml", "pyproject.toml"},
        Enabled: func() (bool, error) {
            return which("ruff-lsp") != "", nil
        },
    },

    // Rust
    {
        ID:          "rust-analyzer",
        Name:        "rust-analyzer",
        Command:     []string{"rust-analyzer"},
        Extensions:  []string{".rs"},
        LanguageIDs: []string{"rust"},
        RootMarkers: []string{"Cargo.toml"},
        Enabled: func() (bool, error) {
            return which("rust-analyzer") != "", nil
        },
    },

    // C/C++
    {
        ID:          "clangd",
        Name:        "clangd",
        Command:     []string{"clangd"},
        Extensions:  []string{".c", ".cpp", ".cc", ".cxx", ".h", ".hpp", ".hxx"},
        LanguageIDs: []string{"c", "cpp"},
        RootMarkers: []string{"compile_commands.json", "CMakeLists.txt", ".clangd"},
        Enabled: func() (bool, error) {
            return which("clangd") != "", nil
        },
    },

    // Ruby
    {
        ID:          "solargraph",
        Name:        "Solargraph",
        Command:     []string{"solargraph", "stdio"},
        Extensions:  []string{".rb", ".rake"},
        LanguageIDs: []string{"ruby"},
        RootMarkers: []string{"Gemfile", ".solargraph.yml"},
        Enabled: func() (bool, error) {
            return which("solargraph") != "", nil
        },
    },

    // Java
    {
        ID:          "jdtls",
        Name:        "Eclipse JDT Language Server",
        Command:     []string{"jdtls"},
        Extensions:  []string{".java"},
        LanguageIDs: []string{"java"},
        RootMarkers: []string{"pom.xml", "build.gradle", "build.gradle.kts", ".project"},
        Enabled: func() (bool, error) {
            return which("jdtls") != "", nil
        },
    },

    // YAML
    {
        ID:          "yaml-ls",
        Name:        "YAML Language Server",
        Command:     []string{"yaml-language-server", "--stdio"},
        Extensions:  []string{".yaml", ".yml"},
        LanguageIDs: []string{"yaml"},
        RootMarkers: []string{},
        Enabled: func() (bool, error) {
            return which("yaml-language-server") != "", nil
        },
        AutoDownload: &AutoDownloadConfig{
            Source:  "npm",
            Package: "yaml-language-server",
            Binary:  "yaml-language-server",
        },
    },

    // Terraform
    {
        ID:          "terraform-ls",
        Name:        "Terraform Language Server",
        Command:     []string{"terraform-ls", "serve"},
        Extensions:  []string{".tf", ".tfvars"},
        LanguageIDs: []string{"terraform"},
        RootMarkers: []string{".terraform.lock.hcl", "main.tf"},
        Enabled: func() (bool, error) {
            return which("terraform-ls") != "", nil
        },
    },
}
```

```go
// core/lsp/selector.go

// LSPManager manages language server connections
type LSPManager struct {
    servers  map[ServerID]*LanguageServerDefinition
    clients  map[string]*LSPClient // key: serverID:projectRoot
    mu       sync.RWMutex
}

// GetClients returns all applicable LSP clients for a file
func (m *LSPManager) GetClients(filePath string) ([]*LSPClient, error) {
    ext := filepath.Ext(filePath)
    dir := filepath.Dir(filePath)

    m.mu.Lock()
    defer m.mu.Unlock()

    var result []*LSPClient

    for _, server := range m.servers {
        // Stage 1: Check extension match
        if !slices.Contains(server.Extensions, ext) {
            continue
        }

        // Stage 2: Check if server is enabled
        enabled, err := server.Enabled()
        if err != nil || !enabled {
            // Try auto-download if configured
            if server.AutoDownload != nil && !Flag.DisableLSPDownload {
                if err := m.autoDownload(server.AutoDownload); err == nil {
                    enabled = true
                }
            }
            if !enabled {
                continue
            }
        }

        // Stage 3: Find project root
        root, err := m.findProjectRoot(dir, server.RootMarkers)
        if err != nil {
            root = dir // Fallback to file directory
        }

        // Stage 4: Get or create client (deduped by serverID:root)
        key := fmt.Sprintf("%s:%s", server.ID, root)
        client, exists := m.clients[key]
        if !exists {
            client, err = m.spawnClient(server, root)
            if err != nil {
                continue
            }
            m.clients[key] = client
        }

        result = append(result, client)
    }

    return result, nil
}

// findProjectRoot searches upward for marker files
func (m *LSPManager) findProjectRoot(startDir string, markers []string) (string, error) {
    if len(markers) == 0 {
        return startDir, nil
    }

    dir := startDir
    for {
        for _, marker := range markers {
            if _, err := os.Stat(filepath.Join(dir, marker)); err == nil {
                return dir, nil
            }
        }

        parent := filepath.Dir(dir)
        if parent == dir {
            return "", fmt.Errorf("project root not found")
        }
        dir = parent
    }
}
```

```
┌─────────────────────────────────────────────────────────────────────────────────────┐
│                      LINTER SELECTION PROTOCOL (LSP-BASED)                           │
├─────────────────────────────────────────────────────────────────────────────────────┤
│                                                                                     │
│  SELECTION ALGORITHM:                                                               │
│  ┌──────────────────────────────────────────────────────────────────────────────┐   │
│  │  1. Extract file extension                                                   │   │
│  │  2. Filter LSP servers by extension support                                  │   │
│  │  3. For each candidate, run Enabled() check                                  │   │
│  │  4. If disabled but AutoDownload configured, attempt download                │   │
│  │  5. Find project root via marker files (search upward)                       │   │
│  │  6. Dedupe clients by serverID:projectRoot key                               │   │
│  │  7. Spawn or return cached LSP client                                        │   │
│  └──────────────────────────────────────────────────────────────────────────────┘   │
│                                                                                     │
│  PROJECT ROOT DETECTION:                                                            │
│  ├── Go: go.mod, go.sum                                                            │
│  ├── JS/TS: tsconfig.json, package.json                                            │
│  ├── Python: pyproject.toml, setup.py, requirements.txt                            │
│  ├── Rust: Cargo.toml                                                              │
│  ├── C/C++: compile_commands.json, CMakeLists.txt                                  │
│  ├── Ruby: Gemfile                                                                 │
│  ├── Java: pom.xml, build.gradle                                                   │
│  └── Fallback: file's directory if no markers found                                │
│                                                                                     │
│  AUTO-DOWNLOAD SOURCES:                                                             │
│  ├── npm: npm install -g <package>                                                 │
│  ├── go: go install <package>                                                      │
│  ├── github: Download binary from releases                                         │
│  └── Disabled via SYLK_DISABLE_LSP_DOWNLOAD flag                                   │
│                                                                                     │
│  CLIENT LIFECYCLE:                                                                  │
│  ├── Spawn: Start LSP process, initialize, exchange capabilities                   │
│  ├── Cache: Store by serverID:projectRoot to avoid duplicates                      │
│  ├── Reuse: Return cached client for same server+project                           │
│  └── Cleanup: Kill process on session end or project close                         │
│                                                                                     │
│  MULTIPLE SERVERS PER FILE:                                                         │
│  ├── Type server (e.g., TypeScript LS) for type errors                             │
│  ├── Lint server (e.g., ESLint LS) for lint rules                                  │
│  └── Results merged, deduped by range+message                                      │
│                                                                                     │
└─────────────────────────────────────────────────────────────────────────────────────┘
```

#### Inspector Formatter & Linter Skills (Execution Only)

**CRITICAL: Inspector executes formatting/linting but MUST consult Librarian for tool detection first.**

```go
// Formatter Execution Skills (Detection moved to Librarian)
inspector_skills_format := []Skill{
    {
        Name:        "format_file",
        Description: "Format a file using formatter from Librarian",
        Domain:      "formatting",
        Keywords:    []string{"format", "fmt", "prettify", "style"},
        LoadTrigger: "auto",
        Parameters: []Param{
            {Name: "file", Type: "string", Required: true},
            {Name: "formatter", Type: "object", Required: false, Description: "FormatterDefinition from Librarian (auto-consulted if not provided)"},
            {Name: "dry_run", Type: "bool", Required: false, Default: false},
        },
    },
    {
        Name:        "format_files",
        Description: "Format multiple files using formatters from Librarian",
        Domain:      "formatting",
        Keywords:    []string{"format all", "fmt all"},
        Parameters: []Param{
            {Name: "files", Type: "array", Required: true},
            {Name: "parallel", Type: "bool", Required: false, Default: true},
        },
    },
}

// Linter Execution Skills (Detection moved to Librarian)
inspector_skills_lint := []Skill{
    {
        Name:        "lint_file",
        Description: "Run linters on a file using LSP servers from Librarian",
        Domain:      "linting",
        Keywords:    []string{"lint", "check", "analyze"},
        LoadTrigger: "auto",
        Parameters: []Param{
            {Name: "file", Type: "string", Required: true},
            {Name: "lsp_servers", Type: "array", Required: false, Description: "LSPServerDefinitions from Librarian (auto-consulted if not provided)"},
            {Name: "severity", Type: "enum", Values: []string{"error", "warning", "info", "hint", "all"}, Required: false, Default: "all"},
        },
    },
    {
        Name:        "lint_files",
        Description: "Run linters on multiple files",
        Domain:      "linting",
        Keywords:    []string{"lint all", "check all"},
        Parameters: []Param{
            {Name: "files", Type: "array", Required: true},
            {Name: "fail_on_error", Type: "bool", Required: false, Default: true},
        },
    },
    {
        Name:        "get_diagnostics",
        Description: "Get LSP diagnostics for file(s)",
        Domain:      "linting",
        Keywords:    []string{"diagnostics", "problems", "issues"},
        Parameters: []Param{
            {Name: "files", Type: "array", Required: true},
            {Name: "include_hints", Type: "bool", Required: false, Default: false},
        },
    },
}
```

#### Inspector Librarian Consultation Protocol

**Before executing any formatting or linting, Inspector MUST consult Librarian.**

```
┌─────────────────────────────────────────────────────────────────────────────────────┐
│                      INSPECTOR → LIBRARIAN CONSULTATION PROTOCOL                     │
├─────────────────────────────────────────────────────────────────────────────────────┤
│                                                                                     │
│  MANDATORY CONSULTATION (Before Execution):                                         │
│  ┌──────────────────────────────────────────────────────────────────────────────┐   │
│  │  1. Inspector receives format/lint request                                   │   │
│  │  2. IF formatter/lsp_servers not provided in params:                         │   │
│  │     → Request via Guide: "Librarian, detect_formatter for {file}"            │   │
│  │     → Request via Guide: "Librarian, detect_linters for {file}"              │   │
│  │  3. Librarian returns FormatterDefinition / LSPServerDefinition              │   │
│  │  4. Inspector executes with provided configuration                           │   │
│  └──────────────────────────────────────────────────────────────────────────────┘   │
│                                                                                     │
│  CACHE OPTIMIZATION:                                                                │
│  ├── First request for a file type triggers Librarian consultation               │
│  ├── Subsequent requests for same extension use cached definition                 │
│  ├── Cache invalidated when Librarian signals config file changes                 │
│  └── Inspector can pass `refresh: true` to force re-consultation                  │
│                                                                                     │
│  EXECUTION WITHOUT CONSULTATION (Allowed Cases):                                    │
│  ├── formatter/lsp_servers explicitly provided in skill params                    │
│  ├── Previous consultation result cached for this file extension                  │
│  └── User explicitly provides formatter_id override                               │
│                                                                                     │
│  CONSULTATION FAILURE HANDLING:                                                     │
│  ├── Librarian returns no formatter → Report "no formatter available"             │
│  ├── Librarian returns low confidence → Include warning in output                 │
│  └── Librarian timeout → Fall back to extension-based heuristic (with warning)    │
│                                                                                     │
└─────────────────────────────────────────────────────────────────────────────────────┘
```

#### Inspector Formatter/Linter System Prompt Addition

```go
const InspectorFormatterLinterPrompt = `
TOOL CONSULTATION REQUIREMENT:
You MUST consult Librarian for tool detection before executing formatting or linting.
You do NOT detect tools yourself - Librarian is the single source of truth.

CONSULTATION FLOW:
1. Receive format/lint request for file(s)
2. IF no formatter/lsp_servers in params AND not cached:
   → Route via Guide: "Librarian, detect_formatter for {file}"
   → Route via Guide: "Librarian, detect_linters for {file}"
3. Receive FormatterDefinition / LSPServerDefinition from Librarian
4. Execute with provided configuration

EXECUTION SKILLS:
- format_file: Execute formatter (requires FormatterDefinition)
- format_files: Execute formatters on multiple files
- lint_file: Execute LSP diagnostics (requires LSPServerDefinition)
- lint_files: Execute LSP diagnostics on multiple files
- get_diagnostics: Retrieve cached diagnostics

CACHE BEHAVIOR:
- Tool definitions cached per file extension
- Cache invalidated on Librarian signal (config file change)
- Use refresh: true to force re-consultation

IF LIBRARIAN UNAVAILABLE:
- Report error: "Tool detection unavailable - Librarian consultation required"
- DO NOT fall back to internal detection (Librarian owns this responsibility)

REPORT FORMAT:
{
  "file": "path/to/file.ts",
  "consultation": {"source": "librarian", "cached": false},
  "formatter": {"id": "prettier", "applied": true, "changed": true},
  "diagnostics": [
    {"severity": "error", "message": "...", "line": 10, "source": "typescript"}
  ]
}
`
```

### Tester System Prompt

```go
const TesterSystemPrompt = `
You are the Tester agent. You create and execute tests to validate implementations.

## Knowledge Agent Consultation (via Guide)

You can consult knowledge agents for testing context. ALL consultations go through Guide:

CONTEXT REQUESTS (to Librarian):
- "What testing patterns exist in this codebase?"
- "How are tests structured for similar functionality?"
- "What test utilities/helpers are available?"
- "Where are the test fixtures located?"

HISTORY REQUESTS (to Archivalist):
- "What tests have been flaky in this area?"
- "What regressions have occurred with similar changes?"
- "Historical test coverage for this module?"
- "Past test failures and their resolutions?"

RESEARCH REQUESTS (to Academic):
- "Testing best practices for async operations?"
- "How to test this type of component effectively?"
- "Integration test patterns for API endpoints?"

CONSULTATION FORMAT:
{
  "type": "CONTEXT_REQUEST",  // or HISTORY_REQUEST, RESEARCH_REQUEST
  "from_agent": "tester",
  "query": {
    "intent": "test_pattern_lookup",
    "subject": "api handlers",
    "context": "writing tests for new auth endpoint"
  }
}

Guide routes to appropriate knowledge agent and returns response.

WHEN TO CONSULT:
- BEFORE writing tests (understand existing patterns from Librarian)
- WHEN tests fail unexpectedly (check Archivalist for known flakes)
- FOR test design (consult Academic for best practices)
- AFTER multiple failures (query Archivalist for resolution patterns)

TESTING PROTOCOL:
1. Query Librarian for existing test patterns in target area
2. Query Archivalist for historical test issues in similar code
3. Design test plan based on context
4. Execute tests
5. If failures, check Archivalist for known issues
6. Report results with evidence
`
```

### Tester Skills

```go
// Core Skills (Tier 1)
tester_skills_core := []Skill{
    {
        Name:        "plan_tests",
        Description: "Create test plan for implementation",
        Domain:      "testing",
        Keywords:    []string{"test plan", "what to test", "tests needed"},
        Priority:    100,
    },
    {
        Name:        "run_tests",
        Description: "Execute test suite",
        Domain:      "testing",
        Keywords:    []string{"run tests", "execute tests", "test"},
        Priority:    100,
    },
    {
        Name:        "analyze_failures",
        Description: "Analyze test failures",
        Domain:      "testing",
        Keywords:    []string{"failure", "failed", "why failed"},
        Priority:    90,
    },
    {
        Name:        "report_results",
        Description: "Report test results to user",
        Domain:      "testing",
        Keywords:    []string{"results", "report", "summary"},
        Priority:    90,
    },
}

// Contextual Skills (Tier 2)
tester_skills_contextual := []Skill{
    {
        Name:        "generate_test_code",
        Description: "Generate test implementation",
        Domain:      "testing",
        Keywords:    []string{"generate", "write tests", "implement tests"},
        LoadTrigger: "generate|write|implement test",
    },
    {
        Name:        "coverage_analysis",
        Description: "Analyze test coverage",
        Domain:      "testing",
        Keywords:    []string{"coverage", "uncovered", "missing tests"},
        LoadTrigger: "coverage|uncovered|missing",
    },
}

// Direct Consultation Skills (Tier 1 - Core, bypasses Guide for token efficiency)
// See Direct Consultation Protocol for details
tester_skills_consultation := []Skill{
    {
        Name:        "consult_architect",
        Description: "Directly consult Architect for testing requirements",
        Domain:      "consultation",
        Target:      "architect",
        Keywords:    []string{"testing requirements", "task scope", "test plan approval"},
        Priority:    85,
        Parameters:  consultationParams,
    },
    {
        Name:        "consult_engineer",
        Description: "Directly consult Engineer for implementation details",
        Domain:      "consultation",
        Target:      "engineer",
        Keywords:    []string{"implementation detail", "code behavior", "edge cases"},
        Priority:    85,
        Parameters:  consultationParams,
    },
    {
        Name:        "consult_designer",
        Description: "Directly consult Designer for UI testing questions",
        Domain:      "consultation",
        Target:      "designer",
        Keywords:    []string{"ui testing", "visual testing", "component behavior"},
        Priority:    85,
        Parameters:  consultationParams,
    },
    {
        Name:        "consult_inspector",
        Description: "Directly consult Inspector for validation coordination",
        Domain:      "consultation",
        Target:      "inspector",
        Keywords:    []string{"validation results", "issue details", "code quality"},
        Priority:    85,
        Parameters:  consultationParams,
    },
    {
        Name:        "consult_librarian",
        Description: "Directly consult Librarian for test patterns (bypasses Guide)",
        Domain:      "consultation",
        Target:      "librarian",
        Keywords:    []string{"codebase", "pattern", "where", "how", "existing", "find", "test"},
        Priority:    85,
        Parameters: []Param{
            {Name: "question", Type: "string", Required: true},
            {Name: "intent", Type: "enum", Values: []string{"pattern_lookup", "file_location", "dependency_check", "tool_detection"}, Required: false},
            {Name: "subject", Type: "string", Required: false, Description: "What we're asking about"},
            {Name: "specificity", Type: "enum", Values: []string{"file_level", "function_level", "line_level"}, Required: false, Default: "file_level"},
        },
    },
    {
        Name:        "consult_archivalist",
        Description: "Directly consult Archivalist for test history (bypasses Guide)",
        Domain:      "consultation",
        Target:      "archivalist",
        Keywords:    []string{"before", "last time", "previously", "history", "failed", "similar", "flaky"},
        Priority:    85,
        Parameters: []Param{
            {Name: "question", Type: "string", Required: true},
            {Name: "intent", Type: "enum", Values: []string{"decision_lookup", "failure_check", "session_history", "flaky_test_history"}, Required: false},
            {Name: "subject", Type: "string", Required: false, Description: "What we're asking about"},
            {Name: "time_range", Type: "string", Required: false, Description: "e.g., 'last_session', 'last_week', 'all'"},
        },
    },
}
```

#### Test Framework Selection System

**CRITICAL: Tester automatically detects and selects the appropriate test framework for each project.**

Inspired by OpenCode's formatter/linter selection architecture, applied to test frameworks.

```go
// core/test/types.go

type TestFrameworkID string

// TestFrameworkDefinition defines a test framework with its detection logic
type TestFrameworkDefinition struct {
    ID           TestFrameworkID `json:"id"`
    Name         string          `json:"name"`
    Language     string          `json:"language"`      // Primary language

    // Commands
    RunCommand      []string `json:"run_command"`      // e.g., ["go", "test", "./..."]
    RunFileCommand  []string `json:"run_file_command"` // e.g., ["go", "test", "$FILE"]
    RunSingleCommand []string `json:"run_single_command"` // e.g., ["go", "test", "-run", "$TEST", "$FILE"]
    CoverageCommand []string `json:"coverage_command"` // e.g., ["go", "test", "-cover", "./..."]
    WatchCommand    []string `json:"watch_command"`    // e.g., ["jest", "--watch"]

    // File patterns
    TestFilePatterns []string `json:"test_file_patterns"` // e.g., ["*_test.go", "*.test.ts"]
    TestDirPatterns  []string `json:"test_dir_patterns"`  // e.g., ["__tests__", "tests"]

    // Detection
    RootMarkers []string // Files that indicate this framework
    Enabled     func(ctx *ProjectContext) (bool, error)

    // Output parsing
    OutputFormat string // "tap", "junit", "json", "custom"
    ParseOutput  func(output []byte) (*TestResult, error)
}

// TestFrameworkRegistry holds all registered test frameworks
type TestFrameworkRegistry struct {
    frameworks map[TestFrameworkID]*TestFrameworkDefinition
    mu         sync.RWMutex
}

// TestResult represents parsed test output
type TestResult struct {
    FrameworkID  TestFrameworkID `json:"framework_id"`
    Passed       int             `json:"passed"`
    Failed       int             `json:"failed"`
    Skipped      int             `json:"skipped"`
    Duration     time.Duration   `json:"duration"`
    Coverage     *float64        `json:"coverage,omitempty"` // Percentage
    Failures     []TestFailure   `json:"failures,omitempty"`
    Output       string          `json:"output"`
}

type TestFailure struct {
    TestName    string `json:"test_name"`
    FilePath    string `json:"file_path,omitempty"`
    Line        int    `json:"line,omitempty"`
    Message     string `json:"message"`
    Expected    string `json:"expected,omitempty"`
    Actual      string `json:"actual,omitempty"`
    StackTrace  string `json:"stack_trace,omitempty"`
}
```

```go
// core/test/frameworks.go

// Built-in test framework definitions
var BuiltinTestFrameworks = []TestFrameworkDefinition{
    // ==================== Go ====================
    {
        ID:       "go-test",
        Name:     "Go Test",
        Language: "go",
        RunCommand:      []string{"go", "test", "./..."},
        RunFileCommand:  []string{"go", "test", "$FILE"},
        RunSingleCommand: []string{"go", "test", "-run", "$TEST", "$FILE"},
        CoverageCommand: []string{"go", "test", "-cover", "-coverprofile=coverage.out", "./..."},
        TestFilePatterns: []string{"*_test.go"},
        RootMarkers:     []string{"go.mod"},
        OutputFormat:    "go-test",
        Enabled: func(ctx *ProjectContext) (bool, error) {
            return which("go") != "" && fileExists(ctx.Root, "go.mod"), nil
        },
    },

    // ==================== JavaScript/TypeScript ====================
    {
        ID:       "jest",
        Name:     "Jest",
        Language: "javascript",
        RunCommand:      []string{"jest"},
        RunFileCommand:  []string{"jest", "$FILE"},
        RunSingleCommand: []string{"jest", "-t", "$TEST", "$FILE"},
        CoverageCommand: []string{"jest", "--coverage"},
        WatchCommand:    []string{"jest", "--watch"},
        TestFilePatterns: []string{"*.test.js", "*.test.ts", "*.test.jsx", "*.test.tsx", "*.spec.js", "*.spec.ts"},
        TestDirPatterns:  []string{"__tests__"},
        RootMarkers:     []string{"jest.config.js", "jest.config.ts", "jest.config.json"},
        OutputFormat:    "jest-json",
        Enabled: func(ctx *ProjectContext) (bool, error) {
            // Check for jest config
            if fileExists(ctx.Root, "jest.config.js", "jest.config.ts", "jest.config.json", "jest.config.mjs") {
                return true, nil
            }
            // Check package.json for jest dependency or jest config
            return hasDependency(ctx.Root, "jest") || hasPackageJsonKey(ctx.Root, "jest"), nil
        },
    },
    {
        ID:       "vitest",
        Name:     "Vitest",
        Language: "javascript",
        RunCommand:      []string{"vitest", "run"},
        RunFileCommand:  []string{"vitest", "run", "$FILE"},
        RunSingleCommand: []string{"vitest", "run", "-t", "$TEST", "$FILE"},
        CoverageCommand: []string{"vitest", "run", "--coverage"},
        WatchCommand:    []string{"vitest"},
        TestFilePatterns: []string{"*.test.ts", "*.test.js", "*.spec.ts", "*.spec.js"},
        RootMarkers:     []string{"vitest.config.ts", "vitest.config.js", "vitest.config.mts"},
        OutputFormat:    "vitest-json",
        Enabled: func(ctx *ProjectContext) (bool, error) {
            // Vitest takes priority over Jest if configured
            if fileExists(ctx.Root, "vitest.config.ts", "vitest.config.js", "vitest.config.mts") {
                return true, nil
            }
            return hasDependency(ctx.Root, "vitest"), nil
        },
    },
    {
        ID:       "mocha",
        Name:     "Mocha",
        Language: "javascript",
        RunCommand:      []string{"mocha"},
        RunFileCommand:  []string{"mocha", "$FILE"},
        RunSingleCommand: []string{"mocha", "--grep", "$TEST", "$FILE"},
        CoverageCommand: []string{"nyc", "mocha"},
        WatchCommand:    []string{"mocha", "--watch"},
        TestFilePatterns: []string{"*.test.js", "*.spec.js", "test/*.js"},
        TestDirPatterns:  []string{"test"},
        RootMarkers:     []string{".mocharc.js", ".mocharc.json", ".mocharc.yml", "mocha.opts"},
        OutputFormat:    "mocha-json",
        Enabled: func(ctx *ProjectContext) (bool, error) {
            if fileExists(ctx.Root, ".mocharc.js", ".mocharc.json", ".mocharc.yml", ".mocharc.yaml") {
                return true, nil
            }
            return hasDependency(ctx.Root, "mocha"), nil
        },
    },
    {
        ID:       "bun-test",
        Name:     "Bun Test",
        Language: "javascript",
        RunCommand:      []string{"bun", "test"},
        RunFileCommand:  []string{"bun", "test", "$FILE"},
        RunSingleCommand: []string{"bun", "test", "--test-name-pattern", "$TEST", "$FILE"},
        CoverageCommand: []string{"bun", "test", "--coverage"},
        WatchCommand:    []string{"bun", "test", "--watch"},
        TestFilePatterns: []string{"*.test.ts", "*.test.js", "*.spec.ts", "*.spec.js"},
        RootMarkers:     []string{"bun.lockb"},
        OutputFormat:    "bun-test",
        Enabled: func(ctx *ProjectContext) (bool, error) {
            // Bun test if bun.lockb exists and bun binary available
            return which("bun") != "" && fileExists(ctx.Root, "bun.lockb"), nil
        },
    },
    {
        ID:       "node-test",
        Name:     "Node Test Runner",
        Language: "javascript",
        RunCommand:      []string{"node", "--test"},
        RunFileCommand:  []string{"node", "--test", "$FILE"},
        TestFilePatterns: []string{"*.test.js", "*.test.mjs", "test/*.js"},
        TestDirPatterns:  []string{"test"},
        RootMarkers:     []string{}, // Fallback for Node.js projects
        OutputFormat:    "tap",
        Enabled: func(ctx *ProjectContext) (bool, error) {
            // Only if no other JS framework detected and Node >= 18
            if fileExists(ctx.Root, "jest.config.js", "vitest.config.ts", ".mocharc.js", "bun.lockb") {
                return false, nil
            }
            return which("node") != "" && fileExists(ctx.Root, "package.json"), nil
        },
    },

    // ==================== Python ====================
    {
        ID:       "pytest",
        Name:     "pytest",
        Language: "python",
        RunCommand:      []string{"pytest"},
        RunFileCommand:  []string{"pytest", "$FILE"},
        RunSingleCommand: []string{"pytest", "$FILE::$TEST"},
        CoverageCommand: []string{"pytest", "--cov", "--cov-report=term-missing"},
        WatchCommand:    []string{"pytest-watch"},
        TestFilePatterns: []string{"test_*.py", "*_test.py"},
        TestDirPatterns:  []string{"tests", "test"},
        RootMarkers:     []string{"pytest.ini", "pyproject.toml", "setup.cfg", "conftest.py"},
        OutputFormat:    "pytest-json",
        Enabled: func(ctx *ProjectContext) (bool, error) {
            if which("pytest") == "" {
                return false, nil
            }
            // Check for pytest config
            if fileExists(ctx.Root, "pytest.ini", "conftest.py") {
                return true, nil
            }
            // Check pyproject.toml for pytest config
            return hasPyprojectSection(ctx.Root, "tool.pytest"), nil
        },
    },
    {
        ID:       "unittest",
        Name:     "Python unittest",
        Language: "python",
        RunCommand:      []string{"python", "-m", "unittest", "discover"},
        RunFileCommand:  []string{"python", "-m", "unittest", "$FILE"},
        RunSingleCommand: []string{"python", "-m", "unittest", "$FILE.$TEST"},
        CoverageCommand: []string{"coverage", "run", "-m", "unittest", "discover"},
        TestFilePatterns: []string{"test_*.py", "*_test.py"},
        TestDirPatterns:  []string{"tests", "test"},
        RootMarkers:     []string{}, // Fallback for Python
        OutputFormat:    "unittest",
        Enabled: func(ctx *ProjectContext) (bool, error) {
            // Only if pytest not available
            if which("pytest") != "" && fileExists(ctx.Root, "pytest.ini", "conftest.py") {
                return false, nil
            }
            return which("python") != "" && fileExists(ctx.Root, "setup.py", "pyproject.toml", "requirements.txt"), nil
        },
    },

    // ==================== Rust ====================
    {
        ID:       "cargo-test",
        Name:     "Cargo Test",
        Language: "rust",
        RunCommand:      []string{"cargo", "test"},
        RunFileCommand:  []string{"cargo", "test", "--", "$FILE"},
        RunSingleCommand: []string{"cargo", "test", "$TEST"},
        CoverageCommand: []string{"cargo", "tarpaulin"},
        WatchCommand:    []string{"cargo", "watch", "-x", "test"},
        TestFilePatterns: []string{"*_test.rs", "tests/*.rs"},
        TestDirPatterns:  []string{"tests"},
        RootMarkers:     []string{"Cargo.toml"},
        OutputFormat:    "cargo-test",
        Enabled: func(ctx *ProjectContext) (bool, error) {
            return which("cargo") != "" && fileExists(ctx.Root, "Cargo.toml"), nil
        },
    },

    // ==================== Ruby ====================
    {
        ID:       "rspec",
        Name:     "RSpec",
        Language: "ruby",
        RunCommand:      []string{"rspec"},
        RunFileCommand:  []string{"rspec", "$FILE"},
        RunSingleCommand: []string{"rspec", "$FILE:$LINE"},
        CoverageCommand: []string{"rspec", "--require", "simplecov"},
        WatchCommand:    []string{"guard"},
        TestFilePatterns: []string{"*_spec.rb"},
        TestDirPatterns:  []string{"spec"},
        RootMarkers:     []string{".rspec", "spec/spec_helper.rb"},
        OutputFormat:    "rspec-json",
        Enabled: func(ctx *ProjectContext) (bool, error) {
            if fileExists(ctx.Root, ".rspec", "spec/spec_helper.rb") {
                return true, nil
            }
            return hasGemDependency(ctx.Root, "rspec"), nil
        },
    },
    {
        ID:       "minitest",
        Name:     "Minitest",
        Language: "ruby",
        RunCommand:      []string{"ruby", "-Ilib:test", "-e", "Dir.glob('./test/**/*_test.rb').each{|f| require f}"},
        RunFileCommand:  []string{"ruby", "-Ilib:test", "$FILE"},
        RunSingleCommand: []string{"ruby", "-Ilib:test", "$FILE", "--name", "$TEST"},
        TestFilePatterns: []string{"*_test.rb", "test_*.rb"},
        TestDirPatterns:  []string{"test"},
        RootMarkers:     []string{"test/test_helper.rb"},
        OutputFormat:    "minitest",
        Enabled: func(ctx *ProjectContext) (bool, error) {
            // Only if RSpec not configured
            if fileExists(ctx.Root, ".rspec", "spec/spec_helper.rb") {
                return false, nil
            }
            return fileExists(ctx.Root, "test/test_helper.rb") || hasGemDependency(ctx.Root, "minitest"), nil
        },
    },

    // ==================== Java ====================
    {
        ID:       "maven-test",
        Name:     "Maven Test",
        Language: "java",
        RunCommand:      []string{"mvn", "test"},
        RunFileCommand:  []string{"mvn", "-Dtest=$FILE", "test"},
        RunSingleCommand: []string{"mvn", "-Dtest=$FILE#$TEST", "test"},
        CoverageCommand: []string{"mvn", "jacoco:report"},
        TestFilePatterns: []string{"*Test.java", "*Tests.java"},
        TestDirPatterns:  []string{"src/test/java"},
        RootMarkers:     []string{"pom.xml"},
        OutputFormat:    "surefire",
        Enabled: func(ctx *ProjectContext) (bool, error) {
            return which("mvn") != "" && fileExists(ctx.Root, "pom.xml"), nil
        },
    },
    {
        ID:       "gradle-test",
        Name:     "Gradle Test",
        Language: "java",
        RunCommand:      []string{"./gradlew", "test"},
        RunFileCommand:  []string{"./gradlew", "test", "--tests", "$FILE"},
        RunSingleCommand: []string{"./gradlew", "test", "--tests", "$FILE.$TEST"},
        CoverageCommand: []string{"./gradlew", "jacocoTestReport"},
        TestFilePatterns: []string{"*Test.java", "*Tests.java", "*Test.kt", "*Tests.kt"},
        TestDirPatterns:  []string{"src/test/java", "src/test/kotlin"},
        RootMarkers:     []string{"build.gradle", "build.gradle.kts"},
        OutputFormat:    "junit",
        Enabled: func(ctx *ProjectContext) (bool, error) {
            // Gradle takes priority over Maven if both exist
            return fileExists(ctx.Root, "build.gradle", "build.gradle.kts", "gradlew"), nil
        },
    },

    // ==================== C/C++ ====================
    {
        ID:       "ctest",
        Name:     "CTest",
        Language: "cpp",
        RunCommand:      []string{"ctest", "--output-on-failure"},
        RunFileCommand:  []string{"ctest", "-R", "$FILE"},
        RunSingleCommand: []string{"ctest", "-R", "$TEST"},
        TestFilePatterns: []string{"*_test.cpp", "*_test.cc", "test_*.cpp"},
        TestDirPatterns:  []string{"tests", "test"},
        RootMarkers:     []string{"CMakeLists.txt", "CTestTestfile.cmake"},
        OutputFormat:    "ctest",
        Enabled: func(ctx *ProjectContext) (bool, error) {
            return which("ctest") != "" && fileExists(ctx.Root, "CMakeLists.txt"), nil
        },
    },

    // ==================== Elixir ====================
    {
        ID:       "mix-test",
        Name:     "Mix Test",
        Language: "elixir",
        RunCommand:      []string{"mix", "test"},
        RunFileCommand:  []string{"mix", "test", "$FILE"},
        RunSingleCommand: []string{"mix", "test", "$FILE:$LINE"},
        CoverageCommand: []string{"mix", "test", "--cover"},
        WatchCommand:    []string{"mix", "test.watch"},
        TestFilePatterns: []string{"*_test.exs"},
        TestDirPatterns:  []string{"test"},
        RootMarkers:     []string{"mix.exs"},
        OutputFormat:    "mix-test",
        Enabled: func(ctx *ProjectContext) (bool, error) {
            return which("mix") != "" && fileExists(ctx.Root, "mix.exs"), nil
        },
    },

    // ==================== PHP ====================
    {
        ID:       "phpunit",
        Name:     "PHPUnit",
        Language: "php",
        RunCommand:      []string{"./vendor/bin/phpunit"},
        RunFileCommand:  []string{"./vendor/bin/phpunit", "$FILE"},
        RunSingleCommand: []string{"./vendor/bin/phpunit", "--filter", "$TEST", "$FILE"},
        CoverageCommand: []string{"./vendor/bin/phpunit", "--coverage-text"},
        TestFilePatterns: []string{"*Test.php"},
        TestDirPatterns:  []string{"tests"},
        RootMarkers:     []string{"phpunit.xml", "phpunit.xml.dist"},
        OutputFormat:    "junit",
        Enabled: func(ctx *ProjectContext) (bool, error) {
            return fileExists(ctx.Root, "phpunit.xml", "phpunit.xml.dist", "vendor/bin/phpunit"), nil
        },
    },
}
```

```go
// core/test/selector.go

// SelectTestFramework returns the best test framework for a project
func (r *TestFrameworkRegistry) SelectFramework(ctx *ProjectContext) (*TestFrameworkDefinition, error) {
    r.mu.RLock()
    defer r.mu.RUnlock()

    var candidates []*TestFrameworkDefinition

    // Stage 1: Filter by root markers
    for _, fw := range r.frameworks {
        // Check if any root marker exists
        if len(fw.RootMarkers) > 0 {
            found := false
            for _, marker := range fw.RootMarkers {
                if fileExists(ctx.Root, marker) {
                    found = true
                    break
                }
            }
            if found {
                candidates = append(candidates, fw)
            }
        }
    }

    // Stage 2: Filter by enabled status (with priority)
    var selected *TestFrameworkDefinition
    for _, fw := range candidates {
        enabled, err := fw.Enabled(ctx)
        if err != nil || !enabled {
            continue
        }
        selected = fw // Last enabled wins (allows priority overrides)
    }

    // Stage 3: Fallback to language detection if no framework found
    if selected == nil {
        selected = r.detectByLanguage(ctx)
    }

    return selected, nil
}

// SelectFrameworkForFile returns the test framework for a specific test file
func (r *TestFrameworkRegistry) SelectFrameworkForFile(ctx *ProjectContext, testFile string) (*TestFrameworkDefinition, error) {
    ext := filepath.Ext(testFile)

    r.mu.RLock()
    defer r.mu.RUnlock()

    for _, fw := range r.frameworks {
        enabled, _ := fw.Enabled(ctx)
        if !enabled {
            continue
        }

        // Check if file matches any test pattern
        for _, pattern := range fw.TestFilePatterns {
            if matched, _ := filepath.Match(pattern, filepath.Base(testFile)); matched {
                return fw, nil
            }
        }
    }

    return nil, fmt.Errorf("no test framework found for file: %s", testFile)
}

// detectByLanguage detects framework based on primary language markers
func (r *TestFrameworkRegistry) detectByLanguage(ctx *ProjectContext) *TestFrameworkDefinition {
    // Check for language-specific markers
    switch {
    case fileExists(ctx.Root, "go.mod"):
        return r.frameworks["go-test"]
    case fileExists(ctx.Root, "Cargo.toml"):
        return r.frameworks["cargo-test"]
    case fileExists(ctx.Root, "package.json"):
        // Default to node test if no other JS framework configured
        return r.frameworks["node-test"]
    case fileExists(ctx.Root, "pyproject.toml", "setup.py", "requirements.txt"):
        return r.frameworks["unittest"]
    case fileExists(ctx.Root, "Gemfile"):
        return r.frameworks["minitest"]
    case fileExists(ctx.Root, "pom.xml"):
        return r.frameworks["maven-test"]
    case fileExists(ctx.Root, "mix.exs"):
        return r.frameworks["mix-test"]
    }
    return nil
}
```

```
┌─────────────────────────────────────────────────────────────────────────────────────┐
│                      TEST FRAMEWORK SELECTION PROTOCOL                               │
├─────────────────────────────────────────────────────────────────────────────────────┤
│                                                                                     │
│  SELECTION ALGORITHM:                                                               │
│  ┌──────────────────────────────────────────────────────────────────────────────┐   │
│  │  1. Check project root for framework-specific markers                        │   │
│  │  2. For each candidate, run Enabled() check:                                 │   │
│  │     a. Binary exists in PATH                                                 │   │
│  │     b. Config file present                                                   │   │
│  │     c. Dependency declared                                                   │   │
│  │     d. No conflict with higher-priority framework                            │   │
│  │  3. If no framework found, fallback to language detection                    │   │
│  │  4. Return best match                                                        │   │
│  └──────────────────────────────────────────────────────────────────────────────┘   │
│                                                                                     │
│  PRIORITY RULES (same language):                                                    │
│  ├── JS/TS: vitest > jest > mocha > bun-test > node-test                           │
│  ├── Python: pytest > unittest                                                      │
│  ├── Ruby: rspec > minitest                                                        │
│  ├── Java: gradle > maven                                                          │
│  └── Config-present frameworks always take priority                                │
│                                                                                     │
│  TEST FILE DETECTION:                                                               │
│  ├── Go: *_test.go                                                                 │
│  ├── JS/TS: *.test.{js,ts}, *.spec.{js,ts}, __tests__/*                            │
│  ├── Python: test_*.py, *_test.py, tests/                                          │
│  ├── Rust: *_test.rs, tests/*.rs                                                   │
│  ├── Ruby: *_spec.rb (RSpec), *_test.rb (Minitest)                                 │
│  └── Java: *Test.java, *Tests.java                                                 │
│                                                                                     │
│  COMMAND PLACEHOLDERS:                                                              │
│  ├── $FILE: Full path to test file                                                 │
│  ├── $TEST: Test name or pattern                                                   │
│  ├── $LINE: Line number (for RSpec-style runners)                                  │
│  └── Commands executed in project root directory                                   │
│                                                                                     │
│  OUTPUT PARSING:                                                                    │
│  ├── Each framework has specific output parser                                     │
│  ├── Extracts: passed, failed, skipped, duration, coverage                         │
│  ├── Failure details: test name, file, line, message, stack trace                  │
│  └── Normalized to TestResult struct                                               │
│                                                                                     │
└─────────────────────────────────────────────────────────────────────────────────────┘
```

#### Tester Test Framework Skills (Execution Only)

**CRITICAL: Tester executes tests but MUST consult Librarian for framework detection first.**

```go
// Test Framework Execution Skills (Detection moved to Librarian)
tester_skills_framework := []Skill{
    {
        Name:        "run_tests_smart",
        Description: "Run tests using framework from Librarian",
        Domain:      "testing",
        Keywords:    []string{"run tests", "test", "execute tests"},
        LoadTrigger: "auto",
        Parameters: []Param{
            {Name: "scope", Type: "enum", Values: []string{"all", "file", "single", "changed"}, Required: false, Default: "all"},
            {Name: "file", Type: "string", Required: false, Description: "Test file (required for file/single scope)"},
            {Name: "test_name", Type: "string", Required: false, Description: "Test name (required for single scope)"},
            {Name: "framework", Type: "object", Required: false, Description: "TestFrameworkDefinition from Librarian (auto-consulted if not provided)"},
            {Name: "parallel", Type: "bool", Required: false, Default: true},
        },
    },
    {
        Name:        "run_tests_with_coverage",
        Description: "Run tests with coverage using framework from Librarian",
        Domain:      "testing",
        Keywords:    []string{"coverage", "test coverage"},
        Parameters: []Param{
            {Name: "threshold", Type: "float", Required: false, Description: "Minimum coverage percentage"},
            {Name: "framework", Type: "object", Required: false, Description: "TestFrameworkDefinition from Librarian"},
            {Name: "changed_only", Type: "bool", Required: false, Default: false},
        },
    },
    {
        Name:        "watch_tests",
        Description: "Run tests in watch mode using framework from Librarian",
        Domain:      "testing",
        Keywords:    []string{"watch", "test watch", "continuous"},
        Parameters: []Param{
            {Name: "scope", Type: "enum", Values: []string{"all", "file"}, Required: false, Default: "all"},
            {Name: "file", Type: "string", Required: false},
            {Name: "framework", Type: "object", Required: false, Description: "TestFrameworkDefinition from Librarian"},
        },
    },
    {
        Name:        "run_changed_tests",
        Description: "Run tests for changed files only",
        Domain:      "testing",
        Keywords:    []string{"changed", "affected", "related tests"},
        Parameters: []Param{
            {Name: "base_ref", Type: "string", Required: false, Default: "HEAD~1", Description: "Git ref to compare against"},
            {Name: "framework", Type: "object", Required: false, Description: "TestFrameworkDefinition from Librarian"},
        },
    },
    {
        Name:        "find_test_files",
        Description: "Find all test files using patterns from Librarian",
        Domain:      "testing",
        Keywords:    []string{"find tests", "test files", "list tests"},
        Parameters: []Param{
            {Name: "path", Type: "string", Required: false},
            {Name: "pattern", Type: "string", Required: false},
            {Name: "framework", Type: "object", Required: false, Description: "TestFrameworkDefinition from Librarian (for file patterns)"},
        },
    },
    {
        Name:        "get_test_for_file",
        Description: "Find the test file corresponding to a source file",
        Domain:      "testing",
        Keywords:    []string{"test for", "corresponding test", "related test"},
        Parameters: []Param{
            {Name: "source_file", Type: "string", Required: true},
            {Name: "framework", Type: "object", Required: false, Description: "TestFrameworkDefinition from Librarian (for naming conventions)"},
        },
    },
}
```

#### Tester Librarian Consultation Protocol

**Before executing any tests, Tester MUST consult Librarian for framework detection.**

```
┌─────────────────────────────────────────────────────────────────────────────────────┐
│                      TESTER → LIBRARIAN CONSULTATION PROTOCOL                        │
├─────────────────────────────────────────────────────────────────────────────────────┤
│                                                                                     │
│  MANDATORY CONSULTATION (Before Execution):                                         │
│  ┌──────────────────────────────────────────────────────────────────────────────┐   │
│  │  1. Tester receives test execution request                                   │   │
│  │  2. IF framework not provided in params:                                     │   │
│  │     → Request via Guide: "Librarian, detect_test_framework for {project}"    │   │
│  │  3. Librarian returns TestFrameworkDefinition                                │   │
│  │  4. Tester executes with provided configuration                              │   │
│  └──────────────────────────────────────────────────────────────────────────────┘   │
│                                                                                     │
│  CACHE OPTIMIZATION:                                                                │
│  ├── First test request triggers Librarian consultation                           │
│  ├── Subsequent requests for same project use cached definition                   │
│  ├── Cache invalidated when Librarian signals config file changes                 │
│  │   (jest.config.*, pytest.ini, package.json, go.mod, etc.)                     │
│  └── Tester can pass `refresh: true` to force re-consultation                     │
│                                                                                     │
│  EXECUTION WITHOUT CONSULTATION (Allowed Cases):                                    │
│  ├── framework explicitly provided in skill params                                │
│  ├── Previous consultation result cached for this project                         │
│  └── User explicitly provides framework override                                  │
│                                                                                     │
│  CONSULTATION FAILURE HANDLING:                                                     │
│  ├── Librarian returns no framework → Report "no test framework detected"         │
│  ├── Librarian returns low confidence → Include warning, proceed anyway           │
│  └── Librarian timeout → Report error, DO NOT guess framework                     │
│                                                                                     │
│  FILE DISCOVERY COORDINATION:                                                       │
│  ├── find_test_files uses TestFilePatterns from Librarian                         │
│  ├── get_test_for_file uses naming conventions from Librarian                     │
│  └── Both skills consult Librarian if framework not provided                      │
│                                                                                     │
└─────────────────────────────────────────────────────────────────────────────────────┘
```

#### Tester Test Framework System Prompt Addition

```go
const TesterTestFrameworkPrompt = `
TOOL CONSULTATION REQUIREMENT:
You MUST consult Librarian for test framework detection before executing tests.
You do NOT detect frameworks yourself - Librarian is the single source of truth.

CONSULTATION FLOW:
1. Receive test execution request
2. IF no framework in params AND not cached:
   → Route via Guide: "Librarian, detect_test_framework for {project}"
3. Receive TestFrameworkDefinition from Librarian
4. Execute with provided configuration

EXECUTION SKILLS:
- run_tests_smart: Execute tests (requires TestFrameworkDefinition)
  - scope: "all" (entire suite), "file" (single file), "single" (specific test), "changed" (affected by git changes)
- run_tests_with_coverage: Execute with coverage reporting
- watch_tests: Continuous test running on file changes
- run_changed_tests: Only tests affected by recent changes

FILE DISCOVERY SKILLS:
- find_test_files: Find test files using patterns from Librarian
- get_test_for_file: Find corresponding test using conventions from Librarian

CACHE BEHAVIOR:
- Framework definition cached per project root
- Cache invalidated on Librarian signal (config file change)
- Use refresh: true to force re-consultation

IF LIBRARIAN UNAVAILABLE:
- Report error: "Test framework detection unavailable - Librarian consultation required"
- DO NOT fall back to internal detection (Librarian owns this responsibility)

OUTPUT PARSING:
Each framework's output is parsed into a standard TestResult:
{
  "consultation": {"source": "librarian", "cached": false},
  "framework": "jest",
  "passed": 42,
  "failed": 2,
  "skipped": 1,
  "duration": "3.5s",
  "coverage": 85.2,
  "failures": [
    {
      "test_name": "TestFoo",
      "file_path": "foo_test.go",
      "line": 42,
      "message": "expected 5, got 3",
      "stack_trace": "..."
    }
  ]
}

FAILURE ANALYSIS:
When tests fail:
1. Parse failure output for structured error info
2. Identify affected test file and line number
3. Extract expected vs actual values when available
4. Suggest potential fixes based on error pattern
`
```

### Completion Evidence Protocol (Inspector & Tester)

**CRITICAL: NO EVIDENCE = NOT COMPLETE. Tasks require verifiable evidence before marking complete.**

```
┌─────────────────────────────────────────────────────────────────────────────────────┐
│                      COMPLETION EVIDENCE PROTOCOL                                    │
├─────────────────────────────────────────────────────────────────────────────────────┤
│                                                                                     │
│  PRINCIPLE: "NO EVIDENCE = NOT COMPLETE"                                            │
│  A task is ONLY complete when ALL required evidence has been collected and stored.  │
│                                                                                     │
│  INSPECTOR EVIDENCE CHECKLIST:                                                      │
│  ┌──────────────────────────────────────────────────────────────────────────────┐   │
│  │  □ BUILD        │ Compilation succeeds (0 errors)                            │   │
│  │                 │ Evidence: build command output                             │   │
│  ├─────────────────┼────────────────────────────────────────────────────────────┤   │
│  │  □ TYPES        │ LSP/type-checker diagnostics clean on changed files        │   │
│  │                 │ Evidence: type-check command output                        │   │
│  ├─────────────────┼────────────────────────────────────────────────────────────┤   │
│  │  □ LINT         │ Linter passes on changed files                             │   │
│  │                 │ Evidence: lint command output                              │   │
│  ├─────────────────┼────────────────────────────────────────────────────────────┤   │
│  │  □ PATTERNS     │ Matches codebase patterns (Librarian verification)         │   │
│  │                 │ Evidence: Librarian pattern_confidence score               │   │
│  ├─────────────────┼────────────────────────────────────────────────────────────┤   │
│  │  □ SECURITY     │ No new vulnerabilities introduced                          │   │
│  │                 │ Evidence: security scan output (if applicable)             │   │
│  └─────────────────┴────────────────────────────────────────────────────────────┘   │
│                                                                                     │
│  TESTER EVIDENCE CHECKLIST:                                                         │
│  ┌──────────────────────────────────────────────────────────────────────────────┐   │
│  │  □ TESTS_PASS   │ All relevant tests pass                                    │   │
│  │                 │ Evidence: test runner output with pass/fail counts         │   │
│  ├─────────────────┼────────────────────────────────────────────────────────────┤   │
│  │  □ COVERAGE     │ Test coverage not decreased                                │   │
│  │                 │ Evidence: coverage report (before/after comparison)        │   │
│  ├─────────────────┼────────────────────────────────────────────────────────────┤   │
│  │  □ REGRESSION   │ No regression in existing tests                            │   │
│  │                 │ Evidence: existing test results unchanged                  │   │
│  ├─────────────────┼────────────────────────────────────────────────────────────┤   │
│  │  □ NEW_TESTS    │ New code has corresponding tests                           │   │
│  │                 │ Evidence: new test files or test functions                 │   │
│  └─────────────────┴────────────────────────────────────────────────────────────┘   │
│                                                                                     │
│  EVIDENCE STORAGE:                                                                  │
│  All evidence is stored in Archivalist with task correlation:                       │
│  {                                                                                  │
│    "task_id": "task_abc123",                                                        │
│    "evidence_type": "BUILD|TYPES|LINT|TESTS_PASS|...",                              │
│    "result": "PASS|FAIL",                                                           │
│    "output": "<command output or summary>",                                         │
│    "timestamp": "2024-01-15T10:30:00Z"                                              │
│  }                                                                                  │
│                                                                                     │
│  EVIDENCE QUERY:                                                                    │
│  Future tasks can query: "What evidence was required for [similar task]?"           │
│                                                                                     │
└─────────────────────────────────────────────────────────────────────────────────────┘
```

#### Inspector Completion Evidence Skills

```go
// Completion Evidence Skills (Tier 1 - Always loaded)
inspector_skills_evidence := []Skill{
    {
        Name:        "collect_build_evidence",
        Description: "Run build and collect evidence",
        Domain:      "evidence",
        Keywords:    []string{"build", "compile"},
        Priority:    100,
        Parameters: []Param{
            {Name: "changed_files", Type: "array", Required: true},
        },
    },
    {
        Name:        "collect_type_evidence",
        Description: "Run type checker and collect evidence",
        Domain:      "evidence",
        Keywords:    []string{"types", "typecheck"},
        Priority:    100,
    },
    {
        Name:        "collect_lint_evidence",
        Description: "Run linter and collect evidence",
        Domain:      "evidence",
        Keywords:    []string{"lint", "style"},
        Priority:    100,
    },
    {
        Name:        "verify_pattern_compliance",
        Description: "Verify code matches codebase patterns via Librarian",
        Domain:      "evidence",
        Keywords:    []string{"patterns", "compliance"},
        Priority:    90,
    },
    {
        Name:        "store_evidence",
        Description: "Store collected evidence in Archivalist",
        Domain:      "evidence",
        Keywords:    []string{"store", "record"},
        Required:    true, // Must store all evidence
    },
}
```

#### Tester Completion Evidence Skills

```go
// Completion Evidence Skills (Tier 1 - Always loaded)
tester_skills_evidence := []Skill{
    {
        Name:        "collect_test_evidence",
        Description: "Run tests and collect evidence",
        Domain:      "evidence",
        Keywords:    []string{"tests", "run"},
        Priority:    100,
        Parameters: []Param{
            {Name: "test_scope", Type: "string", Required: false, Description: "Specific tests or 'all'"},
        },
    },
    {
        Name:        "collect_coverage_evidence",
        Description: "Collect coverage report and compare",
        Domain:      "evidence",
        Keywords:    []string{"coverage"},
        Priority:    100,
    },
    {
        Name:        "verify_no_regression",
        Description: "Verify no regression in existing tests",
        Domain:      "evidence",
        Keywords:    []string{"regression", "existing"},
        Priority:    90,
    },
    {
        Name:        "verify_new_tests_exist",
        Description: "Verify new code has corresponding tests",
        Domain:      "evidence",
        Keywords:    []string{"new tests", "coverage"},
        Priority:    90,
    },
    {
        Name:        "store_evidence",
        Description: "Store collected evidence in Archivalist",
        Domain:      "evidence",
        Keywords:    []string{"store", "record"},
        Required:    true,
    },
}
```

#### Completion Evidence System Prompt Additions

```go
const InspectorCompletionEvidencePrompt = `
COMPLETION EVIDENCE PROTOCOL:
NO EVIDENCE = NOT COMPLETE.

Before marking ANY task complete, you MUST:
1. COLLECT all evidence in your checklist:
   □ BUILD: Run build command, capture output
   □ TYPES: Run type checker on changed files, capture output
   □ LINT: Run linter on changed files, capture output
   □ PATTERNS: Query Librarian for pattern_confidence on changed code
   □ SECURITY: Run security scan if security-sensitive changes

2. VERIFY all evidence passes:
   - Build: 0 errors
   - Types: 0 errors on changed files
   - Lint: 0 errors (warnings acceptable if pre-existing)
   - Patterns: confidence >= 0.7
   - Security: no new high/critical issues

3. STORE evidence in Archivalist:
   For each check, store: task_id, evidence_type, result, output

4. If ANY evidence FAILS:
   - Do NOT mark task complete
   - Report specific failures to Architect
   - Include evidence output in report

EVIDENCE IS IMMUTABLE: Once stored, evidence cannot be modified.
`

const TesterCompletionEvidencePrompt = `
COMPLETION EVIDENCE PROTOCOL:
NO EVIDENCE = NOT COMPLETE.

Before marking ANY task complete, you MUST:
1. COLLECT all evidence in your checklist:
   □ TESTS_PASS: Run test suite, capture output with pass/fail counts
   □ COVERAGE: Run coverage report, compare to baseline
   □ REGRESSION: Verify existing tests still pass
   □ NEW_TESTS: Verify new code has tests (coverage check)

2. VERIFY all evidence passes:
   - Tests: 100% pass rate on relevant tests
   - Coverage: Not decreased (or justified exception)
   - Regression: 0 new failures in existing tests
   - New tests: New code has non-zero coverage

3. STORE evidence in Archivalist:
   For each check, store: task_id, evidence_type, result, output

4. If ANY evidence FAILS:
   - Do NOT mark task complete
   - Report specific failures to Architect
   - Include test output and failure details

EVIDENCE IS IMMUTABLE: Once stored, evidence cannot be modified.
`
```

---

## Agent Skill Definitions

Each agent exposes specific skills that other agents (via Guide) can invoke.

### Skill Naming Convention

```
<agent_name>_<action>_<domain>

Examples:
- archivalist_query_chronicle
- librarian_search_code
- engineer_write_files
- inspector_validate_task
```

### Guide Skill Definitions

```go
guide_skills := []SkillDefinition{
    {
        Name:        "guide_route_message",
        Description: "Route a message to the appropriate agent",
        InputSchema: map[string]any{
            "type": "object",
            "properties": map[string]any{
                "input":    {"type": "string", "description": "The message to route"},
                "target":   {"type": "string", "description": "Optional explicit target agent"},
                "session_id": {"type": "string", "description": "Session context"},
            },
            "required": []string{"input"},
        },
    },
    {
        Name:        "guide_classify_intent",
        Description: "Classify user intent without routing",
        InputSchema: map[string]any{
            "type": "object",
            "properties": map[string]any{
                "input": {"type": "string", "description": "The input to classify"},
            },
            "required": []string{"input"},
        },
    },
    {
        Name:        "guide_get_status",
        Description: "Get system status",
        InputSchema: map[string]any{
            "type": "object",
            "properties": map[string]any{},
        },
    },
}
```

### Archivalist Skill Definitions

```go
archivalist_skills := []SkillDefinition{
    {
        Name:        "archivalist_store_entry",
        Description: "Store an entry in the chronicle",
        InputSchema: map[string]any{
            "type": "object",
            "properties": map[string]any{
                "content":  {"type": "string"},
                "category": {"type": "string", "enum": []string{"decision", "insight", "pattern", "failure", "task_state", "timeline", "user_voice", "hypothesis", "open_thread", "general"}},
                "title":    {"type": "string"},
                "session_id": {"type": "string"},
            },
            "required": []string{"content", "category"},
        },
    },
    {
        Name:        "archivalist_query_chronicle",
        Description: "Query the chronicle for information",
        InputSchema: map[string]any{
            "type": "object",
            "properties": map[string]any{
                "search_text":    {"type": "string"},
                "categories":     {"type": "array", "items": {"type": "string"}},
                "session_ids":    {"type": "array", "items": {"type": "string"}},
                "cross_session":  {"type": "boolean"},
                "limit":          {"type": "integer"},
            },
        },
    },
    {
        Name:        "archivalist_get_briefing",
        Description: "Get session briefing",
        InputSchema: map[string]any{
            "type": "object",
            "properties": map[string]any{
                "tier": {"type": "string", "enum": []string{"micro", "standard", "full"}},
            },
        },
    },
    {
        Name:        "archivalist_record_file",
        Description: "Record file operation",
        InputSchema: map[string]any{
            "type": "object",
            "properties": map[string]any{
                "path":        {"type": "string"},
                "operation":   {"type": "string", "enum": []string{"read", "modified", "created"}},
                "summary":     {"type": "string"},
                "changes":     {"type": "array"},
            },
            "required": []string{"path", "operation"},
        },
    },
    {
        Name:        "archivalist_record_pattern",
        Description: "Record a coding pattern",
        InputSchema: map[string]any{
            "type": "object",
            "properties": map[string]any{
                "category":    {"type": "string"},
                "name":        {"type": "string"},
                "pattern":     {"type": "string"},
                "example":     {"type": "string"},
            },
            "required": []string{"category", "name", "pattern"},
        },
    },
    {
        Name:        "archivalist_record_failure",
        Description: "Record a failure and optionally its resolution",
        InputSchema: map[string]any{
            "type": "object",
            "properties": map[string]any{
                "approach":   {"type": "string"},
                "reason":     {"type": "string"},
                "context":    {"type": "string"},
                "resolution": {"type": "string"},
            },
            "required": []string{"approach", "reason"},
        },
    },
}
```

### Librarian Skill Definitions

```go
librarian_skills := []SkillDefinition{
    {
        Name:        "librarian_search_code",
        Description: "Search codebase for patterns, symbols, or text",
        InputSchema: map[string]any{
            "type": "object",
            "properties": map[string]any{
                "query":       {"type": "string"},
                "file_glob":   {"type": "string"},
                "symbol_type": {"type": "string", "enum": []string{"function", "type", "const", "var", "all"}},
                "limit":       {"type": "integer"},
            },
            "required": []string{"query"},
        },
    },
    {
        Name:        "librarian_get_file",
        Description: "Get file contents with optional line range",
        InputSchema: map[string]any{
            "type": "object",
            "properties": map[string]any{
                "path":       {"type": "string"},
                "start_line": {"type": "integer"},
                "end_line":   {"type": "integer"},
            },
            "required": []string{"path"},
        },
    },
    {
        Name:        "librarian_get_structure",
        Description: "Get codebase structure overview",
        InputSchema: map[string]any{
            "type": "object",
            "properties": map[string]any{
                "path":  {"type": "string"},
                "depth": {"type": "integer"},
            },
        },
    },
    {
        Name:        "librarian_get_dependencies",
        Description: "Get dependency graph for a file or package",
        InputSchema: map[string]any{
            "type": "object",
            "properties": map[string]any{
                "path":      {"type": "string"},
                "direction": {"type": "string", "enum": []string{"imports", "imported_by", "both"}},
            },
            "required": []string{"path"},
        },
    },
}
```

### Engineer Skill Definitions

```go
engineer_skills := []SkillDefinition{
    {
        Name:        "engineer_read_file",
        Description: "Read a file's contents",
        InputSchema: map[string]any{
            "type": "object",
            "properties": map[string]any{
                "path":       {"type": "string"},
                "start_line": {"type": "integer"},
                "end_line":   {"type": "integer"},
            },
            "required": []string{"path"},
        },
    },
    {
        Name:        "engineer_write_file",
        Description: "Write content to a file",
        InputSchema: map[string]any{
            "type": "object",
            "properties": map[string]any{
                "path":    {"type": "string"},
                "content": {"type": "string"},
            },
            "required": []string{"path", "content"},
        },
    },
    {
        Name:        "engineer_edit_file",
        Description: "Edit a file with search/replace",
        InputSchema: map[string]any{
            "type": "object",
            "properties": map[string]any{
                "path":        {"type": "string"},
                "old_content": {"type": "string"},
                "new_content": {"type": "string"},
            },
            "required": []string{"path", "old_content", "new_content"},
        },
    },
    {
        Name:        "engineer_run_command",
        Description: "Execute a shell command",
        InputSchema: map[string]any{
            "type": "object",
            "properties": map[string]any{
                "command": {"type": "string"},
                "timeout": {"type": "integer"},
            },
            "required": []string{"command"},
        },
    },
    {
        Name:        "engineer_signal_complete",
        Description: "Signal task completion",
        InputSchema: map[string]any{
            "type": "object",
            "properties": map[string]any{
                "result":  {"type": "string"},
                "files_modified": {"type": "array", "items": {"type": "string"}},
            },
            "required": []string{"result"},
        },
    },
    {
        Name:        "engineer_request_help",
        Description: "Request clarification via Orchestrator",
        InputSchema: map[string]any{
            "type": "object",
            "properties": map[string]any{
                "question": {"type": "string"},
                "context":  {"type": "string"},
            },
            "required": []string{"question"},
        },
    },
}
```

### Designer Agent Specification

The Designer agent is a UI/UX specialist operating within the Single-Worker Pipeline architecture. It uses **Gemini 3 Pro** via `github.com/googleapis/go-genai` for advanced multimodal reasoning about visual design.

#### Designer Model Configuration

```go
// Designer uses Gemini 3 Pro for multimodal UI/UX capabilities
type DesignerConfig struct {
    Model           string        `json:"model"`            // "gemini-3-pro"
    Provider        string        `json:"provider"`         // "google"
    MaxTokens       int           `json:"max_tokens"`       // 32768
    Temperature     float64       `json:"temperature"`      // 0.3 (lower for consistency)
    TopP            float64       `json:"top_p"`            // 0.9
    VisionEnabled   bool          `json:"vision_enabled"`   // true (for design review)
    ContextWindow   int           `json:"context_window"`   // 128000
}

func DefaultDesignerConfig() DesignerConfig {
    return DesignerConfig{
        Model:         "gemini-3-pro",
        Provider:      "google",
        MaxTokens:     32768,
        Temperature:   0.3,
        TopP:          0.9,
        VisionEnabled: true,
        ContextWindow: 128000,
    }
}
```

#### Designer System Prompt

```go
const DesignerSystemPrompt = `You are an expert UI/UX Designer agent within the Sylk multi-agent system. You operate within a SINGLE-WORKER PIPELINE - you are the only worker in this pipeline.

## Pipeline Architecture

You work within a focused pipeline:
1. YOU (Designer) - Create/modify UI components and designs
2. INSPECTOR (UI mode) - Reviews your work for quality
3. TESTER (UI mode) - Validates accessibility, responsiveness, visual regression

If inspection fails, YOU receive feedback and iterate. You do NOT hand off to other workers mid-pipeline.

## Pipeline Coordination (When Applicable)

Sometimes the Architect sequences pipelines. For example:
- A previous Engineer pipeline may have created API hooks/types you should use
- Your work may inform a follow-up Engineer pipeline

Use these tools for coordination:
- designer_read_pipeline_context: Read artifacts from COMPLETED previous pipelines
- designer_emit_artifact: Emit artifacts for potential downstream pipelines
- designer_signal_dependency: Signal Architect if follow-up work is needed (does NOT create pipelines)

IMPORTANT: You never directly invoke or communicate with other workers. The Architect handles all pipeline orchestration.

## Knowledge Agent Consultation (via Guide)

You can consult knowledge agents for context. ALL consultations go through Guide:

CONTEXT REQUESTS (to Librarian):
- "What UI patterns exist in this codebase?"
- "Show me existing button/form/modal implementations"
- "What design tokens are defined?"

HISTORY REQUESTS (to Archivalist):
- "What UI changes have we made recently?"
- "Have similar components caused issues before?"
- "What accessibility violations have we had?"

RESEARCH REQUESTS (to Academic):
- "Best practices for accessible modals?"
- "UX patterns for form validation?"
- "Mobile-first responsive design guidelines?"

CONSULTATION FORMAT:
{
  "type": "CONTEXT_REQUEST",  // or HISTORY_REQUEST, RESEARCH_REQUEST
  "from_agent": "designer",
  "query": {
    "intent": "pattern_lookup",
    "subject": "modal components",
    "context": "creating new dialog"
  }
}

Guide routes to appropriate knowledge agent and returns response.

WHEN TO CONSULT:
- BEFORE creating new components (check existing patterns)
- WHEN uncertain about design tokens or conventions
- AFTER accessibility issues (check history for similar problems)

## Design System Integration

### CRITICAL: Never Hardcode Values
- Colors: Use color tokens (e.g., color.primary.500, color.semantic.error)
- Spacing: Use spacing tokens (e.g., spacing.md, spacing.lg)
- Typography: Use typography tokens (e.g., font.size.lg, font.weight.medium)
- Shadows: Use shadow tokens (e.g., shadow.sm, shadow.lg)
- Borders: Use border tokens (e.g., border.radius.md)

### Component Creation Protocol
1. ALWAYS call designer_search_components first
2. If similar component exists, use designer_extend_component
3. Only use designer_create_component if no match found
4. Validate with designer_validate_tokens before completing

## Accessibility Requirements

WCAG 2.1 AA compliance is MANDATORY:
- All interactive elements must be keyboard accessible
- Color contrast minimum 4.5:1 for normal text, 3:1 for large text
- All images need alt text
- Form inputs need labels
- Focus states must be visible
- Motion must respect prefers-reduced-motion

Use designer_check_accessibility before marking any work complete.

## Responsive Design

Always design mobile-first with these breakpoints:
- sm: 640px
- md: 768px
- lg: 1024px
- xl: 1280px
- 2xl: 1536px

Use designer_get_breakpoints to confirm project-specific values.

## Output Format

When completing work, emit artifacts for traceability:
- Component references (file paths, exports)
- Design token usage
- Accessibility audit results
- Preview/screenshot if applicable

The Inspector will review these artifacts.`
```

#### Designer Skill Definitions (Tools)

```go
designer_skills := []SkillDefinition{
    // ═══════════════════════════════════════════════════════════════════════════════
    // COMPONENT DISCOVERY & CREATION
    // ═══════════════════════════════════════════════════════════════════════════════
    {
        Name:        "designer_search_components",
        Description: "Search existing component library. MUST be called before any component creation.",
        InputSchema: map[string]any{
            "type": "object",
            "properties": map[string]any{
                "query":     {"type": "string", "description": "Search query (name, purpose, or pattern)"},
                "category":  {"type": "string", "description": "Component category filter"},
                "framework": {"type": "string", "enum": []string{"react", "vue", "svelte", "solid", "all"}},
            },
            "required": []string{"query"},
        },
    },
    {
        Name:        "designer_create_component",
        Description: "Create a new UI component. Only call after designer_search_components confirms no existing match.",
        InputSchema: map[string]any{
            "type": "object",
            "properties": map[string]any{
                "name":          {"type": "string", "description": "PascalCase component name"},
                "category":      {"type": "string", "description": "Component category (button, input, layout, etc.)"},
                "props":         {"type": "array", "items": {"type": "object"}, "description": "Component props with types"},
                "design_tokens": {"type": "array", "items": {"type": "string"}, "description": "Design tokens used (NEVER hardcode values)"},
                "accessibility": {"type": "object", "description": "ARIA attributes and keyboard handling spec"},
                "variants":      {"type": "array", "items": {"type": "string"}, "description": "Component variants (sizes, states)"},
            },
            "required": []string{"name", "category", "props", "design_tokens", "accessibility"},
        },
    },
    {
        Name:        "designer_extend_component",
        Description: "Extend or compose existing components rather than creating from scratch.",
        InputSchema: map[string]any{
            "type": "object",
            "properties": map[string]any{
                "base_component": {"type": "string", "description": "Component to extend"},
                "variant_name":   {"type": "string", "description": "Name for this variant"},
                "modifications":  {"type": "object", "description": "Props/styling modifications"},
            },
            "required": []string{"base_component", "variant_name", "modifications"},
        },
    },
    {
        Name:        "designer_read_component",
        Description: "Read an existing component's source code and props interface.",
        InputSchema: map[string]any{
            "type": "object",
            "properties": map[string]any{
                "component": {"type": "string", "description": "Component name or path"},
                "include":   {"type": "array", "items": {"type": "string"}, "description": "What to include: source, props, styles, stories"},
            },
            "required": []string{"component"},
        },
    },
    {
        Name:        "designer_edit_component",
        Description: "Edit an existing component with targeted changes.",
        InputSchema: map[string]any{
            "type": "object",
            "properties": map[string]any{
                "component":   {"type": "string", "description": "Component name or path"},
                "old_content": {"type": "string", "description": "Content to replace"},
                "new_content": {"type": "string", "description": "Replacement content"},
            },
            "required": []string{"component", "old_content", "new_content"},
        },
    },

    // ═══════════════════════════════════════════════════════════════════════════════
    // DESIGN SYSTEM INTEGRATION
    // ═══════════════════════════════════════════════════════════════════════════════
    {
        Name:        "designer_get_design_tokens",
        Description: "Retrieve design tokens for consistent styling. Always use tokens, never hardcode.",
        InputSchema: map[string]any{
            "type": "object",
            "properties": map[string]any{
                "category": {"type": "string", "enum": []string{"colors", "spacing", "typography", "shadows", "borders", "motion", "all"}},
                "variant":  {"type": "string", "description": "Specific variant or 'all'"},
            },
            "required": []string{"category"},
        },
    },
    {
        Name:        "designer_validate_tokens",
        Description: "Validate that component uses only approved design tokens (no hardcoded values).",
        InputSchema: map[string]any{
            "type": "object",
            "properties": map[string]any{
                "component_code": {"type": "string", "description": "Component source code to validate"},
            },
            "required": []string{"component_code"},
        },
    },
    {
        Name:        "designer_get_theme",
        Description: "Get current theme configuration including dark/light mode tokens.",
        InputSchema: map[string]any{
            "type": "object",
            "properties": map[string]any{
                "mode": {"type": "string", "enum": []string{"light", "dark", "both"}},
            },
        },
    },
    {
        Name:        "designer_create_token",
        Description: "Create a new design token (requires design system ownership).",
        InputSchema: map[string]any{
            "type": "object",
            "properties": map[string]any{
                "name":        {"type": "string", "description": "Token name following naming convention"},
                "category":    {"type": "string", "description": "Token category"},
                "value":       {"type": "string", "description": "Token value"},
                "description": {"type": "string", "description": "Usage description"},
            },
            "required": []string{"name", "category", "value"},
        },
    },

    // ═══════════════════════════════════════════════════════════════════════════════
    // ACCESSIBILITY (WCAG 2.1 AA REQUIRED)
    // ═══════════════════════════════════════════════════════════════════════════════
    {
        Name:        "designer_check_accessibility",
        Description: "Run accessibility audit on component or page. WCAG 2.1 AA is minimum requirement.",
        InputSchema: map[string]any{
            "type": "object",
            "properties": map[string]any{
                "target":     {"type": "string", "description": "Component name or page path"},
                "wcag_level": {"type": "string", "enum": []string{"A", "AA", "AAA"}, "default": "AA"},
                "include":    {"type": "array", "items": {"type": "string"}, "description": "Checks to include: contrast, keyboard, aria, focus, motion"},
            },
            "required": []string{"target"},
        },
    },
    {
        Name:        "designer_check_color_contrast",
        Description: "Verify color contrast ratios meet WCAG requirements.",
        InputSchema: map[string]any{
            "type": "object",
            "properties": map[string]any{
                "foreground": {"type": "string", "description": "Foreground color (token name or hex)"},
                "background": {"type": "string", "description": "Background color (token name or hex)"},
                "text_size":  {"type": "string", "enum": []string{"normal", "large"}, "default": "normal"},
            },
            "required": []string{"foreground", "background"},
        },
    },
    {
        Name:        "designer_generate_aria",
        Description: "Generate appropriate ARIA attributes for component type.",
        InputSchema: map[string]any{
            "type": "object",
            "properties": map[string]any{
                "component_type": {"type": "string", "description": "Semantic type: button, dialog, menu, tab, etc."},
                "interactive":    {"type": "boolean", "description": "Is component interactive?"},
                "states":         {"type": "array", "items": {"type": "string"}, "description": "Possible states: expanded, selected, disabled, etc."},
            },
            "required": []string{"component_type", "interactive"},
        },
    },
    {
        Name:        "designer_test_keyboard_nav",
        Description: "Test keyboard navigation flow for component.",
        InputSchema: map[string]any{
            "type": "object",
            "properties": map[string]any{
                "component": {"type": "string", "description": "Component to test"},
                "expected_flow": {"type": "array", "items": {"type": "string"}, "description": "Expected tab order"},
            },
            "required": []string{"component"},
        },
    },

    // ═══════════════════════════════════════════════════════════════════════════════
    // LAYOUT & RESPONSIVE DESIGN
    // ═══════════════════════════════════════════════════════════════════════════════
    {
        Name:        "designer_create_layout",
        Description: "Create responsive layout using design system grid.",
        InputSchema: map[string]any{
            "type": "object",
            "properties": map[string]any{
                "type":        {"type": "string", "enum": []string{"grid", "flex", "stack", "split"}},
                "breakpoints": {"type": "object", "description": "Responsive configurations per breakpoint"},
                "regions":     {"type": "array", "items": {"type": "string"}, "description": "Named layout regions"},
                "gap":         {"type": "string", "description": "Gap token (e.g., spacing.md)"},
            },
            "required": []string{"type", "breakpoints", "regions"},
        },
    },
    {
        Name:        "designer_get_breakpoints",
        Description: "Get design system breakpoint definitions.",
        InputSchema: map[string]any{
            "type": "object",
            "properties": map[string]any{},
        },
    },
    {
        Name:        "designer_preview_responsive",
        Description: "Preview component/page at different breakpoints.",
        InputSchema: map[string]any{
            "type": "object",
            "properties": map[string]any{
                "target":      {"type": "string", "description": "Component or page to preview"},
                "breakpoints": {"type": "array", "items": {"type": "string"}, "description": "Breakpoints to preview: sm, md, lg, xl, 2xl"},
            },
            "required": []string{"target"},
        },
    },

    // ═══════════════════════════════════════════════════════════════════════════════
    // PREVIEW & VISUAL VALIDATION
    // ═══════════════════════════════════════════════════════════════════════════════
    {
        Name:        "designer_preview_component",
        Description: "Generate preview of component in isolation (Storybook-style).",
        InputSchema: map[string]any{
            "type": "object",
            "properties": map[string]any{
                "component": {"type": "string", "description": "Component to preview"},
                "props":     {"type": "object", "description": "Props to render with"},
                "viewport":  {"type": "string", "enum": []string{"mobile", "tablet", "desktop"}},
                "theme":     {"type": "string", "enum": []string{"light", "dark"}},
            },
            "required": []string{"component"},
        },
    },
    {
        Name:        "designer_run_visual_diff",
        Description: "Compare component against baseline for visual regressions.",
        InputSchema: map[string]any{
            "type": "object",
            "properties": map[string]any{
                "component": {"type": "string", "description": "Component to diff"},
                "baseline":  {"type": "string", "description": "Baseline image path or 'latest'"},
                "threshold": {"type": "number", "description": "Acceptable diff percentage (0-100)"},
            },
            "required": []string{"component"},
        },
    },
    {
        Name:        "designer_capture_screenshot",
        Description: "Capture screenshot of component or page for review.",
        InputSchema: map[string]any{
            "type": "object",
            "properties": map[string]any{
                "target":   {"type": "string", "description": "Component or page path"},
                "viewport": {"type": "string", "description": "Viewport size"},
                "selector": {"type": "string", "description": "CSS selector to capture (optional)"},
            },
            "required": []string{"target"},
        },
    },

    // ═══════════════════════════════════════════════════════════════════════════════
    // ANIMATION & MOTION
    // ═══════════════════════════════════════════════════════════════════════════════
    {
        Name:        "designer_create_animation",
        Description: "Create animation using design system motion tokens.",
        InputSchema: map[string]any{
            "type": "object",
            "properties": map[string]any{
                "name":           {"type": "string", "description": "Animation name"},
                "type":           {"type": "string", "enum": []string{"transition", "keyframe", "spring"}},
                "duration":       {"type": "string", "description": "Duration token (e.g., duration.fast)"},
                "easing":         {"type": "string", "description": "Easing token (e.g., ease.out)"},
                "properties":     {"type": "array", "items": {"type": "string"}, "description": "CSS properties to animate"},
                "reduced_motion": {"type": "object", "description": "Fallback for prefers-reduced-motion"},
            },
            "required": []string{"name", "type", "duration", "easing", "reduced_motion"},
        },
    },
    {
        Name:        "designer_get_motion_tokens",
        Description: "Get available motion/animation tokens.",
        InputSchema: map[string]any{
            "type": "object",
            "properties": map[string]any{
                "category": {"type": "string", "enum": []string{"duration", "easing", "spring", "all"}},
            },
        },
    },

    // ═══════════════════════════════════════════════════════════════════════════════
    // FRONTEND TOOLING
    // ═══════════════════════════════════════════════════════════════════════════════
    {
        Name:        "designer_run_dev_server",
        Description: "Start frontend development server for preview.",
        InputSchema: map[string]any{
            "type": "object",
            "properties": map[string]any{
                "port":       {"type": "integer", "default": 3000},
                "hot_reload": {"type": "boolean", "default": true},
            },
        },
    },
    {
        Name:        "designer_run_storybook",
        Description: "Start Storybook for component development and documentation.",
        InputSchema: map[string]any{
            "type": "object",
            "properties": map[string]any{
                "port":      {"type": "integer", "default": 6006},
                "component": {"type": "string", "description": "Focus on specific component"},
            },
        },
    },
    {
        Name:        "designer_build_styles",
        Description: "Build/compile design system styles (CSS, SCSS, CSS-in-JS).",
        InputSchema: map[string]any{
            "type": "object",
            "properties": map[string]any{
                "format": {"type": "string", "enum": []string{"css", "scss", "js", "ts"}},
                "minify": {"type": "boolean", "default": false},
            },
        },
    },
    {
        Name:        "designer_lint_styles",
        Description: "Run style linter (Stylelint) on component styles.",
        InputSchema: map[string]any{
            "type": "object",
            "properties": map[string]any{
                "target": {"type": "string", "description": "File or directory to lint"},
                "fix":    {"type": "boolean", "description": "Auto-fix issues"},
            },
            "required": []string{"target"},
        },
    },
    {
        Name:        "designer_run_command",
        Description: "Execute a frontend tooling command (npm, yarn, pnpm scripts).",
        InputSchema: map[string]any{
            "type": "object",
            "properties": map[string]any{
                "command": {"type": "string", "description": "Command to execute"},
                "timeout": {"type": "integer", "description": "Timeout in seconds"},
            },
            "required": []string{"command"},
        },
    },

    // ═══════════════════════════════════════════════════════════════════════════════
    // PIPELINE ARTIFACTS (Single-Worker Pipeline Architecture)
    // ═══════════════════════════════════════════════════════════════════════════════
    {
        Name:        "designer_read_pipeline_context",
        Description: "Read artifacts from a completed predecessor pipeline (e.g., Engineer's API types/hooks).",
        InputSchema: map[string]any{
            "type": "object",
            "properties": map[string]any{
                "pipeline_id":   {"type": "string", "description": "ID of completed pipeline to read from"},
                "artifact_type": {"type": "string", "enum": []string{"types", "hooks", "api_schema", "components", "all"}},
            },
            "required": []string{"pipeline_id"},
        },
    },
    {
        Name:        "designer_emit_artifact",
        Description: "Emit artifact for potential downstream pipelines. Called automatically on completion.",
        InputSchema: map[string]any{
            "type": "object",
            "properties": map[string]any{
                "artifact_type": {"type": "string", "enum": []string{"component", "layout", "tokens", "styles", "page"}},
                "artifact_data": {"type": "object", "description": "Artifact data (refs, exports, interfaces)"},
            },
            "required": []string{"artifact_type", "artifact_data"},
        },
    },
    {
        Name:        "designer_signal_dependency",
        Description: "Signal to Architect that a follow-up pipeline is needed. Does NOT create the pipeline directly.",
        InputSchema: map[string]any{
            "type": "object",
            "properties": map[string]any{
                "dependency_type": {"type": "string", "enum": []string{"engineering", "testing", "review", "design"}},
                "description":     {"type": "string", "description": "Description of follow-up work needed"},
                "blocking":        {"type": "boolean", "description": "Is current work blocked until follow-up completes?"},
                "artifacts_needed": {"type": "array", "items": {"type": "string"}, "description": "Specific artifacts needed from follow-up"},
            },
            "required": []string{"dependency_type", "description", "blocking"},
        },
    },
    {
        Name:        "designer_signal_complete",
        Description: "Signal task completion with summary and artifacts.",
        InputSchema: map[string]any{
            "type": "object",
            "properties": map[string]any{
                "result":          {"type": "string", "description": "Summary of completed work"},
                "files_modified":  {"type": "array", "items": {"type": "string"}, "description": "List of modified files"},
                "components_created": {"type": "array", "items": {"type": "string"}, "description": "New components created"},
                "accessibility_passed": {"type": "boolean", "description": "Did accessibility checks pass?"},
            },
            "required": []string{"result", "files_modified"},
        },
    },
}
```

---

## LLM Hooks

Hooks allow agents to inject logic before/after LLM calls and tool executions.

### Hook Types

```go
type HookPriority int

const (
    HookPriorityFirst    HookPriority = 0
    HookPriorityEarly    HookPriority = 25
    HookPriorityNormal   HookPriority = 50
    HookPriorityLate     HookPriority = 75
    HookPriorityLast     HookPriority = 100
)

type PromptHookFunc func(ctx context.Context, data *PromptHookData) (*PromptHookData, error)
type ToolCallHookFunc func(ctx context.Context, data *ToolCallHookData) (*ToolCallHookData, error)
```

### Hook Execution Flow

```
┌─────────────────────────────────────────────────────────────────────────────────────┐
│                              HOOK EXECUTION FLOW                                     │
├─────────────────────────────────────────────────────────────────────────────────────┤
│                                                                                     │
│  LLM CALL:                                                                          │
│  ┌──────────────────────────────────────────────────────────────────────────────┐   │
│  │                                                                              │   │
│  │  1. PRE-PROMPT HOOKS (ordered by priority)                                   │   │
│  │     ├── session_context_injection (inject session state)                     │   │
│  │     ├── skill_loading (load contextual skills)                               │   │
│  │     ├── history_injection (inject relevant history)                          │   │
│  │     └── token_budget_check (verify within limits)                            │   │
│  │                                                                              │   │
│  │  2. LLM CALL                                                                 │   │
│  │                                                                              │   │
│  │  3. POST-PROMPT HOOKS (ordered by priority)                                  │   │
│  │     ├── response_logging (log to Archivalist)                                │   │
│  │     ├── pattern_extraction (extract patterns for learning)                   │   │
│  │     └── token_accounting (track token usage)                                 │   │
│  │                                                                              │   │
│  └──────────────────────────────────────────────────────────────────────────────┘   │
│                                                                                     │
│  TOOL CALL:                                                                         │
│  ┌──────────────────────────────────────────────────────────────────────────────┐   │
│  │                                                                              │   │
│  │  1. PRE-TOOL HOOKS (ordered by priority)                                     │   │
│  │     ├── permission_check (verify allowed in session)                         │   │
│  │     ├── parameter_validation (validate inputs)                               │   │
│  │     └── session_scoping (add session context to params)                      │   │
│  │                                                                              │   │
│  │  2. TOOL EXECUTION                                                           │   │
│  │                                                                              │   │
│  │  3. POST-TOOL HOOKS (ordered by priority)                                    │   │
│  │     ├── result_logging (log to Archivalist)                                  │   │
│  │     ├── file_tracking (record file operations)                               │   │
│  │     └── state_update (update session context)                                │   │
│  │                                                                              │   │
│  └──────────────────────────────────────────────────────────────────────────────┘   │
│                                                                                     │
└─────────────────────────────────────────────────────────────────────────────────────┘
```

### Per-Agent Hooks

#### Guide Hooks

```go
guide_hooks := []Hook{
    // Pre-prompt hooks
    {
        Name:     "session_context_injection",
        Type:     PrePrompt,
        Priority: HookPriorityFirst,
        Handler:  func(ctx context.Context, data *PromptHookData) (*PromptHookData, error) {
            // Inject current session context into system prompt
            session := ctx.Value("session").(*Session)
            data.SystemPrompt = appendSessionContext(data.SystemPrompt, session)
            return data, nil
        },
    },
    {
        Name:     "route_cache_lookup",
        Type:     PrePrompt,
        Priority: HookPriorityEarly,
        Handler:  func(ctx context.Context, data *PromptHookData) (*PromptHookData, error) {
            // Check route cache before LLM classification
            if cached := routeCache.Get(data.Input); cached != nil {
                data.SkipLLM = true
                data.CachedResult = cached
            }
            return data, nil
        },
    },
    // Post-prompt hooks
    {
        Name:     "route_cache_store",
        Type:     PostPrompt,
        Priority: HookPriorityNormal,
        Handler:  func(ctx context.Context, data *PromptHookData) (*PromptHookData, error) {
            // Cache classification result
            if data.ClassificationMethod == "llm" {
                routeCache.Set(data.Input, data.Result)
            }
            return data, nil
        },
    },
}
```

#### Archivalist Hooks

```go
archivalist_hooks := []Hook{
    // Pre-prompt hooks
    {
        Name:     "cross_session_query_handler",
        Type:     PrePrompt,
        Priority: HookPriorityEarly,
        Handler:  func(ctx context.Context, data *PromptHookData) (*PromptHookData, error) {
            // Handle cross-session query requests
            if data.CrossSession {
                data.QueryScope = "all_sessions"
            } else {
                data.QueryScope = ctx.Value("session_id").(string)
            }
            return data, nil
        },
    },
    // Post-tool hooks
    {
        Name:     "entry_session_tagging",
        Type:     PostTool,
        Priority: HookPriorityFirst,
        Handler:  func(ctx context.Context, data *ToolCallHookData) (*ToolCallHookData, error) {
            // Tag all stored entries with session_id
            if data.ToolName == "archivalist_store_entry" {
                entry := data.Result.(*Entry)
                entry.SessionID = ctx.Value("session_id").(string)
            }
            return data, nil
        },
    },
    {
        Name:     "pattern_promotion_check",
        Type:     PostTool,
        Priority: HookPriorityNormal,
        Handler:  func(ctx context.Context, data *ToolCallHookData) (*ToolCallHookData, error) {
            // Check if pattern should be promoted to global
            if data.ToolName == "archivalist_record_pattern" {
                pattern := data.Input.(*Pattern)
                if shouldPromote(pattern) {
                    promoteToGlobal(pattern)
                }
            }
            return data, nil
        },
    },
}
```

#### Engineer Hooks

```go
engineer_hooks := []Hook{
    // Pre-tool hooks
    {
        Name:     "file_operation_tracking",
        Type:     PreTool,
        Priority: HookPriorityFirst,
        Handler:  func(ctx context.Context, data *ToolCallHookData) (*ToolCallHookData, error) {
            // Track file operations for Archivalist
            if isFileOperation(data.ToolName) {
                path := data.Input["path"].(string)
                archivalist.RecordFileOperation(ctx, path, data.ToolName)
            }
            return data, nil
        },
    },
    // Post-tool hooks
    {
        Name:     "consultation_logging",
        Type:     PostTool,
        Priority: HookPriorityNormal,
        Handler:  func(ctx context.Context, data *ToolCallHookData) (*ToolCallHookData, error) {
            // Log consultations to Archivalist
            if isConsultation(data.ToolName) {
                archivalist.StoreEntry(ctx, &Entry{
                    Category: CategoryTimeline,
                    Content:  fmt.Sprintf("Consulted %s: %s", data.ToolName, data.Input["query"]),
                })
            }
            return data, nil
        },
    },

    // ═══════════════════════════════════════════════════════════════════════════════
    // FAILURE RECOVERY PROTOCOL HOOKS
    // ═══════════════════════════════════════════════════════════════════════════════

    // Track consecutive failures
    {
        Name:     "failure_counter",
        Type:     PostTool,
        Priority: HookPriorityFirst,
        Handler:  func(ctx context.Context, data *ToolCallHookData) (*ToolCallHookData, error) {
            state := getEngineerState(ctx)

            if data.Error != nil {
                state.ConsecutiveFailures++

                // Record failure in Archivalist for pattern learning
                archivalist.RecordFailure(ctx, &FailureEntry{
                    ApproachSignature: normalizeApproach(state.CurrentApproach),
                    ErrorPattern:      data.Error.Error(),
                    Context:           state.TaskContext,
                    ToolName:          data.ToolName,
                })

                // Check if threshold exceeded
                if state.ConsecutiveFailures >= 3 {
                    // Trigger escalation
                    state.FailureProtocolActive = true
                    return nil, ErrFailureThresholdExceeded
                }
            } else {
                // Reset counter on success
                state.ConsecutiveFailures = 0
            }
            return data, nil
        },
    },

    // Inject failure warning into system prompt
    {
        Name:     "failure_protocol_injection",
        Type:     PrePrompt,
        Priority: HookPriorityEarly,
        Handler:  func(ctx context.Context, data *PromptHookData) (*PromptHookData, error) {
            state := getEngineerState(ctx)

            if state.ConsecutiveFailures >= 2 {
                warning := fmt.Sprintf(`
⚠️ FAILURE WARNING: %d consecutive failures on this task.
One more failure triggers automatic escalation.

REQUIRED ACTIONS:
1. STOP and reassess your approach
2. Consult Archivalist: "similar failure patterns for [current approach]"
3. Consider alternative approach before retrying

FORBIDDEN:
- Same approach with minor tweaks
- Suppressing errors with any/ignore/@ts-ignore
- Proceeding without consultation`, state.ConsecutiveFailures)

                data.SystemPrompt = appendSection(data.SystemPrompt, "FAILURE_WARNING", warning)
            }

            if state.FailureProtocolActive {
                data.SystemPrompt = appendSection(data.SystemPrompt, "FAILURE_PROTOCOL_ACTIVE",
                    "3 failures reached. STOP ALL EDITS. Revert and escalate with TASK_HELP.")
            }

            return data, nil
        },
    },

    // Create checkpoint before risky operations
    {
        Name:     "checkpoint_creation",
        Type:     PreTool,
        Priority: HookPriorityFirst,
        Handler:  func(ctx context.Context, data *ToolCallHookData) (*ToolCallHookData, error) {
            state := getEngineerState(ctx)

            // Create checkpoint before write/edit operations
            if isWriteOperation(data.ToolName) && state.LastCheckpoint == nil {
                checkpoint, err := createCheckpoint(ctx)
                if err == nil {
                    state.LastCheckpoint = checkpoint
                }
            }
            return data, nil
        },
    },
}
```

#### Engineer Failure Recovery Protocol

**CRITICAL: Engineer tracks consecutive failures and escalates after 3 failures.**

```
┌─────────────────────────────────────────────────────────────────────────────────────┐
│                      ENGINEER FAILURE RECOVERY PROTOCOL                              │
├─────────────────────────────────────────────────────────────────────────────────────┤
│                                                                                     │
│  FAILURE TRACKING:                                                                  │
│  ├── Each tool error increments ConsecutiveFailures counter                         │
│  ├── Each tool success resets counter to 0                                          │
│  ├── Counter is per-task (resets when new task assigned)                            │
│  └── All failures recorded in Archivalist for pattern learning                      │
│                                                                                     │
│  THRESHOLD ACTIONS:                                                                 │
│  ┌──────────────────────────────────────────────────────────────────────────────┐   │
│  │  Failures = 1: Continue normally                                             │   │
│  │  Failures = 2: Warning injected into system prompt                           │   │
│  │                "Reassess approach. Consult Archivalist."                     │   │
│  │  Failures = 3: FAILURE_PROTOCOL_ACTIVE                                       │   │
│  │                1. STOP all edits immediately                                 │   │
│  │                2. Revert to last checkpoint                                  │   │
│  │                3. Document failed approaches                                 │   │
│  │                4. Query Archivalist for similar failures                     │   │
│  │                5. Escalate to Orchestrator with TASK_HELP                    │   │
│  └──────────────────────────────────────────────────────────────────────────────┘   │
│                                                                                     │
│  FORBIDDEN AFTER 2 FAILURES:                                                        │
│  ├── Same approach with minor tweaks                                                │
│  ├── Suppressing errors (any, ignore, @ts-ignore, // @ts-expect-error)             │
│  ├── Proceeding without Archivalist consultation                                   │
│  └── Skipping tests to "make it work"                                              │
│                                                                                     │
│  CHECKPOINT SYSTEM:                                                                 │
│  ├── Checkpoint created before first write operation                               │
│  ├── Revert to checkpoint on 3rd failure                                           │
│  └── Checkpoint = git stash or in-memory file snapshot                             │
│                                                                                     │
└─────────────────────────────────────────────────────────────────────────────────────┘
```

#### Engineer Failure Recovery System Prompt Addition

```go
const EngineerFailureRecoveryPrompt = `
FAILURE RECOVERY PROTOCOL:
You have a 3-strike rule for consecutive failures.

TRACKING:
- Each tool error increments your failure counter
- Each success resets the counter
- Counter tracked by hooks - you'll see warnings when counter rises

AFTER 2 FAILURES (Warning State):
You will see a FAILURE_WARNING in your context.
REQUIRED: Stop and reassess. Query Archivalist for similar failures.
FORBIDDEN: Same approach with tweaks, error suppression, skipping tests.

AFTER 3 FAILURES (Escalation):
Automatic actions by hooks:
1. All edits blocked
2. Revert to checkpoint
3. TASK_HELP signal sent to Orchestrator

YOUR RESPONSE TO ESCALATION:
When you see FAILURE_PROTOCOL_ACTIVE:
1. Document what you tried and why it failed
2. Include Archivalist query results for similar failures
3. Suggest alternative approaches for human review
4. Do NOT attempt more fixes

PROACTIVE CONSULTATION:
When trying something novel or risky:
- Query Archivalist FIRST: "failure patterns for [approach]"
- If similar failures exist, consider alternative
`
```

#### Designer Hooks

```go
designer_hooks := []Hook{
    // ═══════════════════════════════════════════════════════════════════════════════
    // PRE-PROMPT HOOKS (Context Injection)
    // ═══════════════════════════════════════════════════════════════════════════════
    {
        Name:     "inject_pipeline_context",
        Type:     PrePrompt,
        Priority: HookPriorityFirst,
        Handler:  func(ctx context.Context, data *PromptHookData) (*PromptHookData, error) {
            // Inject artifacts from predecessor pipelines (if any exist)
            pipeline := ctx.Value("pipeline").(*Pipeline)
            if pipeline != nil && len(pipeline.PredecessorArtifacts) > 0 {
                data.SystemPrompt = appendPipelineContext(data.SystemPrompt, pipeline.PredecessorArtifacts)
            }
            return data, nil
        },
    },
    {
        Name:     "inject_design_tokens",
        Type:     PrePrompt,
        Priority: HookPriorityEarly,
        Handler:  func(ctx context.Context, data *PromptHookData) (*PromptHookData, error) {
            // Inject design token summary into context
            tokens := getDesignTokensSummary(ctx)
            if tokens != "" {
                data.SystemPrompt = appendSection(data.SystemPrompt, "AVAILABLE_DESIGN_TOKENS", tokens)
            }
            return data, nil
        },
    },
    {
        Name:     "inject_component_index",
        Type:     PrePrompt,
        Priority: HookPriorityEarly,
        Handler:  func(ctx context.Context, data *PromptHookData) (*PromptHookData, error) {
            // Inject component index for deduplication awareness
            index := getComponentIndex(ctx)
            if len(index) > 0 {
                data.SystemPrompt = appendSection(data.SystemPrompt, "EXISTING_COMPONENTS", strings.Join(index, ", "))
            }
            return data, nil
        },
    },

    // ═══════════════════════════════════════════════════════════════════════════════
    // PRE-TOOL HOOKS (Validation)
    // ═══════════════════════════════════════════════════════════════════════════════
    {
        Name:     "validate_component_search_first",
        Type:     PreTool,
        Priority: HookPriorityFirst,
        Trigger:  "designer_create_component",
        Handler:  func(ctx context.Context, data *ToolCallHookData) (*ToolCallHookData, error) {
            // Ensure designer_search_components was called before creating
            history := ctx.Value("tool_history").([]string)
            if !contains(history, "designer_search_components") {
                return nil, fmt.Errorf("BLOCKED: Must call designer_search_components before designer_create_component")
            }
            return data, nil
        },
    },
    {
        Name:     "validate_accessibility_required",
        Type:     PreTool,
        Priority: HookPriorityNormal,
        Trigger:  "designer_signal_complete",
        Handler:  func(ctx context.Context, data *ToolCallHookData) (*ToolCallHookData, error) {
            // Ensure accessibility check was run before completion
            history := ctx.Value("tool_history").([]string)
            if !contains(history, "designer_check_accessibility") {
                // Add warning but don't block
                data.Warnings = append(data.Warnings, "WARNING: No accessibility check was run before completion")
            }
            return data, nil
        },
    },

    // ═══════════════════════════════════════════════════════════════════════════════
    // POST-TOOL HOOKS (Validation & Recording)
    // ═══════════════════════════════════════════════════════════════════════════════
    {
        Name:     "validate_token_usage",
        Type:     PostTool,
        Priority: HookPriorityFirst,
        Trigger:  "designer_create_component,designer_extend_component",
        Handler:  func(ctx context.Context, data *ToolCallHookData) (*ToolCallHookData, error) {
            // Automatically validate token usage in created components
            componentCode := data.Result.Get("component_code")
            if componentCode != "" {
                violations := detectHardcodedValues(componentCode)
                if len(violations) > 0 {
                    data.Warnings = append(data.Warnings, fmt.Sprintf("TOKEN_VIOLATIONS: %v", violations))
                }
            }
            return data, nil
        },
    },
    {
        Name:     "auto_accessibility_check",
        Type:     PostTool,
        Priority: HookPriorityNormal,
        Trigger:  "designer_create_component,designer_extend_component",
        Handler:  func(ctx context.Context, data *ToolCallHookData) (*ToolCallHookData, error) {
            // Queue accessibility check for newly created components
            componentName := data.Result.Get("component_name")
            if componentName != "" {
                queueToolCall(ctx, "designer_check_accessibility", map[string]any{
                    "target":     componentName,
                    "wcag_level": "AA",
                })
            }
            return data, nil
        },
    },
    {
        Name:     "record_component_creation",
        Type:     PostTool,
        Priority: HookPriorityNormal,
        Trigger:  "designer_create_component",
        Handler:  func(ctx context.Context, data *ToolCallHookData) (*ToolCallHookData, error) {
            // Record component creation for Archivalist
            archivalist.StoreEntry(ctx, &Entry{
                Category: CategoryTimeline,
                Content:  fmt.Sprintf("Created component: %s", data.Result.Get("component_name")),
                Metadata: map[string]any{
                    "component": data.Result.Get("component_name"),
                    "category":  data.Input["category"],
                    "tokens":    data.Input["design_tokens"],
                },
            })
            return data, nil
        },
    },
    {
        Name:     "auto_emit_artifact",
        Type:     PostTool,
        Priority: HookPriorityLate,
        Trigger:  "designer_create_component,designer_extend_component,designer_create_layout",
        Handler:  func(ctx context.Context, data *ToolCallHookData) (*ToolCallHookData, error) {
            // Automatically emit artifacts for potential downstream pipelines
            pipeline := ctx.Value("pipeline").(*Pipeline)
            if pipeline != nil {
                artifact := buildArtifactFromResult(data.ToolName, data.Result)
                if artifact != nil {
                    pipeline.AddArtifact(artifact)
                }
            }
            return data, nil
        },
    },
    {
        Name:     "file_operation_tracking",
        Type:     PostTool,
        Priority: HookPriorityLast,
        Handler:  func(ctx context.Context, data *ToolCallHookData) (*ToolCallHookData, error) {
            // Track all file operations for Archivalist
            if isFileModifyingTool(data.ToolName) {
                files := extractModifiedFiles(data)
                for _, file := range files {
                    archivalist.RecordFileOperation(ctx, file, data.ToolName)
                }
            }
            return data, nil
        },
    },
}

// Helper functions for Designer hooks
func detectHardcodedValues(code string) []string {
    violations := []string{}
    // Detect hardcoded colors (hex, rgb, hsl)
    if matched, _ := regexp.MatchString(`#[0-9a-fA-F]{3,8}`, code); matched {
        violations = append(violations, "hardcoded hex color")
    }
    if matched, _ := regexp.MatchString(`rgb\(|rgba\(|hsl\(|hsla\(`, code); matched {
        violations = append(violations, "hardcoded color function")
    }
    // Detect hardcoded pixel values (but allow 0 and 1px for borders)
    if matched, _ := regexp.MatchString(`[^01]\d+px`, code); matched {
        violations = append(violations, "hardcoded pixel value")
    }
    return violations
}

func buildArtifactFromResult(toolName string, result *ToolResult) *PipelineArtifact {
    switch toolName {
    case "designer_create_component":
        return &PipelineArtifact{
            Type: "component",
            Data: map[string]any{
                "name":   result.Get("component_name"),
                "path":   result.Get("file_path"),
                "props":  result.Get("props_interface"),
                "exports": result.Get("exports"),
            },
        }
    case "designer_create_layout":
        return &PipelineArtifact{
            Type: "layout",
            Data: map[string]any{
                "name":    result.Get("layout_name"),
                "regions": result.Get("regions"),
            },
        }
    default:
        return nil
    }
}
```

---

## Dynamic Tool Discovery Protocol

**CRITICAL: Agents MUST NOT rely on hardcoded tool lists. Tool discovery is dynamic and follows a cascading consultation pattern.**

The Engineer, Inspector, and Tester agents need to run code quality tools (linters, formatters, type checkers, test frameworks) but maintaining hardcoded tool registries is unsustainable. Instead, tools are discovered dynamically through a three-tier escalation protocol.

### Discovery Escalation Chain

```
┌─────────────────────────────────────────────────────────────────────────────────────┐
│                        DYNAMIC TOOL DISCOVERY PROTOCOL                               │
├─────────────────────────────────────────────────────────────────────────────────────┤
│                                                                                     │
│  TIER 1: LIBRARIAN CONSULTATION ("What IS configured?")                             │
│  ┌──────────────────────────────────────────────────────────────────────────────┐   │
│  │                                                                              │   │
│  │  Agent → Librarian: "What linters/formatters/test frameworks exist here?"    │   │
│  │                                                                              │   │
│  │  Librarian inspects:                                                         │   │
│  │  ├── Config files (.eslintrc, golangci.yml, pyproject.toml, etc.)            │   │
│  │  ├── Package manifests (package.json devDependencies, requirements-dev.txt) │   │
│  │  ├── CI/CD configs (.github/workflows/*.yml, Makefile, etc.)                 │   │
│  │  └── IDE settings (.vscode/settings.json, .idea/*.xml)                       │   │
│  │                                                                              │   │
│  │  Response includes:                                                          │   │
│  │  ├── Detected tools with exact versions                                      │   │
│  │  ├── Configuration file locations                                            │   │
│  │  ├── Run commands (from scripts, Makefile targets, etc.)                     │   │
│  │  └── Confidence score (HIGH if config found, LOW if inferred)                │   │
│  │                                                                              │   │
│  │  IF confidence >= HIGH: Use discovered tools directly                        │   │
│  │  IF confidence < HIGH: Escalate to Tier 2                                    │   │
│  │                                                                              │   │
│  └──────────────────────────────────────────────────────────────────────────────┘   │
│                                        │                                            │
│                                        ▼ (low confidence)                           │
│                                                                                     │
│  TIER 2: ACADEMIC RESEARCH ("What SHOULD be used?")                                 │
│  ┌──────────────────────────────────────────────────────────────────────────────┐   │
│  │                                                                              │   │
│  │  Librarian → Academic: "What tools are recommended for [Go, TypeScript]?"    │   │
│  │                                                                              │   │
│  │  Academic researches:                                                        │   │
│  │  ├── Current best practices (2024-2025 recommendations)                      │   │
│  │  ├── Community standards (official style guides, popular choices)            │   │
│  │  ├── Tool maturity and maintenance status                                    │   │
│  │  └── Compatibility considerations                                            │   │
│  │                                                                              │   │
│  │  Response includes:                                                          │   │
│  │  ├── Recommended tools with rationale                                        │   │
│  │  ├── Installation commands                                                   │   │
│  │  ├── Default configurations                                                  │   │
│  │  └── Satisfactory flag (TRUE if clear best practice, FALSE if ambiguous)     │   │
│  │                                                                              │   │
│  │  IF satisfactory == TRUE: Use recommended tools                              │   │
│  │  IF satisfactory == FALSE: Escalate to Tier 3                                │   │
│  │                                                                              │   │
│  └──────────────────────────────────────────────────────────────────────────────┘   │
│                                        │                                            │
│                                        ▼ (unsatisfactory)                           │
│                                                                                     │
│  TIER 3: USER DECISION ("What do YOU want?")                                        │
│  ┌──────────────────────────────────────────────────────────────────────────────┐   │
│  │                                                                              │   │
│  │  Guide → User: "Multiple options exist for TypeScript linting:"              │   │
│  │                                                                              │   │
│  │  Presents options with context:                                              │   │
│  │  ├── Option A: ESLint (most configurable, largest ecosystem)                 │   │
│  │  ├── Option B: Biome (fastest, all-in-one)                                   │   │
│  │  ├── Option C: TypeScript compiler only (minimal, built-in)                  │   │
│  │  └── Option D: User specifies custom tool                                    │   │
│  │                                                                              │   │
│  │  User selection is:                                                          │   │
│  │  ├── Applied immediately                                                     │   │
│  │  ├── Stored in Archivalist as user preference                                │   │
│  │  └── Used for future sessions (cross-session knowledge)                      │   │
│  │                                                                              │   │
│  └──────────────────────────────────────────────────────────────────────────────┘   │
│                                                                                     │
└─────────────────────────────────────────────────────────────────────────────────────┘
```

### Protocol Data Structures

```go
// ToolDiscoveryRequest initiates the discovery protocol
type ToolDiscoveryRequest struct {
    SessionID     string           `json:"session_id"`
    RequestingAgent string         `json:"requesting_agent"` // "engineer", "inspector", "tester"
    ToolCategory  ToolCategory     `json:"tool_category"`    // linter, formatter, type_checker, test_framework
    TargetPath    string           `json:"target_path"`      // File or directory to analyze
    Languages     []string         `json:"languages,omitempty"` // If already known
}

type ToolCategory string

const (
    ToolCategoryLinter      ToolCategory = "linter"
    ToolCategoryFormatter   ToolCategory = "formatter"
    ToolCategoryTypeChecker ToolCategory = "type_checker"
    ToolCategoryTestFramework ToolCategory = "test_framework"
    ToolCategoryLSP         ToolCategory = "lsp"
)

// ToolDiscoveryResponse returns discovered tools
type ToolDiscoveryResponse struct {
    Tier          int              `json:"tier"`             // 1, 2, or 3
    Confidence    ConfidenceLevel  `json:"confidence"`
    Tools         []DiscoveredTool `json:"tools"`
    RequiresUser  bool             `json:"requires_user"`    // True if Tier 3 escalation needed
    UserOptions   []ToolOption     `json:"user_options,omitempty"`
}

type ConfidenceLevel string

const (
    ConfidenceHigh   ConfidenceLevel = "high"   // Config file found
    ConfidenceMedium ConfidenceLevel = "medium" // Inferred from dependencies
    ConfidenceLow    ConfidenceLevel = "low"    // Language-only detection
)

// DiscoveredTool represents a tool found through discovery
type DiscoveredTool struct {
    Name          string            `json:"name"`
    Category      ToolCategory      `json:"category"`
    Language      string            `json:"language"`
    Version       string            `json:"version,omitempty"`
    ConfigPath    string            `json:"config_path,omitempty"`
    RunCommand    string            `json:"run_command"`
    InstallCmd    string            `json:"install_cmd,omitempty"`
    IsInstalled   bool              `json:"is_installed"`
    Source        string            `json:"source"` // "config_file", "package_manifest", "academic_research", "user_choice"
    Rationale     string            `json:"rationale,omitempty"`
}

// ToolOption for user selection (Tier 3)
type ToolOption struct {
    Tool          DiscoveredTool    `json:"tool"`
    Pros          []string          `json:"pros"`
    Cons          []string          `json:"cons"`
    Recommended   bool              `json:"recommended"`
}
```

### Librarian Detection Patterns

The Librarian uses pattern matching to detect existing tool configurations:

```go
// ToolDetectionPatterns maps config files to tools
var ToolDetectionPatterns = map[string]ToolDetectionRule{
    // Linters
    ".eslintrc":           {Tool: "eslint", Category: ToolCategoryLinter, Language: "javascript"},
    ".eslintrc.js":        {Tool: "eslint", Category: ToolCategoryLinter, Language: "javascript"},
    ".eslintrc.json":      {Tool: "eslint", Category: ToolCategoryLinter, Language: "javascript"},
    "eslint.config.js":    {Tool: "eslint", Category: ToolCategoryLinter, Language: "javascript"},
    ".golangci.yml":       {Tool: "golangci-lint", Category: ToolCategoryLinter, Language: "go"},
    ".golangci.yaml":      {Tool: "golangci-lint", Category: ToolCategoryLinter, Language: "go"},
    "ruff.toml":           {Tool: "ruff", Category: ToolCategoryLinter, Language: "python"},
    ".ruff.toml":          {Tool: "ruff", Category: ToolCategoryLinter, Language: "python"},
    "clippy.toml":         {Tool: "clippy", Category: ToolCategoryLinter, Language: "rust"},

    // Formatters
    ".prettierrc":         {Tool: "prettier", Category: ToolCategoryFormatter, Language: "javascript"},
    ".prettierrc.js":      {Tool: "prettier", Category: ToolCategoryFormatter, Language: "javascript"},
    "prettier.config.js":  {Tool: "prettier", Category: ToolCategoryFormatter, Language: "javascript"},
    "rustfmt.toml":        {Tool: "rustfmt", Category: ToolCategoryFormatter, Language: "rust"},
    ".editorconfig":       {Tool: "editorconfig", Category: ToolCategoryFormatter, Language: "*"},

    // Type checkers
    "tsconfig.json":       {Tool: "tsc", Category: ToolCategoryTypeChecker, Language: "typescript"},
    "pyrightconfig.json":  {Tool: "pyright", Category: ToolCategoryTypeChecker, Language: "python"},
    "mypy.ini":            {Tool: "mypy", Category: ToolCategoryTypeChecker, Language: "python"},

    // Test frameworks
    "jest.config.js":      {Tool: "jest", Category: ToolCategoryTestFramework, Language: "javascript"},
    "jest.config.ts":      {Tool: "jest", Category: ToolCategoryTestFramework, Language: "typescript"},
    "vitest.config.ts":    {Tool: "vitest", Category: ToolCategoryTestFramework, Language: "typescript"},
    "pytest.ini":          {Tool: "pytest", Category: ToolCategoryTestFramework, Language: "python"},
    "setup.cfg":           {Tool: "pytest", Category: ToolCategoryTestFramework, Language: "python", ParseSection: "[tool:pytest]"},
    ".mocharc.js":         {Tool: "mocha", Category: ToolCategoryTestFramework, Language: "javascript"},
}

// PackageManifestRules for inferring tools from dependencies
var PackageManifestRules = map[string][]PackageRule{
    "package.json": {
        {Package: "eslint", Tool: "eslint", Category: ToolCategoryLinter},
        {Package: "prettier", Tool: "prettier", Category: ToolCategoryFormatter},
        {Package: "jest", Tool: "jest", Category: ToolCategoryTestFramework},
        {Package: "vitest", Tool: "vitest", Category: ToolCategoryTestFramework},
        {Package: "mocha", Tool: "mocha", Category: ToolCategoryTestFramework},
        {Package: "typescript", Tool: "tsc", Category: ToolCategoryTypeChecker},
        {Package: "biome", Tool: "biome", Category: ToolCategoryLinter}, // Also formatter
    },
    "pyproject.toml": {
        {Package: "ruff", Tool: "ruff", Category: ToolCategoryLinter},
        {Package: "black", Tool: "black", Category: ToolCategoryFormatter},
        {Package: "pytest", Tool: "pytest", Category: ToolCategoryTestFramework},
        {Package: "mypy", Tool: "mypy", Category: ToolCategoryTypeChecker},
        {Package: "pyright", Tool: "pyright", Category: ToolCategoryTypeChecker},
    },
    "go.mod": {
        // Go tools are typically not in go.mod, check for tool binaries or Makefile
    },
    "Cargo.toml": {
        // Rust tools are typically via cargo, check for [dev-dependencies]
    },
}
```

### Caching and Persistence

Discovered tools are cached in the Archivalist to avoid repeated discovery:

```go
// ToolDiscoveryCache entry stored in Archivalist
type ToolDiscoveryCacheEntry struct {
    ID            string            `json:"id"`
    Category      Category          `json:"category"` // CategoryToolDiscovery
    SessionID     string            `json:"session_id"`
    ProjectPath   string            `json:"project_path"`
    DiscoveredAt  time.Time         `json:"discovered_at"`
    Tools         []DiscoveredTool  `json:"tools"`
    Tier          int               `json:"tier"`
    UserOverrides map[string]string `json:"user_overrides,omitempty"` // Category -> chosen tool
}

// Cache invalidation triggers
// - File change in config files (via Librarian file watcher)
// - User explicitly requests re-discovery
// - Session references different branch/commit
```

### Protocol Sequence Diagram

```
Engineer/Inspector/Tester              Librarian                    Academic                      User
         │                                │                            │                            │
         │  ToolDiscoveryRequest          │                            │                            │
         │  (category=linter, path=/src)  │                            │                            │
         │ ──────────────────────────────>│                            │                            │
         │                                │                            │                            │
         │                                │ [Scan for config files]    │                            │
         │                                │ [Parse package manifests]  │                            │
         │                                │                            │                            │
         │                     ┌──────────┴──────────┐                 │                            │
         │                     │ Config found?       │                 │                            │
         │                     └──────────┬──────────┘                 │                            │
         │                                │                            │                            │
         │               ┌────────────────┴────────────────┐           │                            │
         │               │                                 │           │                            │
         │             [YES]                             [NO]          │                            │
         │               │                                 │           │                            │
         │               │                                 │ "What tools for [Go]?"                 │
         │               │                                 │ ─────────────────────>│                │
         │               │                                 │                       │                │
         │               │                                 │            [Research] │                │
         │               │                                 │                       │                │
         │               │                                 │         ┌─────────────┴─────────────┐  │
         │               │                                 │         │ Clear best practice?      │  │
         │               │                                 │         └─────────────┬─────────────┘  │
         │               │                                 │                       │                │
         │               │                                 │         ┌─────────────┴─────────────┐  │
         │               │                                 │       [YES]                       [NO] │
         │               │                                 │         │                           │  │
         │               │                                 │<────────┘                           │  │
         │               │                                 │                                     │  │
         │               │                                 │              "Choose linter:"       │  │
         │               │                                 │              ─────────────────────────>│
         │               │                                 │                                     │  │
         │               │                                 │                          [User picks] │
         │               │                                 │<──────────────────────────────────────┘
         │               │                                 │                                        │
         │               ▼                                 ▼                                        │
         │<──────────────────────────────────────────────────                                       │
         │  ToolDiscoveryResponse (tools, commands, etc.)                                           │
         │                                                                                          │
         │ [Install if needed]                                                                      │
         │ [Run tool]                                                                               │
         │                                                                                          │
```

### Language Defaults Registry

When no tools are configured and the user doesn't specify preferences, these defaults are recommended. These represent modern, fast, well-maintained tools as of 2025.

```go
// LanguageDefaults defines recommended tools per language
// Used by Academic when research yields no clear best practice
// Used as recommendations when presenting options to users
var LanguageDefaults = map[string]LanguageToolset{
    "go": {
        PackageManager: "go mod",
        Linter:         ToolDefault{Name: "golangci-lint", InstallCmd: "go install github.com/golangci/golangci-lint/cmd/golangci-lint@latest"},
        Formatter:      ToolDefault{Name: "gofmt", InstallCmd: ""}, // Built-in
        TypeChecker:    ToolDefault{Name: "go vet", InstallCmd: ""}, // Built-in
        LSP:            ToolDefault{Name: "gopls", InstallCmd: "go install golang.org/x/tools/gopls@latest"},
        TestFramework:  ToolDefault{Name: "go test", InstallCmd: ""}, // Built-in
    },
    "python": {
        PackageManager: "uv", // Preferred over pip for speed; fallback to pip if unavailable
        Linter:         ToolDefault{Name: "ruff", InstallCmd: "uv pip install ruff || pip install ruff"},
        Formatter:      ToolDefault{Name: "ruff", InstallCmd: "uv pip install ruff || pip install ruff"}, // ruff format
        TypeChecker:    ToolDefault{Name: "ruff", InstallCmd: "uv pip install ruff || pip install ruff"}, // ruff includes type checking
        LSP:            ToolDefault{Name: "ruff", InstallCmd: "uv pip install ruff-lsp || pip install ruff-lsp"}, // ruff-lsp
        TestFramework:  ToolDefault{Name: "pytest", InstallCmd: "uv pip install pytest || pip install pytest"},
    },
    "javascript": {
        PackageManager: "npm", // Or yarn/pnpm if detected
        Linter:         ToolDefault{Name: "oxlint", InstallCmd: "npm install -D oxlint"}, // Fastest JS linter
        Formatter:      ToolDefault{Name: "prettier", InstallCmd: "npm install -D prettier"},
        TypeChecker:    ToolDefault{Name: "tsc", InstallCmd: "npm install -D typescript"}, // For .ts files
        LSP:            ToolDefault{Name: "typescript-language-server", InstallCmd: "npm install -D typescript-language-server"},
        TestFramework:  ToolDefault{Name: "vitest", InstallCmd: "npm install -D vitest"}, // Modern, fast
    },
    "typescript": {
        PackageManager: "npm",
        Linter:         ToolDefault{Name: "oxlint", InstallCmd: "npm install -D oxlint"},
        Formatter:      ToolDefault{Name: "prettier", InstallCmd: "npm install -D prettier"},
        TypeChecker:    ToolDefault{Name: "tsc", InstallCmd: "npm install -D typescript"},
        LSP:            ToolDefault{Name: "typescript-language-server", InstallCmd: "npm install -D typescript-language-server"},
        TestFramework:  ToolDefault{Name: "vitest", InstallCmd: "npm install -D vitest"},
    },
    "rust": {
        PackageManager: "cargo",
        Linter:         ToolDefault{Name: "clippy", InstallCmd: "rustup component add clippy"},
        Formatter:      ToolDefault{Name: "rustfmt", InstallCmd: "rustup component add rustfmt"},
        TypeChecker:    ToolDefault{Name: "cargo check", InstallCmd: ""}, // Built-in
        LSP:            ToolDefault{Name: "rust-analyzer", InstallCmd: "rustup component add rust-analyzer"},
        TestFramework:  ToolDefault{Name: "cargo test", InstallCmd: ""}, // Built-in
    },
    "ruby": {
        PackageManager: "bundler",
        Linter:         ToolDefault{Name: "rubocop", InstallCmd: "gem install rubocop"},
        Formatter:      ToolDefault{Name: "rubocop", InstallCmd: "gem install rubocop"}, // rubocop -a
        TypeChecker:    ToolDefault{Name: "sorbet", InstallCmd: "gem install sorbet"},
        LSP:            ToolDefault{Name: "ruby-lsp", InstallCmd: "gem install ruby-lsp"},
        TestFramework:  ToolDefault{Name: "rspec", InstallCmd: "gem install rspec"},
    },
    "java": {
        PackageManager: "maven", // Or gradle if detected
        Linter:         ToolDefault{Name: "checkstyle", InstallCmd: ""}, // Usually via Maven/Gradle plugin
        Formatter:      ToolDefault{Name: "google-java-format", InstallCmd: ""},
        TypeChecker:    ToolDefault{Name: "javac", InstallCmd: ""}, // Built-in
        LSP:            ToolDefault{Name: "jdtls", InstallCmd: ""}, // Eclipse JDT Language Server
        TestFramework:  ToolDefault{Name: "junit", InstallCmd: ""}, // Via Maven/Gradle
    },
    "kotlin": {
        PackageManager: "gradle",
        Linter:         ToolDefault{Name: "ktlint", InstallCmd: ""},
        Formatter:      ToolDefault{Name: "ktlint", InstallCmd: ""},
        TypeChecker:    ToolDefault{Name: "kotlinc", InstallCmd: ""},
        LSP:            ToolDefault{Name: "kotlin-language-server", InstallCmd: ""},
        TestFramework:  ToolDefault{Name: "junit", InstallCmd: ""},
    },
    "c": {
        PackageManager: "", // System-dependent
        Linter:         ToolDefault{Name: "clang-tidy", InstallCmd: ""},
        Formatter:      ToolDefault{Name: "clang-format", InstallCmd: ""},
        TypeChecker:    ToolDefault{Name: "clang", InstallCmd: ""},
        LSP:            ToolDefault{Name: "clangd", InstallCmd: ""},
        TestFramework:  ToolDefault{Name: "ctest", InstallCmd: ""},
    },
    "cpp": {
        PackageManager: "", // System-dependent
        Linter:         ToolDefault{Name: "clang-tidy", InstallCmd: ""},
        Formatter:      ToolDefault{Name: "clang-format", InstallCmd: ""},
        TypeChecker:    ToolDefault{Name: "clang", InstallCmd: ""},
        LSP:            ToolDefault{Name: "clangd", InstallCmd: ""},
        TestFramework:  ToolDefault{Name: "gtest", InstallCmd: ""},
    },
    "csharp": {
        PackageManager: "dotnet",
        Linter:         ToolDefault{Name: "dotnet format", InstallCmd: ""},
        Formatter:      ToolDefault{Name: "dotnet format", InstallCmd: ""},
        TypeChecker:    ToolDefault{Name: "dotnet build", InstallCmd: ""},
        LSP:            ToolDefault{Name: "omnisharp", InstallCmd: ""},
        TestFramework:  ToolDefault{Name: "dotnet test", InstallCmd: ""},
    },
    "elixir": {
        PackageManager: "mix",
        Linter:         ToolDefault{Name: "credo", InstallCmd: "mix deps.get"},
        Formatter:      ToolDefault{Name: "mix format", InstallCmd: ""}, // Built-in
        TypeChecker:    ToolDefault{Name: "dialyzer", InstallCmd: ""},
        LSP:            ToolDefault{Name: "elixir-ls", InstallCmd: ""},
        TestFramework:  ToolDefault{Name: "mix test", InstallCmd: ""}, // Built-in
    },
    "bash": {
        PackageManager: "",
        Linter:         ToolDefault{Name: "shellcheck", InstallCmd: ""},
        Formatter:      ToolDefault{Name: "shfmt", InstallCmd: "go install mvdan.cc/sh/v3/cmd/shfmt@latest"},
        TypeChecker:    ToolDefault{Name: "", InstallCmd: ""}, // N/A
        LSP:            ToolDefault{Name: "bash-language-server", InstallCmd: "npm install -g bash-language-server"},
        TestFramework:  ToolDefault{Name: "bats", InstallCmd: ""},
    },
    "php": {
        PackageManager: "composer",
        Linter:         ToolDefault{Name: "phpstan", InstallCmd: "composer require --dev phpstan/phpstan"},
        Formatter:      ToolDefault{Name: "php-cs-fixer", InstallCmd: "composer require --dev friendsofphp/php-cs-fixer"},
        TypeChecker:    ToolDefault{Name: "phpstan", InstallCmd: ""},
        LSP:            ToolDefault{Name: "intelephense", InstallCmd: "npm install -g intelephense"},
        TestFramework:  ToolDefault{Name: "phpunit", InstallCmd: "composer require --dev phpunit/phpunit"},
    },
    "swift": {
        PackageManager: "swift package",
        Linter:         ToolDefault{Name: "swiftlint", InstallCmd: "brew install swiftlint"},
        Formatter:      ToolDefault{Name: "swift-format", InstallCmd: "brew install swift-format"},
        TypeChecker:    ToolDefault{Name: "swiftc", InstallCmd: ""}, // Built-in
        LSP:            ToolDefault{Name: "sourcekit-lsp", InstallCmd: ""}, // Included with Xcode
        TestFramework:  ToolDefault{Name: "swift test", InstallCmd: ""}, // Built-in
    },
    "zig": {
        PackageManager: "zig",
        Linter:         ToolDefault{Name: "zig", InstallCmd: ""}, // zig build has lint-like features
        Formatter:      ToolDefault{Name: "zig fmt", InstallCmd: ""}, // Built-in
        TypeChecker:    ToolDefault{Name: "zig", InstallCmd: ""}, // Built-in
        LSP:            ToolDefault{Name: "zls", InstallCmd: ""},
        TestFramework:  ToolDefault{Name: "zig test", InstallCmd: ""}, // Built-in
    },
    "ocaml": {
        PackageManager: "opam",
        Linter:         ToolDefault{Name: "", InstallCmd: ""},
        Formatter:      ToolDefault{Name: "ocamlformat", InstallCmd: "opam install ocamlformat"},
        TypeChecker:    ToolDefault{Name: "ocaml", InstallCmd: ""}, // Built-in
        LSP:            ToolDefault{Name: "ocaml-lsp", InstallCmd: "opam install ocaml-lsp-server"},
        TestFramework:  ToolDefault{Name: "alcotest", InstallCmd: "opam install alcotest"},
    },
    "dart": {
        PackageManager: "pub",
        Linter:         ToolDefault{Name: "dart analyze", InstallCmd: ""}, // Built-in
        Formatter:      ToolDefault{Name: "dart format", InstallCmd: ""}, // Built-in
        TypeChecker:    ToolDefault{Name: "dart analyze", InstallCmd: ""}, // Built-in
        LSP:            ToolDefault{Name: "dart", InstallCmd: ""}, // Built-in language server
        TestFramework:  ToolDefault{Name: "dart test", InstallCmd: ""}, // Built-in
    },
    "terraform": {
        PackageManager: "",
        Linter:         ToolDefault{Name: "tflint", InstallCmd: ""},
        Formatter:      ToolDefault{Name: "terraform fmt", InstallCmd: ""}, // Built-in
        TypeChecker:    ToolDefault{Name: "terraform validate", InstallCmd: ""}, // Built-in
        LSP:            ToolDefault{Name: "terraform-ls", InstallCmd: ""},
        TestFramework:  ToolDefault{Name: "terratest", InstallCmd: ""},
    },
    "nix": {
        PackageManager: "nix",
        Linter:         ToolDefault{Name: "statix", InstallCmd: "nix-env -iA nixpkgs.statix"},
        Formatter:      ToolDefault{Name: "nixfmt", InstallCmd: "nix-env -iA nixpkgs.nixfmt"},
        TypeChecker:    ToolDefault{Name: "", InstallCmd: ""},
        LSP:            ToolDefault{Name: "nixd", InstallCmd: "nix-env -iA nixpkgs.nixd"},
        TestFramework:  ToolDefault{Name: "", InstallCmd: ""},
    },
    "vue": {
        PackageManager: "npm",
        Linter:         ToolDefault{Name: "eslint", InstallCmd: "npm install -D eslint eslint-plugin-vue"},
        Formatter:      ToolDefault{Name: "prettier", InstallCmd: "npm install -D prettier"},
        TypeChecker:    ToolDefault{Name: "vue-tsc", InstallCmd: "npm install -D vue-tsc"},
        LSP:            ToolDefault{Name: "vue-language-server", InstallCmd: "npm install -D @vue/language-server"},
        TestFramework:  ToolDefault{Name: "vitest", InstallCmd: "npm install -D vitest"},
    },
    "svelte": {
        PackageManager: "npm",
        Linter:         ToolDefault{Name: "eslint", InstallCmd: "npm install -D eslint eslint-plugin-svelte"},
        Formatter:      ToolDefault{Name: "prettier", InstallCmd: "npm install -D prettier prettier-plugin-svelte"},
        TypeChecker:    ToolDefault{Name: "svelte-check", InstallCmd: "npm install -D svelte-check"},
        LSP:            ToolDefault{Name: "svelte-language-server", InstallCmd: "npm install -D svelte-language-server"},
        TestFramework:  ToolDefault{Name: "vitest", InstallCmd: "npm install -D vitest"},
    },
}

type LanguageToolset struct {
    PackageManager string
    Linter         ToolDefault
    Formatter      ToolDefault
    TypeChecker    ToolDefault
    LSP            ToolDefault
    TestFramework  ToolDefault
}

type ToolDefault struct {
    Name       string
    InstallCmd string
}
```

**Key Recommendations:**
- **Python**: Use `uv` for package management (10-100x faster than pip), `ruff` for everything else (linting, formatting, type checking via ruff-lsp)
- **JavaScript/TypeScript**: Use `oxlint` for linting (fastest), `prettier` for formatting, `vitest` for testing (modern, fast)
- **Go**: Built-in tools are excellent; `golangci-lint` aggregates multiple linters
- **Rust**: Official toolchain components are preferred (clippy, rustfmt, rust-analyzer)

### Integration with Agents

Each agent integrates with the protocol through a common skill:

```go
// discover_tools - Common skill for Engineer, Inspector, Tester
skills.NewSkill("discover_tools").
    Description("Discover appropriate tools for the target code via Librarian consultation").
    Domain("tooling").
    Keywords("discover", "tools", "linter", "formatter", "test").
    EnumParam("category", "Tool category", []string{"linter", "formatter", "type_checker", "test_framework", "lsp"}, true).
    StringParam("path", "Target path to analyze", true).
    BoolParam("force_refresh", "Bypass cache and re-discover", false)
```

---

## Lazy Tool Loading

**CRITICAL: This is a universal meta-skill available to ALL agents for on-demand tool schema loading.**

Lazy Tool Loading addresses a fundamental token efficiency problem: agents receive **all** tool definitions on every turn, even when most tools won't be used. This skill enables agents to start with a minimal tool set and load additional tools on demand.

### Problem Statement

```
┌─────────────────────────────────────────────────────────────────────────────────────┐
│                         TOKEN COST: EAGER VS LAZY LOADING                            │
├─────────────────────────────────────────────────────────────────────────────────────┤
│                                                                                     │
│  EAGER LOADING (Current):                                                           │
│  ┌──────────────────────────────────────────────────────────────────────────────┐   │
│  │  Every turn, agent receives ALL tool schemas:                                │   │
│  │                                                                              │   │
│  │  Agent        Tools    Tokens/Tool    Total Cost    Turns    Session Cost   │   │
│  │  ──────────   ─────    ───────────    ──────────    ─────    ────────────   │   │
│  │  Engineer     15       ~200           3,000         20       60,000         │   │
│  │  Designer     30       ~200           6,000         15       90,000         │   │
│  │  Archivalist  20       ~200           4,000         25       100,000        │   │
│  │  Inspector    12       ~200           2,400         10       24,000         │   │
│  │                                                                              │   │
│  │  Problem: Agent uses 2-5 tools per turn but pays for ALL tools              │   │
│  └──────────────────────────────────────────────────────────────────────────────┘   │
│                                                                                     │
│  LAZY LOADING (Proposed):                                                           │
│  ┌──────────────────────────────────────────────────────────────────────────────┐   │
│  │  Agent starts with manifest + core tools, loads on demand:                   │   │
│  │                                                                              │   │
│  │  Component           Tokens                                                  │   │
│  │  ─────────────────   ──────                                                  │   │
│  │  Tool manifest       ~400 (names + 1-line descriptions)                      │   │
│  │  Core tools (2-3)    ~400                                                    │   │
│  │  request_tools       ~100                                                    │   │
│  │  ──────────────────────────                                                  │   │
│  │  Bootstrap total     ~900 tokens (vs 3,000-6,000)                            │   │
│  │                                                                              │   │
│  │  On-demand load: +200 tokens per tool when needed                            │   │
│  │  Average task: 3-5 tools = 600-1,000 additional tokens                       │   │
│  │  Total per turn: ~1,500-2,000 tokens (vs 3,000-6,000)                         │   │
│  │                                                                              │   │
│  │  SAVINGS: 50-70% token reduction per turn                                    │   │
│  └──────────────────────────────────────────────────────────────────────────────┘   │
│                                                                                     │
└─────────────────────────────────────────────────────────────────────────────────────┘
```

### Architecture Overview

```
┌─────────────────────────────────────────────────────────────────────────────────────┐
│                         LAZY TOOL LOADING ARCHITECTURE                               │
├─────────────────────────────────────────────────────────────────────────────────────┤
│                                                                                     │
│  ┌─────────────────────────────────────────────────────────────────────────────┐    │
│  │                           TOOL REGISTRY                                      │    │
│  │  ┌─────────────────┐  ┌─────────────────┐  ┌─────────────────┐              │    │
│  │  │ Tool Manifest   │  │ Tool Schemas    │  │ Tool Bundles    │              │    │
│  │  │                 │  │                 │  │                 │              │    │
│  │  │ • name          │  │ • Full JSON     │  │ • "component"   │              │    │
│  │  │ • description   │  │   schema per    │  │   bundle        │              │    │
│  │  │ • category      │  │   tool          │  │ • "a11y" bundle │              │    │
│  │  │ • bundle_hint   │  │ • Lazy loaded   │  │ • "layout"      │              │    │
│  │  │                 │  │                 │  │   bundle        │              │    │
│  │  └────────┬────────┘  └────────┬────────┘  └────────┬────────┘              │    │
│  │           │                    │                    │                       │    │
│  └───────────┼────────────────────┼────────────────────┼───────────────────────┘    │
│              │                    │                    │                            │
│              ▼                    ▼                    ▼                            │
│  ┌─────────────────────────────────────────────────────────────────────────────┐    │
│  │                        TOOL LOADER SERVICE                                   │    │
│  │                                                                              │    │
│  │  LoadTools(agentID, toolNames) → []ToolSchema                                │    │
│  │  LoadBundle(agentID, bundleName) → []ToolSchema                              │    │
│  │  GetManifest(agentID) → ToolManifest                                         │    │
│  │  GetLoadedTools(agentID, sessionID) → []string                               │    │
│  │                                                                              │    │
│  └──────────────────────────────────┬──────────────────────────────────────────┘    │
│                                     │                                               │
│              ┌──────────────────────┼──────────────────────┐                        │
│              │                      │                      │                        │
│              ▼                      ▼                      ▼                        │
│  ┌───────────────────┐  ┌───────────────────┐  ┌───────────────────┐               │
│  │ ENGINEER          │  │ DESIGNER          │  │ OTHER AGENTS      │               │
│  │                   │  │                   │  │                   │               │
│  │ Loaded:           │  │ Loaded:           │  │ Loaded:           │               │
│  │ • request_tools   │  │ • request_tools   │  │ • request_tools   │               │
│  │ • signal_complete │  │ • signal_complete │  │ • signal_complete │               │
│  │ • [on demand...]  │  │ • [on demand...]  │  │ • [on demand...]  │               │
│  │                   │  │                   │  │                   │               │
│  │ Manifest: 15 tools│  │ Manifest: 30 tools│  │ Manifest: N tools │               │
│  └───────────────────┘  └───────────────────┘  └───────────────────┘               │
│                                                                                     │
└─────────────────────────────────────────────────────────────────────────────────────┘
```

### Core Data Structures

```go
// ToolManifestEntry is a lightweight tool descriptor (~20 tokens each)
type ToolManifestEntry struct {
    Name        string   `json:"name"`
    Description string   `json:"description"`  // One line, <100 chars
    Category    string   `json:"category"`     // "component", "a11y", "layout", etc.
    BundleHint  string   `json:"bundle_hint"`  // Suggested bundle for co-loading
    Keywords    []string `json:"keywords"`     // For intent matching
}

// ToolManifest is the lightweight list sent to agents at bootstrap
type ToolManifest struct {
    AgentID     string              `json:"agent_id"`
    Tools       []ToolManifestEntry `json:"tools"`
    Bundles     []BundleEntry       `json:"bundles"`
    TotalTokens int                 `json:"total_tokens"`  // Manifest cost
}

// BundleEntry groups related tools for efficient loading
type BundleEntry struct {
    Name        string   `json:"name"`
    Description string   `json:"description"`
    Tools       []string `json:"tools"`      // Tool names in bundle
    UseCases    []string `json:"use_cases"`  // When to load this bundle
}

// LoadedToolSet tracks which tools an agent has loaded in a session
type LoadedToolSet struct {
    AgentID   string
    SessionID string

    CoreTools    []string          // Always loaded
    LoadedTools  map[string]bool   // On-demand loaded tools
    LoadHistory  []LoadEvent       // For learning patterns

    mu sync.RWMutex
}

// LoadEvent records tool loading for pattern learning
type LoadEvent struct {
    Timestamp  time.Time
    Tools      []string
    TaskType   string  // Inferred from context
    Successful bool    // Did the tools help complete the task?
}

// ToolLoaderConfig configures the lazy loading behavior
type ToolLoaderConfig struct {
    // Bootstrap configuration
    AlwaysLoadCore     bool     `json:"always_load_core"`      // Load core tools at start
    CoreTools          []string `json:"core_tools"`            // Tools always loaded

    // Loading behavior
    MaxToolsPerRequest int      `json:"max_tools_per_request"` // Limit per request_tools call
    EnableBundles      bool     `json:"enable_bundles"`        // Allow bundle loading
    EnableAutoSuggest  bool     `json:"enable_auto_suggest"`   // Suggest tools based on task

    // Caching
    CacheLoadedTools   bool          `json:"cache_loaded_tools"`   // Keep tools across turns
    CacheTTL           time.Duration `json:"cache_ttl"`            // How long to cache

    // Learning
    EnablePatternLearning bool `json:"enable_pattern_learning"` // Learn tool co-usage
}

func DefaultToolLoaderConfig() ToolLoaderConfig {
    return ToolLoaderConfig{
        AlwaysLoadCore:        true,
        CoreTools:             []string{"request_tools", "signal_complete"},
        MaxToolsPerRequest:    10,
        EnableBundles:         true,
        EnableAutoSuggest:     true,
        CacheLoadedTools:      true,
        CacheTTL:              30 * time.Minute,
        EnablePatternLearning: true,
    }
}
```

### The `request_tools` Universal Skill

```go
// request_tools - Universal meta-skill for all agents
var RequestToolsSkill = SkillDefinition{
    Name:        "request_tools",
    Description: "Load additional tools into your capabilities. Call this when you need tools not currently available.",
    Tier:        0,  // Always loaded, before Tier 1
    Domain:      "meta",
    Priority:    100,

    InputSchema: map[string]any{
        "type": "object",
        "properties": map[string]any{
            "tools": {
                "type":        "array",
                "items":       map[string]any{"type": "string"},
                "description": "List of tool names to load (from manifest)",
            },
            "bundle": {
                "type":        "string",
                "description": "Bundle name to load (loads all tools in bundle)",
            },
            "intent": {
                "type":        "string",
                "description": "Describe what you're trying to do (system suggests tools)",
            },
        },
        "oneOf": []map[string]any{
            {"required": []string{"tools"}},
            {"required": []string{"bundle"}},
            {"required": []string{"intent"}},
        },
    },

    // Response includes loaded tool schemas
    OutputSchema: map[string]any{
        "type": "object",
        "properties": map[string]any{
            "loaded": {
                "type":        "array",
                "items":       map[string]any{"type": "string"},
                "description": "Names of tools now available",
            },
            "schemas": {
                "type":        "array",
                "description": "Full schemas for loaded tools (injected next turn)",
            },
            "suggestions": {
                "type":        "array",
                "description": "Additional tools that might be helpful",
            },
        },
    },
}
```

### State Machine

```
┌─────────────────────────────────────────────────────────────────────────────────────┐
│                         LAZY TOOL LOADING STATE MACHINE                              │
├─────────────────────────────────────────────────────────────────────────────────────┤
│                                                                                     │
│                              ┌─────────────────┐                                    │
│                              │   BOOTSTRAP     │                                    │
│                              │                 │                                    │
│                              │ • Load manifest │                                    │
│                              │ • Load core     │                                    │
│                              │   tools         │                                    │
│                              └────────┬────────┘                                    │
│                                       │                                             │
│                                       ▼                                             │
│                              ┌─────────────────┐                                    │
│                              │     READY       │◄─────────────────────┐             │
│                              │                 │                      │             │
│                              │ Has: manifest + │                      │             │
│                              │ core tools +    │                      │             │
│                              │ any loaded      │                      │             │
│                              └────────┬────────┘                      │             │
│                                       │                               │             │
│                          Agent calls request_tools                    │             │
│                                       │                               │             │
│                                       ▼                               │             │
│                              ┌─────────────────┐                      │             │
│                              │   VALIDATING    │                      │             │
│                              │                 │                      │             │
│                              │ • Check tool    │                      │             │
│                              │   names exist   │                      │             │
│                              │ • Check limits  │                      │             │
│                              │ • Check perms   │                      │             │
│                              └────────┬────────┘                      │             │
│                                       │                               │             │
│                    ┌──────────────────┼──────────────────┐            │             │
│                    │                  │                  │            │             │
│                    ▼                  ▼                  ▼            │             │
│           ┌──────────────┐   ┌──────────────┐   ┌──────────────┐     │             │
│           │    ERROR     │   │   LOADING    │   │  SUGGESTING  │     │             │
│           │              │   │              │   │              │     │             │
│           │ Invalid tool │   │ Fetch full   │   │ Intent-based │     │             │
│           │ name or      │   │ schemas from │   │ tool suggest │     │             │
│           │ permission   │   │ registry     │   │ via embeddings│    │             │
│           │ denied       │   │              │   │              │     │             │
│           └──────┬───────┘   └──────┬───────┘   └──────┬───────┘     │             │
│                  │                  │                  │             │             │
│                  │                  ▼                  │             │             │
│                  │         ┌──────────────┐            │             │             │
│                  │         │   INJECTING  │◄───────────┘             │             │
│                  │         │              │                          │             │
│                  │         │ Add schemas  │                          │             │
│                  │         │ to agent's   │                          │             │
│                  │         │ next turn    │                          │             │
│                  │         │ context      │                          │             │
│                  │         └──────┬───────┘                          │             │
│                  │                │                                  │             │
│                  │                ▼                                  │             │
│                  │         ┌──────────────┐                          │             │
│                  │         │   CACHING    │                          │             │
│                  │         │              │                          │             │
│                  │         │ Update       │                          │             │
│                  │         │ LoadedToolSet│                          │             │
│                  │         │ Record event │                          │             │
│                  │         └──────┬───────┘                          │             │
│                  │                │                                  │             │
│                  └────────────────┼──────────────────────────────────┘             │
│                                   │                                                │
│                                   ▼                                                │
│                              ┌─────────────────┐                                   │
│                              │   RESPONDED     │                                   │
│                              │                 │                                   │
│                              │ Return result   │                                   │
│                              │ with loaded     │                                   │
│                              │ tool names      │                                   │
│                              └─────────────────┘                                   │
│                                                                                    │
└─────────────────────────────────────────────────────────────────────────────────────┘
```

### Loading Flow Sequence

```
┌─────────────────────────────────────────────────────────────────────────────────────┐
│                         LAZY TOOL LOADING SEQUENCE DIAGRAM                           │
├─────────────────────────────────────────────────────────────────────────────────────┤
│                                                                                     │
│  Agent          ToolLoader        Registry         Cache           Hooks            │
│    │                │                │               │               │              │
│    │  BOOTSTRAP     │                │               │               │              │
│    │◄───────────────┤                │               │               │              │
│    │  manifest +    │                │               │               │              │
│    │  core tools    │                │               │               │              │
│    │                │                │               │               │              │
│    │  Task arrives  │                │               │               │              │
│    │────────────────►                │               │               │              │
│    │                │                │               │               │              │
│    │  "I need component tools"       │               │               │              │
│    │                │                │               │               │              │
│    │  request_tools(bundle:"component")              │               │              │
│    │───────────────────────────────────────────────────────────────►│              │
│    │                │                │               │  pre-tool    │              │
│    │                │                │               │  validation  │              │
│    │                │◄───────────────────────────────────────────────│              │
│    │                │                │               │               │              │
│    │                │  GetBundle("component")        │               │              │
│    │                │───────────────►│               │               │              │
│    │                │                │               │               │              │
│    │                │  [search, create, extend,      │               │              │
│    │                │   read, edit]  │               │               │              │
│    │                │◄───────────────│               │               │              │
│    │                │                │               │               │              │
│    │                │  GetSchemas([tools])           │               │              │
│    │                │───────────────►│               │               │              │
│    │                │                │               │               │              │
│    │                │  [full schemas]│               │               │              │
│    │                │◄───────────────│               │               │              │
│    │                │                │               │               │              │
│    │                │  CacheTools(session, tools)    │               │              │
│    │                │───────────────────────────────►│               │              │
│    │                │                │               │               │              │
│    │                │  RecordLoadEvent               │               │              │
│    │                │───────────────────────────────────────────────►│              │
│    │                │                │               │  post-tool   │              │
│    │                │                │               │  hook        │              │
│    │                │                │               │               │              │
│    │  {loaded: [...], schemas: [...]}│               │               │              │
│    │◄──────────────────────────────────────────────────────────────│              │
│    │                │                │               │               │              │
│    │  NEXT TURN: schemas injected via pre-prompt hook               │              │
│    │◄───────────────────────────────────────────────────────────────│              │
│    │                │                │               │               │              │
│    │  Now can call: designer_search_components(...)                 │              │
│    │                │                │               │               │              │
│                                                                                     │
└─────────────────────────────────────────────────────────────────────────────────────┘
```

### Tool Bundles (Predefined)

```go
// Designer tool bundles
var DesignerBundles = []BundleEntry{
    {
        Name:        "component",
        Description: "Tools for component discovery and creation",
        Tools:       []string{
            "designer_search_components",
            "designer_create_component",
            "designer_extend_component",
            "designer_read_component",
            "designer_edit_component",
        },
        UseCases: []string{"create component", "new component", "build component"},
    },
    {
        Name:        "design_system",
        Description: "Tools for design system and tokens",
        Tools:       []string{
            "designer_get_design_tokens",
            "designer_validate_tokens",
            "designer_get_theme",
            "designer_create_token",
        },
        UseCases: []string{"design tokens", "theme", "colors", "spacing"},
    },
    {
        Name:        "accessibility",
        Description: "Tools for accessibility compliance",
        Tools:       []string{
            "designer_check_accessibility",
            "designer_check_color_contrast",
            "designer_generate_aria",
            "designer_test_keyboard_nav",
        },
        UseCases: []string{"accessibility", "a11y", "wcag", "aria", "contrast"},
    },
    {
        Name:        "layout",
        Description: "Tools for layout and responsive design",
        Tools:       []string{
            "designer_create_layout",
            "designer_get_breakpoints",
            "designer_preview_responsive",
        },
        UseCases: []string{"layout", "responsive", "grid", "breakpoints"},
    },
    {
        Name:        "preview",
        Description: "Tools for preview and visual testing",
        Tools:       []string{
            "designer_preview_component",
            "designer_run_visual_diff",
            "designer_capture_screenshot",
        },
        UseCases: []string{"preview", "screenshot", "visual", "diff"},
    },
}

// Engineer tool bundles
var EngineerBundles = []BundleEntry{
    {
        Name:        "file_ops",
        Description: "Tools for file operations",
        Tools:       []string{
            "engineer_read_file",
            "engineer_write_file",
            "engineer_edit_file",
        },
        UseCases: []string{"read", "write", "edit", "file", "modify"},
    },
    {
        Name:        "execution",
        Description: "Tools for running commands and tests",
        Tools:       []string{
            "engineer_run_command",
            "engineer_run_tests",
        },
        UseCases: []string{"run", "execute", "test", "command"},
    },
}
```

### Hook Integration

```go
// Pre-prompt hook: Inject tool manifest and loaded schemas
var InjectToolsHook = Hook{
    Name:     "inject_loaded_tools",
    Type:     PrePrompt,
    Priority: HookPriorityFirst,  // Run before other injections
    Handler: func(ctx context.Context, data *PromptHookData) (*PromptHookData, error) {
        agentID := ctx.Value("agent_id").(string)
        sessionID := ctx.Value("session_id").(string)

        loader := GetToolLoader()

        // Always inject manifest (lightweight)
        manifest := loader.GetManifest(agentID)
        data.SystemPrompt = injectManifestSection(data.SystemPrompt, manifest)

        // Inject schemas for loaded tools
        loadedTools := loader.GetLoadedTools(agentID, sessionID)
        if len(loadedTools.LoadedTools) > 0 {
            schemas := loader.GetSchemas(loadedTools.ToolNames())
            data.Tools = append(data.Tools, schemas...)
        }

        return data, nil
    },
}

// Pre-tool hook: Validate tool loading requests
var ValidateToolLoadHook = Hook{
    Name:     "validate_tool_load",
    Type:     PreTool,
    Priority: HookPriorityFirst,
    Trigger:  "request_tools",
    Handler: func(ctx context.Context, data *ToolCallHookData) (*ToolCallHookData, error) {
        agentID := ctx.Value("agent_id").(string)
        loader := GetToolLoader()
        config := loader.GetConfig()

        // Validate tool names exist
        if tools, ok := data.Input["tools"].([]string); ok {
            manifest := loader.GetManifest(agentID)
            for _, tool := range tools {
                if !manifest.HasTool(tool) {
                    return nil, fmt.Errorf("unknown tool: %s", tool)
                }
            }

            // Check limit
            if len(tools) > config.MaxToolsPerRequest {
                return nil, fmt.Errorf("too many tools requested: %d (max %d)",
                    len(tools), config.MaxToolsPerRequest)
            }
        }

        return data, nil
    },
}

// Post-tool hook: Record loading for pattern learning
var RecordToolLoadHook = Hook{
    Name:     "record_tool_load",
    Type:     PostTool,
    Priority: HookPriorityNormal,
    Trigger:  "request_tools",
    Handler: func(ctx context.Context, data *ToolCallHookData) (*ToolCallHookData, error) {
        if !GetToolLoader().GetConfig().EnablePatternLearning {
            return data, nil
        }

        agentID := ctx.Value("agent_id").(string)
        sessionID := ctx.Value("session_id").(string)
        taskContext := ctx.Value("task_context").(string)

        event := LoadEvent{
            Timestamp: time.Now(),
            Tools:     data.Result.Get("loaded").([]string),
            TaskType:  inferTaskType(taskContext),
        }

        GetToolLoader().RecordLoadEvent(agentID, sessionID, event)

        return data, nil
    },
}
```

### Auto-Suggest Implementation

```go
// IntentBasedSuggester suggests tools based on task description
type IntentBasedSuggester struct {
    embedder    Embedder
    toolEmbeds  map[string][]float32  // tool name -> embedding
    bundleIndex map[string][]string   // use case -> bundle names
}

func (s *IntentBasedSuggester) SuggestTools(ctx context.Context, intent string, manifest *ToolManifest) ([]string, error) {
    // Embed the intent
    intentEmbed, err := s.embedder.Embed(ctx, intent)
    if err != nil {
        return nil, err
    }

    // Find most similar tools
    type scored struct {
        tool  string
        score float64
    }

    var scores []scored
    for _, entry := range manifest.Tools {
        toolEmbed := s.toolEmbeds[entry.Name]
        similarity := cosineSimilarity(intentEmbed, toolEmbed)
        scores = append(scores, scored{entry.Name, similarity})
    }

    // Sort by similarity
    sort.Slice(scores, func(i, j int) bool {
        return scores[i].score > scores[j].score
    })

    // Return top matches above threshold
    var suggestions []string
    for _, s := range scores {
        if s.score > 0.7 && len(suggestions) < 5 {
            suggestions = append(suggestions, s.tool)
        }
    }

    return suggestions, nil
}

// Also check bundles by use case matching
func (s *IntentBasedSuggester) SuggestBundle(intent string, bundles []BundleEntry) *BundleEntry {
    intentLower := strings.ToLower(intent)

    for _, bundle := range bundles {
        for _, useCase := range bundle.UseCases {
            if strings.Contains(intentLower, useCase) {
                return &bundle
            }
        }
    }

    return nil
}
```

### Token Savings Analysis

```
┌─────────────────────────────────────────────────────────────────────────────────────┐
│                         TOKEN SAVINGS: LAZY TOOL LOADING                             │
├─────────────────────────────────────────────────────────────────────────────────────┤
│                                                                                     │
│  SCENARIO: Designer agent, 30 tools, 20 turns per session                           │
│                                                                                     │
│  EAGER LOADING:                                                                     │
│  ┌──────────────────────────────────────────────────────────────────────────────┐   │
│  │  Per turn: 30 tools × 200 tokens = 6,000 tokens                              │   │
│  │  Per session: 6,000 × 20 turns = 120,000 tokens                              │   │
│  └──────────────────────────────────────────────────────────────────────────────┘   │
│                                                                                     │
│  LAZY LOADING:                                                                      │
│  ┌──────────────────────────────────────────────────────────────────────────────┐   │
│  │  Bootstrap (every turn):                                                     │   │
│  │    Manifest: 30 × 20 tokens = 600 tokens                                     │   │
│  │    Core tools: 2 × 200 tokens = 400 tokens                                   │   │
│  │    Subtotal: 1,000 tokens                                                    │   │
│  │                                                                              │   │
│  │  On-demand loading (amortized):                                              │   │
│  │    Turn 1: Load "component" bundle (5 tools) = 1,000 tokens                  │   │
│  │    Turn 2-5: Use cached tools = 0 additional                                 │   │
│  │    Turn 6: Load "accessibility" bundle (4 tools) = 800 tokens                │   │
│  │    Turn 7-10: Use cached tools = 0 additional                                │   │
│  │    ...                                                                       │   │
│  │                                                                              │   │
│  │  Average per turn:                                                           │   │
│  │    Bootstrap: 1,000 tokens                                                   │   │
│  │    Loaded tools (avg 8 cached): 1,600 tokens                                 │   │
│  │    Total: 2,600 tokens                                                       │   │
│  │                                                                              │   │
│  │  Per session: 2,600 × 20 turns = 52,000 tokens                               │   │
│  └──────────────────────────────────────────────────────────────────────────────┘   │
│                                                                                     │
│  SAVINGS:                                                                           │
│  ┌──────────────────────────────────────────────────────────────────────────────┐   │
│  │  Per session: 120,000 - 52,000 = 68,000 tokens saved                         │   │
│  │  Percentage: 57% reduction                                                   │   │
│  │                                                                              │   │
│  │  Projected annual (1000 sessions/day):                                       │   │
│  │    Daily savings: 68,000 × 1000 = 68M tokens                                 │   │
│  │    Monthly savings: ~2B tokens                                               │   │
│  │    Cost savings @ $3/1M tokens: ~$6,000/month                                │   │
│  └──────────────────────────────────────────────────────────────────────────────┘   │
│                                                                                     │
└─────────────────────────────────────────────────────────────────────────────────────┘
```

### Integration Guide

**Step 1: Implement Tool Registry**
```go
// Add to core/tools/registry.go
type ToolRegistry struct {
    manifests map[string]*ToolManifest    // agentID -> manifest
    schemas   map[string]map[string]any   // toolName -> full schema
    bundles   map[string][]BundleEntry    // agentID -> bundles
}
```

**Step 2: Implement Tool Loader Service**
```go
// Add to core/tools/loader.go
type ToolLoader struct {
    registry  *ToolRegistry
    cache     *LoadedToolCache
    suggester *IntentBasedSuggester
    config    ToolLoaderConfig
}
```

**Step 3: Add Hooks to All Agents**
```go
// In each agent's hook registration
agent.RegisterHook(InjectToolsHook)
agent.RegisterHook(ValidateToolLoadHook)
agent.RegisterHook(RecordToolLoadHook)
```

**Step 4: Add `request_tools` to All Agents**
```go
// In each agent's skill registration
agent.RegisterSkill(RequestToolsSkill)
```

**Step 5: Generate Manifests**
```go
// Build time: generate manifests from tool definitions
func GenerateManifest(agentID string, tools []ToolDefinition) *ToolManifest {
    manifest := &ToolManifest{AgentID: agentID}
    for _, tool := range tools {
        manifest.Tools = append(manifest.Tools, ToolManifestEntry{
            Name:        tool.Name,
            Description: truncate(tool.Description, 100),
            Category:    tool.Category,
            Keywords:    tool.Keywords,
        })
    }
    return manifest
}
```

### Considerations

| Aspect | Consideration | Mitigation |
|--------|---------------|------------|
| **Extra round-trip** | First turn may only load tools | Auto-suggest based on task keywords |
| **Agent confusion** | Agent might not know what to request | Provide clear manifest with bundle hints |
| **Cache invalidation** | When do cached tools expire? | TTL + explicit clear on session end |
| **Pattern learning** | What patterns are useful? | Track tool co-usage, success rates |
| **Startup latency** | Manifest generation cost | Pre-compute at build time |

---

## Git Tooling for Agents

**CRITICAL: Git operations are permission-scoped per agent. Each agent has specific capabilities aligned with their role.**

Git tooling enables agents to work with version control in a controlled, auditable manner. Permissions are strictly enforced to prevent unintended repository modifications.

### Permission Matrix

```
┌─────────────────────────────────────────────────────────────────────────────────────┐
│                         GIT PERMISSION MATRIX BY AGENT                               │
├─────────────────────────────────────────────────────────────────────────────────────┤
│                                                                                     │
│  Operation              Engineer  Designer  Inspector  Tester  Architect  Librarian │
│  ────────────────────   ────────  ────────  ─────────  ──────  ─────────  ───────── │
│                                                                                     │
│  READ OPERATIONS                                                                    │
│  git status               ✓         ✓          ✓         ✓        ✓          ✓     │
│  git diff                 ✓         ✓          ✓         ✓        ✓          ✓     │
│  git log                  ✓         ✓          ✓         ✓        ✓          ✓     │
│  git show                 ✓         ✓          ✓         ✓        ✓          ✓     │
│  git blame                ✓         ✓          ✓         ✓        ✓          ✓     │
│  git ls-files             ✓         ✓          ✓         ✓        ✓          ✓     │
│                                                                                     │
│  COMMIT OPERATIONS                                                                  │
│  git add                  ✓         ✓          ✗         ✓*       ✓          ✗     │
│  git commit               ✓         ✓          ✗         ✓*       ✓          ✗     │
│  git stash                ✓         ✓          ✗         ✓        ✓          ✗     │
│                                                                                     │
│  BRANCH OPERATIONS                                                                  │
│  git branch (list)        ✓         ✓          ✓         ✓        ✓          ✓     │
│  git branch (create)      ✗         ✗          ✗         ✗        ✓          ✗     │
│  git checkout             ✗         ✗          ✗         ✗        ✓          ✓     │
│  git switch               ✗         ✗          ✗         ✗        ✓          ✓     │
│                                                                                     │
│  HISTORY OPERATIONS                                                                 │
│  git revert               ✗         ✗          ✗         ✗        ✓          ✗     │
│  git cherry-pick          ✗         ✗          ✗         ✗        ✓          ✗     │
│  git reset (soft)         ✗         ✗          ✗         ✗        ✓          ✗     │
│  git reset (hard)         ✗         ✗          ✗         ✗        ✗          ✗     │
│                                                                                     │
│  REMOTE OPERATIONS                                                                  │
│  git fetch                ✗         ✗          ✗         ✗        ✓          ✓     │
│  git pull                 ✗         ✗          ✗         ✗        ✓**        ✓     │
│  git push                 ✗         ✗          ✗         ✗        ✗          ✗     │
│  git clone                ✗         ✗          ✗         ✗        ✗          ✓***  │
│  git merge                ✗         ✗          ✗         ✗        ✗          ✗     │
│                                                                                     │
│  * Tester: Only when tests pass/added/modified                                      │
│  ** Architect: Only for current repo (verified via git rev-parse --show-toplevel)   │
│  *** Librarian: Into temp directories only, not modifying current repo              │
│                                                                                     │
└─────────────────────────────────────────────────────────────────────────────────────┘
```

### Architecture Overview

```
┌─────────────────────────────────────────────────────────────────────────────────────┐
│                         GIT TOOLING ARCHITECTURE                                     │
├─────────────────────────────────────────────────────────────────────────────────────┤
│                                                                                     │
│  ┌─────────────────────────────────────────────────────────────────────────────┐    │
│  │                          GIT PERMISSION LAYER                                │    │
│  │                                                                              │    │
│  │  ┌─────────────────┐  ┌─────────────────┐  ┌─────────────────┐              │    │
│  │  │ PermissionSet   │  │ RepoValidator   │  │ OperationAudit  │              │    │
│  │  │                 │  │                 │  │                 │              │    │
│  │  │ Per-agent       │  │ Validates ops   │  │ Logs all git    │              │    │
│  │  │ allowed ops     │  │ against current │  │ operations to   │              │    │
│  │  │                 │  │ repo only       │  │ Archivalist     │              │    │
│  │  └─────────────────┘  └─────────────────┘  └─────────────────┘              │    │
│  │                                                                              │    │
│  └──────────────────────────────────┬──────────────────────────────────────────┘    │
│                                     │                                               │
│                                     ▼                                               │
│  ┌─────────────────────────────────────────────────────────────────────────────┐    │
│  │                          GIT SKILLS LAYER                                    │    │
│  │                                                                              │    │
│  │  ┌───────────────┐ ┌───────────────┐ ┌───────────────┐ ┌───────────────┐    │    │
│  │  │ GitReadSkills │ │GitCommitSkills│ │GitBranchSkills│ │GitRemoteSkills│    │    │
│  │  │               │ │               │ │               │ │               │    │    │
│  │  │ • status      │ │ • add         │ │ • list        │ │ • fetch       │    │    │
│  │  │ • diff        │ │ • commit      │ │ • create      │ │ • pull        │    │    │
│  │  │ • log         │ │ • stash       │ │ • checkout    │ │ • clone       │    │    │
│  │  │ • show        │ │               │ │ • switch      │ │               │    │    │
│  │  │ • blame       │ │               │ │ • revert      │ │               │    │    │
│  │  │ • ls-files    │ │               │ │ • cherry-pick │ │               │    │    │
│  │  └───────────────┘ └───────────────┘ └───────────────┘ └───────────────┘    │    │
│  │                                                                              │    │
│  └─────────────────────────────────────────────────────────────────────────────┘    │
│                                                                                     │
└─────────────────────────────────────────────────────────────────────────────────────┘
```

### Core Data Structures

```go
// core/git/permissions.go

type GitPermission string

const (
    GitPermRead      GitPermission = "read"
    GitPermCommit    GitPermission = "commit"
    GitPermBranch    GitPermission = "branch"
    GitPermHistory   GitPermission = "history"
    GitPermRemote    GitPermission = "remote"
    GitPermClone     GitPermission = "clone"
)

type GitPermissionSet struct {
    AgentID     string
    Permissions map[GitPermission]bool
    Conditions  map[string]PermissionCondition  // Tool-specific conditions
}

type PermissionCondition func(ctx context.Context, input map[string]any) error

// Agent permission definitions
var AgentGitPermissions = map[string]*GitPermissionSet{
    "engineer": {
        Permissions: map[GitPermission]bool{
            GitPermRead: true, GitPermCommit: true,
            GitPermBranch: false, GitPermHistory: false,
            GitPermRemote: false, GitPermClone: false,
        },
    },
    "designer": {
        Permissions: map[GitPermission]bool{
            GitPermRead: true, GitPermCommit: true,
            GitPermBranch: false, GitPermHistory: false,
            GitPermRemote: false, GitPermClone: false,
        },
    },
    "inspector": {
        Permissions: map[GitPermission]bool{
            GitPermRead: true, GitPermCommit: false,
            GitPermBranch: false, GitPermHistory: false,
            GitPermRemote: false, GitPermClone: false,
        },
    },
    "tester": {
        Permissions: map[GitPermission]bool{
            GitPermRead: true, GitPermCommit: true,  // Conditional
            GitPermBranch: false, GitPermHistory: false,
            GitPermRemote: false, GitPermClone: false,
        },
        Conditions: map[string]PermissionCondition{
            "git_commit": validateTesterCommitCondition,
        },
    },
    "architect": {
        Permissions: map[GitPermission]bool{
            GitPermRead: true, GitPermCommit: true,
            GitPermBranch: true, GitPermHistory: true,
            GitPermRemote: true, GitPermClone: false,
        },
        Conditions: map[string]PermissionCondition{
            "git_pull":  validateCurrentRepoOnly,
            "git_fetch": validateCurrentRepoOnly,
        },
    },
    "librarian": {
        Permissions: map[GitPermission]bool{
            GitPermRead: true, GitPermCommit: false,
            GitPermBranch: true, GitPermHistory: false,
            GitPermRemote: true, GitPermClone: true,
        },
        Conditions: map[string]PermissionCondition{
            "git_branch_create": denyAlways,
            "git_clone":         validateTempDirOnly,
        },
    },
}
```

### Engineer & Designer Git Skills

**Philosophy**: Focus on atomic commits for completed work. Auto-commit after successful task completion.

```go
var WorkerGitSkills = []SkillDefinition{
    // ═══════════════════════════════════════════════════════════════════════════════
    // READ OPERATIONS - "What exists and what have I changed?"
    // ═══════════════════════════════════════════════════════════════════════════════
    {
        Name:        "git_status",
        Description: "View working tree status - modified, staged, and untracked files",
        Domain:      "git_read",
        Tier:        1,
        InputSchema: map[string]any{
            "type": "object",
            "properties": map[string]any{
                "short":     {"type": "boolean", "description": "Short format output"},
                "porcelain": {"type": "boolean", "description": "Machine-readable format"},
            },
        },
    },
    {
        Name:        "git_diff",
        Description: "View uncommitted changes in working tree",
        Domain:      "git_read",
        Tier:        1,
        InputSchema: map[string]any{
            "type": "object",
            "properties": map[string]any{
                "path":      {"type": "string", "description": "Specific file or directory"},
                "staged":    {"type": "boolean", "description": "Show staged changes only"},
                "stat":      {"type": "boolean", "description": "Show diffstat only"},
                "name_only": {"type": "boolean", "description": "Show only file names"},
            },
        },
    },
    {
        Name:        "git_log",
        Description: "View commit history",
        Domain:      "git_read",
        Tier:        1,
        InputSchema: map[string]any{
            "type": "object",
            "properties": map[string]any{
                "limit":   {"type": "integer", "description": "Number of commits", "default": 10},
                "oneline": {"type": "boolean", "description": "Compact format"},
                "path":    {"type": "string", "description": "Filter by path"},
                "author":  {"type": "string", "description": "Filter by author"},
                "since":   {"type": "string", "description": "Commits since date"},
                "grep":    {"type": "string", "description": "Filter by message"},
            },
        },
    },
    {
        Name:        "git_show",
        Description: "Show commit details and diff",
        Domain:      "git_read",
        Tier:        2,
        InputSchema: map[string]any{
            "type": "object",
            "properties": map[string]any{
                "commit":    {"type": "string", "default": "HEAD"},
                "stat":      {"type": "boolean"},
                "name_only": {"type": "boolean"},
            },
        },
    },
    {
        Name:        "git_blame",
        Description: "Show line-by-line last modification info",
        Domain:      "git_read",
        Tier:        2,
        InputSchema: map[string]any{
            "type": "object",
            "properties": map[string]any{
                "path":       {"type": "string", "description": "File to blame"},
                "line_range": {"type": "string", "description": "Line range (e.g., '10,20')"},
            },
            "required": []string{"path"},
        },
    },
    {
        Name:        "git_ls_files",
        Description: "List tracked files in repository",
        Domain:      "git_read",
        Tier:        2,
        InputSchema: map[string]any{
            "type": "object",
            "properties": map[string]any{
                "pattern":  {"type": "string", "description": "Glob pattern"},
                "modified": {"type": "boolean", "description": "Only modified files"},
            },
        },
    },

    // ═══════════════════════════════════════════════════════════════════════════════
    // COMMIT OPERATIONS - Atomic commits for completed work
    // ═══════════════════════════════════════════════════════════════════════════════
    {
        Name:        "git_add",
        Description: "Stage files for commit",
        Domain:      "git_commit",
        Tier:        1,
        InputSchema: map[string]any{
            "type": "object",
            "properties": map[string]any{
                "paths": {"type": "array", "items": {"type": "string"}, "description": "Files to stage"},
                "all":   {"type": "boolean", "description": "Stage all modified tracked files"},
            },
        },
    },
    {
        Name:        "git_commit",
        Description: "Create atomic commit with staged changes",
        Domain:      "git_commit",
        Tier:        1,
        InputSchema: map[string]any{
            "type": "object",
            "properties": map[string]any{
                "message": {"type": "string", "description": "Commit message (<type>: <description>)", "minLength": 10},
                "body":    {"type": "string", "description": "Extended description"},
            },
            "required": []string{"message"},
        },
    },
    {
        Name:        "git_stash",
        Description: "Temporarily store uncommitted changes",
        Domain:      "git_commit",
        Tier:        2,
        InputSchema: map[string]any{
            "type": "object",
            "properties": map[string]any{
                "message":           {"type": "string"},
                "include_untracked": {"type": "boolean"},
            },
        },
    },
    {
        Name:        "git_stash_pop",
        Description: "Restore most recent stashed changes",
        Domain:      "git_commit",
        Tier:        2,
        InputSchema: map[string]any{
            "type": "object",
            "properties": map[string]any{
                "index": {"type": "integer", "default": 0},
            },
        },
    },
}
```

### Auto-Commit Hook (Engineer/Designer)

```go
// Automatically commit changes after successful task completion
var AutoCommitHook = Hook{
    Name:     "auto_commit_on_success",
    Type:     PostTool,
    Priority: HookPriorityLast,
    Trigger:  "signal_complete",
    Handler: func(ctx context.Context, data *ToolCallHookData) (*ToolCallHookData, error) {
        agentID := ctx.Value("agent_id").(string)

        // Only for Engineer and Designer
        if agentID != "engineer" && agentID != "designer" {
            return data, nil
        }

        // Check if task was successful
        if !data.Result.GetBool("success") {
            return data, nil
        }

        // Get modified files from task context
        modifiedFiles := ctx.Value("modified_files").([]string)
        if len(modifiedFiles) == 0 {
            return data, nil
        }

        // Verify uncommitted changes exist
        status, err := gitStatus(ctx)
        if err != nil || !status.HasChanges() {
            return data, nil
        }

        // Generate atomic commit
        taskDescription := ctx.Value("task_description").(string)
        commitMsg := generateAtomicCommitMessage(agentID, taskDescription, modifiedFiles)

        // Stage only files modified in this task
        if err := gitAdd(ctx, modifiedFiles); err != nil {
            data.Warnings = append(data.Warnings, fmt.Sprintf("Failed to stage: %v", err))
            return data, nil
        }

        // Create commit
        if err := gitCommit(ctx, commitMsg); err != nil {
            data.Warnings = append(data.Warnings, fmt.Sprintf("Failed to commit: %v", err))
            return data, nil
        }

        // Record in Archivalist
        archivalist.StoreEntry(ctx, &Entry{
            Category: CategoryTimeline,
            Content:  fmt.Sprintf("Auto-commit by %s: %s", agentID, commitMsg.Subject),
            Metadata: map[string]any{
                "commit_hash": getHeadCommit(ctx),
                "files":       modifiedFiles,
                "agent":       agentID,
            },
        })

        return data, nil
    },
}

// Commit message following conventional commits
func generateAtomicCommitMessage(agent, taskDesc string, files []string) CommitMessage {
    commitType := inferCommitType(files, taskDesc)
    subject := fmt.Sprintf("%s: %s", commitType, summarizeTask(taskDesc, 50))
    body := fmt.Sprintf("Modified files:\n%s\n\nAgent: %s", formatFileList(files), agent)

    return CommitMessage{Subject: subject, Body: body}
}

func inferCommitType(files []string, task string) string {
    switch {
    case strings.Contains(task, "fix") || strings.Contains(task, "bug"):
        return "fix"
    case containsTestFiles(files):
        return "test"
    case containsStyleFiles(files):
        return "style"
    case containsDocFiles(files):
        return "docs"
    case strings.Contains(task, "refactor"):
        return "refactor"
    default:
        return "feat"
    }
}
```

### Inspector Git Skills (Read-Only)

```go
var InspectorGitSkills = []SkillDefinition{
    // READ-ONLY - Same read operations as Engineer, NO commit operations
    {Name: "git_status", Domain: "git_read", Tier: 1},
    {Name: "git_diff", Domain: "git_read", Tier: 1},
    {Name: "git_log", Domain: "git_read", Tier: 1},
    {Name: "git_show", Domain: "git_read", Tier: 1},
    {Name: "git_blame", Domain: "git_read", Tier: 1},
    {
        Name:        "git_diff_between",
        Description: "Compare two commits/branches for review scope",
        Domain:      "git_read",
        Tier:        2,
        InputSchema: map[string]any{
            "type": "object",
            "properties": map[string]any{
                "from":      {"type": "string", "description": "Base commit/branch"},
                "to":        {"type": "string", "default": "HEAD"},
                "stat":      {"type": "boolean"},
                "name_only": {"type": "boolean"},
            },
            "required": []string{"from"},
        },
    },
}
// Inspector has NO commit operations - enforced at permission layer
```

### Tester Git Skills (Conditional Commit)

```go
var TesterGitSkills = []SkillDefinition{
    // READ OPERATIONS - Same as Engineer
    // COMMIT OPERATIONS - Same tools, but conditional execution via hook
}

// Tester commits only when tests improve
var TesterConditionalCommitHook = Hook{
    Name:     "conditional_commit_on_test_success",
    Type:     PostTool,
    Priority: HookPriorityLast,
    Trigger:  "signal_complete",
    Handler: func(ctx context.Context, data *ToolCallHookData) (*ToolCallHookData, error) {
        agentID := ctx.Value("agent_id").(string)
        if agentID != "tester" {
            return data, nil
        }

        testResults := ctx.Value("test_results").(*TestResults)
        previousResults := ctx.Value("previous_test_results").(*TestResults)

        decision := evaluateCommitCondition(testResults, previousResults, ctx)

        if !decision.Allowed {
            data.Result.Set("commit_skipped", true)
            data.Result.Set("skip_reason", decision.Reason)
            return data, nil
        }

        // Proceed with commit (same as Engineer)
        modifiedFiles := ctx.Value("modified_files").([]string)
        commitMsg := generateTestCommitMessage(decision.Reason, modifiedFiles, testResults)

        if err := gitAdd(ctx, modifiedFiles); err != nil {
            return data, err
        }
        if err := gitCommit(ctx, commitMsg); err != nil {
            return data, err
        }

        return data, nil
    },
}

type CommitDecision struct {
    Allowed bool
    Reason  string
}

func evaluateCommitCondition(current, previous *TestResults, ctx context.Context) CommitDecision {
    modifiedFiles := ctx.Value("modified_files").([]string)

    // Condition 1: Passing tests increased
    if current.Passed > previous.Passed {
        return CommitDecision{true, fmt.Sprintf("test_improvement: %d → %d passing", previous.Passed, current.Passed)}
    }

    // Condition 2: New tests added
    newTestFiles := filterTestFiles(modifiedFiles)
    if hasNewTests(newTestFiles, previous) {
        return CommitDecision{true, fmt.Sprintf("new_tests: added %d test files", len(newTestFiles))}
    }

    // Condition 3: Tests modified and still pass
    if hasModifiedTests(newTestFiles, previous) && current.Failed == 0 {
        return CommitDecision{true, "test_modification: updated tests still pass"}
    }

    return CommitDecision{false, "no_improvement: tests unchanged or regressed"}
}
```

### Architect Git Skills (Full Repo Management)

```go
var ArchitectGitSkills = []SkillDefinition{
    // READ + COMMIT - Same as Engineer

    // ═══════════════════════════════════════════════════════════════════════════════
    // BRANCH OPERATIONS - Repository coordination
    // ═══════════════════════════════════════════════════════════════════════════════
    {
        Name:        "git_branch_list",
        Description: "List all branches",
        Domain:      "git_branch",
        Tier:        1,
        InputSchema: map[string]any{
            "type": "object",
            "properties": map[string]any{
                "all":     {"type": "boolean", "description": "Include remote branches"},
                "verbose": {"type": "boolean"},
            },
        },
    },
    {
        Name:        "git_branch_create",
        Description: "Create a new branch",
        Domain:      "git_branch",
        Tier:        1,
        InputSchema: map[string]any{
            "type": "object",
            "properties": map[string]any{
                "name":        {"type": "string"},
                "start_point": {"type": "string", "default": "HEAD"},
            },
            "required": []string{"name"},
        },
    },
    {
        Name:        "git_checkout",
        Description: "Switch to branch or commit",
        Domain:      "git_branch",
        Tier:        1,
        InputSchema: map[string]any{
            "type": "object",
            "properties": map[string]any{
                "target": {"type": "string"},
                "create": {"type": "boolean"},
            },
            "required": []string{"target"},
        },
    },
    {
        Name:        "git_switch",
        Description: "Switch branches (safer than checkout)",
        Domain:      "git_branch",
        Tier:        1,
        InputSchema: map[string]any{
            "type": "object",
            "properties": map[string]any{
                "branch": {"type": "string"},
                "create": {"type": "boolean"},
            },
            "required": []string{"branch"},
        },
    },

    // ═══════════════════════════════════════════════════════════════════════════════
    // HISTORY OPERATIONS - Controlled history modification
    // ═══════════════════════════════════════════════════════════════════════════════
    {
        Name:        "git_revert",
        Description: "Create commit that undoes a previous commit",
        Domain:      "git_history",
        Tier:        2,
        InputSchema: map[string]any{
            "type": "object",
            "properties": map[string]any{
                "commit":    {"type": "string"},
                "no_commit": {"type": "boolean"},
            },
            "required": []string{"commit"},
        },
    },
    {
        Name:        "git_cherry_pick",
        Description: "Apply changes from specific commits",
        Domain:      "git_history",
        Tier:        2,
        InputSchema: map[string]any{
            "type": "object",
            "properties": map[string]any{
                "commits":   {"type": "array", "items": {"type": "string"}},
                "no_commit": {"type": "boolean"},
            },
            "required": []string{"commits"},
        },
    },
    {
        Name:        "git_reset_soft",
        Description: "Move HEAD, keeping changes staged",
        Domain:      "git_history",
        Tier:        3,
        InputSchema: map[string]any{
            "type": "object",
            "properties": map[string]any{
                "commit": {"type": "string", "default": "HEAD~1"},
            },
        },
    },

    // ═══════════════════════════════════════════════════════════════════════════════
    // REMOTE OPERATIONS - Current repo only (verified via rev-parse)
    // ═══════════════════════════════════════════════════════════════════════════════
    {
        Name:        "git_fetch",
        Description: "Fetch updates from remote (current repo only)",
        Domain:      "git_remote",
        Tier:        1,
        InputSchema: map[string]any{
            "type": "object",
            "properties": map[string]any{
                "remote": {"type": "string", "default": "origin"},
                "prune":  {"type": "boolean"},
            },
        },
    },
    {
        Name:        "git_pull",
        Description: "Pull from remote (current repo only, verified)",
        Domain:      "git_remote",
        Tier:        1,
        InputSchema: map[string]any{
            "type": "object",
            "properties": map[string]any{
                "remote": {"type": "string", "default": "origin"},
                "branch": {"type": "string"},
                "rebase": {"type": "boolean"},
            },
        },
    },
}

// Architect repo-scoping validation
var ArchitectRepoScopeHook = Hook{
    Name:     "validate_repo_scope",
    Type:     PreTool,
    Priority: HookPriorityFirst,
    Trigger:  "git_pull,git_fetch,git_checkout,git_switch",
    Handler: func(ctx context.Context, data *ToolCallHookData) (*ToolCallHookData, error) {
        // Get current repo root via git rev-parse --show-toplevel
        repoRoot, err := exec.Command("git", "rev-parse", "--show-toplevel").Output()
        if err != nil {
            return nil, fmt.Errorf("not in a git repository")
        }

        ctx = context.WithValue(ctx, "repo_root", strings.TrimSpace(string(repoRoot)))

        // Validate any path references are within repo root
        if path, ok := data.Input["path"].(string); ok {
            absPath, _ := filepath.Abs(path)
            if !strings.HasPrefix(absPath, ctx.Value("repo_root").(string)) {
                return nil, fmt.Errorf("operation must be within current repository")
            }
        }

        return data, nil
    },
}
```

### Librarian Git Skills (Read-Only + Exploration)

```go
var LibrarianGitSkills = []SkillDefinition{
    // READ OPERATIONS - Full read access
    {Name: "git_status", Domain: "git_read", Tier: 1},
    {Name: "git_diff", Domain: "git_read", Tier: 1},
    {Name: "git_log", Domain: "git_read", Tier: 1},
    {Name: "git_show", Domain: "git_read", Tier: 1},
    {Name: "git_blame", Domain: "git_read", Tier: 1},
    {Name: "git_ls_files", Domain: "git_read", Tier: 1},
    {
        Name:        "git_ls_tree",
        Description: "List contents of tree object (explore history)",
        Domain:      "git_read",
        Tier:        2,
        InputSchema: map[string]any{
            "type": "object",
            "properties": map[string]any{
                "tree":      {"type": "string", "default": "HEAD"},
                "path":      {"type": "string"},
                "recursive": {"type": "boolean"},
            },
        },
    },

    // ═══════════════════════════════════════════════════════════════════════════════
    // NAVIGATION - Checkout for exploration (read-only intent)
    // ═══════════════════════════════════════════════════════════════════════════════
    {
        Name:        "git_checkout_readonly",
        Description: "Switch to branch/commit for exploration (no modifications)",
        Domain:      "git_navigate",
        Tier:        2,
        InputSchema: map[string]any{
            "type": "object",
            "properties": map[string]any{
                "target": {"type": "string"},
            },
            "required": []string{"target"},
        },
    },
    {
        Name:        "git_branch_list",
        Description: "List all branches",
        Domain:      "git_navigate",
        Tier:        1,
    },
    {
        Name:        "git_tag_list",
        Description: "List all tags",
        Domain:      "git_navigate",
        Tier:        2,
        InputSchema: map[string]any{
            "type": "object",
            "properties": map[string]any{
                "pattern": {"type": "string"},
            },
        },
    },

    // ═══════════════════════════════════════════════════════════════════════════════
    // REMOTE - Fetch and clone for exploration
    // ═══════════════════════════════════════════════════════════════════════════════
    {
        Name:        "git_fetch",
        Description: "Fetch updates without modifying working tree",
        Domain:      "git_remote",
        Tier:        1,
        InputSchema: map[string]any{
            "type": "object",
            "properties": map[string]any{
                "remote": {"type": "string", "default": "origin"},
                "all":    {"type": "boolean"},
            },
        },
    },
    {
        Name:        "git_pull_readonly",
        Description: "Pull remote (stashes local changes first for safety)",
        Domain:      "git_remote",
        Tier:        2,
    },
    {
        Name:        "git_clone_temp",
        Description: "Clone repository to temp directory for analysis",
        Domain:      "git_remote",
        Tier:        3,
        InputSchema: map[string]any{
            "type": "object",
            "properties": map[string]any{
                "url":    {"type": "string"},
                "depth":  {"type": "integer", "default": 1},
                "branch": {"type": "string"},
            },
            "required": []string{"url"},
        },
    },
    {
        Name:        "git_remote_list",
        Description: "List configured remotes",
        Domain:      "git_remote",
        Tier:        1,
    },
}

// Librarian read-only enforcement
var LibrarianReadOnlyHook = Hook{
    Name:     "enforce_readonly",
    Type:     PreTool,
    Priority: HookPriorityFirst,
    Handler: func(ctx context.Context, data *ToolCallHookData) (*ToolCallHookData, error) {
        if ctx.Value("agent_id").(string) != "librarian" {
            return data, nil
        }

        writeOps := []string{"git_add", "git_commit", "git_revert", "git_cherry_pick", "git_reset"}
        for _, op := range writeOps {
            if data.ToolName == op {
                return nil, fmt.Errorf("librarian cannot perform write operation: %s", op)
            }
        }
        return data, nil
    },
}

// Clone to temp directory implementation
func handleGitCloneTemp(ctx context.Context, input map[string]any) (*ToolResult, error) {
    url := input["url"].(string)
    depth := getIntOrDefault(input, "depth", 1)
    branch := getStringOrDefault(input, "branch", "")

    tempDir, err := os.MkdirTemp("", "sylk-librarian-*")
    if err != nil {
        return nil, err
    }

    args := []string{"clone", "--depth", strconv.Itoa(depth)}
    if branch != "" {
        args = append(args, "--branch", branch)
    }
    args = append(args, url, tempDir)

    if err := exec.CommandContext(ctx, "git", args...).Run(); err != nil {
        os.RemoveAll(tempDir)
        return nil, err
    }

    return &ToolResult{
        Success: true,
        Data: map[string]any{
            "path":    tempDir,
            "url":     url,
            "cleanup": "Directory cleaned up after session",
        },
    }, nil
}
```

### Permission Enforcement Hook

```go
// Central permission check - runs before any git operation
var GitPermissionHook = Hook{
    Name:     "git_permission_check",
    Type:     PreTool,
    Priority: HookPriorityFirst,
    Handler: func(ctx context.Context, data *ToolCallHookData) (*ToolCallHookData, error) {
        if !strings.HasPrefix(data.ToolName, "git_") {
            return data, nil
        }

        agentID := ctx.Value("agent_id").(string)
        permSet, ok := AgentGitPermissions[agentID]
        if !ok {
            return nil, fmt.Errorf("no git permissions for agent: %s", agentID)
        }

        // Check base permission
        requiredPerm := getRequiredPermission(data.ToolName)
        if !permSet.Permissions[requiredPerm] {
            return nil, fmt.Errorf("agent %s lacks %s permission for %s",
                agentID, requiredPerm, data.ToolName)
        }

        // Check tool-specific conditions
        if condition, ok := permSet.Conditions[data.ToolName]; ok {
            if err := condition(ctx, data.Input); err != nil {
                return nil, err
            }
        }

        return data, nil
    },
}

func getRequiredPermission(toolName string) GitPermission {
    switch {
    case strings.HasPrefix(toolName, "git_status"), strings.HasPrefix(toolName, "git_diff"),
         strings.HasPrefix(toolName, "git_log"), strings.HasPrefix(toolName, "git_show"),
         strings.HasPrefix(toolName, "git_blame"), strings.HasPrefix(toolName, "git_ls"):
        return GitPermRead
    case strings.HasPrefix(toolName, "git_add"), strings.HasPrefix(toolName, "git_commit"),
         strings.HasPrefix(toolName, "git_stash"):
        return GitPermCommit
    case strings.HasPrefix(toolName, "git_branch"), strings.HasPrefix(toolName, "git_checkout"),
         strings.HasPrefix(toolName, "git_switch"):
        return GitPermBranch
    case strings.HasPrefix(toolName, "git_revert"), strings.HasPrefix(toolName, "git_cherry"),
         strings.HasPrefix(toolName, "git_reset"):
        return GitPermHistory
    case strings.HasPrefix(toolName, "git_fetch"), strings.HasPrefix(toolName, "git_pull"),
         strings.HasPrefix(toolName, "git_remote"):
        return GitPermRemote
    case strings.HasPrefix(toolName, "git_clone"):
        return GitPermClone
    default:
        return GitPermRead
    }
}
```

### Git Operation Audit Hook

```go
// Audit all git operations to Archivalist
var GitAuditHook = Hook{
    Name:     "git_operation_audit",
    Type:     PostTool,
    Priority: HookPriorityLast,
    Handler: func(ctx context.Context, data *ToolCallHookData) (*ToolCallHookData, error) {
        if !strings.HasPrefix(data.ToolName, "git_") {
            return data, nil
        }

        agentID := ctx.Value("agent_id").(string)
        sessionID := ctx.Value("session_id").(string)

        archivalist.StoreEntry(ctx, &Entry{
            Category:  CategoryTimeline,
            SessionID: sessionID,
            Content:   fmt.Sprintf("Git operation: %s by %s", data.ToolName, agentID),
            Metadata: map[string]any{
                "tool":    data.ToolName,
                "agent":   agentID,
                "input":   sanitizeGitInput(data.Input),
                "success": data.Result.Success,
            },
        })

        return data, nil
    },
}
```

### Summary

| Agent | Read | Commit | Branch | History | Remote | Special Behavior |
|-------|------|--------|--------|---------|--------|------------------|
| **Engineer** | ✓ | ✓ | ✗ | ✗ | ✗ | Auto-commit on task success |
| **Designer** | ✓ | ✓ | ✗ | ✗ | ✗ | Auto-commit on task success |
| **Inspector** | ✓ | ✗ | ✗ | ✗ | ✗ | Read-only for review |
| **Tester** | ✓ | ✓* | ✗ | ✗ | ✗ | *Commit only when tests improve |
| **Architect** | ✓ | ✓ | ✓ | ✓ | ✓** | **Current repo only (rev-parse) |
| **Librarian** | ✓ | ✗ | ✓*** | ✗ | ✓ | ***Checkout only; clone to temp |

---

## Analysis Skills Orchestration

Analysis Skills provide **contextual intelligence** on top of existing tools. They do NOT reimplement what LSP, linters, or formatters already do. Instead, they **orchestrate** those tools and **inject context** that static tools lack.

### The Non-Redundancy Principle

Static tools are context-blind. They don't know:

| Context | Why It Matters |
|---------|----------------|
| **What changed** | 50 lint warnings → 3 that matter (in changed code) |
| **What the task is** | "Add auth" means input validation findings are critical |
| **Project patterns** | This project uses `MustUser()` pattern for nil handling |
| **Bug history** | This file had 5 bugs last month, extra scrutiny needed |
| **Test coverage** | High complexity + low coverage = high risk |
| **Intent** | User conversation explains why code exists |

**Key principle:** Analysis skills don't RE-IMPLEMENT what tools do. They ORCHESTRATE tools and ADD CONTEXT.

### Architecture Layers

```
┌─────────────────────────────────────────────────────────────────────────┐
│                          Agent Context                                   │
│      Task intent, session history, conversation, changed files           │
└─────────────────────────────────────────────────────────────────────────┘
                                    │
                                    ▼
┌─────────────────────────────────────────────────────────────────────────┐
│                    Analysis Skills (THIS LAYER)                          │
│         Orchestrate tools, inject context, prioritize, correlate         │
└─────────────────────────────────────────────────────────────────────────┘
                                    │
                ┌───────────────────┼───────────────────┐
                ▼                   ▼                   ▼
┌───────────────────────┐ ┌─────────────────┐ ┌────────────────────────┐
│      LSP Skills       │ │   Lint Skills   │ │   Archivalist Skills   │
│      (existing)       │ │   (existing)    │ │      (existing)        │
└───────────────────────┘ └─────────────────┘ └────────────────────────┘
                │                   │                   │
                ▼                   ▼                   ▼
┌───────────────────────┐ ┌─────────────────┐ ┌────────────────────────┐
│        gopls          │ │  golangci-lint  │ │    VectorGraphDB       │
└───────────────────────┘ └─────────────────┘ └────────────────────────┘
```

### What Tools Already Provide (DO NOT REIMPLEMENT)

| Tool | Capabilities |
|------|--------------|
| **gopls (LSP)** | Type errors, unused vars, unreachable code, references, hover, rename |
| **golangci-lint** | 100+ linters: complexity, security, bugs, style, performance |
| **staticcheck** | SA5011 (nil deref), SA1000 (invalid regex), SA4006 (unused), etc. |
| **gosec** | G101 (hardcoded creds), G201 (SQL injection), G304 (path traversal) |
| **go vet** | Printf format errors, unreachable code, lock copying |
| **go test -cover** | Line/branch coverage measurement |

### What Analysis Skills Add (OUR VALUE)

| Capability | Value Added |
|------------|-------------|
| **Filtering** | 50 warnings → 3 relevant findings in changed code |
| **Prioritization** | Rank by: changed + complexity + bug history |
| **Correlation** | "This pattern caused bug #123 two weeks ago" |
| **Explanation** | Semantic "why it matters" + project-specific fix |
| **Recommendation** | "Use `MustUser()` pattern per project convention" |
| **Strategy** | "Unit test X, integration test Y, skip E2E for Z" |

---

### Inspector Analysis Skills

#### `assess_change_risk`

Orchestrates lint + git + archivalist to produce prioritized, contextual findings.

```go
type ChangeRiskAssessment struct {
    RiskScore        float64              `json:"risk_score"`       // 0.0-1.0
    PriorityFindings []PrioritizedFinding `json:"priority_findings"`
    BlastRadius      []string             `json:"blast_radius"`     // Affected files
    Summary          string               `json:"summary"`
}

type PrioritizedFinding struct {
    Finding        LintFinding   `json:"finding"`
    Priority       Priority      `json:"priority"`       // Critical, High, Medium, Low
    InChangedCode  bool          `json:"in_changed_code"`
    HistoricalBugs []BugRecord   `json:"historical_bugs"`
    Explanation    string        `json:"explanation"`
    Suggestion     string        `json:"suggestion"`
}

func (i *Inspector) AssessChangeRisk(ctx context.Context, files []string) (*ChangeRiskAssessment, error) {
    // 1. Get raw data from existing tools (PARALLEL)
    var (
        lintResult    *LintResult
        diagnostics   []Diagnostic
        changedLines  map[string][]int
        bugHistory    []BugRecord
    )

    g, gCtx := errgroup.WithContext(ctx)

    g.Go(func() error {
        var err error
        lintResult, err = i.lintSkill.Run(gCtx, files)
        return err
    })

    g.Go(func() error {
        var err error
        diagnostics, err = i.lspSkill.Diagnostics(gCtx, files)
        return err
    })

    g.Go(func() error {
        var err error
        changedLines, err = i.gitSkill.ChangedLines(gCtx, files)
        return err
    })

    g.Go(func() error {
        var err error
        bugHistory, err = i.archivalist.QueryBugHistory(gCtx, files)
        return err
    })

    if err := g.Wait(); err != nil {
        return nil, err
    }

    // 2. FILTER: Only findings in changed lines
    relevantFindings := i.filterToChangedLines(lintResult.Findings, changedLines)

    // 3. CORRELATE: Cross-reference with bug history
    prioritized := make([]PrioritizedFinding, 0, len(relevantFindings))
    for _, f := range relevantFindings {
        pf := PrioritizedFinding{
            Finding:       f,
            InChangedCode: true,
        }

        // Check if this pattern caused bugs before
        pf.HistoricalBugs = i.findSimilarBugs(f, bugHistory)
        if len(pf.HistoricalBugs) > 0 {
            pf.Priority = PriorityCritical
            pf.Explanation = fmt.Sprintf(
                "This pattern caused %d previous bugs, most recently: %s",
                len(pf.HistoricalBugs),
                pf.HistoricalBugs[0].Title,
            )
        } else {
            pf.Priority = i.calculatePriority(f)
        }

        // Generate project-specific suggestion
        pf.Suggestion = i.generateSuggestion(ctx, f)

        prioritized = append(prioritized, pf)
    }

    // 4. Sort by priority
    sort.Slice(prioritized, func(a, b int) bool {
        return prioritized[a].Priority < prioritized[b].Priority
    })

    // 5. Calculate risk score
    riskScore := i.calculateRiskScore(prioritized, changedLines)

    // 6. Identify blast radius
    blastRadius := i.identifyBlastRadius(ctx, files)

    return &ChangeRiskAssessment{
        RiskScore:        riskScore,
        PriorityFindings: prioritized[:min(10, len(prioritized))], // Top 10 only
        BlastRadius:      blastRadius,
        Summary:          i.generateSummary(prioritized, riskScore),
    }, nil
}
```

**Value:** 50 lint warnings → 3 prioritized findings with context and history.

#### `explain_finding`

Adds semantic explanation and project-specific fix suggestions.

```go
type FindingExplanation struct {
    What       string   `json:"what"`        // The lint message
    Why        string   `json:"why"`         // Why it matters
    Risk       string   `json:"risk"`        // What could go wrong
    HowToFix   string   `json:"how_to_fix"`  // Project-specific fix
    Example    string   `json:"example"`     // Example from this project
    References []string `json:"references"`  // Links to docs
}

func (i *Inspector) ExplainFinding(ctx context.Context, finding LintFinding) (*FindingExplanation, error) {
    // 1. Get type info from LSP for context
    typeInfo, _ := i.lspSkill.Hover(ctx, finding.Location)

    // 2. Get project patterns from Archivalist
    patterns, _ := i.archivalist.QueryPatterns(ctx, QueryPatterns{
        Category: finding.Category,
        Limit:    5,
    })

    // 3. Build explanation
    explanation := &FindingExplanation{
        What: finding.Message,
        Why:  i.explainWhyItMatters(finding, typeInfo),
        Risk: i.explainRisk(finding),
    }

    // 4. Generate project-specific fix
    if len(patterns.Examples) > 0 {
        explanation.HowToFix = fmt.Sprintf(
            "In this project, this is typically handled with the %s pattern. See %s:%d for an example.",
            patterns.Examples[0].PatternName,
            patterns.Examples[0].File,
            patterns.Examples[0].Line,
        )
        explanation.Example = patterns.Examples[0].Code
    } else {
        explanation.HowToFix = i.generateGenericFix(finding)
    }

    explanation.References = finding.Links

    return explanation, nil
}
```

**Value:** `SA5011: possible nil pointer dereference` becomes:
> "The `user` variable can be nil when `GetUser()` returns no result. This could cause a panic at runtime. In this project, handle this with the `MustUser()` pattern. See `auth/session.go:45` for an example: `user := MustUser(ctx)`"

#### `validate_architecture`

Enforces project-specific architectural rules.

```go
type ArchitectureReport struct {
    Violations []ArchViolation `json:"violations"`
    Score      float64         `json:"score"`      // 0.0-1.0 (1.0 = no violations)
    Summary    string          `json:"summary"`
}

type ArchViolation struct {
    Rule        string        `json:"rule"`
    Location    Location      `json:"location"`
    Description string        `json:"description"`
    Severity    Severity      `json:"severity"`
    Previous    []ArchRecord  `json:"previous"`    // Past occurrences
    Suggestion  string        `json:"suggestion"`
}

func (i *Inspector) ValidateArchitecture(ctx context.Context, files []string) (*ArchitectureReport, error) {
    // 1. Get project's architectural rules from Archivalist
    rules, err := i.archivalist.QueryArchitectureRules(ctx)
    if err != nil {
        return nil, err
    }
    // Example rules:
    // - "handlers/ cannot import repositories/ directly"
    // - "domain/ cannot import infrastructure/"
    // - "All exported functions in api/ must have context.Context as first param"

    // 2. Get import graph from LSP
    imports, err := i.lspSkill.ImportGraph(ctx, files)
    if err != nil {
        return nil, err
    }

    // 3. Check rules against imports
    violations := []ArchViolation{}
    for _, rule := range rules {
        v := rule.Check(imports, files)
        if v != nil {
            // 4. Enrich with historical data
            v.Previous = i.archivalist.QuerySimilarViolations(ctx, v)
            v.Suggestion = i.generateArchSuggestion(rule, v)
            violations = append(violations, *v)
        }
    }

    // 5. Calculate score
    score := 1.0 - (float64(len(violations)) * 0.1) // -10% per violation
    if score < 0 {
        score = 0
    }

    return &ArchitectureReport{
        Violations: violations,
        Score:      score,
        Summary:    i.generateArchSummary(violations),
    }, nil
}
```

**Value:** Enforces THIS PROJECT's conventions, not generic rules.

#### `evaluate_blast_radius`

Identifies what else might break when changing files.

```go
type BlastRadiusReport struct {
    ChangedFiles    []string           `json:"changed_files"`
    DirectDependents []FileDependency  `json:"direct_dependents"`
    TransitiveDeps   []FileDependency  `json:"transitive_deps"`
    RiskLevel        string            `json:"risk_level"` // Low, Medium, High, Critical
    Recommendation   string            `json:"recommendation"`
}

func (i *Inspector) EvaluateBlastRadius(ctx context.Context, files []string) (*BlastRadiusReport, error) {
    // 1. Get references from LSP
    allDependents := make(map[string]*FileDependency)

    for _, file := range files {
        // Get all symbols defined in this file
        symbols, _ := i.lspSkill.DocumentSymbols(ctx, file)

        for _, sym := range symbols {
            if !sym.Exported {
                continue
            }

            // Find all references to this symbol
            refs, _ := i.lspSkill.References(ctx, sym.Location)
            for _, ref := range refs {
                if ref.File == file {
                    continue // Skip self-references
                }

                dep, ok := allDependents[ref.File]
                if !ok {
                    dep = &FileDependency{
                        File:      ref.File,
                        Symbols:   []string{},
                        IsDirect:  true,
                    }
                    allDependents[ref.File] = dep
                }
                dep.Symbols = append(dep.Symbols, sym.Name)
            }
        }
    }

    // 2. Separate direct vs transitive
    directDeps := []FileDependency{}
    for _, dep := range allDependents {
        directDeps = append(directDeps, *dep)
    }

    // 3. Optionally compute transitive (one level deep)
    transitiveDeps := i.computeTransitiveDeps(ctx, directDeps, 1)

    // 4. Calculate risk level
    totalAffected := len(directDeps) + len(transitiveDeps)
    riskLevel := "Low"
    if totalAffected > 20 {
        riskLevel = "Critical"
    } else if totalAffected > 10 {
        riskLevel = "High"
    } else if totalAffected > 5 {
        riskLevel = "Medium"
    }

    return &BlastRadiusReport{
        ChangedFiles:     files,
        DirectDependents: directDeps,
        TransitiveDeps:   transitiveDeps,
        RiskLevel:        riskLevel,
        Recommendation:   i.generateBlastRecommendation(riskLevel, directDeps),
    }, nil
}
```

**Value:** "Changing `user.go` affects 12 files including `auth.go`, `session.go`. Run integration tests."

---

### Tester Analysis Skills

#### `prioritize_test_targets`

Combines coverage, complexity, changes, and history to prioritize what to test.

```go
type TestPriority struct {
    File            string   `json:"file"`
    Functions       []string `json:"functions"`
    PriorityScore   float64  `json:"priority_score"`  // 0-100
    Reasons         []string `json:"reasons"`
    RecommendedType TestType `json:"recommended_type"` // Unit, Integration, E2E
    CurrentCoverage float64  `json:"current_coverage"`
    Complexity      int      `json:"complexity"`
}

type TestType string

const (
    TestTypeUnit        TestType = "unit"
    TestTypeIntegration TestType = "integration"
    TestTypeE2E         TestType = "e2e"
)

func (t *Tester) PrioritizeTestTargets(ctx context.Context) ([]TestPriority, error) {
    // 1. Get coverage from test skill
    coverage, err := t.testSkill.Coverage(ctx)
    if err != nil {
        return nil, err
    }

    // 2. Get complexity from lint skill
    complexity, err := t.lintSkill.ComplexityReport(ctx)
    if err != nil {
        return nil, err
    }

    // 3. Get changed files from git skill
    changed, err := t.gitSkill.ChangedFiles(ctx)
    if err != nil {
        return nil, err
    }

    // 4. Get bug-prone files from Archivalist
    bugDensity, err := t.archivalist.QueryBugDensity(ctx)
    if err != nil {
        return nil, err
    }

    // 5. Score and prioritize each uncovered function
    priorities := []TestPriority{}

    for _, fn := range coverage.UncoveredFunctions() {
        score := 0.0
        reasons := []string{}

        // +40 if in changed files
        if changed.Contains(fn.File) {
            score += 40
            reasons = append(reasons, "recently changed")
        }

        // +30 if high complexity
        fnComplexity := complexity.ForFunction(fn)
        if fnComplexity > 10 {
            score += 30
            reasons = append(reasons, fmt.Sprintf("complexity=%d", fnComplexity))
        } else if fnComplexity > 5 {
            score += 15
            reasons = append(reasons, fmt.Sprintf("complexity=%d", fnComplexity))
        }

        // +30 if file has bug history
        bugs := bugDensity.ForFile(fn.File)
        if bugs > 2 {
            score += 30
            reasons = append(reasons, fmt.Sprintf("%d previous bugs", bugs))
        } else if bugs > 0 {
            score += 15
            reasons = append(reasons, fmt.Sprintf("%d previous bug", bugs))
        }

        priorities = append(priorities, TestPriority{
            File:            fn.File,
            Functions:       []string{fn.Name},
            PriorityScore:   score,
            Reasons:         reasons,
            RecommendedType: t.recommendTestType(fn),
            CurrentCoverage: coverage.ForFunction(fn),
            Complexity:      fnComplexity,
        })
    }

    // 6. Sort by priority score descending
    sort.Slice(priorities, func(i, j int) bool {
        return priorities[i].PriorityScore > priorities[j].PriorityScore
    })

    return priorities, nil
}

func (t *Tester) recommendTestType(fn FunctionInfo) TestType {
    // Heuristics for test type recommendation
    if strings.Contains(fn.File, "handler") || strings.Contains(fn.File, "api") {
        return TestTypeIntegration
    }
    if strings.Contains(fn.File, "e2e") || strings.Contains(fn.File, "acceptance") {
        return TestTypeE2E
    }
    if fn.HasExternalDeps { // DB, HTTP, filesystem
        return TestTypeIntegration
    }
    return TestTypeUnit
}
```

**Value:** "47% coverage" becomes "Test `ProcessPayment` first: changed today, complexity=15, 3 previous bugs."

#### `assess_test_quality`

Evaluates test suite quality beyond coverage.

```go
type TestQualityReport struct {
    CoverageScore     float64            `json:"coverage_score"`     // 0-100
    AssertionDensity  float64            `json:"assertion_density"`  // Assertions per test
    MutationScore     float64            `json:"mutation_score"`     // Mutation kill rate
    Weaknesses        []TestWeakness     `json:"weaknesses"`
    Suggestions       []string           `json:"suggestions"`
    OverallGrade      string             `json:"overall_grade"`      // A, B, C, D, F
}

type TestWeakness struct {
    TestFile    string   `json:"test_file"`
    TestName    string   `json:"test_name"`
    Type        string   `json:"type"`        // "no_assertions", "single_path", "no_error_test"
    Description string   `json:"description"`
    Suggestion  string   `json:"suggestion"`
}

func (t *Tester) AssessTestQuality(ctx context.Context, testFiles []string) (*TestQualityReport, error) {
    // 1. Run tests to get baseline
    results, err := t.testSkill.Run(ctx, testFiles)
    if err != nil {
        return nil, err
    }

    // 2. Get coverage
    coverage, err := t.testSkill.Coverage(ctx)
    if err != nil {
        return nil, err
    }

    // 3. Analyze test patterns (via AST, leveraging existing parsing)
    patterns, err := t.analyzeTestPatterns(ctx, testFiles)
    if err != nil {
        return nil, err
    }

    // 4. Run mutation testing sample (if enabled)
    var mutationScore float64 = -1 // -1 means not run
    if t.config.EnableMutationSampling {
        mutations, err := t.runMutationSample(ctx, testFiles, 10)
        if err == nil {
            mutationScore = mutations.KillRate
        }
    }

    // 5. Identify weaknesses
    weaknesses := []TestWeakness{}

    for _, test := range patterns.Tests {
        if test.AssertionCount == 0 {
            weaknesses = append(weaknesses, TestWeakness{
                TestFile:    test.File,
                TestName:    test.Name,
                Type:        "no_assertions",
                Description: "Test has no assertions - it passes regardless of behavior",
                Suggestion:  "Add assertions to verify expected outcomes",
            })
        }

        if test.BranchesTestedCount == 1 && test.FunctionBranches > 1 {
            weaknesses = append(weaknesses, TestWeakness{
                TestFile:    test.File,
                TestName:    test.Name,
                Type:        "single_path",
                Description: fmt.Sprintf("Only tests 1 of %d branches", test.FunctionBranches),
                Suggestion:  "Add test cases for other branches (error paths, edge cases)",
            })
        }

        if test.FunctionReturnsError && !test.HasErrorTest {
            weaknesses = append(weaknesses, TestWeakness{
                TestFile:    test.File,
                TestName:    test.Name,
                Type:        "no_error_test",
                Description: "Function returns error but no error case is tested",
                Suggestion:  "Add test case that triggers and verifies error handling",
            })
        }
    }

    // 6. Calculate overall grade
    grade := t.calculateGrade(coverage.Percentage, patterns.AssertionDensity, mutationScore, len(weaknesses))

    return &TestQualityReport{
        CoverageScore:    coverage.Percentage,
        AssertionDensity: patterns.AssertionDensity,
        MutationScore:    mutationScore,
        Weaknesses:       weaknesses,
        Suggestions:      t.generateSuggestions(weaknesses),
        OverallGrade:     grade,
    }, nil
}
```

**Value:** "Tests pass" becomes "Tests pass but: 3 tests have no assertions, `TestPayment` doesn't test error path, grade: C."

#### `suggest_test_cases`

Generates specific test case suggestions based on code analysis.

```go
type TestSuggestion struct {
    Name        string            `json:"name"`
    Description string            `json:"description"`
    Inputs      map[string]any    `json:"inputs"`
    Expected    string            `json:"expected"`
    Reason      string            `json:"reason"`
    Priority    int               `json:"priority"`  // 1=highest
}

func (t *Tester) SuggestTestCases(ctx context.Context, fn FunctionInfo) ([]TestSuggestion, error) {
    // 1. Get function signature from LSP
    sig, err := t.lspSkill.Signature(ctx, fn.Location)
    if err != nil {
        return nil, err
    }

    // 2. Get function body for analysis
    body, err := t.readSkill.FunctionBody(ctx, fn.Location)
    if err != nil {
        return nil, err
    }

    // 3. Extract boundaries from code
    boundaries := t.extractBoundaries(body)
    // Examples:
    // `if age < 18` → boundaries at 17, 18, 19
    // `for i := 0; i < len(items); i++` → boundaries at 0, 1, len-1, len
    // `switch status` → each case value

    // 4. Get existing test cases
    existing, _ := t.findExistingTests(ctx, fn)

    // 5. Get patterns from similar functions
    similar, _ := t.archivalist.QuerySimilarFunctionTests(ctx, fn)

    suggestions := []TestSuggestion{}
    priority := 1

    // 6. Suggest boundary tests
    for _, b := range boundaries {
        if existing.Covers(b) {
            continue
        }

        for _, input := range b.BoundaryInputs() {
            suggestions = append(suggestions, TestSuggestion{
                Name:        fmt.Sprintf("Test%s_%s_%v", fn.Name, b.Name, input.Value),
                Description: fmt.Sprintf("Test boundary condition: %s = %v", b.Name, input.Value),
                Inputs:      input.AsMap(),
                Expected:    input.ExpectedBehavior,
                Reason:      fmt.Sprintf("Boundary at '%s' not covered", b.Description),
                Priority:    priority,
            })
            priority++
        }
    }

    // 7. Suggest error case if function returns error
    if sig.ReturnsError() && !existing.HasErrorTest() {
        suggestions = append(suggestions, TestSuggestion{
            Name:        fmt.Sprintf("Test%s_Error", fn.Name),
            Description: "Test error handling path",
            Inputs:      t.generateErrorInputs(sig),
            Expected:    "Returns non-nil error",
            Reason:      "Error path not tested",
            Priority:    priority,
        })
        priority++
    }

    // 8. Suggest nil/empty cases for pointer/slice params
    for _, param := range sig.Parameters {
        if (param.IsPointer || param.IsSlice) && !existing.HasNilTest(param.Name) {
            suggestions = append(suggestions, TestSuggestion{
                Name:        fmt.Sprintf("Test%s_%sNil", fn.Name, param.Name),
                Description: fmt.Sprintf("Test nil %s handling", param.Name),
                Inputs:      map[string]any{param.Name: nil},
                Expected:    "Handles nil gracefully (no panic)",
                Reason:      fmt.Sprintf("Nil %s case not tested", param.Name),
                Priority:    priority,
            })
            priority++
        }
    }

    // 9. Suggest patterns from similar functions
    for _, pattern := range similar.CommonPatterns() {
        if !existing.HasPattern(pattern) {
            suggestions = append(suggestions, pattern.AsSuggestion(fn, priority))
            priority++
        }
    }

    return suggestions, nil
}
```

**Value:** Automated boundary detection + historical patterns = specific, actionable test suggestions.

#### `identify_coverage_gaps`

Finds coverage gaps specifically in changed code.

```go
type CoverageGap struct {
    File        string   `json:"file"`
    Function    string   `json:"function"`
    Lines       []int    `json:"lines"`      // Uncovered line numbers
    IsChanged   bool     `json:"is_changed"` // Was this line changed recently?
    Complexity  int      `json:"complexity"`
    Suggestion  string   `json:"suggestion"`
}

func (t *Tester) IdentifyCoverageGaps(ctx context.Context) ([]CoverageGap, error) {
    // 1. Get coverage
    coverage, err := t.testSkill.Coverage(ctx)
    if err != nil {
        return nil, err
    }

    // 2. Get changed lines
    changedLines, err := t.gitSkill.ChangedLines(ctx, nil) // All files
    if err != nil {
        return nil, err
    }

    // 3. Get complexity
    complexity, err := t.lintSkill.ComplexityReport(ctx)
    if err != nil {
        return nil, err
    }

    gaps := []CoverageGap{}

    // 4. For each uncovered region, check if it's in changed code
    for _, uncovered := range coverage.UncoveredRegions() {
        isChanged := false
        changedUncovered := []int{}

        if lines, ok := changedLines[uncovered.File]; ok {
            for _, line := range uncovered.Lines {
                if contains(lines, line) {
                    isChanged = true
                    changedUncovered = append(changedUncovered, line)
                }
            }
        }

        // Prioritize changed + uncovered
        if isChanged {
            gaps = append(gaps, CoverageGap{
                File:       uncovered.File,
                Function:   uncovered.Function,
                Lines:      changedUncovered,
                IsChanged:  true,
                Complexity: complexity.ForFunction(uncovered.Function),
                Suggestion: t.generateCoverageSuggestion(uncovered),
            })
        }
    }

    // Sort by: changed first, then by complexity
    sort.Slice(gaps, func(i, j int) bool {
        if gaps[i].IsChanged != gaps[j].IsChanged {
            return gaps[i].IsChanged // Changed first
        }
        return gaps[i].Complexity > gaps[j].Complexity // Higher complexity first
    })

    return gaps, nil
}
```

**Value:** "Changed lines 45-67 in `payment.go` have no test coverage. Add test for `validateAmount()`."

---

### Skill Bundles for Lazy Loading

```go
var AnalysisSkillBundles = map[string][]string{
    "inspector_analysis": {
        "assess_change_risk",
        "explain_finding",
        "validate_architecture",
        "evaluate_blast_radius",
    },
    "tester_analysis": {
        "prioritize_test_targets",
        "assess_test_quality",
        "suggest_test_cases",
        "identify_coverage_gaps",
    },
}

// Manifest entries (~20 tokens each, vs ~150 for full schema)
var AnalysisSkillManifests = []SkillManifest{
    {Name: "assess_change_risk", Brief: "Prioritized findings in changed code with history"},
    {Name: "explain_finding", Brief: "Semantic explanation with project-specific fix"},
    {Name: "validate_architecture", Brief: "Check project architectural rules"},
    {Name: "evaluate_blast_radius", Brief: "Identify files affected by changes"},
    {Name: "prioritize_test_targets", Brief: "Rank what to test by risk"},
    {Name: "assess_test_quality", Brief: "Evaluate test suite beyond coverage"},
    {Name: "suggest_test_cases", Brief: "Generate test suggestions from code"},
    {Name: "identify_coverage_gaps", Brief: "Find uncovered changed code"},
}
```

---

### Integration with Existing Skills

```
Analysis Skills call existing skills (never external tools directly):

┌─────────────────────────────────────────────────────────────────┐
│                    assess_change_risk                            │
└─────────────────────────────────────────────────────────────────┘
                              │
        ┌─────────────────────┼─────────────────────┬────────────────────┐
        │                     │                     │                    │
        ▼                     ▼                     ▼                    ▼
┌─────────────┐       ┌─────────────┐       ┌─────────────┐      ┌─────────────┐
│  lint_run   │       │lsp_diagnostics│     │git_changed  │      │archivalist_ │
│  (existing) │       │  (existing)  │      │  (existing) │      │query_bugs   │
└─────────────┘       └─────────────┘       └─────────────┘      └─────────────┘
        │                     │                     │                    │
        ▼                     ▼                     ▼                    ▼
┌─────────────┐       ┌─────────────┐       ┌─────────────┐      ┌─────────────┐
│golangci-lint│       │    gopls    │       │     git     │      │VectorGraphDB│
└─────────────┘       └─────────────┘       └─────────────┘      └─────────────┘
```

This ensures:
1. No direct tool invocation (always through skills)
2. Consistent error handling
3. Proper caching at skill level
4. Audit logging for all operations

---

## Single-Worker Pipeline Architecture

Pipelines are the execution units for task completion. They are designed to be **fast**, **focused**, and **ephemeral**.

### Core Principle: One Pipeline = One Worker Type

Each pipeline has exactly ONE worker type (Designer OR Engineer). This enables:
- Clear ownership and blame attribution
- Minimal context size
- Focused Inspector/Tester modes
- Parallel execution of independent pipelines

```
┌─────────────────────────────────────────────────────────────────────────────────────┐
│                    TDD SINGLE-WORKER PIPELINE (RED → GREEN → REFACTOR)               │
├─────────────────────────────────────────────────────────────────────────────────────┤
│                                                                                     │
│  PHASE 1: CRITERIA DEFINITION (Inspector defines what "done" means)                 │
│  ════════════════════════════════════════════════════════════════════              │
│                        ┌──────────────────┐                                         │
│                        │    INSPECTOR     │                                         │
│                        │                  │  ← Mode derived from worker type        │
│                        │ DEFINES:         │    Designer → UI criteria               │
│                        │ • Success critera│    Engineer → Code criteria             │
│                        │ • Quality gates  │                                         │
│                        │ • Constraints    │                                         │
│                        └────────┬─────────┘                                         │
│                                 │                                                   │
│                                 ▼ Criteria passed to Tester                         │
│                                                                                     │
│  PHASE 2: TEST CREATION - RED (Tester creates tests that WILL FAIL)                 │
│  ════════════════════════════════════════════════════════════════════              │
│                        ┌──────────────────┐                                         │
│                        │      TESTER      │                                         │
│                        │                  │  ← Mode derived from worker type        │
│                        │ CREATES:         │    Designer → UI tests                  │
│                        │ • Tests based on │    Engineer → Code tests                │
│                        │   Inspector      │                                         │
│                        │   criteria       │                                         │
│                        │ • Expected: FAIL │  ← RED phase (no implementation yet)    │
│                        └────────┬─────────┘                                         │
│                                 │                                                   │
│                                 ▼ Tests + Criteria passed to Worker                 │
│                                                                                     │
│  PHASE 3: IMPLEMENTATION - GREEN (Worker makes tests pass)                          │
│  ════════════════════════════════════════════════════════════════════              │
│                        ┌──────────────────┐                                         │
│                        │      WORKER      │                                         │
│                        │                  │  ← Architect assigns one type           │
│                        │ Designer OR      │                                         │
│                        │ Engineer         │                                         │
│                        │                  │                                         │
│                        │ IMPLEMENTS:      │                                         │
│                        │ • Task work      │  ← GREEN phase (make tests pass)        │
│                        │ • Satisfies      │                                         │
│                        │   criteria       │                                         │
│                        └────────┬─────────┘                                         │
│                                 │                                                   │
│                                 ▼                                                   │
│                                                                                     │
│  PHASE 4: VALIDATION (Both Inspector AND Tester must pass)                          │
│  ════════════════════════════════════════════════════════════════════              │
│           ┌─────────────────────────────────────────────────────┐                   │
│           │              PARALLEL VALIDATION                     │                   │
│           │  ┌──────────────────┐     ┌──────────────────┐      │                   │
│           │  │    INSPECTOR     │     │      TESTER      │      │                   │
│           │  │    (validate)    │     │    (run tests)   │      │                   │
│           │  │                  │     │                  │      │                   │
│           │  │ Checks criteria  │     │ Runs test suite  │      │                   │
│           │  │ are satisfied    │     │ created earlier  │      │                   │
│           │  └────────┬─────────┘     └────────┬─────────┘      │                   │
│           │           │                        │                 │                   │
│           └───────────┼────────────────────────┼─────────────────┘                   │
│                       │                        │                                     │
│                       ▼                        ▼                                     │
│              ┌─────────────────────────────────────────┐                            │
│              │     COMPLETION CHECK                     │                            │
│              │                                          │                            │
│              │  Inspector.pass == true                  │                            │
│              │        AND                               │                            │
│              │  Tester.pass == true                     │                            │
│              └───────────────────┬─────────────────────┘                            │
│                                  │                                                   │
│              ┌───────────────────┼───────────────────────┐                          │
│              │                   │                       │                          │
│              ▼                   │                       ▼                          │
│       ┌──────────────┐          │              ┌──────────────┐                     │
│       │ EITHER FAIL  │          │              │  BOTH PASS   │                     │
│       └──────┬───────┘          │              └──────┬───────┘                     │
│              │                  │                     │                             │
│              │                  │                     ▼                             │
│              │                  │            ┌──────────────┐                       │
│              │                  │            │  COMPLETED   │                       │
│              │ Loop back to     │            │              │                       │
│              │ INSPECTOR        │            │ Report to    │                       │
│              │ (refine criteria │            │ Architect    │                       │
│              │  if needed)      │            └──────────────┘                       │
│              │                  │                                                   │
│              ▼                  │                                                   │
│    ┌──────────────────┐        │                                                   │
│    │ LOOP TO PHASE 1  │────────┘                                                   │
│    │                  │                                                            │
│    │ • Inspector may  │                                                            │
│    │   refine criteria│                                                            │
│    │ • Tester updates │                                                            │
│    │   tests          │                                                            │
│    │ • Worker retries │                                                            │
│    └──────────────────┘                                                            │
│                                                                                     │
└─────────────────────────────────────────────────────────────────────────────────────┘
```

### Pipeline Data Structures

```go
type WorkerType string

const (
    WorkerDesigner WorkerType = "designer"
    WorkerEngineer WorkerType = "engineer"
)

// TDD Pipeline Phases: INSPECT → TEST (RED) → WORK (GREEN) → VALIDATE → loop/complete
type PipelineStatus string

const (
    PipelinePending          PipelineStatus = "pending"           // Not yet started
    PipelineDefiningCriteria PipelineStatus = "defining_criteria" // Phase 1: Inspector defines criteria
    PipelineCreatingTests    PipelineStatus = "creating_tests"    // Phase 2: Tester creates tests (RED)
    PipelineExecuting        PipelineStatus = "executing"         // Phase 3: Worker implements (GREEN)
    PipelineValidating       PipelineStatus = "validating"        // Phase 4: Both validate in parallel
    PipelineCompleted        PipelineStatus = "completed"         // Both Inspector AND Tester passed
    PipelineFailed           PipelineStatus = "failed"            // Max loops exceeded
)

type Pipeline struct {
    ID          string         `json:"id"`
    TaskID      string         `json:"task_id"`
    WorkerType  WorkerType     `json:"worker_type"`
    Status      PipelineStatus `json:"status"`
    LoopCount   int            `json:"loop_count"`   // Current TDD loop iteration
    MaxLoops    int            `json:"max_loops"`    // Max allowed loops before failure
    CreatedAt   time.Time      `json:"created_at"`
    CompletedAt *time.Time     `json:"completed_at,omitempty"`

    // TDD Phase Outputs (populated sequentially, then validated)
    InspectorCriteria *InspectorCriteria `json:"inspector_criteria,omitempty"` // Phase 1 output
    TesterTests       *TesterTests       `json:"tester_tests,omitempty"`       // Phase 2 output
    WorkerOutput      *WorkerOutput      `json:"worker_output,omitempty"`      // Phase 3 output

    // Validation Results (Phase 4 - both must pass)
    InspectorResult   *InspectorResult   `json:"inspector_result,omitempty"`
    TesterResult      *TesterResult      `json:"tester_result,omitempty"`
}

// InspectorCriteria defines what "done" means for this task (Phase 1)
type InspectorCriteria struct {
    TaskID           string             `json:"task_id"`
    SuccessCriteria  []SuccessCriterion `json:"success_criteria"`
    QualityGates     []QualityGate      `json:"quality_gates"`
    Constraints      []Constraint       `json:"constraints"`
    CreatedAt        time.Time          `json:"created_at"`
}

type SuccessCriterion struct {
    ID          string `json:"id"`
    Description string `json:"description"`
    Verifiable  bool   `json:"verifiable"` // Can be tested programmatically
    Priority    string `json:"priority"`   // "required", "recommended", "optional"
}

type QualityGate struct {
    Name      string `json:"name"`
    Threshold string `json:"threshold"` // e.g., "coverage >= 80%", "no critical issues"
    Automated bool   `json:"automated"` // Can be checked automatically
}

type Constraint struct {
    Type        string `json:"type"`        // "security", "performance", "accessibility", etc.
    Requirement string `json:"requirement"`
    Rationale   string `json:"rationale"`
}

// TesterTests contains the test suite created in Phase 2 (RED)
type TesterTests struct {
    TaskID        string     `json:"task_id"`
    TestFiles     []TestFile `json:"test_files"`
    InitialRun    *TestRun   `json:"initial_run"`     // Should FAIL (RED phase)
    BasedOnCriteria []string `json:"based_on_criteria"` // Links to InspectorCriteria.SuccessCriteria IDs
    CreatedAt     time.Time  `json:"created_at"`
}

type TestFile struct {
    Path      string   `json:"path"`
    TestNames []string `json:"test_names"`
    Framework string   `json:"framework"` // "go_test", "jest", "pytest", etc.
}

type TestRun struct {
    Timestamp   time.Time `json:"timestamp"`
    TotalTests  int       `json:"total_tests"`
    Passed      int       `json:"passed"`
    Failed      int       `json:"failed"`
    Skipped     int       `json:"skipped"`
    Duration    string    `json:"duration"`
    AllPassed   bool      `json:"all_passed"`
    Failures    []TestFailure `json:"failures,omitempty"`
}

// Inspector/Tester modes are DERIVED from WorkerType
func (p *Pipeline) InspectorMode() InspectorMode {
    if p.WorkerType == WorkerDesigner {
        return InspectorUIMode
    }
    return InspectorCodeMode
}

func (p *Pipeline) TesterMode() TesterMode {
    if p.WorkerType == WorkerDesigner {
        return TesterUIMode
    }
    return TesterCodeMode
}

// IsComplete returns true only when BOTH Inspector AND Tester pass validation
func (p *Pipeline) IsComplete() bool {
    return p.InspectorResult != nil && p.InspectorResult.Passed &&
           p.TesterResult != nil && p.TesterResult.Passed
}

// NeedsLoop returns true if either validation failed and loops remain
func (p *Pipeline) NeedsLoop() bool {
    if p.LoopCount >= p.MaxLoops {
        return false
    }
    inspectorFailed := p.InspectorResult != nil && !p.InspectorResult.Passed
    testerFailed := p.TesterResult != nil && !p.TesterResult.Passed
    return inspectorFailed || testerFailed
}
```

### Inspector Modes

The Inspector operates in TWO phases per TDD cycle, with mode determined by pipeline worker type:

**Phase 1 (Criteria Definition):** Inspector analyzes the task and defines success criteria, quality gates, and constraints BEFORE any tests are written or code is implemented.

**Phase 4 (Validation):** Inspector validates that Worker output satisfies the criteria defined in Phase 1.

```
┌─────────────────────────────────────────────────────────────────────────────────────┐
│                      INSPECTOR MODES (TDD-Aware)                                     │
├─────────────────────────────────────────────────────────────────────────────────────┤
│                                                                                     │
│   CODE MODE (Engineer Pipeline)          UI MODE (Designer Pipeline)               │
│   ═════════════════════════════          ══════════════════════════                │
│                                                                                     │
│   PHASE 1 - DEFINES:                     PHASE 1 - DEFINES:                         │
│   • Function signatures required         • Component API contracts                  │
│   • Error handling requirements          • Accessibility requirements (WCAG)        │
│   • Security constraints                 • Design token usage rules                 │
│   • Performance thresholds               • Responsive breakpoint coverage           │
│   • Test coverage minimums               • Keyboard navigation paths                │
│   • API contract specifications          • Color contrast ratios                    │
│   • Type safety requirements             • Semantic HTML structure                  │
│                                          • Animation performance budgets            │
│                                                                                     │
│   PHASE 4 - VALIDATES:                   PHASE 4 - VALIDATES:                       │
│   • Code style consistency               • Design token compliance                  │
│   • Security vulnerabilities             • Accessibility compliance                 │
│   • Error handling patterns              • Component reuse                          │
│   • Test coverage achieved               • Responsive design                        │
│   • Dependency usage                     • Keyboard navigation                      │
│   • API contracts honored                • Color contrast                           │
│   • Performance patterns                 • Focus management                         │
│   • Type safety                          • Semantic HTML                            │
│                                                                                     │
│   Skills Loaded:                         Skills Loaded:                             │
│   • define_code_criteria (Phase 1)       • define_ui_criteria (Phase 1)             │
│   • check_style (Phase 4)                • check_accessibility (Phase 4)            │
│   • check_security (Phase 4)             • check_design_tokens (Phase 4)            │
│   • check_dependencies (Phase 4)         • check_component_reuse (Phase 4)          │
│   • check_error_handling (Phase 4)       • check_responsive (Phase 4)               │
│   • check_test_coverage (Phase 4)        • check_color_contrast (Phase 4)           │
│   • check_types (Phase 4)                • check_semantic_html (Phase 4)            │
│   • receiving_code_review (feedback)     • check_focus_management (Phase 4)         │
│   • verification_before_completion       • verification_before_completion           │
│   • subagent_two_stage_validation        • subagent_two_stage_validation            │
│                                                                                     │
└─────────────────────────────────────────────────────────────────────────────────────┘
```

### Tester Modes

The Tester operates in TWO phases per TDD cycle, with mode determined by pipeline worker type:

**Phase 2 (Test Creation - RED):** Tester receives criteria from Inspector and creates tests that SHOULD FAIL (no implementation exists yet). This is the "RED" in RED-GREEN-REFACTOR.

**Phase 4 (Validation - GREEN check):** Tester runs the test suite created in Phase 2 to verify Worker's implementation makes tests pass.

```
┌─────────────────────────────────────────────────────────────────────────────────────┐
│                       TESTER MODES (TDD-Aware)                                       │
├─────────────────────────────────────────────────────────────────────────────────────┤
│                                                                                     │
│   CODE MODE (Engineer Pipeline)          UI MODE (Designer Pipeline)               │
│   ═════════════════════════════          ══════════════════════════                │
│                                                                                     │
│   PHASE 2 - CREATES (RED):               PHASE 2 - CREATES (RED):                   │
│   • Unit tests from criteria             • Component tests from criteria            │
│   • Integration test stubs               • Visual regression baselines              │
│   • API test cases                       • Accessibility test cases                 │
│   • Performance benchmark setup          • Interaction test scenarios               │
│   • Error case test scenarios            • Responsive test viewports                │
│   • Expected: ALL TESTS FAIL             • Expected: ALL TESTS FAIL                 │
│     (no implementation yet)                (no implementation yet)                  │
│                                                                                     │
│   PHASE 4 - VALIDATES (GREEN check):     PHASE 4 - VALIDATES (GREEN check):         │
│   • Run unit tests                       • Run component tests                      │
│   • Run integration tests                • Run visual regression tests              │
│   • Run API tests                        • Run accessibility tests (axe-core)       │
│   • Run performance benchmarks           • Run interaction tests                    │
│   • Run error case tests                 • Run responsive tests                     │
│   • Expected: ALL TESTS PASS             • Expected: ALL TESTS PASS                 │
│     (Worker implemented correctly)         (Worker implemented correctly)           │
│                                                                                     │
│   Tools:                                 Tools:                                     │
│   • go test / jest / pytest              • Testing Library                          │
│   • Benchmark suites                     • Playwright/Cypress                       │
│   • API test frameworks                  • axe-core                                 │
│   • Coverage tools                       • Chromatic/Percy                          │
│                                          • Storybook                                │
│                                                                                     │
│   Skills Loaded:                         Skills Loaded:                             │
│   • write_unit_test (Phase 2)            • write_component_test (Phase 2)           │
│   • write_integration_test (Phase 2)     • write_visual_test (Phase 2)              │
│   • write_api_test (Phase 2)             • write_a11y_test (Phase 2)                │
│   • run_tests (Phase 4)                  • run_component_tests (Phase 4)            │
│   • check_coverage (Phase 4)             • check_visual_regression (Phase 4)        │
│   • test_driven_development              • test_driven_development                  │
│   • verification_before_completion       • verification_before_completion           │
│                                                                                     │
└─────────────────────────────────────────────────────────────────────────────────────┘
```

### Failure Handling

**TDD Loop Failure (Phase 4):** When EITHER Inspector OR Tester fails validation, the pipeline loops back to Phase 1 (Inspector refines criteria if needed, Tester updates tests, Worker retries implementation).

**Max Loops Exceeded:** Pipeline enters `PipelineFailed` state. User is prompted to intervene.

**Outside Pipeline:** Integration failures between pipelines loop back to the ARCHITECT.

```
┌─────────────────────────────────────────────────────────────────────────────────────┐
│                      FAILURE ROUTING (TDD-Aware)                                     │
├─────────────────────────────────────────────────────────────────────────────────────┤
│                                                                                     │
│   PHASE 4 VALIDATION FAILURE (Either Inspector OR Tester fails)                    │
│   ═════════════════════════════════════════════════════════════                    │
│                                                                                     │
│   Case 1: Inspector fails, Tester passes                                           │
│   ────────────────────────────────────────                                         │
│   Inspector: "Color contrast is too low"                                           │
│        │                                                                            │
│        └──► Loop to Phase 1                                                        │
│             • Inspector refines contrast criterion                                 │
│             • Tester may add contrast-specific tests                               │
│             • Worker (Designer) fixes contrast                                     │
│                                                                                     │
│   Case 2: Tester fails, Inspector passes                                           │
│   ────────────────────────────────────────                                         │
│   Tester: "3 of 5 unit tests failing"                                              │
│        │                                                                            │
│        └──► Loop to Phase 1                                                        │
│             • Inspector reviews if criteria need adjustment                        │
│             • Tester keeps existing tests (they define requirements)               │
│             • Worker (Engineer) fixes implementation                               │
│                                                                                     │
│   Case 3: BOTH Inspector AND Tester fail                                           │
│   ────────────────────────────────────────                                         │
│   Inspector: "Security constraint violated"                                        │
│   Tester: "API tests failing"                                                      │
│        │                                                                            │
│        └──► Loop to Phase 1 with combined feedback                                 │
│             • Inspector consolidates all failed criteria                           │
│             • Tester updates tests to cover all failures                           │
│             • Worker receives comprehensive fix requirements                       │
│                                                                                     │
│   MAX LOOPS EXCEEDED                                                               │
│   ═════════════════                                                                │
│                                                                                     │
│   After max_loops iterations (default: 3):                                         │
│        │                                                                            │
│        ├──► PipelineStatus = "failed"                                              │
│        ├──► User prompted: "Pipeline exceeded 3 loops. Options:"                   │
│        │    1. Increase max_loops and continue                                     │
│        │    2. /task <name> ignore_inspector - bypass Inspector                    │
│        │    3. /task <name> ignore_tester - bypass Tester                          │
│        │    4. Cancel pipeline and escalate to Architect                           │
│        └──► Architect notified of repeated failure pattern                         │
│                                                                                     │
│   ───────────────────────────────────────────────────────────────────────────────  │
│                                                                                     │
│   OUTSIDE PIPELINE (Integration issue between pipelines)                           │
│   ══════════════════════════════════════════════════════                           │
│                                                                                     │
│   ┌────────────────┐         ┌────────────────┐                                    │
│   │ Designer       │         │ Engineer       │                                    │
│   │ Pipeline ✓     │         │ Pipeline ✓     │                                    │
│   │ (BOTH pass)    │         │ (BOTH pass)    │                                    │
│   └───────┬────────┘         └───────┬────────┘                                    │
│           │                          │                                              │
│           └──────────┬───────────────┘                                              │
│                      ▼                                                              │
│           ┌────────────────────┐                                                   │
│           │ INTEGRATION CHECK  │  ← Outside any pipeline                           │
│           │                    │                                                   │
│           │ "UI doesn't match  │                                                   │
│           │  API response"     │                                                   │
│           └─────────┬──────────┘                                                   │
│                     │                                                               │
│                     ▼                                                               │
│           ┌────────────────────┐                                                   │
│           │    ARCHITECT       │  ← Decides which pipeline to re-run               │
│           │                    │                                                   │
│           │ Analyzes issue,    │  Skills used:                                     │
│           │ creates new        │  • systematic_debugging                           │
│           │ corrective task    │  • dispatching_parallel_agents                    │
│           └────────────────────┘  • brainstorming (if novel issue)                 │
│                                                                                     │
└─────────────────────────────────────────────────────────────────────────────────────┘
```

### Hybrid Task Decomposition

When a task requires both UI and code work, the Architect decomposes it into separate focused pipelines:

```
┌─────────────────────────────────────────────────────────────────────────────────────┐
│                    ARCHITECT TASK DECOMPOSITION                                      │
├─────────────────────────────────────────────────────────────────────────────────────┤
│                                                                                     │
│   User: "Add settings page with form validation"                                   │
│                                                                                     │
│   ARCHITECT ANALYSIS:                                                               │
│   ┌─────────────────────────────────────────────────────────────────────────────┐  │
│   │                                                                              │  │
│   │   1. IDENTIFY WORK TYPES                                                    │  │
│   │      ├── UI: Page layout, form components, styling                          │  │
│   │      └── Code: Validation logic, API integration, state                     │  │
│   │                                                                              │  │
│   │   2. IDENTIFY DEPENDENCIES                                                  │  │
│   │      ├── Does UI need code output? (data shapes, hooks)                     │  │
│   │      ├── Does code need UI output? (component refs, events)                 │  │
│   │      └── Are they independent?                                              │  │
│   │                                                                              │  │
│   │   3. DETERMINE EXECUTION ORDER                                              │  │
│   │      ├── UI depends on Code → Engineer first, Designer second               │  │
│   │      ├── Code depends on UI → Designer first, Engineer second               │  │
│   │      └── Independent → Parallel execution                                   │  │
│   │                                                                              │  │
│   └─────────────────────────────────────────────────────────────────────────────┘  │
│                                                                                     │
│   SCENARIO A: UI depends on Code (Engineer → Designer)                             │
│   ───────────────────────────────────────────────────                              │
│   "Create settings form that displays user preferences from API"                   │
│                                                                                     │
│   ┌────────────────────┐              ┌────────────────────┐                       │
│   │ Pipeline 1         │   output     │ Pipeline 2         │                       │
│   │ Worker: Engineer   │─────────────▶│ Worker: Designer   │                       │
│   │                    │   feeds      │                    │                       │
│   │ Create API +       │   into       │ Create form using  │                       │
│   │ usePreferences()   │              │ usePreferences()   │                       │
│   └────────────────────┘              └────────────────────┘                       │
│                                                                                     │
│   SCENARIO B: Code depends on UI (Designer → Engineer)                             │
│   ───────────────────────────────────────────────────                              │
│   "Add validation logic to this form component"                                    │
│                                                                                     │
│   ┌────────────────────┐              ┌────────────────────┐                       │
│   │ Pipeline 1         │   output     │ Pipeline 2         │                       │
│   │ Worker: Designer   │─────────────▶│ Worker: Engineer   │                       │
│   │                    │   feeds      │                    │                       │
│   │ Create form        │   into       │ Add validation     │                       │
│   │ structure          │              │ to form fields     │                       │
│   └────────────────────┘              └────────────────────┘                       │
│                                                                                     │
│   SCENARIO C: Independent (Parallel)                                               │
│   ──────────────────────────────────                                               │
│   "Add settings page" + "Add user preferences API"                                 │
│                                                                                     │
│   ┌────────────────────┐                                                           │
│   │ Pipeline 1         │──────┐                                                    │
│   │ Worker: Designer   │      │      ┌────────────────────┐                        │
│   │ Settings page UI   │      ├─────▶│ Integration test   │                        │
│   └────────────────────┘      │      │ (if needed)        │                        │
│                               │      └────────────────────┘                        │
│   ┌────────────────────┐      │                                                    │
│   │ Pipeline 2         │──────┘      PARALLEL EXECUTION                            │
│   │ Worker: Engineer   │             (faster completion)                           │
│   │ Preferences API    │                                                           │
│   └────────────────────┘                                                           │
│                                                                                     │
│   SCENARIO D: Tightly Coupled (Single Pipeline)                                    │
│   ─────────────────────────────────────────────                                    │
│   "Add a button that calls an API when clicked"                                    │
│                                                                                     │
│   Work is inseparable - Architect assigns to PRIMARY worker type:                  │
│   • Button with simple onClick → Designer (UI-primary)                             │
│   • Complex API orchestration → Engineer (Code-primary)                            │
│                                                                                     │
│   ┌────────────────────┐                                                           │
│   │ Single Pipeline    │   Architect decides based on task emphasis               │
│   │ Worker: Designer   │                                                           │
│   │ OR Engineer        │                                                           │
│   └────────────────────┘                                                           │
│                                                                                     │
└─────────────────────────────────────────────────────────────────────────────────────┘
```

### Pipeline Type Determination

The Architect determines pipeline worker type through analysis or explicit user request:

```
┌─────────────────────────────────────────────────────────────────────────────────────┐
│                    PIPELINE TYPE DETERMINATION                                       │
├─────────────────────────────────────────────────────────────────────────────────────┤
│                                                                                     │
│   PATH 1: AUTOMATIC (Architect analyzes task)                                      │
│   ════════════════════════════════════════════                                     │
│                                                                                     │
│   DESIGNER signals:                      ENGINEER signals:                         │
│   • "component", "modal", "button"       • "api", "endpoint", "backend"            │
│   • "style", "css", "tailwind"           • "database", "query", "model"            │
│   • "responsive", "layout", "grid"       • "algorithm", "logic", "service"         │
│   • "animation", "transition"            • "integration", "auth", "config"         │
│   • "accessibility", "a11y", "aria"      • "cli", "script", "migration"            │
│   • "design system", "theme"             • "test", "benchmark", "performance"      │
│   • "UI", "UX", "user interface"         • "refactor", "optimize", "fix"           │
│                                                                                     │
│   PATH 2: EXPLICIT (User requests via /task command)                               │
│   ══════════════════════════════════════════════════                               │
│                                                                                     │
│   /task designer "Create a card component with hover states"                       │
│        └──► Designer Pipeline (explicit)                                           │
│                                                                                     │
│   /task engineer "Add retry logic to the HTTP client"                              │
│        └──► Engineer Pipeline (explicit)                                           │
│                                                                                     │
│   /task "Add settings page"                                                        │
│        └──► Architect analyzes, may create multiple pipelines                      │
│                                                                                     │
└─────────────────────────────────────────────────────────────────────────────────────┘
```

### Token Efficiency Benefits

```
┌─────────────────────────────────────────────────────────────────────────────────────┐
│                    TOKEN EFFICIENCY: SINGLE-WORKER PIPELINES                         │
├─────────────────────────────────────────────────────────────────────────────────────┤
│                                                                                     │
│   Hybrid task with Inspector finding a code issue:                                 │
│                                                                                     │
│   SEPARATE FOCUSED PIPELINES                                                       │
│   ══════════════════════════                                                       │
│                                                                                     │
│   Designer Pipeline (completes successfully):                                       │
│   ├── Task context:          ~250 tokens                                           │
│   ├── Designer work:         ~700 tokens                                           │
│   ├── Inspector (UI):        ~400 tokens                                           │
│   ├── Tester (UI):           ~400 tokens                                           │
│   └── TOTAL:                 ~1,750 tokens ✓ DONE                                  │
│                                                                                     │
│   Engineer Pipeline (needs retry):                                                  │
│   ├── Task context:          ~250 tokens                                           │
│   ├── Engineer work:         ~700 tokens                                           │
│   ├── Inspector (code):      ~400 tokens  ← FAILS                                  │
│   ├── Engineer retry:        ~500 tokens  ← Focused context                        │
│   ├── Inspector re-check:    ~400 tokens                                           │
│   ├── Tester (code):         ~400 tokens                                           │
│   └── TOTAL:                 ~2,650 tokens                                         │
│                                                                                     │
│   COMBINED: ~4,400 tokens                                                          │
│                                                                                     │
│   vs MIXED PIPELINE (both workers):                                                │
│   ├── Full context on retry: ~1,600 tokens wasted                                  │
│   └── TOTAL:                 ~5,600 tokens                                         │
│                                                                                     │
│   SAVINGS: ~1,200 tokens (21% reduction)                                           │
│   BONUS: Parallel execution when independent (up to 50% faster)                    │
│                                                                                     │
└─────────────────────────────────────────────────────────────────────────────────────┘
```

### Direct Task Commands

Designers and Engineers can be directly addressed via `/task` commands:

```go
// User can explicitly create pipelines
/task designer "Create a responsive navigation component"
/task engineer "Add authentication middleware"
/task "Build user profile page"  // Architect decides decomposition

// Task command handling
func (g *Guide) HandleTaskCommand(ctx context.Context, cmd TaskCommand) error {
    switch {
    case cmd.WorkerType == "designer":
        // Create Designer pipeline directly
        return g.createPipeline(ctx, cmd.Task, WorkerDesigner)

    case cmd.WorkerType == "engineer":
        // Create Engineer pipeline directly
        return g.createPipeline(ctx, cmd.Task, WorkerEngineer)

    default:
        // Route to Architect for analysis and decomposition
        return g.routeToArchitect(ctx, cmd.Task)
    }
}
```

### Designer Pipeline Integration

The Designer operates within the Single-Worker Pipeline architecture with specific UI/UX-focused behaviors.

#### Designer Pipeline Flow

```
┌─────────────────────────────────────────────────────────────────────────────────────┐
│                         DESIGNER PIPELINE EXECUTION                                  │
├─────────────────────────────────────────────────────────────────────────────────────┤
│                                                                                     │
│  INITIALIZATION:                                                                    │
│  ┌──────────────────────────────────────────────────────────────────────────────┐   │
│  │  1. Load design tokens summary into context                                  │   │
│  │  2. Load component index for deduplication awareness                         │   │
│  │  3. Load predecessor artifacts (if sequenced by Architect)                   │   │
│  │  4. Configure Inspector/Tester for UI mode                                   │   │
│  └──────────────────────────────────────────────────────────────────────────────┘   │
│                                                                                     │
│  EXECUTION:                                                                         │
│  ┌──────────────────────────────────────────────────────────────────────────────┐   │
│  │                                                                              │   │
│  │  ┌─────────────────────┐                                                     │   │
│  │  │ DESIGNER            │                                                     │   │
│  │  │ (Gemini 3 Pro)      │                                                     │   │
│  │  │                     │                                                     │   │
│  │  │ Tools:              │                                                     │   │
│  │  │ • Component CRUD    │                                                     │   │
│  │  │ • Design tokens     │                                                     │   │
│  │  │ • Accessibility     │                                                     │   │
│  │  │ • Layout/Responsive │                                                     │   │
│  │  │ • Preview/Visual    │                                                     │   │
│  │  │ • Animation/Motion  │                                                     │   │
│  │  │ • Frontend tooling  │                                                     │   │
│  │  │ • Pipeline artifacts│                                                     │   │
│  │  └──────────┬──────────┘                                                     │   │
│  │             │                                                                │   │
│  │             │ Emits: component refs, token usage, file paths                 │   │
│  │             ▼                                                                │   │
│  │  ┌─────────────────────┐                                                     │   │
│  │  │ INSPECTOR (UI Mode) │                                                     │   │
│  │  │                     │                                                     │   │
│  │  │ Checks:             │                                                     │   │
│  │  │ • Token compliance  │◄── No hardcoded colors/spacing/typography           │   │
│  │  │ • Component reuse   │◄── Searched before creating?                        │   │
│  │  │ • Accessibility     │◄── WCAG 2.1 AA compliance                           │   │
│  │  │ • Responsive design │◄── Mobile-first, all breakpoints                    │   │
│  │  │ • Design system fit │◄── Follows patterns, naming conventions             │   │
│  │  └──────────┬──────────┘                                                     │   │
│  │             │                                                                │   │
│  │     ┌───────┴───────┐                                                        │   │
│  │     │               │                                                        │   │
│  │     ▼               ▼                                                        │   │
│  │  ┌──────┐       ┌──────┐                                                     │   │
│  │  │ FAIL │       │ PASS │                                                     │   │
│  │  └──┬───┘       └──┬───┘                                                     │   │
│  │     │              │                                                         │   │
│  │     │              ▼                                                         │   │
│  │     │    ┌─────────────────────┐                                             │   │
│  │     │    │ TESTER (UI Mode)    │                                             │   │
│  │     │    │                     │                                             │   │
│  │     │    │ Tests:              │                                             │   │
│  │     │    │ • Visual regression │◄── Compare against baseline                 │   │
│  │     │    │ • Responsive render │◄── All viewport sizes                       │   │
│  │     │    │ • A11y automation   │◄── axe-core, lighthouse                     │   │
│  │     │    │ • Keyboard nav      │◄── Focus management                         │   │
│  │     │    │ • Theme switching   │◄── Light/dark mode                          │   │
│  │     │    └──────────┬──────────┘                                             │   │
│  │     │               │                                                        │   │
│  │     │       ┌───────┴───────┐                                                │   │
│  │     │       │               │                                                │   │
│  │     │       ▼               ▼                                                │   │
│  │     │   ┌──────┐       ┌──────────┐                                          │   │
│  │     │   │ FAIL │       │ COMPLETE │                                          │   │
│  │     │   └──┬───┘       └────┬─────┘                                          │   │
│  │     │      │                │                                                │   │
│  │     ▼      ▼                │                                                │   │
│  │  ┌─────────────┐            │                                                │   │
│  │  │ Loop to     │            │                                                │   │
│  │  │ DESIGNER    │            │                                                │   │
│  │  │ with        │            │                                                │   │
│  │  │ feedback    │            │                                                │   │
│  │  └─────────────┘            │                                                │   │
│  │                             │                                                │   │
│  └─────────────────────────────┼────────────────────────────────────────────────┘   │
│                                │                                                    │
│  COMPLETION:                   ▼                                                    │
│  ┌──────────────────────────────────────────────────────────────────────────────┐   │
│  │  Pipeline artifacts emitted:                                                 │   │
│  │  • Component definitions (name, path, props interface, exports)              │   │
│  │  • Layout specifications (regions, breakpoints)                              │   │
│  │  • Token usage report                                                        │   │
│  │  • Accessibility audit results                                               │   │
│  │  • Files modified list                                                       │   │
│  │                                                                              │   │
│  │  If follow-up needed:                                                        │   │
│  │  • designer_signal_dependency notifies Architect                             │   │
│  │  • Architect may spawn Engineer pipeline with these artifacts                │   │
│  └──────────────────────────────────────────────────────────────────────────────┘   │
│                                                                                     │
└─────────────────────────────────────────────────────────────────────────────────────┘
```

#### Designer-Specific Inspector Rules (UI Mode)

```go
type DesignerInspectorRules struct {
    // Token Compliance
    TokenCompliance struct {
        EnforceTokens       bool     `json:"enforce_tokens"`       // true = block on violations
        AllowedPatterns     []string `json:"allowed_patterns"`     // e.g., ["var(--", "theme."]
        ForbiddenPatterns   []string `json:"forbidden_patterns"`   // e.g., ["#[0-9a-f]", "rgb("]
        ExemptProperties    []string `json:"exempt_properties"`    // e.g., ["border-width: 1px"]
    }

    // Component Reuse
    ComponentReuse struct {
        RequireSearchFirst  bool `json:"require_search_first"`  // Must call search before create
        SimilarityThreshold int  `json:"similarity_threshold"`  // Block if >80% similar exists
    }

    // Accessibility
    Accessibility struct {
        MinimumLevel        string   `json:"minimum_level"`        // "AA" (WCAG 2.1)
        RequiredChecks      []string `json:"required_checks"`      // ["contrast", "keyboard", "aria"]
        AutoFix             bool     `json:"auto_fix"`             // Auto-fix simple issues
    }

    // Responsive
    Responsive struct {
        RequiredBreakpoints []string `json:"required_breakpoints"` // ["sm", "md", "lg"]
        MobileFirst         bool     `json:"mobile_first"`         // Enforce mobile-first
    }
}

func DefaultDesignerInspectorRules() DesignerInspectorRules {
    return DesignerInspectorRules{
        TokenCompliance: struct {
            EnforceTokens     bool
            AllowedPatterns   []string
            ForbiddenPatterns []string
            ExemptProperties  []string
        }{
            EnforceTokens:     true,
            AllowedPatterns:   []string{"var(--", "theme.", "tokens.", "color.", "spacing."},
            ForbiddenPatterns: []string{`#[0-9a-fA-F]{3,8}`, `rgba?\(`, `hsla?\(`},
            ExemptProperties:  []string{"border-width: 1px", "border: 0", "outline: 0"},
        },
        ComponentReuse: struct {
            RequireSearchFirst  bool
            SimilarityThreshold int
        }{
            RequireSearchFirst:  true,
            SimilarityThreshold: 80,
        },
        Accessibility: struct {
            MinimumLevel   string
            RequiredChecks []string
            AutoFix        bool
        }{
            MinimumLevel:   "AA",
            RequiredChecks: []string{"contrast", "keyboard", "aria", "focus"},
            AutoFix:        false,
        },
        Responsive: struct {
            RequiredBreakpoints []string
            MobileFirst         bool
        }{
            RequiredBreakpoints: []string{"sm", "md", "lg"},
            MobileFirst:         true,
        },
    }
}
```

#### Designer-Specific Tester Rules (UI Mode)

```go
type DesignerTesterRules struct {
    // Visual Regression
    VisualRegression struct {
        Enabled         bool    `json:"enabled"`
        Threshold       float64 `json:"threshold"`        // Max % diff allowed (0.1 = 0.1%)
        BaselineDir     string  `json:"baseline_dir"`     // Where to store baselines
        UpdateBaseline  bool    `json:"update_baseline"`  // Auto-update on pass
    }

    // Responsive Testing
    ResponsiveTest struct {
        Viewports []Viewport `json:"viewports"`
        CheckOverflow bool   `json:"check_overflow"`    // Detect horizontal scroll
        CheckTruncation bool `json:"check_truncation"`  // Detect text truncation
    }

    // Accessibility Testing
    AccessibilityTest struct {
        Engine         string   `json:"engine"`           // "axe-core", "lighthouse"
        FailOn         []string `json:"fail_on"`          // Severity levels that fail
        SkipRules      []string `json:"skip_rules"`       // Rules to skip
    }

    // Theme Testing
    ThemeTest struct {
        TestBothThemes bool `json:"test_both_themes"`  // Test light AND dark
        CheckContrast  bool `json:"check_contrast"`    // Per-theme contrast check
    }
}

type Viewport struct {
    Name   string `json:"name"`
    Width  int    `json:"width"`
    Height int    `json:"height"`
}

func DefaultDesignerTesterRules() DesignerTesterRules {
    return DesignerTesterRules{
        VisualRegression: struct {
            Enabled        bool
            Threshold      float64
            BaselineDir    string
            UpdateBaseline bool
        }{
            Enabled:        true,
            Threshold:      0.1,
            BaselineDir:    ".visual-baselines",
            UpdateBaseline: false,
        },
        ResponsiveTest: struct {
            Viewports       []Viewport
            CheckOverflow   bool
            CheckTruncation bool
        }{
            Viewports: []Viewport{
                {Name: "mobile", Width: 375, Height: 667},
                {Name: "tablet", Width: 768, Height: 1024},
                {Name: "desktop", Width: 1280, Height: 720},
                {Name: "wide", Width: 1920, Height: 1080},
            },
            CheckOverflow:   true,
            CheckTruncation: true,
        },
        AccessibilityTest: struct {
            Engine    string
            FailOn    []string
            SkipRules []string
        }{
            Engine:    "axe-core",
            FailOn:    []string{"critical", "serious"},
            SkipRules: []string{},
        },
        ThemeTest: struct {
            TestBothThemes bool
            CheckContrast  bool
        }{
            TestBothThemes: true,
            CheckContrast:  true,
        },
    }
}
```

#### Designer Client (Gemini 3 Pro via go-genai)

```go
package designer

import (
    "context"
    "github.com/googleapis/go-genai"
)

type DesignerClient struct {
    client  *genai.Client
    model   *genai.GenerativeModel
    config  DesignerConfig
    session *genai.ChatSession
}

func NewDesignerClient(ctx context.Context, cfg DesignerConfig) (*DesignerClient, error) {
    client, err := genai.NewClient(ctx, &genai.ClientConfig{
        APIKey: os.Getenv("GOOGLE_AI_API_KEY"),
    })
    if err != nil {
        return nil, fmt.Errorf("failed to create genai client: %w", err)
    }

    model := client.GenerativeModel(cfg.Model)
    model.SetTemperature(float32(cfg.Temperature))
    model.SetTopP(float32(cfg.TopP))
    model.SetMaxOutputTokens(int32(cfg.MaxTokens))

    // Configure safety settings for design work
    model.SafetySettings = []*genai.SafetySetting{
        {Category: genai.HarmCategoryHarassment, Threshold: genai.HarmBlockMediumAndAbove},
        {Category: genai.HarmCategoryDangerousContent, Threshold: genai.HarmBlockMediumAndAbove},
    }

    return &DesignerClient{
        client: client,
        model:  model,
        config: cfg,
    }, nil
}

func (dc *DesignerClient) StartSession(systemPrompt string) error {
    dc.session = dc.model.StartChat()

    // Send system prompt as first message
    _, err := dc.session.SendMessage(context.Background(), genai.Text(systemPrompt))
    return err
}

func (dc *DesignerClient) Send(ctx context.Context, message string) (*genai.GenerateContentResponse, error) {
    return dc.session.SendMessage(ctx, genai.Text(message))
}

func (dc *DesignerClient) SendWithImage(ctx context.Context, message string, imagePath string) (*genai.GenerateContentResponse, error) {
    // Read image for multimodal analysis (design review, visual comparison)
    imageData, err := os.ReadFile(imagePath)
    if err != nil {
        return nil, fmt.Errorf("failed to read image: %w", err)
    }

    return dc.session.SendMessage(ctx,
        genai.Text(message),
        genai.ImageData("image/png", imageData),
    )
}

func (dc *DesignerClient) Close() {
    if dc.client != nil {
        dc.client.Close()
    }
}
```

---

## DAG Planning (Architect Skill)

### DAG Output Schema

```json
{
  "id": "workflow-uuid",
  "session_id": "session-uuid",
  "prompt": "Original user request",
  "nodes": [
    {
      "id": "t1",
      "agent": "librarian",
      "prompt": "Index current middleware patterns",
      "context": {
        "relevant_files": ["src/middleware/"],
        "reason": "Need to understand existing patterns"
      },
      "metadata": {
        "priority": "high",
        "timeout_ms": 60000,
        "retry": {"max": 1, "backoff_ms": 1000},
        "file_operations": ["read"]
      },
      "depends_on": []
    },
    {
      "id": "t2",
      "agent": "engineer",
      "prompt": "Create rate limiter interface following existing middleware pattern",
      "context": {
        "upstream_outputs": ["t1"],
        "constraints": ["Follow existing middleware pattern"]
      },
      "metadata": {
        "priority": "normal",
        "file_operations": ["create", "write"]
      },
      "depends_on": ["t1"]
    }
  ],
  "execution_order": [
    ["t1"],
    ["t2", "t3"],
    ["t4"]
  ],
  "policy": {
    "max_concurrency": 4,
    "fail_fast": true,
    "default_retry": {"max": 2, "backoff_ms": 2000}
  }
}
```

---

## Pipeline Architecture

**CRITICAL: Pipelines are isolated execution contexts that enable tight feedback loops for individual tasks while preserving the session-wide quality assurance flow.**

### Two-Level Quality Assurance

Sylk implements quality assurance at TWO levels:

1. **Pipeline-Level (task-specific)**: Direct feedback loops within an isolated context
2. **Session-Level (integration)**: Full validation after all pipelines complete, routed through Architect

```
┌─────────────────────────────────────────────────────────────────────────────────────┐
│                        TWO-LEVEL QUALITY ASSURANCE                                   │
├─────────────────────────────────────────────────────────────────────────────────────┤
│                                                                                     │
│  LEVEL 1: PIPELINE-INTERNAL (per-task, direct feedback)                             │
│  ┌───────────────────────────────────────────────────────────────────────────────┐  │
│  │                                                                               │  │
│  │   Engineer ◄─────────────────────────────────────────────────┐                │  │
│  │      │                                                       │                │  │
│  │      │ completes task                                        │                │  │
│  │      ▼                                                       │                │  │
│  │   Inspector (task-specific)                                  │                │  │
│  │      │ - Lint this file                                      │                │  │
│  │      │ - Format this file                                    │                │  │
│  │      │ - Type-check this file                                │                │  │
│  │      │ - Task compliance check                               │                │  │
│  │      ▼                                                       │                │  │
│  │   ┌──────┐                                                   │                │  │
│  │   │ PASS?├──No──► DIRECT FEEDBACK (no Architect) ────────────┘                │  │
│  │   └──┬───┘                                                                    │  │
│  │      │ Yes                                                                    │  │
│  │      ▼                                                       ┌────────────────┘  │
│  │   Tester (task-specific)                                     │                   │
│  │      │ - Generate tests for THIS requirement                 │                   │
│  │      │ - Run tests for THIS task only                        │                   │
│  │      ▼                                                       │                   │
│  │   ┌──────┐                                                   │                   │
│  │   │ PASS?├──No──► DIRECT FEEDBACK (no Architect) ────────────┘                   │
│  │   └──┬───┘                                                                       │
│  │      │ Yes                                                                       │
│  │      ▼                                                                           │
│  │   PIPELINE COMPLETE                                                              │
│  │                                                                                  │
│  └──────────────────────────────────────────────────────────────────────────────────┘
│                                                                                     │
│  LEVEL 2: SESSION-WIDE (post-DAG, through Architect)                                │
│  ┌───────────────────────────────────────────────────────────────────────────────┐  │
│  │                                                                               │  │
│  │   ALL Pipelines Complete                                                      │  │
│  │      │                                                                        │  │
│  │      ▼                                                                        │  │
│  │   Orchestrator → Architect: "DAG execution done"                              │  │
│  │      │                                                                        │  │
│  │      ▼                                                                        │  │
│  │   INSPECTOR (session-wide)                                                    │  │
│  │      │ - Full scan of ALL changes together                                    │  │
│  │      │ - Integration validation                                               │  │
│  │      │ - Cross-cutting concerns (security, patterns)                          │  │
│  │      │ - Codebase-wide compliance                                             │  │
│  │      ▼                                                                        │  │
│  │   ┌──────┐                                                                    │  │
│  │   │ PASS?├──No──► Architect creates FIX DAG ──► New Pipelines ──► Loop        │  │
│  │   └──┬───┘                                                                    │  │
│  │      │ Yes                                                                    │  │
│  │      ▼                                                                        │  │
│  │   TESTER (session-wide)                                                       │  │
│  │      │ - Full test suite (integration, regression, e2e)                       │  │
│  │      │ - Cross-change validation                                              │  │
│  │      │ - Tests affected by ANY change in DAG                                  │  │
│  │      ▼                                                                        │  │
│  │   ┌──────┐                                                                    │  │
│  │   │ PASS?├──No──► Architect creates FIX DAG ──► New Pipelines ──► Loop        │  │
│  │   └──┬───┘                                                                    │  │
│  │      │ Yes                                                                    │  │
│  │      ▼                                                                        │  │
│  │   WORKFLOW COMPLETE → Architect → User                                        │  │
│  │                                                                               │  │
│  └──────────────────────────────────────────────────────────────────────────────────┘
│                                                                                     │
└─────────────────────────────────────────────────────────────────────────────────────┘
```

### Pipeline vs Session-Level QA Scope

| Concern | Pipeline-Level | Session-Level |
|---------|----------------|---------------|
| **Scope** | Single task's changes | All DAG changes combined |
| **Feedback routing** | Direct to Engineer | Through Architect |
| **Creates new DAG?** | No (internal loop) | Yes (fix DAG) |
| **Inspector focus** | File-level lint/format/types | Integration & cross-cutting |
| **Tester focus** | Task-specific unit tests | Full suite, regression, e2e |
| **User override** | `/task <id>` can ignore | `USER_OVERRIDE` message |

### Pipeline Data Model

```go
// core/pipeline/pipeline.go

type PipelineID string

type PipelineState string

const (
    PipelineStateCreated    PipelineState = "created"
    PipelineStateRunning    PipelineState = "running"
    PipelineStateInspecting PipelineState = "inspecting"
    PipelineStateTesting    PipelineState = "testing"
    PipelineStateCompleted  PipelineState = "completed"
    PipelineStateFailed     PipelineState = "failed"
)

// Pipeline is an isolated execution context containing Engineer + Inspector + Tester
type Pipeline struct {
    ID            PipelineID             `json:"id"`
    SessionID     string                 `json:"session_id"`
    DAGID         string                 `json:"dag_id"`
    TaskID        string                 `json:"task_id"`

    // State
    State         PipelineState          `json:"state"`
    CreatedAt     time.Time              `json:"created_at"`
    CompletedAt   *time.Time             `json:"completed_at,omitempty"`

    // Co-located agents (pipeline-scoped instances)
    EngineerID    string                 `json:"engineer_id"`
    InspectorID   string                 `json:"inspector_id"`
    TesterID      string                 `json:"tester_id"`

    // Task context (shared by all three agents)
    Context       *PipelineContext       `json:"context"`

    // Iteration tracking
    InspectorLoops int                   `json:"inspector_loops"`
    TesterLoops    int                   `json:"tester_loops"`
    MaxLoops       int                   `json:"max_loops"`

    // Results
    EngineerResult  *EngineerResult      `json:"engineer_result,omitempty"`
    InspectorResult *InspectorResult     `json:"inspector_result,omitempty"`
    TesterResult    *TesterResult        `json:"tester_result,omitempty"`
}

// PipelineContext is shared state within a pipeline (all three agents can access)
type PipelineContext struct {
    // Task definition
    TaskPrompt       string              `json:"task_prompt"`
    TaskConstraints  []string            `json:"task_constraints,omitempty"`
    ComplianceCriteria []string          `json:"compliance_criteria,omitempty"`

    // Upstream context (from DAG dependencies)
    UpstreamOutputs  map[string]any      `json:"upstream_outputs,omitempty"`

    // Files touched by this pipeline
    ModifiedFiles    []string            `json:"modified_files,omitempty"`
    CreatedFiles     []string            `json:"created_files,omitempty"`

    // Feedback history (for context in subsequent loops)
    InspectorFeedback []InspectorFeedback `json:"inspector_feedback,omitempty"`
    TesterFeedback    []TesterFeedback    `json:"tester_feedback,omitempty"`
}

// InspectorFeedback is direct feedback from Inspector to Engineer
type InspectorFeedback struct {
    Loop        int                     `json:"loop"`
    Timestamp   time.Time               `json:"timestamp"`
    Issues      []InspectorIssue        `json:"issues"`
    Passed      bool                    `json:"passed"`
}

type InspectorIssue struct {
    File        string                  `json:"file"`
    Line        int                     `json:"line,omitempty"`
    Category    string                  `json:"category"` // lint, format, type, compliance
    Severity    string                  `json:"severity"` // error, warning
    Message     string                  `json:"message"`
    Suggestion  string                  `json:"suggestion,omitempty"`
}

// TesterFeedback is direct feedback from Tester to Engineer
type TesterFeedback struct {
    Loop        int                     `json:"loop"`
    Timestamp   time.Time               `json:"timestamp"`
    TestsRun    int                     `json:"tests_run"`
    TestsPassed int                     `json:"tests_passed"`
    Failures    []TestFailure           `json:"failures,omitempty"`
    Passed      bool                    `json:"passed"`
}

type TestFailure struct {
    TestName    string                  `json:"test_name"`
    File        string                  `json:"file"`
    Message     string                  `json:"message"`
    Expected    string                  `json:"expected,omitempty"`
    Actual      string                  `json:"actual,omitempty"`
    StackTrace  string                  `json:"stack_trace,omitempty"`
}
```

### Pipeline Internal Bus

Pipelines use a dedicated internal bus for direct Engineer ↔ Inspector ↔ Tester communication that bypasses the Guide:

```go
// core/pipeline/bus.go

// PipelineBus handles direct communication within a pipeline
// This is NOT routed through Guide - it's pipeline-internal only
type PipelineBus struct {
    pipelineID PipelineID

    // Direct feedback channels
    inspectorToEngineer chan *InspectorFeedback
    testerToEngineer    chan *TesterFeedback

    // Control channels
    engineerDone        chan *EngineerResult
    inspectorDone       chan *InspectorResult
    testerDone          chan *TesterResult

    // Cancellation
    ctx    context.Context
    cancel context.CancelFunc

    closed atomic.Bool
}

func NewPipelineBus(ctx context.Context, pipelineID PipelineID) *PipelineBus {
    ctx, cancel := context.WithCancel(ctx)
    return &PipelineBus{
        pipelineID:          pipelineID,
        inspectorToEngineer: make(chan *InspectorFeedback, 8),
        testerToEngineer:    make(chan *TesterFeedback, 8),
        engineerDone:        make(chan *EngineerResult, 1),
        inspectorDone:       make(chan *InspectorResult, 1),
        testerDone:          make(chan *TesterResult, 1),
        ctx:                 ctx,
        cancel:              cancel,
    }
}

// SendInspectorFeedback sends feedback directly to Engineer (no Guide routing)
func (b *PipelineBus) SendInspectorFeedback(feedback *InspectorFeedback) error {
    if b.closed.Load() {
        return ErrPipelineClosed
    }
    select {
    case b.inspectorToEngineer <- feedback:
        return nil
    case <-b.ctx.Done():
        return b.ctx.Err()
    }
}

// SendTesterFeedback sends feedback directly to Engineer (no Guide routing)
func (b *PipelineBus) SendTesterFeedback(feedback *TesterFeedback) error {
    if b.closed.Load() {
        return ErrPipelineClosed
    }
    select {
    case b.testerToEngineer <- feedback:
        return nil
    case <-b.ctx.Done():
        return b.ctx.Err()
    }
}
```

### Pipeline Lifecycle

```
┌─────────────────────────────────────────────────────────────────────────────────────┐
│                           PIPELINE LIFECYCLE                                         │
├─────────────────────────────────────────────────────────────────────────────────────┤
│                                                                                     │
│  Orchestrator receives TASK_DISPATCH for engineer task                              │
│      │                                                                              │
│      ▼                                                                              │
│  ┌──────────┐                                                                       │
│  │ CREATED  │  Pipeline created with:                                               │
│  │          │  - Engineer instance                                                  │
│  └────┬─────┘  - Inspector instance (task-scoped)                                   │
│       │        - Tester instance (task-scoped)                                      │
│       │        - Shared PipelineContext                                             │
│       │        - Internal PipelineBus                                               │
│       ▼                                                                             │
│  ┌──────────┐                                                                       │
│  │ RUNNING  │  Engineer executes task                                               │
│  │          │  - Reads/writes files                                                 │
│  │          │  - Updates PipelineContext.ModifiedFiles                              │
│  └────┬─────┘                                                                       │
│       │                                                                             │
│       ▼                                                                             │
│  ┌────────────┐                                                                     │
│  │ INSPECTING │  Inspector validates Engineer's output                              │
│  │            │  - Runs lint/format/type checks on ModifiedFiles                    │
│  │            │  - Checks task compliance                                           │
│  └────┬───────┘                                                                     │
│       │                                                                             │
│       ├── Issues found & loops < max ──► SendInspectorFeedback ──► RUNNING          │
│       │                                                                             │
│       ├── Issues found & loops >= max ──► User prompted (ignore/fail)               │
│       │                                                                             │
│       ▼                                                                             │
│  ┌──────────┐                                                                       │
│  │ TESTING  │  Tester generates & runs task-specific tests                          │
│  │          │  - Creates tests for THIS requirement                                 │
│  │          │  - Runs only relevant tests                                           │
│  └────┬─────┘                                                                       │
│       │                                                                             │
│       ├── Failures & loops < max ──► SendTesterFeedback ──► RUNNING                 │
│       │                                                                             │
│       ├── Failures & loops >= max ──► User prompted (ignore/fail)                   │
│       │                                                                             │
│       ▼                                                                             │
│  ┌───────────┐                                                                      │
│  │ COMPLETED │  Pipeline done, results sent to Orchestrator                         │
│  └───────────┘                                                                      │
│                                                                                     │
│  At any point:                                                                      │
│  ├── User /task <id> ──► Guide routes to Pipeline ──► Engineer receives             │
│  ├── Unrecoverable error ──► FAILED                                                 │
│  └── User cancellation ──► FAILED                                                   │
│                                                                                     │
└─────────────────────────────────────────────────────────────────────────────────────┘
```

### Pipeline Manager

```go
// core/pipeline/manager.go

type PipelineManager interface {
    // Lifecycle
    Create(ctx context.Context, cfg CreatePipelineConfig) (*Pipeline, error)
    Start(ctx context.Context, id PipelineID) error
    Cancel(ctx context.Context, id PipelineID) error

    // Queries
    Get(id PipelineID) (*Pipeline, bool)
    GetBySession(sessionID string) []*Pipeline
    GetByDAG(dagID string) []*Pipeline
    GetActive() []*Pipeline

    // User interaction (via Guide)
    RouteUserMessage(pipelineID PipelineID, msg string) error

    // Results
    GetResult(id PipelineID) (*PipelineResult, error)

    // Cleanup
    CloseAll() error
}

type CreatePipelineConfig struct {
    SessionID          string
    DAGID              string
    TaskID             string
    TaskPrompt         string
    TaskConstraints    []string
    ComplianceCriteria []string
    UpstreamOutputs    map[string]any
    MaxLoops           int  // Default: 3
}

type PipelineResult struct {
    PipelineID      PipelineID          `json:"pipeline_id"`
    Success         bool                `json:"success"`
    EngineerResult  *EngineerResult     `json:"engineer_result"`
    InspectorResult *InspectorResult    `json:"inspector_result"`
    TesterResult    *TesterResult       `json:"tester_result"`
    ModifiedFiles   []string            `json:"modified_files"`
    CreatedFiles    []string            `json:"created_files"`
    LoopsUsed       int                 `json:"loops_used"`
    Duration        time.Duration       `json:"duration"`
}
```

### Guide Integration for /task Command

The Guide routes user messages to specific pipelines via the `/task` command:

```go
// Guide skill for pipeline interaction
{
    Name:        "task_interact",
    Description: "Route user message to a specific pipeline's engineer",
    Domain:      "routing",
    Keywords:    []string{"/task"},
    Priority:    100,
    Parameters: []Param{
        {Name: "pipeline_id", Type: "string", Required: true, Description: "Pipeline ID or index"},
        {Name: "action", Type: "enum", Values: []string{"prompt", "query", "interrupt", "ignore_inspector", "ignore_tester"}, Required: true},
        {Name: "message", Type: "string", Required: false},
    },
}
```

User interaction examples:
```
/task 1 prompt "Focus on error handling first"     → Guide → Pipeline 1 → Engineer
/task 2 query "What files have you modified?"      → Guide → Pipeline 2 → Engineer
/task 1 interrupt                                  → Guide → Pipeline 1 → Pause
/task 3 ignore_inspector                           → Guide → Pipeline 3 → Skip inspector loop
/task 2 ignore_tester                              → Guide → Pipeline 2 → Skip tester loop
```

### Orchestrator Changes for Pipelines

The Orchestrator's task dispatch now creates pipelines instead of bare engineers:

```go
// Before (current): Orchestrator spawns Engineer directly
func (o *Orchestrator) dispatchTask(task *DAGNode) error {
    engineer := o.engineerPool.Acquire()
    result, err := engineer.Execute(task)
    // ... handle result, report to Architect
}

// After (with pipelines): Orchestrator creates Pipeline
func (o *Orchestrator) dispatchTask(task *DAGNode) error {
    pipeline, err := o.pipelineManager.Create(ctx, CreatePipelineConfig{
        SessionID:          task.SessionID,
        DAGID:              task.DAGID,
        TaskID:             task.ID,
        TaskPrompt:         task.Prompt,
        TaskConstraints:    task.Constraints,
        ComplianceCriteria: task.ComplianceCriteria,
        UpstreamOutputs:    o.gatherUpstreamOutputs(task),
        MaxLoops:           3,
    })
    if err != nil {
        return err
    }

    // Start pipeline (runs Engineer → Inspector → Tester loop internally)
    if err := o.pipelineManager.Start(ctx, pipeline.ID); err != nil {
        return err
    }

    // Wait for pipeline completion
    result, err := o.pipelineManager.GetResult(pipeline.ID)
    // ... handle result, report to Architect
}
```

### Message Types

New pipeline-internal messages (NOT routed through Guide):
```
INSPECTOR_FEEDBACK      Inspector → Engineer: task-specific issues (pipeline-internal)
TESTER_FEEDBACK         Tester → Engineer: task-specific test failures (pipeline-internal)
PIPELINE_COMPLETE       Pipeline → Orchestrator: all loops passed
PIPELINE_FAILED         Pipeline → Orchestrator: max loops exceeded or error
```

Existing messages (unchanged - used for session-level QA):
```
INSPECTION_REQUEST      Architect → Inspector: validate ALL changes (session-wide)
INSPECTION_RESULTS      Inspector → Architect: session-wide issues found
TEST_PLAN_REQUEST       Architect → Tester: create test plan (session-wide)
TEST_RESULTS            Tester → Architect: session-wide test results
TEST_CORRECTIONS        Tester → Architect: impl fixes needed (creates fix DAG)
```

New Guide-routed messages:
```
USER_TASK_PROMPT        User → Guide → Pipeline: prompt specific engineer
USER_TASK_QUERY         User → Guide → Pipeline: query specific engineer
USER_TASK_INTERRUPT     User → Guide → Pipeline: interrupt pipeline
USER_IGNORE_INSPECTOR   User → Guide → Pipeline: skip inspector for this pipeline
USER_IGNORE_TESTER      User → Guide → Pipeline: skip tester for this pipeline
```

### Orchestrator Batch & Parallel Execution Skills

**CRITICAL: Orchestrator uses batch and parallel execution to maximize throughput while respecting dependencies.**

Inspired by anomalyco/opencode batch execution and oh-my-opencode background_task patterns.

```go
// Orchestrator Execution Skills (Internal - not user-facing)
orchestrator_skills_execution := []Skill{
    {
        Name:        "batch_dispatch",
        Description: "Dispatch multiple independent tasks in parallel",
        Domain:      "execution",
        Keywords:    []string{"parallel", "batch", "concurrent"},
        Parameters: []Param{
            {Name: "tasks", Type: "array", Required: true, Description: "Array of tasks to dispatch"},
            {Name: "max_concurrency", Type: "int", Required: false, Default: 4},
            {Name: "fail_fast", Type: "bool", Required: false, Default: true, Description: "Stop on first failure"},
        },
    },
    {
        Name:        "background_task",
        Description: "Run a long-running task in background with monitoring",
        Domain:      "execution",
        Keywords:    []string{"background", "async", "long-running"},
        Parameters: []Param{
            {Name: "task", Type: "object", Required: true},
            {Name: "timeout", Type: "duration", Required: false},
            {Name: "progress_interval", Type: "duration", Required: false, Default: "30s"},
        },
    },
    {
        Name:        "task_monitor",
        Description: "Monitor background task progress",
        Domain:      "execution",
        Keywords:    []string{"monitor", "progress", "status"},
        Parameters: []Param{
            {Name: "task_id", Type: "string", Required: true},
        },
    },
    {
        Name:        "parallel_validate",
        Description: "Run multiple validations in parallel across pipelines",
        Domain:      "execution",
        Keywords:    []string{"validate", "parallel", "check"},
        Parameters: []Param{
            {Name: "pipeline_ids", Type: "array", Required: true},
            {Name: "validation_type", Type: "enum", Values: []string{"build", "lint", "type", "test", "all"}, Required: true},
        },
    },
    {
        Name:        "kill_task",
        Description: "Kill a running background task",
        Domain:      "execution",
        Keywords:    []string{"kill", "stop", "cancel"},
        Parameters: []Param{
            {Name: "task_id", Type: "string", Required: true},
            {Name: "force", Type: "bool", Required: false, Default: false},
        },
    },
}
```

#### Batch Execution Protocol

```
┌─────────────────────────────────────────────────────────────────────────────────────┐
│                        BATCH EXECUTION PROTOCOL                                      │
├─────────────────────────────────────────────────────────────────────────────────────┤
│                                                                                     │
│  PARALLEL TASK DISPATCH:                                                            │
│  ┌──────────────────────────────────────────────────────────────────────────────┐   │
│  │  1. Analyze DAG for independent tasks (no dependencies between them)         │   │
│  │  2. Group into batches respecting max_concurrency                            │   │
│  │  3. Dispatch batch using batch_dispatch                                       │   │
│  │  4. Wait for all tasks in batch OR fail_fast on first error                  │   │
│  │  5. Proceed to next batch (tasks that depended on completed batch)           │   │
│  └──────────────────────────────────────────────────────────────────────────────┘   │
│                                                                                     │
│  BACKGROUND TASK MANAGEMENT:                                                        │
│  ├── Long-running tasks (builds, tests) run in background                          │
│  ├── Progress signals emitted at progress_interval                                 │
│  ├── Tasks can be monitored via task_monitor                                       │
│  ├── Tasks can be killed via kill_task                                             │
│  └── Timeout enforcement with graceful shutdown                                     │
│                                                                                     │
│  PARALLEL VALIDATION:                                                               │
│  ├── After pipeline batches complete, run validations in parallel                  │
│  ├── Build validation runs once (shared)                                           │
│  ├── Lint/type validation can run per-file in parallel                             │
│  ├── Test validation groups by test file                                           │
│  └── Results aggregated before reporting to Architect                              │
│                                                                                     │
│  EXECUTION ORDER EXAMPLE:                                                           │
│  "execution_order": [["t1"], ["t2", "t3"], ["t4"]]                                 │
│  ├── Batch 1: [t1] runs alone                                                      │
│  ├── Batch 2: [t2, t3] run in parallel (both depend on t1)                         │
│  └── Batch 3: [t4] runs alone (depends on t2 AND t3)                               │
│                                                                                     │
└─────────────────────────────────────────────────────────────────────────────────────┘
```

---

## Pipeline Variants

Pipeline variants enable parallel exploration of alternative implementations. When a user requests a variant mid-execution, the system creates an isolated parallel pipeline starting from the same state as the original, allowing both to run concurrently with full isolation.

### Core Concepts

**Variant Group**: A collection of pipelines exploring the same task with different approaches. Contains the original pipeline plus one or more variant pipelines.

**Starting Point**: The git HEAD commit when a pipeline begins execution. Captured once, used for:
- Seeding VFS when switching to isolated mode
- Rollback target for the working directory
- Ensuring variants start from identical state

**VFS Isolation**: When variants are created, all pipelines in the group switch to VFS mode. Each pipeline writes to its own staging directory while reading from a shared base (the starting point state).

### Data Structures

```go
// VariantGroupStatus tracks the lifecycle of a variant group
type VariantGroupStatus string

const (
    VariantGroupActive    VariantGroupStatus = "active"     // Variants still executing
    VariantGroupPending   VariantGroupStatus = "pending"    // Awaiting user selection
    VariantGroupSelected  VariantGroupStatus = "selected"   // User made selection
    VariantGroupCancelled VariantGroupStatus = "cancelled"  // All variants cancelled
)

// VariantStatus tracks individual variant lifecycle
type VariantStatus string

const (
    VariantRunning   VariantStatus = "running"    // Currently executing
    VariantReady     VariantStatus = "ready"      // Completed, awaiting selection
    VariantSelected  VariantStatus = "selected"   // User chose this variant
    VariantDiscarded VariantStatus = "discarded"  // User chose different variant
    VariantCancelled VariantStatus = "cancelled"  // Cancelled before completion
    VariantFailed    VariantStatus = "failed"     // Execution failed
)

// VariantGroup manages a set of competing implementations
type VariantGroup struct {
    ID            string                  `json:"id"`
    SessionID     string                  `json:"session_id"`
    OriginalStep  int                     `json:"original_step"`     // DAG step that spawned variants
    StartingPoint string                  `json:"starting_point"`    // Git HEAD when original started
    Status        VariantGroupStatus      `json:"status"`
    Variants      map[string]*VariantInfo `json:"variants"`          // variantID -> info
    SelectedID    *string                 `json:"selected_id,omitempty"`
    CreatedAt     time.Time               `json:"created_at"`
    CompletedAt   *time.Time              `json:"completed_at,omitempty"`
}

// VariantInfo tracks a single variant within a group
type VariantInfo struct {
    ID          string        `json:"id"`
    PipelineID  string        `json:"pipeline_id"`
    Label       string        `json:"label"`         // User-friendly name ("original", "variant-1")
    Approach    string        `json:"approach"`      // Description of this variant's approach
    Status      VariantStatus `json:"status"`
    StartedAt   time.Time     `json:"started_at"`
    CompletedAt *time.Time    `json:"completed_at,omitempty"`
    Error       string        `json:"error,omitempty"`
}

// Pipeline additions for variant support
type Pipeline struct {
    // ... existing fields ...

    StartingPoint  string  `json:"starting_point"`            // Git HEAD when started
    VariantGroupID *string `json:"variant_group_id,omitempty"` // Non-nil if part of variant group
    VFS            *VirtualFilesystem                         // Non-nil when isolated
}

// CachedDiff stores computed diff with TTL
type CachedDiff struct {
    GroupID    string
    VariantID  string
    Diff       string
    ComputedAt time.Time
    ExpiresAt  time.Time
}

// VariantDiffCache provides lazy diff computation with caching
type VariantDiffCache struct {
    mu    sync.RWMutex
    diffs map[string]*CachedDiff  // "groupID:variantID" -> diff
    ttl   time.Duration           // Default: 30 seconds
}

// Notification represents a variant-related notification
type Notification struct {
    Type      string    `json:"type"`       // "variant_ready", "all_ready", "variant_failed"
    GroupID   string    `json:"group_id"`
    VariantID string    `json:"variant_id,omitempty"`
    Message   string    `json:"message"`
    Timestamp time.Time `json:"timestamp"`
}

// NotificationAggregator batches notifications to prevent spam
type NotificationAggregator struct {
    mu         sync.Mutex
    pending    map[string][]Notification  // sessionID -> pending notifications
    flushTimer *time.Timer
    flushDelay time.Duration              // Default: 100ms
    onFlush    func(sessionID string, notifications []Notification)
}
```

### Variant Creation Flow

```
┌─────────────────────────────────────────────────────────────────────────────────────┐
│                           VARIANT CREATION FLOW                                      │
├─────────────────────────────────────────────────────────────────────────────────────┤
│                                                                                     │
│  USER REQUEST                                                                       │
│  ════════════                                                                       │
│  "Try this with a different approach: use channels instead of mutexes"              │
│       │                                                                             │
│       ▼                                                                             │
│  ┌─────────┐                                                                        │
│  │  Guide  │ ──── Intent Classification: VARIANT_REQUEST                            │
│  └────┬────┘      Extracts: target_task, modified_prompt                            │
│       │                                                                             │
│       ▼                                                                             │
│  ┌───────────┐                                                                      │
│  │ Architect │ ──── Validates variant is feasible                                   │
│  └─────┬─────┘      Creates VARIANT_INJECT signal                                   │
│        │                                                                            │
│        ▼                                                                            │
│  ┌──────────────┐                                                                   │
│  │ Orchestrator │ ──── Receives VARIANT_INJECT                                      │
│  └──────┬───────┘                                                                   │
│         │                                                                           │
│         ▼                                                                           │
│  ┌─────────────────────────────────────────────────────────────────────────────┐    │
│  │                     VARIANT INJECTION SEQUENCE                               │    │
│  ├─────────────────────────────────────────────────────────────────────────────┤    │
│  │                                                                             │    │
│  │  1. IDENTIFY STATE                                                          │    │
│  │     ├── Locate original pipeline (must be running or pending)               │    │
│  │     ├── Get StartingPoint (S0) from original                                │    │
│  │     └── Query changed files: git diff --name-only {S0} HEAD                 │    │
│  │                                                                             │    │
│  │  2. CREATE VARIANT GROUP                                                    │    │
│  │     ├── Generate group ID                                                   │    │
│  │     ├── Register original as first variant                                  │    │
│  │     └── Set group status = ACTIVE                                           │    │
│  │                                                                             │    │
│  │  3. SEED ORIGINAL VFS                                                       │    │
│  │     ├── Create VFS with staging dir: /tmp/sylk/variants/{groupID}/original  │    │
│  │     ├── For each changed file:                                              │    │
│  │     │   └── vfs.Seed(path, currentDiskContent)                              │    │
│  │     └── Original now writes to VFS, not disk                                │    │
│  │                                                                             │    │
│  │  4. ROLLBACK WORKING DIR                                                    │    │
│  │     └── git checkout {S0}  // Instant, working dir now at S0                │    │
│  │                                                                             │    │
│  │  5. CREATE VARIANT PIPELINE                                                 │    │
│  │     ├── Create new pipeline with modified prompt                            │    │
│  │     ├── Set StartingPoint = S0                                              │    │
│  │     ├── Create VFS: /tmp/sylk/variants/{groupID}/variant-1                  │    │
│  │     ├── Register in variant group                                           │    │
│  │     └── Start execution                                                     │    │
│  │                                                                             │    │
│  │  6. SIGNAL ORIGINAL                                                         │    │
│  │     └── VARIANT_CREATED signal to original pipeline                         │    │
│  │         (Original continues, now aware it's in a variant group)             │    │
│  │                                                                             │    │
│  └─────────────────────────────────────────────────────────────────────────────┘    │
│                                                                                     │
│  PARALLEL EXECUTION                                                                 │
│  ══════════════════                                                                 │
│                                                                                     │
│      Working Dir (at S0)           VFS: original              VFS: variant-1        │
│      ┌──────────────────┐         ┌──────────────────┐       ┌──────────────────┐   │
│      │ main.go (v0)     │ ◄─READ──│ main.go (v1)     │       │ main.go (v1')    │   │
│      │ utils.go (v0)    │         │ utils.go (v1)    │       │ utils.go (v1')   │   │
│      │ config.go (v0)   │         │                  │       │                  │   │
│      └──────────────────┘         └──────────────────┘       └──────────────────┘   │
│             │                            │                          │               │
│             │                            │                          │               │
│             └─────── Reads fall through to working dir ─────────────┘               │
│                      (unchanged files read from S0)                                 │
│                                                                                     │
│  COMPLETION                                                                         │
│  ══════════                                                                         │
│                                                                                     │
│      ┌─────────────┐                                                                │
│      │ Variant     │ ──► VARIANT_READY signal to Guide                              │
│      │ Completes   │     (includes: groupID, variantID, summary)                    │
│      └─────────────┘                                                                │
│                                                                                     │
│      When ALL variants complete:                                                    │
│      ┌─────────────┐                                                                │
│      │   Guide     │ ──► Aggregated notification to user                            │
│      │ (notifies)  │     "2 variants ready for task X"                              │
│      └─────────────┘                                                                │
│                                                                                     │
│  USER SELECTION                                                                     │
│  ══════════════                                                                     │
│                                                                                     │
│      User runs: /variants                                                           │
│      ┌─────────────────────────────────────────────────────────────────────────┐    │
│      │  Variant Group: abc123                                                  │    │
│      │  Task: "Implement connection pooling"                                   │    │
│      │                                                                         │    │
│      │  [1] original - mutex-based synchronization (ready)                     │    │
│      │  [2] variant-1 - channel-based synchronization (ready)                  │    │
│      │                                                                         │    │
│      │  Commands: /variants diff 1 2, /variants select 2                       │    │
│      └─────────────────────────────────────────────────────────────────────────┘    │
│                                                                                     │
│      User runs: /variants select 2                                                  │
│      ┌─────────────┐                                                                │
│      │ Orchestrator│ ──► Commit variant-1 VFS to working dir                        │
│      │ (commits)   │     Delete variant group staging dirs                          │
│      └─────────────┘     Continue pipeline (Inspector/Tester)                       │
│                                                                                     │
└─────────────────────────────────────────────────────────────────────────────────────┘
```

### State Transitions

#### VariantGroup State Machine

```
                    ┌──────────────────────────────────────────────────┐
                    │                                                  │
                    │            VariantGroup States                   │
                    │                                                  │
                    └──────────────────────────────────────────────────┘

                              ACTIVE
                                │
                                │ (all variants complete OR fail)
                                ▼
        ┌─────────────────── PENDING ───────────────────┐
        │                       │                       │
        │ (user cancels all)    │ (user selects)        │ (timeout/abandon)
        ▼                       ▼                       ▼
    CANCELLED               SELECTED                CANCELLED
```

#### Individual Variant State Machine

```
                              RUNNING
                                │
            ┌───────────────────┼───────────────────┐
            │                   │                   │
            │ (error)           │ (complete)        │ (cancelled)
            ▼                   ▼                   ▼
          FAILED              READY             CANCELLED
                                │
                    ┌───────────┴───────────┐
                    │                       │
                    │ (selected)            │ (other selected)
                    ▼                       ▼
                SELECTED                DISCARDED
```

### Cancellation Semantics

Variants are independently cancellable. Cancelling one variant does not affect others.

```go
// CancelVariant cancels a single variant, others continue
func (o *Orchestrator) CancelVariant(groupID, variantID string) error {
    group := o.variantGroups[groupID]
    variant := group.Variants[variantID]

    // Signal pipeline to stop
    pipeline := o.pipelines[variant.PipelineID]
    pipeline.Cancel()

    // Update variant status
    variant.Status = VariantCancelled

    // Check if all variants now terminal
    allTerminal := true
    for _, v := range group.Variants {
        if v.Status == VariantRunning {
            allTerminal = false
            break
        }
    }

    if allTerminal {
        // Count ready variants
        readyCount := 0
        for _, v := range group.Variants {
            if v.Status == VariantReady {
                readyCount++
            }
        }

        if readyCount > 0 {
            group.Status = VariantGroupPending
        } else {
            group.Status = VariantGroupCancelled
        }
    }

    return nil
}

// CancelVariantGroup cancels entire group
func (o *Orchestrator) CancelVariantGroup(groupID string) error {
    group := o.variantGroups[groupID]

    for _, variant := range group.Variants {
        if variant.Status == VariantRunning {
            o.CancelVariant(groupID, variant.ID)
        }
    }

    group.Status = VariantGroupCancelled

    // Cleanup: restore working dir, delete staging dirs
    o.cleanupVariantGroup(group)

    return nil
}
```

### Gotchas and Mitigations

| Gotcha | Mitigation |
|--------|------------|
| **Dirty working dir at variant request** | Check for uncommitted changes. If dirty and not from our pipeline, warn user. Use `git stash` if needed before rollback. |
| **VFS switch during tool execution** | Only switch at tool boundaries. Tool calls are atomic - wait for current tool to complete before switching pipeline to VFS. |
| **Subprocesses bypass VFS** | VFS intercepts subprocess writes via LD_PRELOAD/DYLD_INSERT_LIBRARIES. Subprocesses write to VFS staging, not real disk. |
| **Large file seeding** | Stream files to VFS staging rather than loading into memory. Seed lazily on first read if not yet written. |
| **Concurrent git operations** | Lock git operations during variant injection. Only Orchestrator runs git commands during injection sequence. |
| **User edits during variant execution** | Variants operate on S0 state. User edits go to working dir. Warn user that edits may conflict with variant selection. |
| **Network/external state** | Variants share external state (databases, APIs). Document this limitation. Consider mock/replay for stateful operations. |
| **VFS staging disk usage** | Periodic cleanup of old staging dirs. Auto-cleanup on variant group completion. Configurable staging location. |
| **Selection timeout** | Optional timeout for pending groups. Configurable auto-select or auto-cancel policy. Default: wait indefinitely. |

### Guide Signaling Protocol

Guide acts as the central signal hub for variant-related messages, ensuring responsive UX at scale.

```go
// VariantSignal types routed through Guide
const (
    SignalVariantRequest  = "VARIANT_REQUEST"   // User requests variant
    SignalVariantCreated  = "VARIANT_CREATED"   // Variant successfully created
    SignalVariantReady    = "VARIANT_READY"     // Single variant completed
    SignalAllReady        = "ALL_VARIANTS_READY" // All variants in group completed
    SignalVariantFailed   = "VARIANT_FAILED"    // Variant execution failed
    SignalVariantSelected = "VARIANT_SELECTED"  // User selected a variant
)

// Guide's variant signal handler
func (g *Guide) HandleVariantSignal(signal Signal) {
    switch signal.Type {
    case SignalVariantReady:
        g.aggregator.Add(signal.SessionID, Notification{
            Type:      "variant_ready",
            GroupID:   signal.GroupID,
            VariantID: signal.VariantID,
            Message:   fmt.Sprintf("Variant '%s' ready", signal.Label),
            Timestamp: time.Now(),
        })

    case SignalAllReady:
        // Immediate notification, bypass aggregator
        g.notifyUser(signal.SessionID, Notification{
            Type:    "all_ready",
            GroupID: signal.GroupID,
            Message: fmt.Sprintf("%d variants ready for review", signal.Count),
        })

    case SignalVariantFailed:
        g.aggregator.Add(signal.SessionID, Notification{
            Type:      "variant_failed",
            GroupID:   signal.GroupID,
            VariantID: signal.VariantID,
            Message:   fmt.Sprintf("Variant '%s' failed: %s", signal.Label, signal.Error),
        })
    }
}
```

### Notification Aggregation

Prevents notification spam when multiple variants complete in rapid succession.

```go
func NewNotificationAggregator(flushDelay time.Duration, onFlush func(string, []Notification)) *NotificationAggregator {
    return &NotificationAggregator{
        pending:    make(map[string][]Notification),
        flushDelay: flushDelay,  // Recommend: 100ms
        onFlush:    onFlush,
    }
}

func (a *NotificationAggregator) Add(sessionID string, n Notification) {
    a.mu.Lock()
    defer a.mu.Unlock()

    a.pending[sessionID] = append(a.pending[sessionID], n)

    // Reset or start flush timer
    if a.flushTimer != nil {
        a.flushTimer.Stop()
    }
    a.flushTimer = time.AfterFunc(a.flushDelay, func() {
        a.flush()
    })
}

func (a *NotificationAggregator) flush() {
    a.mu.Lock()
    pending := a.pending
    a.pending = make(map[string][]Notification)
    a.mu.Unlock()

    for sessionID, notifications := range pending {
        a.onFlush(sessionID, notifications)
    }
}
```

### Lazy Diff Computation

Diffs are computed on-demand and cached to avoid overhead for unused comparisons.

```go
func NewVariantDiffCache(ttl time.Duration) *VariantDiffCache {
    return &VariantDiffCache{
        diffs: make(map[string]*CachedDiff),
        ttl:   ttl,  // Recommend: 30 seconds
    }
}

func (c *VariantDiffCache) GetDiff(groupID, variantID string, computeFn func() string) string {
    key := fmt.Sprintf("%s:%s", groupID, variantID)

    c.mu.RLock()
    if cached, ok := c.diffs[key]; ok && time.Now().Before(cached.ExpiresAt) {
        c.mu.RUnlock()
        return cached.Diff
    }
    c.mu.RUnlock()

    // Compute diff
    diff := computeFn()

    c.mu.Lock()
    c.diffs[key] = &CachedDiff{
        GroupID:    groupID,
        VariantID:  variantID,
        Diff:       diff,
        ComputedAt: time.Now(),
        ExpiresAt:  time.Now().Add(c.ttl),
    }
    c.mu.Unlock()

    return diff
}

// ComputeVariantDiff generates diff between variant VFS and starting point
func (o *Orchestrator) ComputeVariantDiff(groupID, variantID string) string {
    group := o.variantGroups[groupID]
    variant := group.Variants[variantID]
    pipeline := o.pipelines[variant.PipelineID]

    return o.diffCache.GetDiff(groupID, variantID, func() string {
        // Get files changed in this variant's VFS
        changedFiles := pipeline.VFS.ChangedFiles()

        var diff strings.Builder
        for _, path := range changedFiles {
            // Get original content (from git at StartingPoint)
            original, _ := git.Show(group.StartingPoint, path)
            // Get variant content (from VFS)
            variant, _ := pipeline.VFS.Read(path)

            // Generate unified diff
            diff.WriteString(unifiedDiff(path, original, variant))
        }
        return diff.String()
    })
}
```

### Non-Blocking Selection UI

User interacts with variants via commands, never blocking prompts.

```
CLI COMMANDS
════════════

/variants
    List all pending variant groups for current session.
    Shows: group ID, task description, variant count, status.

/variants show <group_id>
    Show details of specific variant group.
    Shows: all variants with labels, approaches, status, completion time.

/variants diff <group_id> [variant1] [variant2]
    Show diff between variants or between variant and starting point.
    If one variant: shows changes from starting point.
    If two variants: shows diff between the two.

/variants preview <group_id> <variant_id>
    Preview full content of a variant's changes.
    Opens in pager if large.

/variants select <group_id> <variant_id>
    Select a variant for commit. Commits VFS to working dir.
    Other variants are discarded. Pipeline continues to Inspector/Tester.

/variants cancel <group_id> [variant_id]
    Cancel a specific variant or entire group.
    If variant: cancels that variant, others continue.
    If group: cancels all variants, cleans up.
```

### Status Line Integration

Variant status appears in the session status line for ambient awareness.

```
┌─────────────────────────────────────────────────────────────────────────────┐
│  sylk > session abc123                                                      │
│  ─────────────────────────────────────────────────────────────────────────  │
│  Pipeline: running | Tasks: 3/5 | Variants: 2 pending (task-4)             │
│                                           ▲                                 │
│                                           │                                 │
│                           Subtle indicator of pending variants              │
└─────────────────────────────────────────────────────────────────────────────┘
```

When variants complete:

```
┌─────────────────────────────────────────────────────────────────────────────┐
│  sylk > session abc123                                                      │
│  ─────────────────────────────────────────────────────────────────────────  │
│  Pipeline: waiting | Tasks: 3/5 | Variants: 2 ready ◄── /variants          │
│                                                  ▲                          │
│                                                  │                          │
│                              Attention indicator + command hint             │
└─────────────────────────────────────────────────────────────────────────────┘
```

### VFS Commit Protocol

When user selects a variant, its VFS contents are committed to the working directory.

```go
func (o *Orchestrator) CommitVariant(groupID, variantID string) error {
    group := o.variantGroups[groupID]
    variant := group.Variants[variantID]
    pipeline := o.pipelines[variant.PipelineID]

    // 1. Get all files changed in this VFS
    changedFiles := pipeline.VFS.ChangedFiles()

    // 2. Write each to working directory
    for _, path := range changedFiles {
        content, err := pipeline.VFS.Read(path)
        if err != nil {
            return fmt.Errorf("read vfs file %s: %w", path, err)
        }

        fullPath := filepath.Join(o.workDir, path)
        if err := os.MkdirAll(filepath.Dir(fullPath), 0755); err != nil {
            return fmt.Errorf("create dir for %s: %w", path, err)
        }

        if err := os.WriteFile(fullPath, content, 0644); err != nil {
            return fmt.Errorf("write file %s: %w", path, err)
        }
    }

    // 3. Update variant statuses
    variant.Status = VariantSelected
    for _, v := range group.Variants {
        if v.ID != variantID && v.Status == VariantReady {
            v.Status = VariantDiscarded
        }
    }
    group.SelectedID = &variantID
    group.Status = VariantGroupSelected
    now := time.Now()
    group.CompletedAt = &now

    // 4. Cleanup staging directories
    o.cleanupVariantGroup(group)

    // 5. Continue pipeline (Inspector/Tester will run on committed changes)
    o.resumePipeline(pipeline.ID)

    return nil
}

func (o *Orchestrator) cleanupVariantGroup(group *VariantGroup) {
    stagingBase := filepath.Join(os.TempDir(), "sylk", "variants", group.ID)
    os.RemoveAll(stagingBase)
}
```

### Layer Wait Semantics

When a variant is injected, the topological layer containing the original task must wait for ALL variants to complete before the pipeline can proceed.

```go
func (o *Orchestrator) checkLayerCompletion(layer int) bool {
    for _, taskID := range o.dag.Layer(layer) {
        pipeline := o.pipelines[taskID]

        // If part of variant group, check group status
        if pipeline.VariantGroupID != nil {
            group := o.variantGroups[*pipeline.VariantGroupID]
            if group.Status != VariantGroupSelected {
                return false  // Still waiting for selection
            }
        } else {
            // Normal task, check individual completion
            if pipeline.Status != PipelineComplete {
                return false
            }
        }
    }
    return true
}
```

---

## Agent Memory Management

Each agent has specific memory management strategies based on their role, model, and context window characteristics. All checkpoints and compaction summaries are submitted to the Archivalist for persistence.

### Agent Model Assignments

| Agent | Model | Context Strategy |
|-------|-------|------------------|
| **Librarian** | (TBD) | Frequent checkpoints (25%, 50%, 75%), compact at 75% |
| **Guide** | (TBD) | Routing-focused checkpoints (50%, 75%, 90%), compact at 95% |
| **Academic** | Opus 4.5 | Research paper at 85%, compact at 95% |
| **Architect** | OpenAI Codex 5.2 | Workflow/plan at 85%, compact at 95% |
| **Engineer** | Opus 4.5 | **PIPELINE HANDOFF at 95%** (special case) |
| **Inspector** | OpenAI Codex 5.2 | Findings summary at 85%, compact at 95% |
| **Tester** | OpenAI Codex 5.2 | Test summary at 85%, compact at 95% |

### Memory Management Summary

```
┌─────────────────────────────────────────────────────────────────────────────────────┐
│                     AGENT MEMORY MANAGEMENT THRESHOLDS                               │
├─────────────────────────────────────────────────────────────────────────────────────┤
│                                                                                     │
│  LIBRARIAN:    25%──────50%──────75%                                                │
│                 │        │        │                                                 │
│                 ▼        ▼        ▼                                                 │
│              CKPT     CKPT    CKPT+COMPACT                                          │
│                                                                                     │
│  GUIDE:                 50%──────75%──────90%──────95%                              │
│                          │        │        │        │                               │
│                          ▼        ▼        ▼        ▼                               │
│                        CKPT     CKPT     CKPT    COMPACT                            │
│                                                                                     │
│  ACADEMIC:                               85%──────95%                               │
│  (Opus 4.5)                               │        │                                │
│                                           ▼        ▼                                │
│                                      RESEARCH   COMPACT                             │
│                                       PAPER                                         │
│                                                                                     │
│  ARCHITECT:                              85%──────95%                               │
│  (Codex 5.2)                              │        │                                │
│                                           ▼        ▼                                │
│                                       WORKFLOW  COMPACT                             │
│                                        + PLAN                                       │
│                                                                                     │
│  ENGINEER:                                       95%                                │
│  (Opus 4.5)                                       │                                 │
│                                                   ▼                                 │
│                                           *** PIPELINE ***                          │
│                                           *** HANDOFF  ***                          │
│                                                                                     │
│  INSPECTOR:                              85%──────95%                               │
│  (Codex 5.2)                              │        │                                │
│                                           ▼        ▼                                │
│                                       FINDINGS  COMPACT                             │
│                                       SUMMARY   (local)                             │
│                                                                                     │
│  TESTER:                                 85%──────95%                               │
│  (Codex 5.2)                              │        │                                │
│                                           ▼        ▼                                │
│                                         TEST    COMPACT                             │
│                                       SUMMARY   (local)                             │
│                                                                                     │
│  All checkpoints/summaries → ARCHIVALIST (persistent storage)                       │
│                                                                                     │
└─────────────────────────────────────────────────────────────────────────────────────┘
```

---

### Librarian Memory Management

The Librarian continuously learns about the codebase through indexing and queries. This knowledge must be persisted to survive context window limits.

**Thresholds**: 25%, 50%, 75% (checkpoint) | 75% (compact)

**Checkpoint Summary (Onboarding-Style)**:

```go
type CodebaseSummary struct {
    Timestamp          time.Time         `json:"timestamp"`
    SessionID          string            `json:"session_id"`
    ContextUsage       float64           `json:"context_usage"`
    CheckpointIndex    int               `json:"checkpoint_index"`

    // Onboarding information
    DirectoryStructure string            `json:"directory_structure"`
    KeyPaths           []string          `json:"key_paths"`
    CodeStyle          string            `json:"code_style"`
    Architecture       string            `json:"architecture"`
    TestingStrategy    string            `json:"testing_strategy"`
    Tooling            ToolingSummary    `json:"tooling"`
    PackageManagers    []string          `json:"package_managers"`
    Patterns           []string          `json:"patterns"`
    Conventions        []string          `json:"conventions"`
    NewDiscoveries     []string          `json:"new_discoveries"`
}
```

**Archivalist Category**: `librarian_checkpoint`

**Consult Archivalist For**: Agent activity (what files changed, what other agents did)

---

### Librarian Query Caching (Intent-Aware)

The Librarian handles two fundamentally different query types that require different caching strategies:

**Specific Queries** (Location-based):
- "where is the auth code"
- "show me the User struct"
- "find the CreateSession function"
- Answer: A specific location in the codebase

**Abstract Queries** (Pattern-based):
- "what is our caching strategy"
- "how do we handle errors in this repo"
- "what patterns do we use for authentication"
- Answer: Synthesized understanding from multiple sources

#### Why Simple Embedding Similarity Fails

With typical embedding models and a 0.95 similarity threshold:

| Query A | Query B | Typical Similarity |
|---------|---------|-------------------|
| "where can I find the auth code" | "where is authentication located" | ~0.82-0.88 |
| "what is our caching strategy" | "how do we handle caching" | ~0.78-0.85 |
| "where is X" | "find X" | ~0.85-0.90 |

**A 0.95 threshold misses most natural language variations.** Users don't ask the same question the same way twice.

#### Intent-Aware Caching Architecture

```
┌─────────────────────────────────────────────────────────────────────────────────────┐
│                         LIBRARIAN QUERY PROCESSING PIPELINE                          │
├─────────────────────────────────────────────────────────────────────────────────────┤
│                                                                                     │
│  User Query: "what patterns do we use for error handling"                           │
│       │                                                                             │
│       ▼                                                                             │
│  ┌─────────────────────────────────────────────────────────────────────────────┐   │
│  │  STEP 1: Intent Classification (provided by Guide during routing)           │   │
│  │                                                                             │   │
│  │  Intent Types:                                                              │   │
│  │    LOCATE  - "where is", "find", "show me", "which file"                   │   │
│  │    PATTERN - "strategy", "approach", "pattern", "how do we"                │   │
│  │    EXPLAIN - "how does", "explain", "what does X do"                       │   │
│  │    GENERAL - other codebase questions                                       │   │
│  │                                                                             │   │
│  │  Result: Intent = PATTERN                                                   │   │
│  └─────────────────────────────────────────────────────────────────────────────┘   │
│       │                                                                             │
│       ▼                                                                             │
│  ┌─────────────────────────────────────────────────────────────────────────────┐   │
│  │  STEP 2: Subject/Concept Extraction (provided by Guide)                     │   │
│  │                                                                             │   │
│  │  Query: "what patterns do we use for error handling"                        │   │
│  │                           │                                                 │   │
│  │                           ▼                                                 │   │
│  │  Concept: "error_handling"                                                  │   │
│  │  Related: ["errors", "error patterns", "exception handling"]               │   │
│  └─────────────────────────────────────────────────────────────────────────────┘   │
│       │                                                                             │
│       ▼                                                                             │
│  ┌─────────────────────────────────────────────────────────────────────────────┐   │
│  │  STEP 3: Intent-Specific Cache Lookup                                       │   │
│  │                                                                             │   │
│  │  LOCATE:  Key = entity name, Invalidate = file change                      │   │
│  │  PATTERN: Key = concept, Invalidate = TTL + structural change              │   │
│  │  EXPLAIN: Key = subject, Invalidate = file change + TTL                    │   │
│  │  GENERAL: Key = embedding (0.80 threshold), Invalidate = TTL               │   │
│  │                                                                             │   │
│  │  Cache Key: "pattern:error_handling"                                        │   │
│  └─────────────────────────────────────────────────────────────────────────────┘   │
│       │                                                                             │
│       ▼                                                                             │
│  ┌───────────────────────────────┐    ┌────────────────────────────────────────┐   │
│  │  CACHE HIT                    │    │  CACHE MISS                            │   │
│  │  Return cached response       │    │  Synthesize → Cache → Return           │   │
│  │  (0 tokens, <5ms)            │    │  (2,500-10,000 tokens, 500-2000ms)     │   │
│  └───────────────────────────────┘    └────────────────────────────────────────┘   │
│                                                                                     │
└─────────────────────────────────────────────────────────────────────────────────────┘
```

#### Cache Strategies by Intent

```go
type LibrarianQueryCache struct {
    // Layer 1: Intent + Subject based (high hit rate for repeated concepts)
    locateCache  map[string]*LocateCacheEntry   // entity → location + file_hash
    patternCache map[string]*PatternCacheEntry  // concept → synthesis + sources
    explainCache map[string]*ExplainCacheEntry  // subject → explanation + sources

    // Layer 2: Semantic fallback (lower threshold than Archivalist)
    semanticCache *SemanticCache  // embedding similarity at 0.80, not 0.95

    // File change tracking for invalidation
    fileHashes map[string]string  // path → content hash

    mu sync.RWMutex
}

// Cache entry for LOCATE intent ("where is X")
type LocateCacheEntry struct {
    Entity       string              `json:"entity"`        // normalized entity name
    Locations    []FileLocation      `json:"locations"`     // where it was found
    FileHashes   map[string]string   `json:"file_hashes"`   // path → hash at cache time
    CreatedAt    time.Time           `json:"created_at"`
    HitCount     int64               `json:"hit_count"`
}

// Cache entry for PATTERN intent ("what is our X strategy")
type PatternCacheEntry struct {
    Concept      string              `json:"concept"`       // normalized concept
    Synthesis    string              `json:"synthesis"`     // synthesized answer
    SourceFiles  []string            `json:"source_files"`  // files used to generate
    TTL          time.Duration       `json:"ttl"`           // 60 min default
    CreatedAt    time.Time           `json:"created_at"`
    HitCount     int64               `json:"hit_count"`
}

// Cache entry for EXPLAIN intent ("how does X work")
type ExplainCacheEntry struct {
    Subject      string              `json:"subject"`       // what's being explained
    Explanation  string              `json:"explanation"`   // the explanation
    SourceFiles  []string            `json:"source_files"`  // files referenced
    FileHashes   map[string]string   `json:"file_hashes"`   // for invalidation
    TTL          time.Duration       `json:"ttl"`           // 30 min default
    CreatedAt    time.Time           `json:"created_at"`
    HitCount     int64               `json:"hit_count"`
}
```

#### Cache Invalidation by Intent

| Intent | Cache Key | Invalidation Trigger | TTL |
|--------|-----------|---------------------|-----|
| **LOCATE** | Entity name | Any file in result changes | Until file change |
| **PATTERN** | Concept | TTL + major structural change | 60 min |
| **EXPLAIN** | Subject | Referenced files change + TTL | 30 min |
| **GENERAL** | Embedding (0.80) | TTL only | 15 min |

```go
func (lc *LibrarianQueryCache) IsStale(entry any, intent QueryIntent) bool {
    switch intent {
    case IntentLocate:
        e := entry.(*LocateCacheEntry)
        // Check if any referenced file has changed
        for path, cachedHash := range e.FileHashes {
            if currentHash := lc.fileHashes[path]; currentHash != cachedHash {
                return true  // File changed, invalidate
            }
        }
        return false  // No TTL for location queries

    case IntentPattern:
        e := entry.(*PatternCacheEntry)
        // TTL-based + check for major structural changes
        if time.Since(e.CreatedAt) > e.TTL {
            return true
        }
        // Check if any source directory has new files
        return lc.hasStructuralChanges(e.SourceFiles)

    case IntentExplain:
        e := entry.(*ExplainCacheEntry)
        // Hybrid: file change OR TTL
        if time.Since(e.CreatedAt) > e.TTL {
            return true
        }
        for path, cachedHash := range e.FileHashes {
            if currentHash := lc.fileHashes[path]; currentHash != cachedHash {
                return true
            }
        }
        return false

    default:
        // General: TTL only
        return time.Since(entry.(*GeneralCacheEntry).CreatedAt) > 15*time.Minute
    }
}
```

#### File Change Detection & Cache Invalidation

The Librarian uses fsnotify to watch for file changes and invalidate cache entries in real-time.

```
┌─────────────────────────────────────────────────────────────────────────────────────┐
│                         FILE CHANGE DETECTION PIPELINE                               │
├─────────────────────────────────────────────────────────────────────────────────────┤
│                                                                                     │
│  ┌──────────────┐     ┌──────────────┐     ┌──────────────┐     ┌──────────────┐   │
│  │   fsnotify   │────▶│   Debouncer  │────▶│ Hash Computer│────▶│ Invalidator  │   │
│  │   Watcher    │     │  (100ms)     │     │              │     │              │   │
│  └──────────────┘     └──────────────┘     └──────────────┘     └──────────────┘   │
│         │                    │                    │                    │            │
│         ▼                    ▼                    ▼                    ▼            │
│  Watch all files      Batch rapid         Compute xxHash64      Invalidate:        │
│  in repo (recursive)  changes together    (fast, 1GB/s)         - LOCATE entries   │
│                                                                 - EXPLAIN entries  │
│                                                                 - Update fileHashes│
│                                                                                     │
└─────────────────────────────────────────────────────────────────────────────────────┘
```

##### File Watcher Integration (fsnotify)

```go
type FileWatcher struct {
    watcher     *fsnotify.Watcher
    cache       *LibrarianQueryCache
    index       *CodebaseIndex
    debouncer   *Debouncer
    hashCache   map[string]string  // path → current hash

    // Configuration
    ignorePaths []string  // .git, node_modules, vendor, etc.

    mu          sync.RWMutex
    ctx         context.Context
    cancel      context.CancelFunc
}

func NewFileWatcher(cache *LibrarianQueryCache, index *CodebaseIndex) *FileWatcher {
    watcher, _ := fsnotify.NewWatcher()
    ctx, cancel := context.WithCancel(context.Background())

    fw := &FileWatcher{
        watcher:   watcher,
        cache:     cache,
        index:     index,
        debouncer: NewDebouncer(100 * time.Millisecond),
        hashCache: make(map[string]string),
        ignorePaths: []string{
            ".git", "node_modules", "vendor", "__pycache__",
            ".idea", ".vscode", "dist", "build", ".cache",
        },
        ctx:    ctx,
        cancel: cancel,
    }

    go fw.run()
    return fw
}

func (fw *FileWatcher) run() {
    for {
        select {
        case event, ok := <-fw.watcher.Events:
            if !ok {
                return
            }
            fw.handleEvent(event)

        case err, ok := <-fw.watcher.Errors:
            if !ok {
                return
            }
            log.Printf("file watcher error: %v", err)

        case <-fw.ctx.Done():
            return
        }
    }
}

func (fw *FileWatcher) handleEvent(event fsnotify.Event) {
    // Ignore non-relevant events
    if fw.shouldIgnore(event.Name) {
        return
    }

    // Debounce rapid changes (e.g., editor save creates multiple events)
    fw.debouncer.Debounce(event.Name, func() {
        fw.processFileChange(event)
    })
}
```

##### Debouncing Rapid File Changes

Editors often trigger multiple events for a single save (write, chmod, rename). Debouncing batches these together.

```go
type Debouncer struct {
    delay   time.Duration
    timers  map[string]*time.Timer
    mu      sync.Mutex
}

func NewDebouncer(delay time.Duration) *Debouncer {
    return &Debouncer{
        delay:  delay,
        timers: make(map[string]*time.Timer),
    }
}

func (d *Debouncer) Debounce(key string, fn func()) {
    d.mu.Lock()
    defer d.mu.Unlock()

    // Cancel existing timer for this key
    if timer, ok := d.timers[key]; ok {
        timer.Stop()
    }

    // Set new timer
    d.timers[key] = time.AfterFunc(d.delay, func() {
        d.mu.Lock()
        delete(d.timers, key)
        d.mu.Unlock()
        fn()
    })
}
```

##### Hash Computation (xxHash64)

Using xxHash64 for fast file hashing (~1GB/s, much faster than SHA256).

```go
import "github.com/cespare/xxhash/v2"

func (fw *FileWatcher) computeHash(path string) (string, error) {
    file, err := os.Open(path)
    if err != nil {
        return "", err
    }
    defer file.Close()

    hasher := xxhash.New()
    if _, err := io.Copy(hasher, file); err != nil {
        return "", err
    }

    return fmt.Sprintf("%x", hasher.Sum64()), nil
}

func (fw *FileWatcher) processFileChange(event fsnotify.Event) {
    path := event.Name

    fw.mu.Lock()
    defer fw.mu.Unlock()

    var newHash string
    var err error

    switch {
    case event.Op&fsnotify.Remove == fsnotify.Remove:
        // File deleted
        newHash = ""
        delete(fw.hashCache, path)

    case event.Op&fsnotify.Write == fsnotify.Write,
         event.Op&fsnotify.Create == fsnotify.Create:
        // File created or modified
        newHash, err = fw.computeHash(path)
        if err != nil {
            return
        }

        // Check if hash actually changed (content change, not just touch)
        if oldHash, ok := fw.hashCache[path]; ok && oldHash == newHash {
            return  // No actual content change
        }
        fw.hashCache[path] = newHash
    }

    // Notify cache of file change
    fw.cache.OnFileChanged(path, newHash)

    // Also update the index
    fw.index.OnFileChanged(path, event.Op)
}
```

##### Cache Invalidation on File Change

```go
func (lc *LibrarianQueryCache) OnFileChanged(path string, newHash string) {
    lc.mu.Lock()
    defer lc.mu.Unlock()

    // Update current hash
    if newHash == "" {
        delete(lc.fileHashes, path)
    } else {
        lc.fileHashes[path] = newHash
    }

    // Invalidate LOCATE entries that reference this file
    for entity, entry := range lc.locateCache {
        if _, ok := entry.FileHashes[path]; ok {
            delete(lc.locateCache, entity)
            lc.stats.LocateInvalidations++
        }
    }

    // Invalidate EXPLAIN entries that reference this file
    for subject, entry := range lc.explainCache {
        if _, ok := entry.FileHashes[path]; ok {
            delete(lc.explainCache, subject)
            lc.stats.ExplainInvalidations++
        }
    }

    // Check PATTERN entries for structural changes (new/deleted files)
    dir := filepath.Dir(path)
    for concept, entry := range lc.patternCache {
        for _, sourceFile := range entry.SourceFiles {
            if filepath.Dir(sourceFile) == dir {
                // File added/removed in a directory we synthesized from
                delete(lc.patternCache, concept)
                lc.stats.PatternInvalidations++
                break
            }
        }
    }
}
```

##### Index Synchronization

The cache invalidation coordinates with the Librarian's code index:

```go
func (idx *CodebaseIndex) OnFileChanged(path string, op fsnotify.Op) {
    switch {
    case op&fsnotify.Remove == fsnotify.Remove:
        idx.RemoveFile(path)

    case op&fsnotify.Create == fsnotify.Create:
        idx.IndexFile(path)

    case op&fsnotify.Write == fsnotify.Write:
        idx.ReindexFile(path)
    }
}
```

##### Batch Operations for Large Changes

For operations like `git checkout` that change many files at once:

```go
func (fw *FileWatcher) OnBatchChange(paths []string) {
    fw.mu.Lock()
    defer fw.mu.Unlock()

    // Pause normal watching during batch
    fw.debouncer.Pause()
    defer fw.debouncer.Resume()

    // Collect all changed hashes
    changedPaths := make(map[string]string)
    for _, path := range paths {
        if hash, err := fw.computeHash(path); err == nil {
            if oldHash := fw.hashCache[path]; oldHash != hash {
                changedPaths[path] = hash
                fw.hashCache[path] = hash
            }
        }
    }

    // Batch invalidate
    fw.cache.OnBatchFileChanged(changedPaths)
    fw.index.OnBatchFileChanged(changedPaths)
}

func (lc *LibrarianQueryCache) OnBatchFileChanged(changes map[string]string) {
    lc.mu.Lock()
    defer lc.mu.Unlock()

    // Update all hashes
    for path, hash := range changes {
        lc.fileHashes[path] = hash
    }

    // For large batches, it's more efficient to clear caches entirely
    if len(changes) > 100 {
        lc.locateCache = make(map[string]*LocateCacheEntry)
        lc.explainCache = make(map[string]*ExplainCacheEntry)
        lc.patternCache = make(map[string]*PatternCacheEntry)
        return
    }

    // For smaller batches, selectively invalidate
    for path := range changes {
        lc.invalidateByPath(path)
    }
}
```

#### Query Handling with Pre-Classified Intent

The Guide classifies intent during routing (no additional cost to Librarian):

```go
// Message from Guide includes intent classification
type LibrarianRequest struct {
    Query       string      `json:"query"`
    SessionID   string      `json:"session_id"`

    // Pre-classified by Guide (no additional LLM cost)
    Intent      QueryIntent `json:"intent"`      // LOCATE, PATTERN, EXPLAIN, GENERAL
    Subject     string      `json:"subject"`     // extracted entity/concept
    Confidence  float64     `json:"confidence"`  // Guide's classification confidence
}

func (l *Librarian) HandleQuery(req *LibrarianRequest) (*Response, error) {
    // Use Guide's pre-classification if confident
    if req.Confidence >= 0.8 {
        switch req.Intent {
        case IntentLocate:
            if cached, ok := l.cache.GetLocate(req.Subject); ok {
                return cached.ToResponse(), nil
            }
        case IntentPattern:
            if cached, ok := l.cache.GetPattern(req.Subject); ok {
                return cached.ToResponse(), nil
            }
        case IntentExplain:
            if cached, ok := l.cache.GetExplain(req.Subject); ok {
                return cached.ToResponse(), nil
            }
        }
    }

    // Cache miss or low confidence - do full synthesis
    response, sources := l.synthesize(req.Query)

    // Cache the result using the classified intent
    l.cache.Store(req.Intent, req.Subject, response, sources)

    return response, nil
}
```

#### Expected Cache Performance

| Query Type | Without Cache | With Intent Cache | Improvement |
|------------|---------------|-------------------|-------------|
| "where is auth.go" (repeated) | 3,000 tokens | 0 tokens | 100% |
| "where is authentication" (variation) | 3,000 tokens | 0 tokens | 100% |
| "what is our caching strategy" | 5,000 tokens | 0 tokens | 100% |
| "how do we handle caching" (variation) | 5,000 tokens | 0 tokens | 100% |
| "explain the auth flow" | 4,000 tokens | 0 tokens | 100% |

**Overall expected hit rate**: 70-85% for typical usage patterns

**Token savings per session**: 60-75% reduction

---

### Guide Memory Management

The Guide tracks routing decisions, matches, and request patterns. This information helps optimize future routing.

**Thresholds**: 50%, 75%, 90% (checkpoint) | 95% (compact)

**Checkpoint Summary**:

```go
type GuideSummary struct {
    Timestamp          time.Time                `json:"timestamp"`
    SessionID          string                   `json:"session_id"`
    ContextUsage       float64                  `json:"context_usage"`
    CheckpointIndex    int                      `json:"checkpoint_index"`

    // Routing knowledge
    KnownRoutings      map[string]string        `json:"known_routings"`      // pattern → agent
    FrequentMatches    []RoutingMatch           `json:"frequent_matches"`    // common routes
    FailedRoutings     []FailedRouting          `json:"failed_routings"`     // routes that didn't work
    AgentCapabilities  map[string][]string      `json:"agent_capabilities"`  // agent → capabilities observed
    RequestPatterns    []RequestPattern         `json:"request_patterns"`    // common request types
    SessionRoutingStats RoutingStats            `json:"session_routing_stats"`
}

type RoutingMatch struct {
    Pattern     string `json:"pattern"`
    Agent       string `json:"agent"`
    Confidence  float64 `json:"confidence"`
    UsageCount  int    `json:"usage_count"`
}
```

**Archivalist Category**: `guide_checkpoint`

---

### Archivalist Query Caching (Intent-Aware)

The Archivalist handles historical queries - past decisions, agent activity, workflow outcomes. Users phrase these queries in many ways that miss the standard 0.95 embedding similarity threshold.

#### Why Intent-Aware Caching for Archivalist

**The Problem**: Same intent, different phrasing:

| Query A | Query B | Similarity | Result at 0.95 |
|---------|---------|------------|----------------|
| "What did we do before for auth" | "Past solutions for authentication" | ~0.82 | MISS |
| "What files changed" | "Recent modifications by engineer" | ~0.80 | MISS |
| "Did the tests pass" | "What was the test outcome" | ~0.78 | MISS |
| "Have we seen this error before" | "Similar past failures" | ~0.75 | MISS |

**Key Difference from Librarian**: Historical data is **immutable**. Once stored, it doesn't change. Invalidation is simpler - mostly TTL-based or "new data added", not file-change-based.

#### Archivalist Query Intents

```
┌─────────────────────────────────────────────────────────────────────────────────────┐
│                         ARCHIVALIST QUERY INTENT TYPES                               │
├─────────────────────────────────────────────────────────────────────────────────────┤
│                                                                                     │
│  HISTORICAL - Past solutions, decisions, approaches                                 │
│  ├── "What did we do before for X"                                                 │
│  ├── "Past solutions for X"                                                        │
│  ├── "How did we handle X previously"                                              │
│  └── Cache: topic-based, TTL 30-60 min (history doesn't change)                    │
│                                                                                     │
│  ACTIVITY - Agent activity, file changes, task results                             │
│  ├── "What files did the engineer change"                                          │
│  ├── "What happened in the last task"                                              │
│  ├── "Show me recent modifications"                                                │
│  └── Cache: session-scoped, short TTL 5 min (new activity happens frequently)      │
│                                                                                     │
│  OUTCOME - Results, status, completion                                             │
│  ├── "Did the tests pass"                                                          │
│  ├── "What issues did the inspector find"                                          │
│  ├── "What was the result of the workflow"                                         │
│  └── Cache: task/workflow ID, invalidate when result updated                       │
│                                                                                     │
│  SIMILAR - Pattern matching, similarity search                                     │
│  ├── "Have we seen this error before"                                              │
│  ├── "Find similar past decisions"                                                 │
│  ├── "What worked for problems like this"                                          │
│  └── Cache: embedding-based with 0.80 threshold, TTL 30 min                        │
│                                                                                     │
│  RESUME - Session state, continuation                                              │
│  ├── "Where did we leave off"                                                      │
│  ├── "Resume context"                                                              │
│  ├── "What's the current status"                                                   │
│  └── Cache: session ID, very short TTL 1-2 min (state changes constantly)          │
│                                                                                     │
└─────────────────────────────────────────────────────────────────────────────────────┘
```

#### Intent-Specific Cache Structures

```go
type ArchivalistQueryCache struct {
    // Intent-based caches
    historicalCache map[string]*HistoricalCacheEntry  // topic → past solutions
    activityCache   map[string]*ActivityCacheEntry    // session+type → activity
    outcomeCache    map[string]*OutcomeCacheEntry     // task/workflow ID → result
    similarCache    *SimilarityCache                  // embedding-based, 0.80 threshold
    resumeCache     map[string]*ResumeCacheEntry      // session ID → state

    // Existing query cache (fallback)
    legacyCache     *QueryCache  // Original 0.95 threshold cache

    mu sync.RWMutex
}

type HistoricalCacheEntry struct {
    Topic        string        `json:"topic"`         // "authentication", "caching", etc.
    Solutions    []Solution    `json:"solutions"`     // Past approaches found
    SessionIDs   []string      `json:"session_ids"`   // Sessions these came from
    TTL          time.Duration `json:"ttl"`           // 30-60 min
    CreatedAt    time.Time     `json:"created_at"`
    HitCount     int64         `json:"hit_count"`
}

type ActivityCacheEntry struct {
    SessionID    string        `json:"session_id"`
    ActivityType string        `json:"activity_type"` // "file_changes", "task_results", etc.
    Activities   []Activity    `json:"activities"`
    TTL          time.Duration `json:"ttl"`           // 5 min (short, new activity frequent)
    CreatedAt    time.Time     `json:"created_at"`
    LastActivity time.Time     `json:"last_activity"` // Invalidate if new activity after this
}

type OutcomeCacheEntry struct {
    WorkflowID   string        `json:"workflow_id"`
    TaskID       string        `json:"task_id,omitempty"`
    Outcome      *Outcome      `json:"outcome"`
    IsComplete   bool          `json:"is_complete"`   // If false, short TTL
    TTL          time.Duration `json:"ttl"`
    CreatedAt    time.Time     `json:"created_at"`
}

type ResumeCacheEntry struct {
    SessionID    string        `json:"session_id"`
    State        *SessionState `json:"state"`
    TTL          time.Duration `json:"ttl"`           // 1-2 min (state changes constantly)
    CreatedAt    time.Time     `json:"created_at"`
    StateVersion int64         `json:"state_version"` // Invalidate if version changed
}
```

#### Cache Invalidation by Intent

| Intent | Cache Key | Invalidation Trigger | TTL |
|--------|-----------|---------------------|-----|
| **HISTORICAL** | Topic (normalized) | TTL only (history is immutable) | 30-60 min |
| **ACTIVITY** | Session + activity type | New activity recorded OR TTL | 5 min |
| **OUTCOME** | Workflow/Task ID | Result updated OR TTL | Until complete, then 30 min |
| **SIMILAR** | Embedding (0.80 threshold) | TTL only | 30 min |
| **RESUME** | Session ID | Any session activity OR TTL | 1-2 min |

```go
func (ac *ArchivalistQueryCache) IsStale(entry any, intent ArchivalistIntent) bool {
    switch intent {
    case IntentHistorical:
        e := entry.(*HistoricalCacheEntry)
        // History doesn't change - TTL only
        return time.Since(e.CreatedAt) > e.TTL

    case IntentActivity:
        e := entry.(*ActivityCacheEntry)
        // Check if new activity since cache
        if ac.hasNewActivity(e.SessionID, e.LastActivity) {
            return true
        }
        return time.Since(e.CreatedAt) > e.TTL

    case IntentOutcome:
        e := entry.(*OutcomeCacheEntry)
        // If workflow not complete, check if result updated
        if !e.IsComplete {
            if ac.outcomeUpdated(e.WorkflowID, e.TaskID) {
                return true
            }
        }
        return time.Since(e.CreatedAt) > e.TTL

    case IntentSimilar:
        // Similarity cache uses TTL only
        e := entry.(*SimilarCacheEntry)
        return time.Since(e.CreatedAt) > e.TTL

    case IntentResume:
        e := entry.(*ResumeCacheEntry)
        // Check if session state version changed
        if ac.sessionStateVersion(e.SessionID) != e.StateVersion {
            return true
        }
        return time.Since(e.CreatedAt) > e.TTL

    default:
        return true
    }
}
```

#### Guide Intent Classification for Archivalist

The Guide classifies Archivalist query intent during routing (same LLM call, no extra cost):

```go
type ArchivalistIntent int

const (
    ArchivalistIntentHistorical ArchivalistIntent = iota  // Past solutions
    ArchivalistIntentActivity                              // Agent activity
    ArchivalistIntentOutcome                               // Results/status
    ArchivalistIntentSimilar                               // Similarity search
    ArchivalistIntentResume                                // Session state
    ArchivalistIntentGeneral                               // Fallback
)

// Intent classification patterns
var archivalistIntentPatterns = map[ArchivalistIntent][]string{
    ArchivalistIntentHistorical: {
        "what did we do", "before", "previously", "past", "last time",
        "how did we handle", "prior", "earlier", "history",
    },
    ArchivalistIntentActivity: {
        "what changed", "what did .* do", "modifications", "recent",
        "files changed", "activity", "what happened",
    },
    ArchivalistIntentOutcome: {
        "did .* pass", "result", "outcome", "issues found",
        "status of", "complete", "succeed", "fail",
    },
    ArchivalistIntentSimilar: {
        "similar", "like this", "seen before", "pattern",
        "resembles", "related", "comparable",
    },
    ArchivalistIntentResume: {
        "where did we", "resume", "continue", "left off",
        "current state", "pick up",
    },
}

// Message to Archivalist includes intent
type ArchivalistRequest struct {
    Query       string             `json:"query"`
    SessionID   string             `json:"session_id"`
    Intent      ArchivalistIntent  `json:"intent"`
    Subject     string             `json:"subject"`     // Topic/entity being queried
    Confidence  float64            `json:"confidence"`
}
```

#### Expected Performance

| Intent | Typical Query Cost | With Cache Hit | Savings |
|--------|-------------------|----------------|---------|
| HISTORICAL | ~4,000 tokens | 0 tokens | 100% |
| ACTIVITY | ~2,000 tokens | 0 tokens | 100% |
| OUTCOME | ~1,500 tokens | 0 tokens | 100% |
| SIMILAR | ~5,000 tokens | 0 tokens | 100% |
| RESUME | ~1,000 tokens | 0 tokens | 100% |

**Expected hit rates by intent**:
- HISTORICAL: 80-90% (same topics asked repeatedly)
- ACTIVITY: 60-70% (frequent but session-specific)
- OUTCOME: 70-80% (asked multiple times during workflow)
- SIMILAR: 50-60% (varied queries, but patterns repeat)
- RESUME: 40-50% (short TTL, but frequently asked)

**Overall expected hit rate**: 60-75%
**Token savings per session**: 50-65% reduction

---

### Academic Memory Management

The Academic researches topics and produces findings. At checkpoint, it produces a "research paper" summarizing its findings.

**Model**: Opus 4.5

**Thresholds**: 85% (checkpoint) | 95% (compact)

**Checkpoint Summary (Research Paper Format)**:

```go
type AcademicResearchPaper struct {
    Timestamp          time.Time         `json:"timestamp"`
    SessionID          string            `json:"session_id"`
    ContextUsage       float64           `json:"context_usage"`

    // Research paper structure
    Title              string            `json:"title"`
    Abstract           string            `json:"abstract"`
    TopicsResearched   []string          `json:"topics_researched"`
    KeyFindings        []Finding         `json:"key_findings"`
    SourcesCited       []Source          `json:"sources_cited"`
    Recommendations    []string          `json:"recommendations"`
    OpenQuestions      []string          `json:"open_questions"`
    RelatedTopics      []string          `json:"related_topics"`
}

type Finding struct {
    Topic       string   `json:"topic"`
    Summary     string   `json:"summary"`
    Confidence  string   `json:"confidence"` // high, medium, low
    Sources     []string `json:"sources"`
}
```

**Archivalist Category**: `academic_research_paper`

---

### Architect Memory Management

The Architect creates implementation plans and DAGs. At checkpoint, it produces a retrievable workflow/plan document.

**Model**: OpenAI Codex 5.2

**Thresholds**: 85% (checkpoint) | 95% (compact)

**Checkpoint Summary (Retrievable Workflow Format)**:

```go
type ArchitectWorkflowSummary struct {
    Timestamp          time.Time         `json:"timestamp"`
    SessionID          string            `json:"session_id"`
    ContextUsage       float64           `json:"context_usage"`

    // Workflow state (parseable by other Architects)
    OriginalRequest    string            `json:"original_request"`
    ImplementationPlan string            `json:"implementation_plan"`
    CurrentDAG         *DAGSummary       `json:"current_dag"`
    CompletedTasks     []TaskSummary     `json:"completed_tasks"`
    PendingTasks       []TaskSummary     `json:"pending_tasks"`
    BlockedTasks       []TaskSummary     `json:"blocked_tasks"`
    ArchitecturalDecisions []Decision   `json:"architectural_decisions"`
    Risks              []Risk            `json:"risks"`
    Assumptions        []string          `json:"assumptions"`
}

type DAGSummary struct {
    ID             string            `json:"id"`
    TotalNodes     int               `json:"total_nodes"`
    CompletedNodes int               `json:"completed_nodes"`
    CurrentLayer   int               `json:"current_layer"`
    ExecutionOrder [][]string        `json:"execution_order"` // layer → node IDs
}

type TaskSummary struct {
    ID          string `json:"id"`
    Description string `json:"description"`
    Status      string `json:"status"`
    AssignedTo  string `json:"assigned_to,omitempty"`
    Result      string `json:"result,omitempty"`
}
```

**Archivalist Category**: `architect_workflow`

---

### Inspector Memory Management

The Inspector validates code and tracks issues. At checkpoint, it summarizes findings, fixes, and priorities.

**Model**: OpenAI Codex 5.2

**Thresholds**: 85% (checkpoint) | 95% (compact locally)

**Note**: Inspector compacts locally at 95%. Does NOT trigger pipeline handoff.

**Checkpoint Summary**:

```go
type InspectorFindingsSummary struct {
    Timestamp          time.Time         `json:"timestamp"`
    SessionID          string            `json:"session_id"`
    PipelineID         string            `json:"pipeline_id,omitempty"`
    ContextUsage       float64           `json:"context_usage"`

    // Findings state
    ChecksPerformed    []string          `json:"checks_performed"`
    IssuesFound        int               `json:"issues_found"`
    IssuesResolved     int               `json:"issues_resolved"`
    IssuesRemaining    []InspectorIssue  `json:"issues_remaining"`
    FixesCompleted     []CompletedFix    `json:"fixes_completed"`
    FixesPending       []PendingFix      `json:"fixes_pending"`
    ValidationState    map[string]bool   `json:"validation_state"` // category → pass
}

type CompletedFix struct {
    IssueID     string `json:"issue_id"`
    File        string `json:"file"`
    Line        int    `json:"line"`
    Description string `json:"description"`
    FixApplied  string `json:"fix_applied"`
}

type PendingFix struct {
    IssueID     string `json:"issue_id"`
    File        string `json:"file"`
    Line        int    `json:"line"`
    Category    string `json:"category"`
    Severity    string `json:"severity"` // critical, high, medium, low
    Priority    int    `json:"priority"` // 1 = highest
    Description string `json:"description"`
    SuggestedFix string `json:"suggested_fix"`
}
```

**Archivalist Category**: `inspector_findings`

---

### Tester Memory Management

The Tester creates and runs tests. At checkpoint, it summarizes test state and results.

**Model**: OpenAI Codex 5.2

**Thresholds**: 85% (checkpoint) | 95% (compact locally)

**Note**: Tester compacts locally at 95%. Does NOT trigger pipeline handoff.

**Checkpoint Summary**:

```go
type TesterSummary struct {
    Timestamp          time.Time         `json:"timestamp"`
    SessionID          string            `json:"session_id"`
    PipelineID         string            `json:"pipeline_id,omitempty"`
    ContextUsage       float64           `json:"context_usage"`

    // Test state
    TestsCreated       []TestInfo        `json:"tests_created"`
    TestsRun           []TestResult      `json:"tests_run"`
    PassCount          int               `json:"pass_count"`
    FailCount          int               `json:"fail_count"`
    SkipCount          int               `json:"skip_count"`
    CoverageNeeded     []string          `json:"coverage_needed"`
    FailureDescriptions []FailureDesc    `json:"failure_descriptions"`
}

type TestInfo struct {
    File     string `json:"file"`
    TestName string `json:"test_name"`
    Type     string `json:"type"` // unit, integration, e2e
    ForTask  string `json:"for_task,omitempty"`
}

type TestResult struct {
    TestName string `json:"test_name"`
    Status   string `json:"status"` // pass, fail, skip
    Duration string `json:"duration,omitempty"`
}

type FailureDesc struct {
    TestName    string `json:"test_name"`
    Error       string `json:"error"`
    Expected    string `json:"expected,omitempty"`
    Actual      string `json:"actual,omitempty"`
    Suggestion  string `json:"suggestion,omitempty"`
}
```

**Archivalist Category**: `tester_summary`

---

### Engineer Memory Management: Pipeline Handoff

**CRITICAL**: The Engineer does NOT compact locally. At 95% context, it triggers a **PIPELINE HANDOFF**.

**Model**: Opus 4.5

**Threshold**: 95% → **PIPELINE HANDOFF**

This is a special mechanism where the entire pipeline (Engineer + Inspector + Tester) transfers state to a new pipeline, minimizing re-learning.

```
┌─────────────────────────────────────────────────────────────────────────────────────┐
│                    PIPELINE HANDOFF (ENGINEER AT 95%)                                │
├─────────────────────────────────────────────────────────────────────────────────────┤
│                                                                                     │
│  OLD PIPELINE                                                         NEW PIPELINE  │
│  (Eng+Insp+Test)       GUIDE          ARCHITECT       ORCHESTRATOR   (Eng+Insp+Test)│
│       │                  │                │                │               │        │
│  ┌────┴────┐             │                │                │               │        │
│  │Engineer │             │                │                │               │        │
│  │ at 95%  │             │                │                │               │        │
│  └────┬────┘             │                │                │               │        │
│       │                  │                │                │               │        │
│       │ Prepare state    │                │                │               │        │
│       │ (E + I + T)      │                │                │               │        │
│       │                  │                │                │               │        │
│       │══"HANDOFF_REQ"══▶│                │                │               │        │
│       │  + full state    │══════════════▶│                │               │        │
│       │  (E+I+T summary) │                │                │               │        │
│       │                  │                │                │               │        │
│       │                  │          ┌─────┴─────┐          │               │        │
│       │                  │          │ Examine   │          │               │        │
│       │                  │          │ state,    │          │               │        │
│       │                  │          │ adjust    │          │               │        │
│       │                  │          │ workflow  │          │               │        │
│       │                  │          │ if needed │          │               │        │
│       │                  │          └─────┬─────┘          │               │        │
│       │                  │                │                │               │        │
│       │                  │                │══"CREATE_NEW"═▶│               │        │
│       │                  │                │  + state       │               │        │
│       │                  │                │  + "close old" │               │        │
│       │                  │                │                │               │        │
│       │                  │                │                │──create──────▶│        │
│       │                  │                │                │  (inject      │        │
│       │                  │                │                │   state)      │        │
│       │                  │                │                │               │        │
│       │                  │                │                │◀──"STARTED"───│        │
│       │                  │                │                │               │        │
│       │◀══"CLOSE_NOW"════│◀═══════════════│◀═══════════════│               │        │
│       │                  │                │                │               │        │
│       X                  │                │                │         ┌─────┴─────┐  │
│  (old closes)            │                │                │         │ EXECUTING │  │
│                          │                │                │         │ (resumed) │  │
│                          │                │                │         └───────────┘  │
│                                                                                     │
└─────────────────────────────────────────────────────────────────────────────────────┘
```

#### Handoff State Structure

```go
type PipelineHandoff struct {
    // Metadata
    OldPipelineID   PipelineID    `json:"old_pipeline_id"`
    SessionID       string        `json:"session_id"`
    DAGID           string        `json:"dag_id"`
    TaskID          string        `json:"task_id"`
    HandoffReason   string        `json:"handoff_reason"` // "engineer_context_95%"
    Timestamp       time.Time     `json:"timestamp"`
    HandoffIndex    int           `json:"handoff_index"`  // For chaining (1, 2, 3...)

    // Bundled state from all three agents
    EngineerState   *EngineerHandoffState   `json:"engineer_state"`
    InspectorState  *InspectorHandoffState  `json:"inspector_state"`
    TesterState     *TesterHandoffState     `json:"tester_state"`
}

type EngineerHandoffState struct {
    OriginalPrompt    string            `json:"original_prompt"`    // Verbatim
    Accomplished      []string          `json:"accomplished"`       // What was done
    FilesChanged      []FileChange      `json:"files_changed"`      // Specific changes
    Remaining         []string          `json:"remaining"`          // TODOs to complete
    ContextNotes      string            `json:"context_notes"`      // Critical context
}

type InspectorHandoffState struct {
    ChecksPerformed   []string          `json:"checks_performed"`
    FixesCompleted    []CompletedFix    `json:"fixes_completed"`    // With file:line refs
    FixesRemaining    []PendingFix      `json:"fixes_remaining"`    // With priority
    ValidationState   map[string]bool   `json:"validation_state"`   // category → pass/fail
}

type TesterHandoffState struct {
    TestsCreated      []TestInfo        `json:"tests_created"`      // File, name
    TestResults       []TestResult      `json:"test_results"`       // Pass/fail
    FailureDescs      []FailureDesc     `json:"failure_descs"`      // Brief errors
    CoverageNeeded    []string          `json:"coverage_needed"`    // What still needs tests
}
```

#### Handoff Message Flow

```
1. Engineer (old) prepares bundled state (Engineer + Inspector + Tester)

2. Engineer (old) ──GUIDE──▶ Architect    : "HANDOFF_REQUEST" + full state

3. Architect examines state, adjusts workflow if necessary

4. Architect ──GUIDE──▶ Orchestrator      : "CREATE_PIPELINE_WITH_STATE" {
                                              state: <bundled state>,
                                              close_pipeline: <old pipeline ID>
                                            }

5. Orchestrator:
   - Creates new pipeline
   - Injects state into new pipeline (all three agents start with context)
   - New pipeline starts executing
   - Closes old pipeline

6. Orchestrator ──GUIDE──▶ Architect      : "HANDOFF_COMPLETE"
```

#### Handoff Rules

| Rule | Description |
|------|-------------|
| **Trigger** | Engineer at 95% OR user request |
| **Who triggers** | Only Engineer triggers handoff (Inspector/Tester compact locally) |
| **State bundling** | Engineer collects state from Inspector + Tester |
| **Architect review** | Architect examines and may adjust workflow |
| **Archivalist** | Handoff state also stored for audit/recovery |
| **Retry** | If handoff fails, retry. If retries fail, fallback to summarize → Archivalist → compact |
| **Chaining** | Handoffs can chain infinitely (tracked by HandoffIndex) |
| **User control** | User can stop a handoff chain if desired |

#### New Pipeline Receives

The new pipeline's agents start with full context:

- **New Engineer**: Knows original prompt, what was done, what remains
- **New Inspector**: Knows what passed, what failed, pending fixes with priorities
- **New Tester**: Knows existing tests, results, what coverage is still needed

This minimizes re-learning and re-discovery.

**Archivalist Category**: `pipeline_handoff`

---

### Designer Memory Management: Pipeline Handoff

**CRITICAL**: The Designer does NOT compact locally. At 95% context, it triggers a **PIPELINE HANDOFF**.

**Model**: Gemini 3 Pro

**Threshold**: 95% → **PIPELINE HANDOFF**

This mirrors the Engineer pipeline handoff mechanism. The entire Designer pipeline (Designer + UIInspector + UITester) transfers state to a new pipeline, minimizing re-learning for UI/UX work.

```
┌─────────────────────────────────────────────────────────────────────────────────────┐
│                    PIPELINE HANDOFF (DESIGNER AT 95%)                                │
├─────────────────────────────────────────────────────────────────────────────────────┤
│                                                                                     │
│  OLD PIPELINE                                                         NEW PIPELINE  │
│  (Des+UIInsp+UITest)     GUIDE          ARCHITECT       ORCHESTRATOR (Des+UIInsp+UT)│
│       │                  │                │                │               │        │
│  ┌────┴────┐             │                │                │               │        │
│  │Designer │             │                │                │               │        │
│  │ at 95%  │             │                │                │               │        │
│  └────┬────┘             │                │                │               │        │
│       │                  │                │                │               │        │
│       │ Prepare state    │                │                │               │        │
│       │ (D + UI + UT)    │                │                │               │        │
│       │                  │                │                │               │        │
│       │══"HANDOFF_REQ"══▶│                │                │               │        │
│       │  + full state    │══════════════▶│                │               │        │
│       │  (D+UI+UT summ)  │                │                │               │        │
│       │                  │                │                │               │        │
│       │                  │          ┌─────┴─────┐          │               │        │
│       │                  │          │ Examine   │          │               │        │
│       │                  │          │ state,    │          │               │        │
│       │                  │          │ adjust    │          │               │        │
│       │                  │          │ workflow  │          │               │        │
│       │                  │          │ if needed │          │               │        │
│       │                  │          └─────┬─────┘          │               │        │
│       │                  │                │                │               │        │
│       │                  │                │══"CREATE_NEW"═▶│               │        │
│       │                  │                │  + state       │               │        │
│       │                  │                │  + "close old" │               │        │
│       │                  │                │                │               │        │
│       │                  │                │                │──create──────▶│        │
│       │                  │                │                │  (inject      │        │
│       │                  │                │                │   state)      │        │
│       │                  │                │                │               │        │
│       │                  │                │                │◀──"STARTED"───│        │
│       │                  │                │                │               │        │
│       │◀══"CLOSE_NOW"════│◀═══════════════│◀═══════════════│               │        │
│       │                  │                │                │               │        │
│       X                  │                │                │         ┌─────┴─────┐  │
│  (old closes)            │                │                │         │ EXECUTING │  │
│                          │                │                │         │ (resumed) │  │
│                          │                │                │         └───────────┘  │
│                                                                                     │
└─────────────────────────────────────────────────────────────────────────────────────┘
```

#### Designer Handoff State Structure

```go
type DesignerPipelineHandoff struct {
    // Metadata
    OldPipelineID   PipelineID    `json:"old_pipeline_id"`
    SessionID       string        `json:"session_id"`
    DAGID           string        `json:"dag_id"`
    TaskID          string        `json:"task_id"`
    HandoffReason   string        `json:"handoff_reason"` // "designer_context_95%"
    Timestamp       time.Time     `json:"timestamp"`
    HandoffIndex    int           `json:"handoff_index"`  // For chaining (1, 2, 3...)

    // Bundled state from all three agents
    DesignerState     *DesignerHandoffState     `json:"designer_state"`
    UIInspectorState  *UIInspectorHandoffState  `json:"ui_inspector_state"`
    UITesterState     *UITesterHandoffState     `json:"ui_tester_state"`
}

type DesignerHandoffState struct {
    OriginalPrompt      string              `json:"original_prompt"`      // Verbatim
    Accomplished        []string            `json:"accomplished"`         // What was done
    ComponentsCreated   []ComponentInfo     `json:"components_created"`   // New components
    ComponentsModified  []ComponentChange   `json:"components_modified"`  // Changed components
    TokensUsed          []TokenUsage        `json:"tokens_used"`          // Design tokens applied
    Remaining           []string            `json:"remaining"`            // TODOs to complete
    ContextNotes        string              `json:"context_notes"`        // Critical context
}

type ComponentInfo struct {
    Name        string   `json:"name"`
    Path        string   `json:"path"`
    Type        string   `json:"type"`        // "component", "layout", "page"
    Props       []string `json:"props"`       // Prop names
    Exports     []string `json:"exports"`     // Exported items
}

type ComponentChange struct {
    Name        string   `json:"name"`
    Path        string   `json:"path"`
    ChangeType  string   `json:"change_type"` // "props", "styling", "structure", "a11y"
    Description string   `json:"description"`
}

type TokenUsage struct {
    TokenName   string `json:"token_name"`
    Category    string `json:"category"`    // "color", "spacing", "typography"
    UsedIn      string `json:"used_in"`     // File/component path
}

type UIInspectorHandoffState struct {
    ChecksPerformed     []string              `json:"checks_performed"`
    TokenViolations     []TokenViolation      `json:"token_violations"`
    A11yIssuesFound     []A11yIssue           `json:"a11y_issues_found"`
    A11yIssuesResolved  []A11yIssue           `json:"a11y_issues_resolved"`
    A11yIssuesRemaining []A11yIssue           `json:"a11y_issues_remaining"`
    ResponsiveIssues    []ResponsiveIssue     `json:"responsive_issues"`
    ComponentReuseConcerns []ReuseConcern     `json:"component_reuse_concerns"`
    ValidationState     map[string]bool       `json:"validation_state"`   // category → pass/fail
}

type TokenViolation struct {
    File        string `json:"file"`
    Line        int    `json:"line"`
    Pattern     string `json:"pattern"`      // e.g., "#ff0000"
    SuggestedFix string `json:"suggested_fix"` // e.g., "var(--color-error)"
    Status      string `json:"status"`       // "pending", "fixed"
}

type A11yIssue struct {
    File        string `json:"file"`
    Component   string `json:"component"`
    Rule        string `json:"rule"`         // WCAG rule ID
    Severity    string `json:"severity"`     // "critical", "serious", "moderate", "minor"
    Description string `json:"description"`
    SuggestedFix string `json:"suggested_fix"`
}

type ResponsiveIssue struct {
    File        string `json:"file"`
    Breakpoint  string `json:"breakpoint"`   // "sm", "md", "lg"
    Issue       string `json:"issue"`
    SuggestedFix string `json:"suggested_fix"`
}

type ReuseConcern struct {
    NewComponent      string `json:"new_component"`
    SimilarExisting   string `json:"similar_existing"`
    SimilarityPercent int    `json:"similarity_percent"`
    Recommendation    string `json:"recommendation"`
}

type UITesterHandoffState struct {
    VisualTests       []VisualTestResult    `json:"visual_tests"`
    A11yTests         []A11yTestResult      `json:"a11y_tests"`
    ResponsiveTests   []ResponsiveTestResult `json:"responsive_tests"`
    KeyboardNavTests  []KeyboardTestResult  `json:"keyboard_nav_tests"`
    ThemeTests        []ThemeTestResult     `json:"theme_tests"`
    CoverageNeeded    []string              `json:"coverage_needed"`
}

type VisualTestResult struct {
    Component   string  `json:"component"`
    Viewport    string  `json:"viewport"`
    Status      string  `json:"status"`      // "pass", "fail", "baseline"
    DiffPercent float64 `json:"diff_percent,omitempty"`
    SnapshotPath string `json:"snapshot_path"`
}

type A11yTestResult struct {
    Component   string `json:"component"`
    Tool        string `json:"tool"`        // "axe-core", "lighthouse"
    Passed      int    `json:"passed"`
    Failed      int    `json:"failed"`
    Score       int    `json:"score,omitempty"`
}

type ResponsiveTestResult struct {
    Component   string `json:"component"`
    Breakpoint  string `json:"breakpoint"`
    Status      string `json:"status"`
    Issues      []string `json:"issues,omitempty"`
}

type KeyboardTestResult struct {
    Component   string `json:"component"`
    TestCase    string `json:"test_case"`   // "tab-order", "focus-visible", "escape-closes"
    Status      string `json:"status"`
}

type ThemeTestResult struct {
    Component   string `json:"component"`
    Theme       string `json:"theme"`       // "light", "dark", "high-contrast"
    Status      string `json:"status"`
    Issues      []string `json:"issues,omitempty"`
}
```

#### Designer Handoff Message Flow

```
1. Designer (old) prepares bundled state (Designer + UIInspector + UITester)

2. Designer (old) ──GUIDE──▶ Architect    : "HANDOFF_REQUEST" + full state

3. Architect examines state, adjusts workflow if necessary
   - May rebalance remaining work
   - May split into multiple smaller tasks
   - May identify components that need different handling

4. Architect ──GUIDE──▶ Orchestrator      : "CREATE_PIPELINE_WITH_STATE" {
                                              state: <bundled state>,
                                              close_pipeline: <old pipeline ID>
                                            }

5. Orchestrator:
   - Creates new Designer pipeline
   - Injects state into new pipeline (all three agents start with context)
   - New pipeline starts executing
   - Closes old pipeline

6. Orchestrator ──GUIDE──▶ Architect      : "HANDOFF_COMPLETE"
```

#### Designer Handoff Rules

| Rule | Description |
|------|-------------|
| **Trigger** | Designer at 95% OR user request |
| **Who triggers** | Only Designer triggers handoff (UIInspector/UITester compact locally) |
| **State bundling** | Designer collects state from UIInspector + UITester |
| **Architect review** | Architect examines and may adjust workflow |
| **Archivalist** | Handoff state also stored for audit/recovery |
| **Retry** | If handoff fails, retry. If retries fail, fallback to summarize → Archivalist → compact |
| **Chaining** | Handoffs can chain infinitely (tracked by HandoffIndex) |
| **User control** | User can stop a handoff chain if desired |

#### New Designer Pipeline Receives

The new pipeline's agents start with full context:

- **New Designer**: Knows original prompt, what was done (components, tokens), what remains
- **New UIInspector**: Knows token violations, a11y issues (resolved/remaining), responsive issues, reuse concerns
- **New UITester**: Knows visual regression state, a11y test results, responsive test results, keyboard/theme test state

This minimizes re-learning and re-discovery for UI/UX work.

**Archivalist Category**: `designer_pipeline_handoff`

---

## VectorGraphDB: Unified Knowledge Graph

### Overview

The VectorGraphDB is a unified, embedded knowledge graph that combines **vector similarity search** with **graph-based structural queries** across three domains: Code (Librarian), History (Archivalist), and Academic (external knowledge). It uses SQLite as the storage backend with in-memory HNSW indexes for fast vector search - no external dependencies required.

```
┌─────────────────────────────────────────────────────────────────────────────┐
│                    VECTORGRAPHDB: THREE-DOMAIN ARCHITECTURE                  │
├─────────────────────────────────────────────────────────────────────────────┤
│                                                                             │
│  ┌─────────────────┐  ┌─────────────────┐  ┌─────────────────┐             │
│  │   LIBRARIAN     │  │   ARCHIVALIST   │  │    ACADEMIC     │             │
│  │   (Code)        │  │   (History)     │  │   (Knowledge)   │             │
│  │                 │  │                 │  │                 │             │
│  │  Domain: 0      │  │  Domain: 1      │  │  Domain: 2      │             │
│  │                 │  │                 │  │                 │             │
│  │  Nodes:         │  │  Nodes:         │  │  Nodes:         │             │
│  │  • File         │  │  • Entry        │  │  • Paper        │             │
│  │  • Function     │  │  • Session      │  │  • Docs         │             │
│  │  • Struct       │  │  • Workflow     │  │  • BestPractice │             │
│  │  • Interface    │  │  • Outcome      │  │  • RFC          │             │
│  │  • Package      │  │  • Decision     │  │  • Tutorial     │             │
│  └────────┬────────┘  └────────┬────────┘  └────────┬────────┘             │
│           │                    │                    │                       │
│           └────────────────────┼────────────────────┘                       │
│                                │                                            │
│                    CROSS-DOMAIN EDGES                                       │
│           ┌────────────────────┼────────────────────┐                       │
│           │                    │                    │                       │
│           ▼                    ▼                    ▼                       │
│  ┌─────────────────────────────────────────────────────────────────────┐   │
│  │                      VECTORGRAPHDB                                   │   │
│  │                    (Single SQLite File)                              │   │
│  │                                                                     │   │
│  │  ┌─────────────────────────────────────────────────────────────┐   │   │
│  │  │  NODES TABLE                                                 │   │   │
│  │  │  • Unified schema for all domains                           │   │   │
│  │  │  • Domain field partitions data                             │   │   │
│  │  │  • Type field identifies node kind                          │   │   │
│  │  └─────────────────────────────────────────────────────────────┘   │   │
│  │                                                                     │   │
│  │  ┌─────────────────────────────────────────────────────────────┐   │   │
│  │  │  EDGES TABLE                                                 │   │   │
│  │  │  • Structural: calls, imports, implements                   │   │   │
│  │  │  • Temporal: produced_by, resulted_in                       │   │   │
│  │  │  • Cross-domain: modified, based_on, documents              │   │   │
│  │  └─────────────────────────────────────────────────────────────┘   │   │
│  │                                                                     │   │
│  │  ┌─────────────────────────────────────────────────────────────┐   │   │
│  │  │  VECTORS TABLE (BLOBs)        HNSW INDEX (In-Memory)        │   │   │
│  │  │  • Embeddings as binary       • O(log n) search             │   │   │
│  │  │  • Pre-computed magnitudes    • Loaded on startup           │   │   │
│  │  │  • Persisted to disk          • ~50MB per 10K nodes         │   │   │
│  │  └─────────────────────────────────────────────────────────────┘   │   │
│  │                                                                     │   │
│  └─────────────────────────────────────────────────────────────────────┘   │
│                                                                             │
└─────────────────────────────────────────────────────────────────────────────┘
```

### SQLite Schema

```sql
-- =============================================================================
-- DOMAIN AND TYPE ENUMS (stored as integers)
-- =============================================================================
-- Domain: 0 = code, 1 = history, 2 = academic
-- NodeType: See node type constants below

-- =============================================================================
-- NODES TABLE (unified for all domains)
-- =============================================================================
CREATE TABLE nodes (
    id TEXT PRIMARY KEY,
    domain INTEGER NOT NULL,
    node_type INTEGER NOT NULL,
    name TEXT NOT NULL,

    -- Code domain fields
    path TEXT,
    package TEXT,
    line_start INTEGER,
    line_end INTEGER,
    signature TEXT,

    -- History domain fields
    session_id TEXT,
    timestamp DATETIME,
    category TEXT,

    -- Academic domain fields
    url TEXT,
    source TEXT,
    authors JSON,
    published_at DATETIME,

    -- Common fields
    content TEXT,
    content_hash TEXT,
    metadata JSON,

    -- Verification and trust
    verified BOOLEAN DEFAULT FALSE,
    verification_type INTEGER,
    confidence REAL DEFAULT 1.0,
    trust_level INTEGER DEFAULT 50,

    -- Temporal tracking
    created_at DATETIME DEFAULT CURRENT_TIMESTAMP,
    updated_at DATETIME DEFAULT CURRENT_TIMESTAMP,
    expires_at DATETIME,
    superseded_by TEXT,

    CHECK (domain IN (0, 1, 2)),
    FOREIGN KEY (superseded_by) REFERENCES nodes(id) ON DELETE SET NULL
);

-- =============================================================================
-- EDGES TABLE
-- =============================================================================
CREATE TABLE edges (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    source_id TEXT NOT NULL,
    target_id TEXT NOT NULL,
    edge_type INTEGER NOT NULL,
    weight REAL DEFAULT 1.0,
    metadata JSON,
    created_at DATETIME DEFAULT CURRENT_TIMESTAMP,

    FOREIGN KEY (source_id) REFERENCES nodes(id) ON DELETE CASCADE,
    FOREIGN KEY (target_id) REFERENCES nodes(id) ON DELETE CASCADE,
    UNIQUE (source_id, target_id, edge_type)
);

-- =============================================================================
-- VECTORS TABLE (embeddings as BLOBs)
-- =============================================================================
CREATE TABLE vectors (
    node_id TEXT PRIMARY KEY,
    embedding BLOB NOT NULL,
    magnitude REAL NOT NULL,
    dimensions INTEGER NOT NULL DEFAULT 768,
    domain INTEGER NOT NULL,
    node_type INTEGER NOT NULL,

    FOREIGN KEY (node_id) REFERENCES nodes(id) ON DELETE CASCADE
);

-- =============================================================================
-- PROVENANCE TABLE (tracks source of information)
-- =============================================================================
CREATE TABLE provenance (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    node_id TEXT NOT NULL,
    source_type INTEGER NOT NULL,
    source_node_id TEXT,
    source_path TEXT,
    source_url TEXT,
    confidence REAL NOT NULL,
    verified_at DATETIME,

    FOREIGN KEY (node_id) REFERENCES nodes(id) ON DELETE CASCADE,
    FOREIGN KEY (source_node_id) REFERENCES nodes(id) ON DELETE SET NULL
);

-- =============================================================================
-- CONFLICTS TABLE (detected contradictions)
-- =============================================================================
CREATE TABLE conflicts (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    conflict_type INTEGER NOT NULL,
    subject TEXT NOT NULL,
    node_id_a TEXT NOT NULL,
    node_id_b TEXT NOT NULL,
    description TEXT NOT NULL,
    resolution TEXT,
    resolved BOOLEAN DEFAULT FALSE,
    detected_at DATETIME DEFAULT CURRENT_TIMESTAMP,
    resolved_at DATETIME,

    FOREIGN KEY (node_id_a) REFERENCES nodes(id) ON DELETE CASCADE,
    FOREIGN KEY (node_id_b) REFERENCES nodes(id) ON DELETE CASCADE
);

-- =============================================================================
-- HNSW INDEX PERSISTENCE
-- =============================================================================
CREATE TABLE hnsw_meta (
    key TEXT PRIMARY KEY,
    value TEXT NOT NULL
);

CREATE TABLE hnsw_edges (
    source_id TEXT NOT NULL,
    target_id TEXT NOT NULL,
    level INTEGER NOT NULL,
    PRIMARY KEY (source_id, target_id, level)
);

-- =============================================================================
-- ACADEMIC-SPECIFIC TABLES
-- =============================================================================
CREATE TABLE academic_sources (
    id TEXT PRIMARY KEY,
    name TEXT NOT NULL,
    base_url TEXT NOT NULL,
    api_endpoint TEXT,
    rate_limit_per_min INTEGER,
    requires_auth BOOLEAN DEFAULT FALSE,
    last_crawled_at DATETIME
);

CREATE TABLE academic_chunks (
    id TEXT PRIMARY KEY,
    node_id TEXT NOT NULL,
    chunk_index INTEGER NOT NULL,
    content TEXT NOT NULL,
    FOREIGN KEY (node_id) REFERENCES nodes(id) ON DELETE CASCADE,
    UNIQUE (node_id, chunk_index)
);

CREATE TABLE library_docs (
    library_path TEXT PRIMARY KEY,
    doc_node_id TEXT,
    FOREIGN KEY (doc_node_id) REFERENCES nodes(id) ON DELETE SET NULL
);

-- =============================================================================
-- INDEXES
-- =============================================================================
CREATE INDEX idx_nodes_domain_type ON nodes(domain, node_type);
CREATE INDEX idx_nodes_path ON nodes(path) WHERE path IS NOT NULL;
CREATE INDEX idx_nodes_name ON nodes(name);
CREATE INDEX idx_nodes_session ON nodes(session_id) WHERE session_id IS NOT NULL;
CREATE INDEX idx_nodes_hash ON nodes(content_hash) WHERE content_hash IS NOT NULL;
CREATE INDEX idx_nodes_superseded ON nodes(superseded_by) WHERE superseded_by IS NOT NULL;

CREATE INDEX idx_edges_source ON edges(source_id, edge_type);
CREATE INDEX idx_edges_target ON edges(target_id, edge_type);
CREATE INDEX idx_edges_type ON edges(edge_type);

CREATE INDEX idx_vectors_domain ON vectors(domain);
CREATE INDEX idx_vectors_domain_type ON vectors(domain, node_type);

CREATE INDEX idx_provenance_node ON provenance(node_id);
CREATE INDEX idx_conflicts_unresolved ON conflicts(resolved) WHERE resolved = FALSE;

CREATE INDEX idx_hnsw_edges_level ON hnsw_edges(level, source_id);
CREATE INDEX idx_chunks_node ON academic_chunks(node_id);
```

### Node and Edge Types

```go
// =============================================================================
// DOMAIN CONSTANTS
// =============================================================================

type Domain int

const (
    DomainCode     Domain = 0  // Librarian - live codebase
    DomainHistory  Domain = 1  // Archivalist - historical data
    DomainAcademic Domain = 2  // Academic - external knowledge
)

// =============================================================================
// NODE TYPES BY DOMAIN
// =============================================================================

type NodeType int

// Code domain (0-99)
const (
    NodeTypeFile      NodeType = 0
    NodeTypePackage   NodeType = 1
    NodeTypeFunction  NodeType = 2
    NodeTypeMethod    NodeType = 3
    NodeTypeStruct    NodeType = 4
    NodeTypeInterface NodeType = 5
    NodeTypeVariable  NodeType = 6
    NodeTypeConstant  NodeType = 7
    NodeTypeImport    NodeType = 8
)

// History domain (100-199)
const (
    NodeTypeHistoryEntry NodeType = 100
    NodeTypeSession      NodeType = 101
    NodeTypeWorkflow     NodeType = 102
    NodeTypeOutcome      NodeType = 103
    NodeTypeDecision     NodeType = 104
)

// Academic domain (200-299)
const (
    NodeTypePaper         NodeType = 200
    NodeTypeDocumentation NodeType = 201
    NodeTypeBestPractice  NodeType = 202
    NodeTypeRFC           NodeType = 203
    NodeTypeStackOverflow NodeType = 204
    NodeTypeBlogPost      NodeType = 205
    NodeTypeTutorial      NodeType = 206
)

// =============================================================================
// EDGE TYPES
// =============================================================================

type EdgeType int

// Code structural edges (0-49)
const (
    EdgeTypeCalls         EdgeType = 0   // function calls function
    EdgeTypeCalledBy      EdgeType = 1   // reverse of Calls
    EdgeTypeImports       EdgeType = 2   // file imports package
    EdgeTypeImportedBy    EdgeType = 3   // reverse of Imports
    EdgeTypeImplements    EdgeType = 4   // struct implements interface
    EdgeTypeImplementedBy EdgeType = 5   // reverse of Implements
    EdgeTypeEmbeds        EdgeType = 6   // struct embeds struct
    EdgeTypeHasField      EdgeType = 7   // struct has field of type
    EdgeTypeHasMethod     EdgeType = 8   // type has method
    EdgeTypeDefines       EdgeType = 9   // file defines symbol
    EdgeTypeDefinedIn     EdgeType = 10  // reverse of Defines
    EdgeTypeReturns       EdgeType = 11  // function returns type
    EdgeTypeReceives      EdgeType = 12  // function receives param type
)

// History edges (50-99)
const (
    EdgeTypeProducedBy  EdgeType = 50  // entry produced by session
    EdgeTypeResultedIn  EdgeType = 51  // workflow resulted in outcome
    EdgeTypeSimilarTo   EdgeType = 52  // semantically similar
    EdgeTypeFollowedBy  EdgeType = 53  // temporal sequence
    EdgeTypeSupersedes  EdgeType = 54  // newer supersedes older
)

// Cross-domain edges (100-149)
const (
    EdgeTypeModified    EdgeType = 100  // history entry modified code
    EdgeTypeCreated     EdgeType = 101  // history entry created code
    EdgeTypeDeleted     EdgeType = 102  // history entry deleted code
    EdgeTypeBasedOn     EdgeType = 103  // decision based on academic
    EdgeTypeReferences  EdgeType = 104  // entry references academic
    EdgeTypeValidatedBy EdgeType = 105  // approach validated by academic
    EdgeTypeDocuments   EdgeType = 106  // academic documents code
    EdgeTypeUsesLibrary EdgeType = 107  // code uses library (→ docs)
    EdgeTypeImplementsPattern EdgeType = 108  // code implements pattern
)

// Academic edges (150-199)
const (
    EdgeTypeCites       EdgeType = 150  // paper cites paper
    EdgeTypeRelatedTo   EdgeType = 151  // conceptually related
)

// =============================================================================
// VERIFICATION AND TRUST TYPES
// =============================================================================

type VerificationType int

const (
    VerificationNone         VerificationType = 0
    VerificationAgainstCode  VerificationType = 1
    VerificationAgainstHistory VerificationType = 2
    VerificationByUser       VerificationType = 3
)

type SourceType int

const (
    SourceTypeCode         SourceType = 0
    SourceTypeHistory      SourceType = 1
    SourceTypeAcademic     SourceType = 2
    SourceTypeLLMInference SourceType = 3
    SourceTypeUserProvided SourceType = 4
)

type TrustLevel int

const (
    TrustLevelGround     TrustLevel = 100  // Current code (source of truth)
    TrustLevelRecent     TrustLevel = 80   // Recent verified history
    TrustLevelStandard   TrustLevel = 70   // RFCs, official docs
    TrustLevelAcademic   TrustLevel = 60   // Peer-reviewed papers
    TrustLevelOldHistory TrustLevel = 40   // Old history (may be stale)
    TrustLevelBlog       TrustLevel = 30   // Blog posts, SO answers
    TrustLevelLLM        TrustLevel = 20   // LLM inference (unverified)
)

type ConflictType int

const (
    ConflictTypeTemporal       ConflictType = 0  // Old vs new data
    ConflictTypeSourceMismatch ConflictType = 1  // Code vs history
    ConflictTypeSemantic       ConflictType = 2  // Contradictory claims
)
```

### HNSW Index Implementation (Pure Go, No Extensions)

The HNSW (Hierarchical Navigable Small World) index provides O(log n) approximate nearest neighbor search without requiring sqlite-vec or any external dependencies.

```go
// =============================================================================
// HNSW INDEX (Hierarchical Navigable Small World)
// =============================================================================

// HNSWIndex provides O(log n) approximate nearest neighbor search
type HNSWIndex struct {
    mu sync.RWMutex

    // Graph layers (layer 0 = all nodes, higher layers = fewer nodes)
    layers []map[string][]string  // layer -> nodeID -> neighbor IDs

    // Node data
    vectors    map[string][]float32
    magnitudes map[string]float64
    domains    map[string]Domain
    nodeTypes  map[string]NodeType

    // Parameters
    M            int     // Max connections per node (default: 16)
    efConstruct  int     // Beam width during construction (default: 200)
    efSearch     int     // Beam width during search (default: 50)
    levelMult    float64 // Level multiplier (default: 1/ln(M))
    maxLevel     int     // Current max level
    entryPoint   string  // Entry node ID
}

// HNSWConfig configures the HNSW index
type HNSWConfig struct {
    M           int  // Max connections per node
    EfConstruct int  // Beam width during construction
    EfSearch    int  // Beam width during search
}

// DefaultHNSWConfig returns sensible defaults
func DefaultHNSWConfig() HNSWConfig {
    return HNSWConfig{
        M:           16,
        EfConstruct: 200,
        EfSearch:    50,
    }
}

// NewHNSWIndex creates a new HNSW index
func NewHNSWIndex(config HNSWConfig) *HNSWIndex {
    if config.M == 0 {
        config = DefaultHNSWConfig()
    }

    return &HNSWIndex{
        layers:      make([]map[string][]string, 0),
        vectors:     make(map[string][]float32),
        magnitudes:  make(map[string]float64),
        domains:     make(map[string]Domain),
        nodeTypes:   make(map[string]NodeType),
        M:           config.M,
        efConstruct: config.EfConstruct,
        efSearch:    config.EfSearch,
        levelMult:   1.0 / math.Log(float64(config.M)),
    }
}

// Add inserts a node into the index
func (h *HNSWIndex) Add(nodeID string, embedding []float32, domain Domain, nodeType NodeType) {
    h.mu.Lock()
    defer h.mu.Unlock()

    mag := magnitude(embedding)
    h.vectors[nodeID] = embedding
    h.magnitudes[nodeID] = mag
    h.domains[nodeID] = domain
    h.nodeTypes[nodeID] = nodeType

    // Random level for this node
    level := h.randomLevel()

    // Ensure we have enough layers
    for len(h.layers) <= level {
        h.layers = append(h.layers, make(map[string][]string))
    }

    if h.entryPoint == "" {
        // First node
        h.entryPoint = nodeID
        h.maxLevel = level
        for l := 0; l <= level; l++ {
            h.layers[l][nodeID] = []string{}
        }
        return
    }

    // Find entry point at top level, descend
    currNode := h.entryPoint

    for l := h.maxLevel; l > level; l-- {
        currNode = h.greedySearchSingle(embedding, currNode, l)
    }

    // Insert at each level
    for l := min(level, h.maxLevel); l >= 0; l-- {
        neighbors := h.searchLayer(embedding, currNode, h.efConstruct, l)
        selected := h.selectNeighbors(embedding, neighbors, h.M)

        h.layers[l][nodeID] = selected
        for _, neighbor := range selected {
            h.layers[l][neighbor] = append(h.layers[l][neighbor], nodeID)
            if len(h.layers[l][neighbor]) > h.M*2 {
                h.layers[l][neighbor] = h.selectNeighbors(
                    h.vectors[neighbor],
                    h.layers[l][neighbor],
                    h.M,
                )
            }
        }

        if l > 0 && len(neighbors) > 0 {
            currNode = neighbors[0]
        }
    }

    if level > h.maxLevel {
        h.maxLevel = level
        h.entryPoint = nodeID
    }
}

// Search finds k nearest neighbors
func (h *HNSWIndex) Search(query []float32, k int, filter *SearchFilter) []ScoredNode {
    h.mu.RLock()
    defer h.mu.RUnlock()

    if h.entryPoint == "" {
        return nil
    }

    // Descend from top to layer 0
    currNode := h.entryPoint
    for l := h.maxLevel; l > 0; l-- {
        currNode = h.greedySearchSingle(query, currNode, l)
    }

    // Search at layer 0
    ef := h.efSearch
    if k > ef {
        ef = k
    }

    candidates := h.searchLayer(query, currNode, ef*2, 0)  // Get extra for filtering

    // Apply filter and score
    queryMag := magnitude(query)
    var results []ScoredNode

    for _, nodeID := range candidates {
        // Apply domain/type filter
        if filter != nil {
            if filter.Domain != nil && h.domains[nodeID] != *filter.Domain {
                continue
            }
            if filter.NodeType != nil && h.nodeTypes[nodeID] != *filter.NodeType {
                continue
            }
        }

        sim := cosineSimilarityPrecomputed(query, h.vectors[nodeID], queryMag, h.magnitudes[nodeID])
        results = append(results, ScoredNode{
            NodeID:     nodeID,
            Similarity: sim,
            Domain:     h.domains[nodeID],
            NodeType:   h.nodeTypes[nodeID],
        })
    }

    // Sort by similarity
    sort.Slice(results, func(i, j int) bool {
        return results[i].Similarity > results[j].Similarity
    })

    if len(results) > k {
        results = results[:k]
    }

    return results
}

// Delete removes a node from the index
func (h *HNSWIndex) Delete(nodeID string) {
    h.mu.Lock()
    defer h.mu.Unlock()

    delete(h.vectors, nodeID)
    delete(h.magnitudes, nodeID)
    delete(h.domains, nodeID)
    delete(h.nodeTypes, nodeID)

    // Remove from all layers
    for level := range h.layers {
        delete(h.layers[level], nodeID)
        // Remove references from neighbors
        for nid, neighbors := range h.layers[level] {
            newNeighbors := make([]string, 0, len(neighbors))
            for _, n := range neighbors {
                if n != nodeID {
                    newNeighbors = append(newNeighbors, n)
                }
            }
            h.layers[level][nid] = newNeighbors
        }
    }

    // Update entry point if deleted
    if h.entryPoint == nodeID {
        h.entryPoint = ""
        for _, layer := range h.layers {
            for nid := range layer {
                h.entryPoint = nid
                break
            }
            if h.entryPoint != "" {
                break
            }
        }
    }
}

// Helper functions
func (h *HNSWIndex) randomLevel() int {
    level := 0
    for rand.Float64() < 0.5 && level < 16 {
        level++
    }
    return level
}

func (h *HNSWIndex) greedySearchSingle(query []float32, entry string, level int) string {
    curr := entry
    currDist := h.distance(query, curr)

    for {
        changed := false
        for _, neighbor := range h.layers[level][curr] {
            dist := h.distance(query, neighbor)
            if dist < currDist {
                curr = neighbor
                currDist = dist
                changed = true
            }
        }
        if !changed {
            break
        }
    }

    return curr
}

func (h *HNSWIndex) searchLayer(query []float32, entry string, ef int, level int) []string {
    visited := make(map[string]bool)
    visited[entry] = true

    candidates := []string{entry}
    results := []string{entry}

    for len(candidates) > 0 {
        // Get closest candidate
        closest := candidates[0]
        closestDist := h.distance(query, closest)
        closestIdx := 0
        for i, c := range candidates[1:] {
            d := h.distance(query, c)
            if d < closestDist {
                closest = c
                closestDist = d
                closestIdx = i + 1
            }
        }
        candidates = append(candidates[:closestIdx], candidates[closestIdx+1:]...)

        // Get furthest result
        furthest := results[len(results)-1]
        furthestDist := h.distance(query, furthest)
        for _, r := range results {
            d := h.distance(query, r)
            if d > furthestDist {
                furthest = r
                furthestDist = d
            }
        }

        if closestDist > furthestDist {
            break
        }

        // Explore neighbors
        for _, neighbor := range h.layers[level][closest] {
            if visited[neighbor] {
                continue
            }
            visited[neighbor] = true

            neighborDist := h.distance(query, neighbor)
            if neighborDist < furthestDist || len(results) < ef {
                candidates = append(candidates, neighbor)
                results = append(results, neighbor)

                // Keep only ef best
                if len(results) > ef {
                    sort.Slice(results, func(i, j int) bool {
                        return h.distance(query, results[i]) < h.distance(query, results[j])
                    })
                    results = results[:ef]
                }
            }
        }
    }

    return results
}

func (h *HNSWIndex) selectNeighbors(query []float32, candidates []string, m int) []string {
    if len(candidates) <= m {
        return candidates
    }

    // Sort by distance
    sort.Slice(candidates, func(i, j int) bool {
        return h.distance(query, candidates[i]) < h.distance(query, candidates[j])
    })

    return candidates[:m]
}

func (h *HNSWIndex) distance(query []float32, nodeID string) float64 {
    return 1.0 - cosineSimilarityPrecomputed(
        query,
        h.vectors[nodeID],
        magnitude(query),
        h.magnitudes[nodeID],
    )
}

// SearchFilter filters search results
type SearchFilter struct {
    Domain   *Domain
    NodeType *NodeType
}

// ScoredNode is a search result with similarity score
type ScoredNode struct {
    NodeID     string
    Similarity float64
    Domain     Domain
    NodeType   NodeType
}

// Utility functions
func magnitude(v []float32) float64 {
    var sum float64
    for _, x := range v {
        sum += float64(x) * float64(x)
    }
    return math.Sqrt(sum)
}

func cosineSimilarityPrecomputed(a, b []float32, magA, magB float64) float64 {
    var dot float64
    for i := range a {
        dot += float64(a[i]) * float64(b[i])
    }
    if magA == 0 || magB == 0 {
        return 0
    }
    return dot / (magA * magB)
}

func serializeEmbedding(embedding []float32) []byte {
    buf := make([]byte, len(embedding)*4)
    for i, v := range embedding {
        binary.LittleEndian.PutUint32(buf[i*4:], math.Float32bits(v))
    }
    return buf
}

func deserializeEmbedding(data []byte) []float32 {
    embedding := make([]float32, len(data)/4)
    for i := range embedding {
        embedding[i] = math.Float32frombits(binary.LittleEndian.Uint32(data[i*4:]))
    }
    return embedding
}
```

### Core VectorGraphDB Structure

```go
// =============================================================================
// VECTORGRAPHDB CORE
// =============================================================================

// VectorGraphDB is the unified knowledge graph with vector search
type VectorGraphDB struct {
    mu sync.RWMutex

    // SQLite connection
    db *sql.DB

    // In-memory HNSW index
    hnsw *HNSWIndex

    // Embedder for generating vectors
    embedder Embedder

    // Prepared statements
    stmtInsertNode   *sql.Stmt
    stmtInsertEdge   *sql.Stmt
    stmtInsertVector *sql.Stmt
    stmtGetNode      *sql.Stmt
    stmtGetEdges     *sql.Stmt

    // Configuration
    config VectorGraphDBConfig
}

// VectorGraphDBConfig configures the database
type VectorGraphDBConfig struct {
    DBPath          string
    EmbeddingDims   int
    HNSWConfig      HNSWConfig
    MaxNodes        int
    EnableWAL       bool
}

// DefaultVectorGraphDBConfig returns sensible defaults
func DefaultVectorGraphDBConfig(dbPath string) VectorGraphDBConfig {
    return VectorGraphDBConfig{
        DBPath:        dbPath,
        EmbeddingDims: 768,
        HNSWConfig:    DefaultHNSWConfig(),
        MaxNodes:      1000000,
        EnableWAL:     true,
    }
}

// NewVectorGraphDB creates a new VectorGraphDB
func NewVectorGraphDB(config VectorGraphDBConfig, embedder Embedder) (*VectorGraphDB, error) {
    // Open SQLite with WAL mode
    dsn := config.DBPath
    if config.EnableWAL {
        dsn += "?_journal_mode=WAL&_synchronous=NORMAL"
    }

    db, err := sql.Open("sqlite3", dsn)
    if err != nil {
        return nil, fmt.Errorf("failed to open database: %w", err)
    }

    vdb := &VectorGraphDB{
        db:       db,
        hnsw:     NewHNSWIndex(config.HNSWConfig),
        embedder: embedder,
        config:   config,
    }

    // Initialize schema
    if err := vdb.initSchema(); err != nil {
        return nil, fmt.Errorf("failed to init schema: %w", err)
    }

    // Prepare statements
    if err := vdb.prepareStatements(); err != nil {
        return nil, fmt.Errorf("failed to prepare statements: %w", err)
    }

    // Load HNSW index from disk
    if err := vdb.loadHNSWIndex(); err != nil {
        return nil, fmt.Errorf("failed to load HNSW index: %w", err)
    }

    return vdb, nil
}

// AddNode adds a node to the graph
func (vdb *VectorGraphDB) AddNode(ctx context.Context, node *Node) error {
    vdb.mu.Lock()
    defer vdb.mu.Unlock()

    // Generate embedding if content provided
    var embedding []float32
    var mag float64
    if node.Content != "" && vdb.embedder != nil {
        var err error
        embedding, err = vdb.embedder.Embed(ctx, node.Name+" "+node.Content)
        if err != nil {
            return fmt.Errorf("failed to generate embedding: %w", err)
        }
        mag = magnitude(embedding)
    }

    // Insert node
    _, err := vdb.stmtInsertNode.ExecContext(ctx,
        node.ID, node.Domain, node.Type, node.Name,
        node.Path, node.Package, node.LineStart, node.LineEnd, node.Signature,
        node.SessionID, node.Timestamp, node.Category,
        node.URL, node.Source, node.Authors, node.PublishedAt,
        node.Content, node.ContentHash, node.Metadata,
        node.Verified, node.VerificationType, node.Confidence, node.TrustLevel,
        node.ExpiresAt, node.SupersededBy,
    )
    if err != nil {
        return fmt.Errorf("failed to insert node: %w", err)
    }

    // Insert vector if embedding generated
    if len(embedding) > 0 {
        blob := serializeEmbedding(embedding)
        _, err = vdb.stmtInsertVector.ExecContext(ctx,
            node.ID, blob, mag, len(embedding), node.Domain, node.Type,
        )
        if err != nil {
            return fmt.Errorf("failed to insert vector: %w", err)
        }

        // Add to HNSW index
        vdb.hnsw.Add(node.ID, embedding, node.Domain, node.Type)
    }

    return nil
}

// AddEdge adds an edge between nodes
func (vdb *VectorGraphDB) AddEdge(ctx context.Context, sourceID, targetID string, edgeType EdgeType, weight float64, metadata map[string]any) error {
    vdb.mu.Lock()
    defer vdb.mu.Unlock()

    metaJSON, _ := json.Marshal(metadata)

    _, err := vdb.stmtInsertEdge.ExecContext(ctx,
        sourceID, targetID, edgeType, weight, metaJSON,
    )
    if err != nil {
        return fmt.Errorf("failed to insert edge: %w", err)
    }

    return nil
}

// SimilarNodes finds similar nodes using vector search
func (vdb *VectorGraphDB) SimilarNodes(ctx context.Context, query string, k int, filter *SearchFilter) ([]ScoredNode, error) {
    // Generate query embedding
    embedding, err := vdb.embedder.Embed(ctx, query)
    if err != nil {
        return nil, fmt.Errorf("failed to embed query: %w", err)
    }

    // Search HNSW index
    vdb.mu.RLock()
    results := vdb.hnsw.Search(embedding, k, filter)
    vdb.mu.RUnlock()

    return results, nil
}

// SimilarToNode finds nodes similar to an existing node
func (vdb *VectorGraphDB) SimilarToNode(ctx context.Context, nodeID string, k int, filter *SearchFilter) ([]ScoredNode, error) {
    vdb.mu.RLock()
    embedding, ok := vdb.hnsw.vectors[nodeID]
    vdb.mu.RUnlock()

    if !ok {
        return nil, fmt.Errorf("node not found: %s", nodeID)
    }

    vdb.mu.RLock()
    results := vdb.hnsw.Search(embedding, k+1, filter)  // +1 because it will find itself
    vdb.mu.RUnlock()

    // Remove self from results
    filtered := make([]ScoredNode, 0, k)
    for _, r := range results {
        if r.NodeID != nodeID {
            filtered = append(filtered, r)
        }
    }

    return filtered, nil
}

// TraverseEdges traverses edges from a node
func (vdb *VectorGraphDB) TraverseEdges(ctx context.Context, nodeID string, edgeType EdgeType, direction string) ([]*Node, error) {
    var query string
    if direction == "outgoing" {
        query = `
            SELECT n.* FROM nodes n
            JOIN edges e ON e.target_id = n.id
            WHERE e.source_id = ? AND e.edge_type = ?
        `
    } else {
        query = `
            SELECT n.* FROM nodes n
            JOIN edges e ON e.source_id = n.id
            WHERE e.target_id = ? AND e.edge_type = ?
        `
    }

    return vdb.queryNodes(ctx, query, nodeID, edgeType)
}

// GetNode retrieves a node by ID
func (vdb *VectorGraphDB) GetNode(ctx context.Context, nodeID string) (*Node, error) {
    row := vdb.stmtGetNode.QueryRowContext(ctx, nodeID)
    return vdb.scanNode(row)
}

// Close closes the database
func (vdb *VectorGraphDB) Close() error {
    // Save HNSW index
    if err := vdb.saveHNSWIndex(); err != nil {
        return fmt.Errorf("failed to save HNSW index: %w", err)
    }

    return vdb.db.Close()
}
```

### Cross-Domain Query Examples

```
┌─────────────────────────────────────────────────────────────────────────────┐
│                      CROSS-DOMAIN QUERY EXAMPLES                             │
├─────────────────────────────────────────────────────────────────────────────┤
│                                                                             │
│  QUERY: "What patterns does our auth code implement?"                       │
│  ═══════════════════════════════════════════════════                        │
│                                                                             │
│       LIBRARIAN                           ACADEMIC                          │
│       (Code)                              (Knowledge)                       │
│           │                                   │                             │
│           ▼                                   │                             │
│  1. Vector search: "authentication"          │                             │
│     Returns: auth/middleware.go,             │                             │
│              auth/jwt.go, etc.               │                             │
│           │                                   │                             │
│           ▼                                   │                             │
│  2. Traverse IMPLEMENTS_PATTERN edges ──────▶ 3. Get pattern nodes         │
│                                               │                             │
│                                               ▼                             │
│                                            Result:                          │
│                                            • "JWT Bearer Token Pattern"     │
│                                            • "Middleware Chain Pattern"     │
│  Token cost: 0 (graph traversal only)                                      │
│                                                                             │
│  ───────────────────────────────────────────────────────────────────────   │
│                                                                             │
│  QUERY: "Why did we choose JWT over sessions?"                             │
│  ═════════════════════════════════════════════                              │
│                                                                             │
│       ARCHIVALIST                         ACADEMIC                          │
│       (History)                           (Knowledge)                       │
│           │                                   │                             │
│           ▼                                   │                             │
│  1. Search history: "JWT sessions decision"  │                             │
│     Returns: decision entry from 2024-06     │                             │
│           │                                   │                             │
│           ▼                                   │                             │
│  2. Traverse BASED_ON edges ────────────────▶ 3. Get referenced sources    │
│                                               │                             │
│                                               ▼                             │
│                                            Result:                          │
│                                            • RFC 7519 (JWT spec)            │
│                                            • "Stateless Auth at Scale"      │
│  Token cost: 0 (graph traversal only)                                      │
│                                                                             │
│  ───────────────────────────────────────────────────────────────────────   │
│                                                                             │
│  QUERY: "What changes did we make to auth code last week?"                 │
│  ═════════════════════════════════════════════════════════                  │
│                                                                             │
│       ARCHIVALIST              LIBRARIAN                                    │
│       (History)                (Code)                                       │
│           │                        │                                        │
│           ▼                        │                                        │
│  1. Query entries from             │                                        │
│     last 7 days with               │                                        │
│     subject "auth"                 │                                        │
│           │                        │                                        │
│           ▼                        │                                        │
│  2. Traverse MODIFIED edges ──────▶ 3. Resolve to current code nodes       │
│                                    │                                        │
│                                    ▼                                        │
│                                 Result:                                     │
│                                 • auth/jwt.go:ValidateToken (modified)     │
│                                 • auth/refresh.go (created)                │
│  Token cost: 0 (graph traversal only)                                      │
│                                                                             │
│  ───────────────────────────────────────────────────────────────────────   │
│                                                                             │
│  QUERY: "Have we had issues with this function before?"                    │
│  ══════════════════════════════════════════════════════                     │
│                                                                             │
│       LIBRARIAN                ARCHIVALIST                                  │
│       (Code)                   (History)                                    │
│           │                        │                                        │
│           ▼                        │                                        │
│  1. Identify function node         │                                        │
│     (from context or query)        │                                        │
│           │                        │                                        │
│           ▼                        │                                        │
│  2. Traverse MODIFIED edges ──────▶ 3. Get history entries that            │
│     (reverse direction)               modified this function               │
│                                    │                                        │
│                                    ▼                                        │
│                                 4. Filter for outcomes with                │
│                                    category "bug_fix" or "issue"           │
│                                    │                                        │
│                                    ▼                                        │
│                                 Result:                                     │
│                                 • 2024-01-05: Token expiry bypass (fixed)  │
│                                 • 2024-02-12: Algorithm confusion (fixed)  │
│  Token cost: 0 (graph traversal only)                                      │
│                                                                             │
└─────────────────────────────────────────────────────────────────────────────┘
```

```go
// Cross-domain query implementations

// GetHistoryForCode returns history entries that modified a code node
func (vdb *VectorGraphDB) GetHistoryForCode(ctx context.Context, codeNodeID string) ([]*Node, error) {
    query := `
        SELECT n.*
        FROM nodes n
        JOIN edges e ON e.source_id = n.id
        WHERE e.target_id = ?
          AND e.edge_type IN (?, ?, ?)
          AND n.domain = ?
        ORDER BY n.timestamp DESC
    `
    return vdb.queryNodes(ctx, query,
        codeNodeID,
        EdgeTypeModified, EdgeTypeCreated, EdgeTypeDeleted,
        DomainHistory,
    )
}

// GetCodeForHistory returns code nodes referenced by a history entry
func (vdb *VectorGraphDB) GetCodeForHistory(ctx context.Context, historyNodeID string) ([]*Node, error) {
    query := `
        SELECT n.*
        FROM nodes n
        JOIN edges e ON e.target_id = n.id
        WHERE e.source_id = ?
          AND e.edge_type IN (?, ?, ?)
          AND n.domain = ?
    `
    return vdb.queryNodes(ctx, query,
        historyNodeID,
        EdgeTypeModified, EdgeTypeCreated, EdgeTypeDeleted,
        DomainCode,
    )
}

// GetAcademicForCode returns academic nodes that document code
func (vdb *VectorGraphDB) GetAcademicForCode(ctx context.Context, codeNodeID string) ([]*Node, error) {
    query := `
        SELECT n.*
        FROM nodes n
        JOIN edges e ON e.source_id = n.id
        WHERE e.target_id = ?
          AND e.edge_type IN (?, ?)
          AND n.domain = ?
    `
    return vdb.queryNodes(ctx, query,
        codeNodeID,
        EdgeTypeDocuments, EdgeTypeImplementsPattern,
        DomainAcademic,
    )
}

// GetAcademicForDecision returns academic nodes that a decision was based on
func (vdb *VectorGraphDB) GetAcademicForDecision(ctx context.Context, historyNodeID string) ([]*Node, error) {
    query := `
        SELECT n.*
        FROM nodes n
        JOIN edges e ON e.target_id = n.id
        WHERE e.source_id = ?
          AND e.edge_type IN (?, ?, ?)
          AND n.domain = ?
    `
    return vdb.queryNodes(ctx, query,
        historyNodeID,
        EdgeTypeBasedOn, EdgeTypeReferences, EdgeTypeValidatedBy,
        DomainAcademic,
    )
}

// SimilarAcrossDomainsWithContext performs vector search with graph context
func (vdb *VectorGraphDB) SimilarAcrossDomainsWithContext(
    ctx context.Context,
    query string,
    k int,
    pathFilter string,
) (*CrossDomainResult, error) {
    // 1. Vector search across all domains
    embedding, err := vdb.embedder.Embed(ctx, query)
    if err != nil {
        return nil, err
    }

    similar := vdb.hnsw.Search(embedding, k*3, nil)  // Get extra for filtering

    // 2. Filter by path if specified
    var filtered []ScoredNode
    for _, node := range similar {
        if pathFilter != "" {
            n, err := vdb.GetNode(ctx, node.NodeID)
            if err != nil {
                continue
            }
            if n.Path != "" && !strings.Contains(n.Path, pathFilter) {
                continue
            }
        }
        filtered = append(filtered, node)
        if len(filtered) >= k {
            break
        }
    }

    // 3. Enrich with cross-domain context
    result := &CrossDomainResult{
        Query:   query,
        Results: make([]EnrichedNode, len(filtered)),
    }

    for i, scored := range filtered {
        node, _ := vdb.GetNode(ctx, scored.NodeID)
        enriched := EnrichedNode{
            Node:       node,
            Similarity: scored.Similarity,
        }

        // Get cross-domain context based on domain
        switch node.Domain {
        case DomainCode:
            enriched.History, _ = vdb.GetHistoryForCode(ctx, node.ID)
            enriched.Academic, _ = vdb.GetAcademicForCode(ctx, node.ID)
        case DomainHistory:
            enriched.Code, _ = vdb.GetCodeForHistory(ctx, node.ID)
            enriched.Academic, _ = vdb.GetAcademicForDecision(ctx, node.ID)
        case DomainAcademic:
            // Get code that references this academic node
            enriched.Code, _ = vdb.queryNodes(ctx, `
                SELECT n.* FROM nodes n
                JOIN edges e ON e.source_id = n.id
                WHERE e.target_id = ? AND n.domain = ?
            `, node.ID, DomainCode)
        }

        result.Results[i] = enriched
    }

    return result, nil
}

// CrossDomainResult contains enriched search results
type CrossDomainResult struct {
    Query   string
    Results []EnrichedNode
}

// EnrichedNode is a node with cross-domain context
type EnrichedNode struct {
    Node       *Node
    Similarity float64
    Code       []*Node  // Related code nodes
    History    []*Node  // Related history nodes
    Academic   []*Node  // Related academic nodes
}
```

### Mitigation 1: Hallucination Firewall (Verify Before Store)

**Risk**: LLM hallucinates facts, they get stored, then retrieved as "evidence" creating a feedback loop.

**Mitigation**: Verify all LLM-generated content against source of truth before storing.

```go
// =============================================================================
// MITIGATION 1: HALLUCINATION FIREWALL
// =============================================================================

// HallucinationFirewall verifies content before storage
type HallucinationFirewall struct {
    vdb       *VectorGraphDB
    librarian *Librarian  // For code verification
}

// VerificationResult contains verification outcome
type VerificationResult struct {
    Verified       bool
    VerificationType VerificationType
    Confidence     float64
    Warnings       []string
    Contradictions []string
    SourceNodes    []string  // Nodes used for verification
}

// VerifyBeforeStore verifies an entry before storing
func (hf *HallucinationFirewall) VerifyBeforeStore(ctx context.Context, entry *HistoryEntry) (*VerificationResult, error) {
    result := &VerificationResult{
        Verified:   true,
        Confidence: 1.0,
    }

    // 1. Extract claims from content
    claims := hf.extractCodeClaims(entry.Content)

    // 2. Verify each claim against code
    for _, claim := range claims {
        verified, sources, err := hf.verifyClaimAgainstCode(ctx, claim)
        if err != nil {
            result.Warnings = append(result.Warnings,
                fmt.Sprintf("Could not verify claim: %s", claim.Text))
            continue
        }

        if !verified {
            result.Verified = false
            result.Contradictions = append(result.Contradictions,
                fmt.Sprintf("Claim '%s' contradicts code", claim.Text))
            result.Confidence *= 0.5
        } else {
            result.SourceNodes = append(result.SourceNodes, sources...)
        }
    }

    // 3. Verify referenced files exist
    for _, filePath := range entry.ReferencedFiles {
        exists, err := hf.librarian.FileExists(ctx, filePath)
        if err != nil || !exists {
            result.Warnings = append(result.Warnings,
                fmt.Sprintf("Referenced file not found: %s", filePath))
            result.Confidence *= 0.8
        }
    }

    // 4. Check for contradictions with recent history
    contradictions, err := hf.findHistoryContradictions(ctx, entry)
    if err == nil && len(contradictions) > 0 {
        result.Contradictions = append(result.Contradictions, contradictions...)
        result.Confidence *= 0.7
    }

    // 5. Set verification type
    if result.Verified && result.Confidence >= 0.8 {
        result.VerificationType = VerificationAgainstCode
    } else {
        result.VerificationType = VerificationNone
    }

    return result, nil
}

// Claim represents an extractable claim from content
type Claim struct {
    Text    string
    Subject string
    Type    string  // "uses", "implements", "has", "is"
}

// extractCodeClaims extracts verifiable claims from content
func (hf *HallucinationFirewall) extractCodeClaims(content string) []Claim {
    var claims []Claim

    // Pattern: "we use X for Y"
    usePatterns := regexp.MustCompile(`(?i)we use (\w+) for (\w+)`)
    matches := usePatterns.FindAllStringSubmatch(content, -1)
    for _, m := range matches {
        claims = append(claims, Claim{
            Text:    m[0],
            Subject: m[1],
            Type:    "uses",
        })
    }

    // Pattern: "X implements Y"
    implPatterns := regexp.MustCompile(`(?i)(\w+) implements (\w+)`)
    matches = implPatterns.FindAllStringSubmatch(content, -1)
    for _, m := range matches {
        claims = append(claims, Claim{
            Text:    m[0],
            Subject: m[1],
            Type:    "implements",
        })
    }

    // Pattern: "the X is in Y"
    locPatterns := regexp.MustCompile(`(?i)the (\w+) is in (\S+)`)
    matches = locPatterns.FindAllStringSubmatch(content, -1)
    for _, m := range matches {
        claims = append(claims, Claim{
            Text:    m[0],
            Subject: m[1],
            Type:    "location",
        })
    }

    return claims
}

// verifyClaimAgainstCode verifies a claim against the codebase
func (hf *HallucinationFirewall) verifyClaimAgainstCode(ctx context.Context, claim Claim) (bool, []string, error) {
    // Search code for evidence
    results, err := hf.vdb.SimilarNodes(ctx, claim.Subject, 10, &SearchFilter{
        Domain: ptr(DomainCode),
    })
    if err != nil {
        return false, nil, err
    }

    if len(results) == 0 {
        return false, nil, nil  // No evidence found
    }

    // Check if any result supports the claim
    var sources []string
    for _, result := range results {
        node, err := hf.vdb.GetNode(ctx, result.NodeID)
        if err != nil {
            continue
        }

        // Simple heuristic: if claim subject appears in code content
        if strings.Contains(strings.ToLower(node.Content), strings.ToLower(claim.Subject)) {
            sources = append(sources, node.ID)
        }
    }

    return len(sources) > 0, sources, nil
}

// findHistoryContradictions finds contradictions with recent history
func (hf *HallucinationFirewall) findHistoryContradictions(ctx context.Context, entry *HistoryEntry) ([]string, error) {
    // Get recent entries about same subject
    results, err := hf.vdb.SimilarNodes(ctx, entry.Summary, 20, &SearchFilter{
        Domain: ptr(DomainHistory),
    })
    if err != nil {
        return nil, err
    }

    var contradictions []string

    // Check for contradicting claims
    entryClaims := hf.extractCodeClaims(entry.Content)

    for _, result := range results {
        if result.Similarity < 0.7 {
            continue  // Not similar enough to be relevant
        }

        node, err := hf.vdb.GetNode(ctx, result.NodeID)
        if err != nil {
            continue
        }

        historyClaims := hf.extractCodeClaims(node.Content)

        for _, ec := range entryClaims {
            for _, hc := range historyClaims {
                if ec.Subject == hc.Subject && ec.Type == hc.Type {
                    // Same subject and type but different claim
                    if ec.Text != hc.Text {
                        contradictions = append(contradictions,
                            fmt.Sprintf("New: '%s' vs History: '%s'", ec.Text, hc.Text))
                    }
                }
            }
        }
    }

    return contradictions, nil
}

// StoreWithVerification stores an entry with verification
func (hf *HallucinationFirewall) StoreWithVerification(ctx context.Context, entry *HistoryEntry) error {
    // Verify first
    result, err := hf.VerifyBeforeStore(ctx, entry)
    if err != nil {
        return err
    }

    // Create node with verification metadata
    node := &Node{
        ID:               entry.ID,
        Domain:           DomainHistory,
        Type:             NodeTypeHistoryEntry,
        Name:             entry.Summary,
        Content:          entry.Content,
        SessionID:        entry.SessionID,
        Timestamp:        entry.Timestamp,
        Category:         entry.Category,
        Verified:         result.Verified,
        VerificationType: result.VerificationType,
        Confidence:       result.Confidence,
        TrustLevel:       int(TrustLevelRecent),
    }

    // Mark as unverified if failed
    if !result.Verified {
        node.TrustLevel = int(TrustLevelLLM)
        node.Content = "[UNVERIFIED] " + node.Content

        // Store warnings in metadata
        meta := map[string]any{
            "warnings":       result.Warnings,
            "contradictions": result.Contradictions,
            "needs_review":   true,
        }
        metaJSON, _ := json.Marshal(meta)
        node.Metadata = string(metaJSON)
    }

    // Store node
    if err := hf.vdb.AddNode(ctx, node); err != nil {
        return err
    }

    // Create provenance edges to source nodes
    for _, sourceID := range result.SourceNodes {
        hf.vdb.AddEdge(ctx, node.ID, sourceID, EdgeTypeReferences, 1.0, nil)
    }

    return nil
}

func ptr[T any](v T) *T { return &v }
```

### Mitigation 2: Freshness Tracking & Decay

**Risk**: Stale data gets retrieved and presented as current truth.

**Mitigation**: Track creation time, apply decay, prefer fresh data.

```go
// =============================================================================
// MITIGATION 2: FRESHNESS TRACKING & DECAY
// =============================================================================

// FreshnessTracker manages temporal relevance of nodes
type FreshnessTracker struct {
    vdb *VectorGraphDB
}

// FreshnessScore represents a node's temporal relevance
type FreshnessScore struct {
    Score           float64
    Age             time.Duration
    IsSuperseded    bool
    SupersededBy    string
    Warning         string
}

// CalculateFreshness calculates freshness score for a node
func (ft *FreshnessTracker) CalculateFreshness(ctx context.Context, node *Node) FreshnessScore {
    result := FreshnessScore{
        Score: 1.0,
    }

    // Check if superseded
    if node.SupersededBy != "" {
        result.IsSuperseded = true
        result.SupersededBy = node.SupersededBy
        result.Score = 0.1  // Heavily penalize superseded nodes
        result.Warning = "⚠️ SUPERSEDED - newer information available"
        return result
    }

    // Calculate age
    if !node.Timestamp.IsZero() {
        result.Age = time.Since(node.Timestamp)
    } else {
        result.Age = time.Since(node.CreatedAt)
    }

    // Apply domain-specific decay
    switch node.Domain {
    case DomainCode:
        // Code freshness based on content hash (checked separately)
        result.Score = 1.0  // Assume fresh if hash matches
        result.Warning = ""

    case DomainHistory:
        // History decays over time
        // 1 day = 0.95, 1 week = 0.8, 1 month = 0.6, 6 months = 0.3
        days := result.Age.Hours() / 24
        result.Score = math.Max(0.2, 1.0-days/180.0*0.8)

        if days > 30 {
            result.Warning = fmt.Sprintf("⚠️ %d days old - may be outdated", int(days))
        }

    case DomainAcademic:
        // Academic content decays slower
        days := result.Age.Hours() / 24
        result.Score = math.Max(0.3, 1.0-days/365.0*0.7)

        if days > 180 {
            result.Warning = fmt.Sprintf("⚠️ %d days old - verify still current", int(days))
        }
    }

    // Check expiration
    if node.ExpiresAt != nil && time.Now().After(*node.ExpiresAt) {
        result.Score *= 0.5
        result.Warning = "⚠️ EXPIRED - should be refreshed"
    }

    return result
}

// GetWithFreshness retrieves a node with freshness information
func (ft *FreshnessTracker) GetWithFreshness(ctx context.Context, nodeID string) (*NodeWithFreshness, error) {
    node, err := ft.vdb.GetNode(ctx, nodeID)
    if err != nil {
        return nil, err
    }

    freshness := ft.CalculateFreshness(ctx, node)

    // If superseded, optionally get the newer version
    var currentVersion *Node
    if freshness.IsSuperseded {
        currentVersion, _ = ft.vdb.GetNode(ctx, freshness.SupersededBy)
    }

    return &NodeWithFreshness{
        Node:           node,
        Freshness:      freshness,
        CurrentVersion: currentVersion,
    }, nil
}

// NodeWithFreshness wraps a node with freshness metadata
type NodeWithFreshness struct {
    Node           *Node
    Freshness      FreshnessScore
    CurrentVersion *Node  // If superseded, this is the newer version
}

// SupersedeNode marks a node as superseded by another
func (ft *FreshnessTracker) SupersedeNode(ctx context.Context, oldNodeID, newNodeID string) error {
    _, err := ft.vdb.db.ExecContext(ctx, `
        UPDATE nodes SET superseded_by = ?, updated_at = CURRENT_TIMESTAMP
        WHERE id = ?
    `, newNodeID, oldNodeID)
    if err != nil {
        return err
    }

    // Add supersedes edge
    return ft.vdb.AddEdge(ctx, newNodeID, oldNodeID, EdgeTypeSupersedes, 1.0, nil)
}

// RefreshRanking applies freshness to search results
func (ft *FreshnessTracker) RefreshRanking(ctx context.Context, results []ScoredNode) []ScoredNodeWithFreshness {
    ranked := make([]ScoredNodeWithFreshness, len(results))

    for i, result := range results {
        node, err := ft.vdb.GetNode(ctx, result.NodeID)
        if err != nil {
            ranked[i] = ScoredNodeWithFreshness{
                ScoredNode:   result,
                Freshness:    FreshnessScore{Score: 0.5},
                CombinedScore: result.Similarity * 0.5,
            }
            continue
        }

        freshness := ft.CalculateFreshness(ctx, node)

        // Combined score: similarity * freshness
        combined := result.Similarity * freshness.Score

        ranked[i] = ScoredNodeWithFreshness{
            ScoredNode:    result,
            Freshness:     freshness,
            CombinedScore: combined,
        }
    }

    // Sort by combined score
    sort.Slice(ranked, func(i, j int) bool {
        return ranked[i].CombinedScore > ranked[j].CombinedScore
    })

    return ranked
}

// ScoredNodeWithFreshness extends ScoredNode with freshness
type ScoredNodeWithFreshness struct {
    ScoredNode
    Freshness     FreshnessScore
    CombinedScore float64
}

// CleanupExpired removes or archives expired nodes
func (ft *FreshnessTracker) CleanupExpired(ctx context.Context) (int, error) {
    result, err := ft.vdb.db.ExecContext(ctx, `
        DELETE FROM nodes
        WHERE expires_at IS NOT NULL
          AND expires_at < datetime('now', '-30 days')
          AND domain = ?
    `, DomainHistory)  // Only auto-cleanup history, not code or academic

    if err != nil {
        return 0, err
    }

    affected, _ := result.RowsAffected()
    return int(affected), nil
}
```

### Mitigation 3: Source Attribution & Provenance

**Risk**: User can't tell where information came from (code? history? academic? LLM inference?).

**Mitigation**: Track and expose provenance for all information.

```go
// =============================================================================
// MITIGATION 3: SOURCE ATTRIBUTION & PROVENANCE
// =============================================================================

// ProvenanceTracker tracks and exposes information sources
type ProvenanceTracker struct {
    vdb *VectorGraphDB
}

// ProvenanceChain represents the full source chain for information
type ProvenanceChain struct {
    Sources []ProvenanceSource
}

// ProvenanceSource represents a single source in the chain
type ProvenanceSource struct {
    Type       SourceType
    NodeID     string
    Path       string    // File path (if code)
    URL        string    // URL (if academic)
    Timestamp  time.Time
    Confidence float64
    Verified   bool
}

// AddProvenance records provenance for a node
func (pt *ProvenanceTracker) AddProvenance(ctx context.Context, nodeID string, source ProvenanceSource) error {
    _, err := pt.vdb.db.ExecContext(ctx, `
        INSERT INTO provenance (node_id, source_type, source_node_id, source_path, source_url, confidence, verified_at)
        VALUES (?, ?, ?, ?, ?, ?, ?)
    `, nodeID, source.Type, source.NodeID, source.Path, source.URL, source.Confidence,
        func() any { if source.Verified { return time.Now() } else { return nil } }())

    return err
}

// GetProvenance retrieves provenance chain for a node
func (pt *ProvenanceTracker) GetProvenance(ctx context.Context, nodeID string) (*ProvenanceChain, error) {
    rows, err := pt.vdb.db.QueryContext(ctx, `
        SELECT source_type, source_node_id, source_path, source_url, confidence, verified_at
        FROM provenance
        WHERE node_id = ?
        ORDER BY confidence DESC
    `, nodeID)
    if err != nil {
        return nil, err
    }
    defer rows.Close()

    chain := &ProvenanceChain{}

    for rows.Next() {
        var source ProvenanceSource
        var verifiedAt sql.NullTime

        err := rows.Scan(&source.Type, &source.NodeID, &source.Path, &source.URL, &source.Confidence, &verifiedAt)
        if err != nil {
            continue
        }

        source.Verified = verifiedAt.Valid
        if verifiedAt.Valid {
            source.Timestamp = verifiedAt.Time
        }

        chain.Sources = append(chain.Sources, source)
    }

    return chain, nil
}

// AnnotatedResponse is a response with source annotations
type AnnotatedResponse struct {
    Text     string
    Segments []AnnotatedSegment
}

// AnnotatedSegment is a text segment with provenance
type AnnotatedSegment struct {
    Text       string
    Start      int
    End        int
    Provenance ProvenanceChain
    SourceRef  string  // [1], [2], etc.
}

// AnnotateResponse adds source citations to a response
func (pt *ProvenanceTracker) AnnotateResponse(ctx context.Context, response string, usedNodes []string) (*AnnotatedResponse, error) {
    annotated := &AnnotatedResponse{
        Text: response,
    }

    // Build source references
    sourceRefs := make(map[string]string)  // nodeID -> [n]
    var sourceList []string

    for i, nodeID := range usedNodes {
        ref := fmt.Sprintf("[%d]", i+1)
        sourceRefs[nodeID] = ref
        sourceList = append(sourceList, nodeID)
    }

    // Create segments for each source mention
    for nodeID, ref := range sourceRefs {
        provenance, err := pt.GetProvenance(ctx, nodeID)
        if err != nil {
            continue
        }

        // Find where this source's content appears in response
        node, err := pt.vdb.GetNode(ctx, nodeID)
        if err != nil {
            continue
        }

        // Simple heuristic: look for key terms from the node
        keyTerms := extractKeyTerms(node.Content)
        for _, term := range keyTerms {
            idx := strings.Index(strings.ToLower(response), strings.ToLower(term))
            if idx >= 0 {
                annotated.Segments = append(annotated.Segments, AnnotatedSegment{
                    Text:       term,
                    Start:      idx,
                    End:        idx + len(term),
                    Provenance: *provenance,
                    SourceRef:  ref,
                })
                break  // One segment per source
            }
        }
    }

    return annotated, nil
}

// FormatSourceList formats sources for display
func (pt *ProvenanceTracker) FormatSourceList(ctx context.Context, nodeIDs []string) (string, error) {
    var lines []string

    for i, nodeID := range nodeIDs {
        node, err := pt.vdb.GetNode(ctx, nodeID)
        if err != nil {
            continue
        }

        provenance, _ := pt.GetProvenance(ctx, nodeID)

        var sourceDesc string
        var warning string

        switch node.Domain {
        case DomainCode:
            sourceDesc = fmt.Sprintf("Code: %s", node.Path)
            if node.LineStart > 0 {
                sourceDesc += fmt.Sprintf(":%d-%d", node.LineStart, node.LineEnd)
            }
            warning = "(verified, current)"

        case DomainHistory:
            age := time.Since(node.Timestamp)
            sourceDesc = fmt.Sprintf("History: %s", node.Timestamp.Format("2006-01-02"))
            if age.Hours() > 24*30 {
                warning = fmt.Sprintf("(%d days old)", int(age.Hours()/24))
            } else {
                warning = "(recent)"
            }

        case DomainAcademic:
            sourceDesc = fmt.Sprintf("Academic: %s", node.Name)
            if node.URL != "" {
                sourceDesc += fmt.Sprintf(" (%s)", node.URL)
            }
            switch node.Type {
            case NodeTypeRFC:
                warning = "(authoritative)"
            case NodeTypeDocumentation:
                warning = "(official docs)"
            default:
                warning = "(external)"
            }
        }

        if !node.Verified {
            warning = "⚠️ UNVERIFIED"
        }

        // Add provenance details
        if provenance != nil && len(provenance.Sources) > 0 {
            sourceDesc += " via " + sourceTypeName(provenance.Sources[0].Type)
        }

        lines = append(lines, fmt.Sprintf("[%d] %s %s", i+1, sourceDesc, warning))
    }

    return strings.Join(lines, "\n"), nil
}

func sourceTypeName(t SourceType) string {
    switch t {
    case SourceTypeCode:
        return "code analysis"
    case SourceTypeHistory:
        return "history lookup"
    case SourceTypeAcademic:
        return "external reference"
    case SourceTypeLLMInference:
        return "inference"
    case SourceTypeUserProvided:
        return "user input"
    default:
        return "unknown"
    }
}

func extractKeyTerms(content string) []string {
    // Simple term extraction (in practice, use NLP)
    words := strings.Fields(content)
    var terms []string
    for _, word := range words {
        if len(word) > 5 {  // Only meaningful words
            terms = append(terms, word)
        }
        if len(terms) >= 5 {
            break
        }
    }
    return terms
}
```

### Mitigation 4: Trust Hierarchy

**Risk**: LLM doesn't know which sources to trust when they conflict.

**Mitigation**: Explicit trust hierarchy with code as ground truth.

```go
// =============================================================================
// MITIGATION 4: TRUST HIERARCHY
// =============================================================================

// TrustHierarchy manages source trustworthiness
type TrustHierarchy struct {
    vdb *VectorGraphDB
}

// GetTrustLevel returns the trust level for a node
func (th *TrustHierarchy) GetTrustLevel(ctx context.Context, node *Node) TrustLevel {
    // Code is always ground truth
    if node.Domain == DomainCode {
        return TrustLevelGround
    }

    // Check if verified
    if !node.Verified {
        return TrustLevelLLM
    }

    // History trust depends on age and verification
    if node.Domain == DomainHistory {
        age := time.Since(node.Timestamp)
        if age < 7*24*time.Hour {
            return TrustLevelRecent
        } else if age < 30*24*time.Hour {
            return TrustLevel(60)  // Between recent and academic
        } else {
            return TrustLevelOldHistory
        }
    }

    // Academic trust depends on type
    if node.Domain == DomainAcademic {
        switch node.Type {
        case NodeTypeRFC:
            return TrustLevelStandard
        case NodeTypeDocumentation:
            return TrustLevelStandard
        case NodeTypePaper:
            return TrustLevelAcademic
        case NodeTypeBestPractice:
            return TrustLevelAcademic
        case NodeTypeStackOverflow, NodeTypeBlogPost:
            return TrustLevelBlog
        default:
            return TrustLevelAcademic
        }
    }

    return TrustLevel(node.TrustLevel)
}

// TrustLevelName returns a human-readable trust level name
func TrustLevelName(level TrustLevel) string {
    switch {
    case level >= TrustLevelGround:
        return "Ground Truth (Code)"
    case level >= TrustLevelRecent:
        return "Recent Verified"
    case level >= TrustLevelStandard:
        return "Authoritative"
    case level >= TrustLevelAcademic:
        return "Academic"
    case level >= TrustLevelOldHistory:
        return "Old History"
    case level >= TrustLevelBlog:
        return "Informal"
    default:
        return "Unverified/Inferred"
    }
}

// TrustWarning returns a warning message for low-trust content
func (th *TrustHierarchy) TrustWarning(level TrustLevel) string {
    switch {
    case level >= TrustLevelGround:
        return ""
    case level >= TrustLevelRecent:
        return "From recent history, may need verification"
    case level >= TrustLevelStandard:
        return "From external docs, verify applies to your context"
    case level >= TrustLevelAcademic:
        return "Academic source, may not apply directly"
    case level >= TrustLevelOldHistory:
        return "⚠️ OLD DATA - verify this is still accurate"
    case level >= TrustLevelBlog:
        return "⚠️ Informal source - verify independently"
    default:
        return "⚠️ UNVERIFIED - LLM inference, may be incorrect"
    }
}

// RankByTrust sorts nodes by trust level
func (th *TrustHierarchy) RankByTrust(ctx context.Context, nodes []*Node) []*NodeWithTrust {
    ranked := make([]*NodeWithTrust, len(nodes))

    for i, node := range nodes {
        trust := th.GetTrustLevel(ctx, node)
        ranked[i] = &NodeWithTrust{
            Node:    node,
            Trust:   trust,
            Warning: th.TrustWarning(trust),
        }
    }

    // Sort by trust level (highest first)
    sort.Slice(ranked, func(i, j int) bool {
        return ranked[i].Trust > ranked[j].Trust
    })

    return ranked
}

// NodeWithTrust wraps a node with trust information
type NodeWithTrust struct {
    Node    *Node
    Trust   TrustLevel
    Warning string
}

// ResolveConflict resolves conflicts between nodes using trust hierarchy
func (th *TrustHierarchy) ResolveConflict(ctx context.Context, nodeA, nodeB *Node) *ConflictResolution {
    trustA := th.GetTrustLevel(ctx, nodeA)
    trustB := th.GetTrustLevel(ctx, nodeB)

    resolution := &ConflictResolution{
        NodeA:      nodeA,
        NodeB:      nodeB,
        TrustA:     trustA,
        TrustB:     trustB,
    }

    // Code always wins
    if nodeA.Domain == DomainCode && nodeB.Domain != DomainCode {
        resolution.Winner = nodeA
        resolution.Reason = "Code is source of truth"
        return resolution
    }
    if nodeB.Domain == DomainCode && nodeA.Domain != DomainCode {
        resolution.Winner = nodeB
        resolution.Reason = "Code is source of truth"
        return resolution
    }

    // Higher trust wins
    if trustA > trustB {
        resolution.Winner = nodeA
        resolution.Reason = fmt.Sprintf("%s > %s", TrustLevelName(trustA), TrustLevelName(trustB))
    } else if trustB > trustA {
        resolution.Winner = nodeB
        resolution.Reason = fmt.Sprintf("%s > %s", TrustLevelName(trustB), TrustLevelName(trustA))
    } else {
        // Same trust - prefer newer
        if nodeA.Timestamp.After(nodeB.Timestamp) {
            resolution.Winner = nodeA
            resolution.Reason = "Same trust level, preferring newer"
        } else {
            resolution.Winner = nodeB
            resolution.Reason = "Same trust level, preferring newer"
        }
    }

    return resolution
}

// ConflictResolution contains the resolution of a conflict
type ConflictResolution struct {
    NodeA   *Node
    NodeB   *Node
    TrustA  TrustLevel
    TrustB  TrustLevel
    Winner  *Node
    Reason  string
}
```

### Mitigation 5: Conflict Detection

**Risk**: Contradictory information retrieved without the user knowing.

**Mitigation**: Detect and surface conflicts before presenting to LLM/user.

```go
// =============================================================================
// MITIGATION 5: CONFLICT DETECTION
// =============================================================================

// ConflictDetector detects contradictions in retrieved context
type ConflictDetector struct {
    vdb   *VectorGraphDB
    trust *TrustHierarchy
}

// Conflict represents a detected contradiction
type Conflict struct {
    Type        ConflictType
    Subject     string
    NodeA       *Node
    NodeB       *Node
    Description string
    Resolution  string
    Resolved    bool
}

// DetectConflicts finds conflicts in a set of retrieved nodes
func (cd *ConflictDetector) DetectConflicts(ctx context.Context, nodes []*Node) ([]Conflict, error) {
    var conflicts []Conflict

    // 1. Temporal conflicts (old vs new on same subject)
    temporal := cd.detectTemporalConflicts(nodes)
    conflicts = append(conflicts, temporal...)

    // 2. Source conflicts (code says X, history says Y)
    source := cd.detectSourceConflicts(ctx, nodes)
    conflicts = append(conflicts, source...)

    // 3. Semantic contradictions (claim A contradicts claim B)
    semantic := cd.detectSemanticConflicts(nodes)
    conflicts = append(conflicts, semantic...)

    // Store detected conflicts
    for _, c := range conflicts {
        cd.storeConflict(ctx, c)
    }

    return conflicts, nil
}

// detectTemporalConflicts finds conflicts due to time differences
func (cd *ConflictDetector) detectTemporalConflicts(nodes []*Node) []Conflict {
    var conflicts []Conflict

    // Group by subject
    bySubject := make(map[string][]*Node)
    for _, node := range nodes {
        subject := extractSubject(node)
        bySubject[subject] = append(bySubject[subject], node)
    }

    for subject, group := range bySubject {
        if len(group) < 2 {
            continue
        }

        // Sort by time
        sort.Slice(group, func(i, j int) bool {
            return group[i].Timestamp.Before(group[j].Timestamp)
        })

        oldest := group[0]
        newest := group[len(group)-1]

        // If >30 days apart, flag as potential conflict
        if newest.Timestamp.Sub(oldest.Timestamp) > 30*24*time.Hour {
            conflicts = append(conflicts, Conflict{
                Type:        ConflictTypeTemporal,
                Subject:     subject,
                NodeA:       oldest,
                NodeB:       newest,
                Description: fmt.Sprintf("Information about '%s' spans %d days - older data may be stale",
                    subject, int(newest.Timestamp.Sub(oldest.Timestamp).Hours()/24)),
                Resolution:  "Prefer newer information unless older is from code",
            })
        }
    }

    return conflicts
}

// detectSourceConflicts finds conflicts between domains
func (cd *ConflictDetector) detectSourceConflicts(ctx context.Context, nodes []*Node) []Conflict {
    var conflicts []Conflict

    // Separate by domain
    var codeNodes, historyNodes []*Node
    for _, node := range nodes {
        switch node.Domain {
        case DomainCode:
            codeNodes = append(codeNodes, node)
        case DomainHistory:
            historyNodes = append(historyNodes, node)
        }
    }

    // Check if history claims contradict code
    for _, histNode := range historyNodes {
        claims := extractClaimsSimple(histNode.Content)

        for _, claim := range claims {
            for _, codeNode := range codeNodes {
                if contradictsClaim(claim, codeNode) {
                    conflicts = append(conflicts, Conflict{
                        Type:        ConflictTypeSourceMismatch,
                        Subject:     claim,
                        NodeA:       histNode,
                        NodeB:       codeNode,
                        Description: fmt.Sprintf("History claims '%s' but code shows otherwise", claim),
                        Resolution:  "Code is source of truth - history may be outdated",
                    })
                }
            }
        }
    }

    return conflicts
}

// detectSemanticConflicts finds contradictory statements
func (cd *ConflictDetector) detectSemanticConflicts(nodes []*Node) []Conflict {
    var conflicts []Conflict

    // Extract claims from all nodes
    type claimWithNode struct {
        claim string
        node  *Node
    }
    var allClaims []claimWithNode

    for _, node := range nodes {
        claims := extractClaimsSimple(node.Content)
        for _, c := range claims {
            allClaims = append(allClaims, claimWithNode{c, node})
        }
    }

    // Check for contradictions
    for i, a := range allClaims {
        for j, b := range allClaims {
            if i >= j {
                continue
            }

            if claimsContradict(a.claim, b.claim) {
                conflicts = append(conflicts, Conflict{
                    Type:        ConflictTypeSemantic,
                    Subject:     extractSubjectFromClaim(a.claim),
                    NodeA:       a.node,
                    NodeB:       b.node,
                    Description: fmt.Sprintf("'%s' contradicts '%s'", a.claim, b.claim),
                    Resolution:  "Requires human review to determine which is correct",
                })
            }
        }
    }

    return conflicts
}

// storeConflict persists a detected conflict
func (cd *ConflictDetector) storeConflict(ctx context.Context, conflict Conflict) error {
    _, err := cd.vdb.db.ExecContext(ctx, `
        INSERT INTO conflicts (conflict_type, subject, node_id_a, node_id_b, description, resolution)
        VALUES (?, ?, ?, ?, ?, ?)
    `, conflict.Type, conflict.Subject, conflict.NodeA.ID, conflict.NodeB.ID,
        conflict.Description, conflict.Resolution)
    return err
}

// GetUnresolvedConflicts returns conflicts needing attention
func (cd *ConflictDetector) GetUnresolvedConflicts(ctx context.Context) ([]Conflict, error) {
    rows, err := cd.vdb.db.QueryContext(ctx, `
        SELECT c.conflict_type, c.subject, c.description, c.resolution,
               c.node_id_a, c.node_id_b
        FROM conflicts c
        WHERE c.resolved = FALSE
        ORDER BY c.detected_at DESC
    `)
    if err != nil {
        return nil, err
    }
    defer rows.Close()

    var conflicts []Conflict
    for rows.Next() {
        var c Conflict
        var nodeAID, nodeBID string

        err := rows.Scan(&c.Type, &c.Subject, &c.Description, &c.Resolution, &nodeAID, &nodeBID)
        if err != nil {
            continue
        }

        c.NodeA, _ = cd.vdb.GetNode(ctx, nodeAID)
        c.NodeB, _ = cd.vdb.GetNode(ctx, nodeBID)
        conflicts = append(conflicts, c)
    }

    return conflicts, nil
}

// ResolveConflict marks a conflict as resolved
func (cd *ConflictDetector) ResolveConflict(ctx context.Context, conflictID int, resolution string) error {
    _, err := cd.vdb.db.ExecContext(ctx, `
        UPDATE conflicts
        SET resolved = TRUE, resolution = ?, resolved_at = CURRENT_TIMESTAMP
        WHERE id = ?
    `, resolution, conflictID)
    return err
}

// Helper functions
func extractSubject(node *Node) string {
    // Extract primary subject from node name or content
    if node.Name != "" {
        return strings.ToLower(node.Name)
    }
    words := strings.Fields(node.Content)
    if len(words) > 0 {
        return strings.ToLower(words[0])
    }
    return ""
}

func extractClaimsSimple(content string) []string {
    // Simple claim extraction
    var claims []string
    sentences := strings.Split(content, ".")
    for _, s := range sentences {
        s = strings.TrimSpace(s)
        if len(s) > 10 && len(s) < 200 {
            claims = append(claims, s)
        }
    }
    return claims
}

func contradictsClaim(claim string, codeNode *Node) bool {
    // Simple contradiction check: claim mentions something not in code
    claimLower := strings.ToLower(claim)
    codeLower := strings.ToLower(codeNode.Content)

    // Check for "uses X" claims
    if strings.Contains(claimLower, "uses ") {
        words := strings.Fields(claimLower)
        for i, w := range words {
            if w == "uses" && i+1 < len(words) {
                tech := words[i+1]
                if !strings.Contains(codeLower, tech) {
                    return true
                }
            }
        }
    }

    return false
}

func claimsContradict(a, b string) bool {
    // Simple contradiction detection
    aLower := strings.ToLower(a)
    bLower := strings.ToLower(b)

    // "uses X" vs "uses Y" for same purpose
    if strings.Contains(aLower, "uses ") && strings.Contains(bLower, "uses ") {
        // Extract what's being used
        aWords := strings.Fields(aLower)
        bWords := strings.Fields(bLower)

        for i, w := range aWords {
            if w == "uses" && i+1 < len(aWords) {
                aTech := aWords[i+1]
                for j, v := range bWords {
                    if v == "uses" && j+1 < len(bWords) {
                        bTech := bWords[j+1]
                        // Different tech for same context might contradict
                        if aTech != bTech && haveSameContext(a, b) {
                            return true
                        }
                    }
                }
            }
        }
    }

    return false
}

func haveSameContext(a, b string) bool {
    // Check if claims are about the same thing
    aWords := strings.Fields(strings.ToLower(a))
    bWords := strings.Fields(strings.ToLower(b))

    commonWords := 0
    for _, aw := range aWords {
        for _, bw := range bWords {
            if aw == bw && len(aw) > 3 {
                commonWords++
            }
        }
    }

    return commonWords >= 2
}

func extractSubjectFromClaim(claim string) string {
    words := strings.Fields(strings.ToLower(claim))
    if len(words) > 2 {
        return words[1]  // Usually the subject is the second word
    }
    return claim
}
```

### Mitigation 6: Context Quality Scoring

**Risk**: Too much low-quality context dilutes LLM attention.

**Mitigation**: Score and filter context to maximize quality-to-tokens ratio.

```go
// =============================================================================
// MITIGATION 6: CONTEXT QUALITY SCORING
// =============================================================================

// ContextQualityScorer scores and selects optimal context
type ContextQualityScorer struct {
    vdb        *VectorGraphDB
    trust      *TrustHierarchy
    freshness  *FreshnessTracker
    maxTokens  int
}

// QualityScorerConfig configures the scorer
type QualityScorerConfig struct {
    MaxTokens         int
    MinQualityScore   float64
    SimilarityWeight  float64
    TrustWeight       float64
    FreshnessWeight   float64
    VerificationBonus float64
}

// DefaultQualityScorerConfig returns sensible defaults
func DefaultQualityScorerConfig() QualityScorerConfig {
    return QualityScorerConfig{
        MaxTokens:         8000,
        MinQualityScore:   0.3,
        SimilarityWeight:  0.35,
        TrustWeight:       0.25,
        FreshnessWeight:   0.25,
        VerificationBonus: 0.15,
    }
}

// ScoredContext represents scored context items
type ScoredContext struct {
    Node          *Node
    Similarity    float64
    TrustLevel    TrustLevel
    Freshness     FreshnessScore
    QualityScore  float64
    EstTokens     int
    Warning       string
    Included      bool
}

// SelectBestContext selects optimal context within token budget
func (cqs *ContextQualityScorer) SelectBestContext(
    ctx context.Context,
    query string,
    retrieved []ScoredNode,
    config QualityScorerConfig,
) ([]ScoredContext, error) {

    // Score all candidates
    scored := make([]ScoredContext, len(retrieved))

    for i, r := range retrieved {
        node, err := cqs.vdb.GetNode(ctx, r.NodeID)
        if err != nil {
            continue
        }

        trust := cqs.trust.GetTrustLevel(ctx, node)
        fresh := cqs.freshness.CalculateFreshness(ctx, node)

        // Calculate quality score
        quality := cqs.calculateQualityScore(r.Similarity, trust, fresh, node.Verified, config)

        scored[i] = ScoredContext{
            Node:         node,
            Similarity:   r.Similarity,
            TrustLevel:   trust,
            Freshness:    fresh,
            QualityScore: quality,
            EstTokens:    estimateTokens(node.Content),
            Warning:      cqs.trust.TrustWarning(trust),
        }
    }

    // Sort by quality score
    sort.Slice(scored, func(i, j int) bool {
        return scored[i].QualityScore > scored[j].QualityScore
    })

    // Select until token budget exhausted
    tokenCount := 0
    for i := range scored {
        // Skip low-quality items
        if scored[i].QualityScore < config.MinQualityScore {
            continue
        }

        if tokenCount+scored[i].EstTokens > config.MaxTokens {
            break
        }

        scored[i].Included = true
        tokenCount += scored[i].EstTokens
    }

    return scored, nil
}

// calculateQualityScore computes overall quality
func (cqs *ContextQualityScorer) calculateQualityScore(
    similarity float64,
    trust TrustLevel,
    fresh FreshnessScore,
    verified bool,
    config QualityScorerConfig,
) float64 {

    // Normalize trust to 0-1
    normalizedTrust := float64(trust) / 100.0

    score := similarity*config.SimilarityWeight +
        normalizedTrust*config.TrustWeight +
        fresh.Score*config.FreshnessWeight

    if verified {
        score += config.VerificationBonus
    }

    // Cap at 1.0
    if score > 1.0 {
        score = 1.0
    }

    return score
}

// estimateTokens estimates token count for content
func estimateTokens(content string) int {
    // Rough estimate: ~4 characters per token
    return len(content) / 4
}

// BuildContextWindow builds the final context string
func (cqs *ContextQualityScorer) BuildContextWindow(scored []ScoredContext) string {
    var builder strings.Builder

    included := 0
    for _, s := range scored {
        if !s.Included {
            continue
        }

        included++
        builder.WriteString(fmt.Sprintf("\n[%d] Source: %s (Trust: %s, Quality: %.2f)\n",
            included,
            sourceDescription(s.Node),
            TrustLevelName(s.TrustLevel),
            s.QualityScore))

        if s.Warning != "" {
            builder.WriteString(fmt.Sprintf("    %s\n", s.Warning))
        }

        builder.WriteString(fmt.Sprintf("    Content: %s\n", truncate(s.Node.Content, 500)))
    }

    return builder.String()
}

func sourceDescription(node *Node) string {
    switch node.Domain {
    case DomainCode:
        return fmt.Sprintf("Code: %s", node.Path)
    case DomainHistory:
        return fmt.Sprintf("History: %s (%s)", node.Name, node.Timestamp.Format("2006-01-02"))
    case DomainAcademic:
        return fmt.Sprintf("Academic: %s", node.Name)
    default:
        return node.Name
    }
}

func truncate(s string, maxLen int) string {
    if len(s) <= maxLen {
        return s
    }
    return s[:maxLen] + "..."
}
```

### Mitigation 7: LLM Prompt Engineering

**Risk**: LLM doesn't understand trust hierarchy or how to handle conflicts.

**Mitigation**: Explicit instructions in system prompt about trust and conflict handling.

```go
// =============================================================================
// MITIGATION 7: LLM PROMPT ENGINEERING
// =============================================================================

// LLMContextBuilder builds prompts with trust and conflict awareness
type LLMContextBuilder struct {
    provenance *ProvenanceTracker
    conflicts  *ConflictDetector
    scorer     *ContextQualityScorer
}

// SystemPromptWithGraphContext is the system prompt for graph-aware LLM
const SystemPromptWithGraphContext = `
You are an assistant with access to a knowledge graph containing:
1. CODE: Current source code (MOST TRUSTWORTHY - this is ground truth)
2. HISTORY: Past decisions and changes (may be outdated)
3. ACADEMIC: External docs and papers (may not apply to this project)

CRITICAL RULES:
1. When code and history conflict, CODE IS CORRECT (history may be stale)
2. Always cite your sources using [source_id] notation
3. If you're uncertain, SAY SO - don't guess
4. If retrieved context seems contradictory, SURFACE THE CONFLICT
5. Distinguish between "the code does X" (fact) vs "you should do X" (advice)
6. For advice, prefer ACADEMIC sources over your training
7. NEVER claim something is in the code unless it's in the CODE context
8. Mark any inference not directly from sources as [INFERENCE]

TRUST HIERARCHY (highest to lowest):
1. Current code (ground truth) - Trust Level: 100
2. Recent verified history (<7 days) - Trust Level: 80
3. Official documentation / RFCs - Trust Level: 70
4. Academic papers / best practices - Trust Level: 60
5. Older history (>30 days) - Trust Level: 40
6. Blog posts / SO answers - Trust Level: 30
7. Your inference (unverified) - Trust Level: 20

When you see ⚠️ warnings on sources, mention them to the user.
When sources conflict, explain the conflict and recommend which to trust.
`

// BuildPrompt builds a complete prompt with context and conflicts
func (lcb *LLMContextBuilder) BuildPrompt(
    ctx context.Context,
    query string,
    scored []ScoredContext,
    conflicts []Conflict,
) (string, error) {

    var prompt strings.Builder

    prompt.WriteString(SystemPromptWithGraphContext)
    prompt.WriteString("\n\n---\n\n")

    // Add conflicts first if any
    if len(conflicts) > 0 {
        prompt.WriteString("⚠️ CONFLICTS DETECTED:\n")
        for _, c := range conflicts {
            prompt.WriteString(fmt.Sprintf("- %s: %s\n", conflictTypeName(c.Type), c.Description))
            if c.Resolution != "" {
                prompt.WriteString(fmt.Sprintf("  Suggested resolution: %s\n", c.Resolution))
            }
        }
        prompt.WriteString("\n---\n\n")
    }

    // Add context with trust annotations
    prompt.WriteString("RETRIEVED CONTEXT:\n\n")

    sourceNum := 0
    for _, item := range scored {
        if !item.Included {
            continue
        }

        sourceNum++
        prompt.WriteString(fmt.Sprintf("[%d] %s (Trust: %s, Score: %.2f)\n",
            sourceNum,
            sourceDescription(item.Node),
            TrustLevelName(item.TrustLevel),
            item.QualityScore))

        if item.Warning != "" {
            prompt.WriteString(fmt.Sprintf("    %s\n", item.Warning))
        }

        prompt.WriteString(fmt.Sprintf("    Content: %s\n\n", truncate(item.Node.Content, 1000)))
    }

    prompt.WriteString("---\n\n")
    prompt.WriteString(fmt.Sprintf("USER QUERY: %s\n\n", query))
    prompt.WriteString("Respond with source citations [n]. Surface any conflicts or uncertainties.")

    return prompt.String(), nil
}

// AnnotatedLLMResponse represents a response with source tracking
type AnnotatedLLMResponse struct {
    Response    string
    SourcesUsed []string  // Node IDs
    Inferences  []string  // Claims not from sources
    Conflicts   []Conflict
    Warnings    []string
}

// ParseLLMResponse extracts annotations from LLM response
func (lcb *LLMContextBuilder) ParseLLMResponse(response string, scoredContext []ScoredContext) *AnnotatedLLMResponse {
    result := &AnnotatedLLMResponse{
        Response: response,
    }

    // Extract source references [n]
    refPattern := regexp.MustCompile(`\[(\d+)\]`)
    matches := refPattern.FindAllStringSubmatch(response, -1)

    usedIndices := make(map[int]bool)
    for _, m := range matches {
        idx, _ := strconv.Atoi(m[1])
        usedIndices[idx] = true
    }

    // Map to node IDs
    sourceNum := 0
    for _, item := range scoredContext {
        if item.Included {
            sourceNum++
            if usedIndices[sourceNum] {
                result.SourcesUsed = append(result.SourcesUsed, item.Node.ID)
            }
        }
    }

    // Detect inferences
    inferencePattern := regexp.MustCompile(`\[INFERENCE\]([^[]+)`)
    inferenceMatches := inferencePattern.FindAllStringSubmatch(response, -1)
    for _, m := range inferenceMatches {
        result.Inferences = append(result.Inferences, strings.TrimSpace(m[1]))
    }

    // Check for warnings in response
    if strings.Contains(response, "⚠️") {
        lines := strings.Split(response, "\n")
        for _, line := range lines {
            if strings.Contains(line, "⚠️") {
                result.Warnings = append(result.Warnings, line)
            }
        }
    }

    return result
}

func conflictTypeName(t ConflictType) string {
    switch t {
    case ConflictTypeTemporal:
        return "Temporal"
    case ConflictTypeSourceMismatch:
        return "Source Mismatch"
    case ConflictTypeSemantic:
        return "Semantic Contradiction"
    default:
        return "Unknown"
    }
}
```

### Token Savings Analysis

```
┌─────────────────────────────────────────────────────────────────────────────┐
│                    TOKEN SAVINGS: ALL THREE DOMAINS                          │
├─────────────────────────────────────────────────────────────────────────────┤
│                                                                             │
│  BASELINE (No caching, no graph - all LLM):                                 │
│  ══════════════════════════════════════════                                 │
│  Librarian:   100 queries × 3000 tokens = 300,000 tokens                   │
│  Archivalist: 100 queries × 2500 tokens = 250,000 tokens                   │
│  Academic:    100 queries × 4000 tokens = 400,000 tokens                   │
│  TOTAL: 950,000 tokens                                                      │
│                                                                             │
│  ──────────────────────────────────────────────────────────────────────────│
│                                                                             │
│  WITH INTENT CACHE ONLY:                                                    │
│  ═══════════════════════                                                    │
│  Librarian:   70 hits × 0 + 30 misses × 3000 = 90,000 tokens               │
│  Archivalist: 75 hits × 0 + 25 misses × 2500 = 62,500 tokens               │
│  Academic:    60 hits × 0 + 40 misses × 4000 = 160,000 tokens              │
│  TOTAL: 312,500 tokens (67% savings)                                        │
│                                                                             │
│  ──────────────────────────────────────────────────────────────────────────│
│                                                                             │
│  WITH INTENT CACHE + VECTORGRAPHDB:                                         │
│  ═══════════════════════════════════                                        │
│                                                                             │
│  LIBRARIAN (100 queries):                                                   │
│  ├─ 70 intent cache hits        → 0 tokens                                 │
│  ├─ 20 graph-only retrievals    → 0 tokens (LOCATE, structural)            │
│  │   • "where is X" - direct lookup                                        │
│  │   • "what calls Y" - graph traversal                                    │
│  │   • "find similar" - vector search                                      │
│  └─ 10 LLM synthesis            → 30,000 tokens (EXPLAIN, PATTERN)         │
│  Librarian total: 30,000 tokens (90% savings)                              │
│                                                                             │
│  ARCHIVALIST (100 queries):                                                 │
│  ├─ 75 intent cache hits        → 0 tokens                                 │
│  ├─ 15 graph-only retrievals    → 0 tokens (HISTORICAL, ACTIVITY)          │
│  │   • "what did we change" - graph traversal                              │
│  │   • "issues with this" - cross-domain query                             │
│  └─ 10 LLM synthesis            → 25,000 tokens (reasoning)                │
│  Archivalist total: 25,000 tokens (90% savings)                            │
│                                                                             │
│  ACADEMIC (100 queries):                                                    │
│  ├─ 60 intent cache hits        → 0 tokens                                 │
│  ├─ 25 graph-only retrievals    → 0 tokens (docs, patterns)                │
│  └─ 15 LLM synthesis            → 60,000 tokens (summarization)            │
│  Academic total: 60,000 tokens (85% savings)                               │
│                                                                             │
│  CROSS-DOMAIN (50 queries):                                                 │
│  ├─ 30 graph traversals         → 0 tokens                                 │
│  └─ 20 LLM synthesis            → 50,000 tokens                            │
│  Cross-domain total: 50,000 tokens (NEW CAPABILITY)                        │
│                                                                             │
│  ──────────────────────────────────────────────────────────────────────────│
│                                                                             │
│  GRAND TOTAL WITH VECTORGRAPHDB:                                            │
│  ═══════════════════════════════                                            │
│  Librarian:    30,000 tokens   (90% savings vs baseline)                   │
│  Archivalist:  25,000 tokens   (90% savings vs baseline)                   │
│  Academic:     60,000 tokens   (85% savings vs baseline)                   │
│  Cross-domain: 50,000 tokens   (new capability)                            │
│  ────────────────────────────────────────                                   │
│  TOTAL: 165,000 tokens (83% savings vs baseline 950K)                      │
│                                                                             │
│  ──────────────────────────────────────────────────────────────────────────│
│                                                                             │
│  EMBEDDING COST OVERHEAD:                                                   │
│  ════════════════════════                                                   │
│  Initial indexing: 50,000 nodes × $0.00002 = $1.00                         │
│  Per query: ~$0.00002 (embedding the query)                                │
│  Daily queries (1000): ~$0.02                                              │
│  Monthly embedding cost: ~$1.00                                            │
│                                                                             │
│  vs Monthly LLM savings at baseline rates:                                 │
│  • Baseline: 950K tokens × 30 days × $0.01/1K = $285/month                │
│  • With VectorGraphDB: 165K tokens × 30 days × $0.01/1K = $50/month       │
│  • NET SAVINGS: $235/month - $1 embedding = $234/month                     │
│                                                                             │
│  ROI: 234x return on embedding investment                                   │
│                                                                             │
└─────────────────────────────────────────────────────────────────────────────┘
```

### Comprehensive Token & Cost Savings Analysis

The VectorGraphDB system achieves **~75% token reduction** through intent caching and context reduction. The agent (LLM) always runs on cache misses - it **decides** when to invoke VectorGraphDB skills. Savings come from curated context, not from avoiding LLM calls.

#### The Two-Layer Resolution Architecture

```
┌─────────────────────────────────────────────────────────────────────────────┐
│                      QUERY RESOLUTION LAYERS                                │
├─────────────────────────────────────────────────────────────────────────────┤
│                                                                             │
│   L1: INTENT CACHE                    L2: AGENT (LLM)                       │
│   ─────────────────                   ───────────────                       │
│   • <1ms latency                      • 500-3000ms latency                  │
│   • 0 tokens                          • Agent reasons about query           │
│   • Exact + similar match             • DECIDES whether to call VectorGraphDB│
│   • ~68% of queries                   • Processes results, generates response│
│                                       • ~32% of queries                     │
│                                                                             │
│   On cache miss, agent runs and may invoke VectorGraphDB skills:            │
│   ┌─────────────────────────────────────────────────────────────────────┐  │
│   │  Agent: "I need to find authentication code"                        │  │
│   │    → Invokes: search_code(query="authentication", limit=10)         │  │
│   │    → VectorGraphDB returns curated, scored results                  │  │
│   │    → Agent synthesizes response from smaller context                │  │
│   └─────────────────────────────────────────────────────────────────────┘  │
│                                                                             │
└─────────────────────────────────────────────────────────────────────────────┘
```

#### Where Savings Actually Come From

**1. Intent Cache (L1) - ~68% of queries cost 0 tokens**

```
Librarian:   70% cache hit rate → 0 tokens
Archivalist: 75% cache hit rate → 0 tokens
Academic:    60% cache hit rate → 0 tokens
```

This is the biggest savings. Similar queries hit cached responses via embedding similarity.

**2. Context Reduction (L2) - ~60% smaller context per query**

When the agent DOES run, VectorGraphDB provides curated context instead of raw files:

```
WITHOUT VectorGraphDB:
──────────────────────
Agent receives: Raw file contents, grep results, full history
Context size: ~2,000-5,000 tokens per query
Agent must parse, filter, and reason about everything

WITH VectorGraphDB:
───────────────────
Agent invokes: search_code, find_patterns, get_dependencies
Returns: Scored nodes, relevant snippets, structured metadata
Context size: ~800-1,500 tokens per query
Agent works with pre-filtered, ranked results
```

**3. Progressive Retrieval - Agent asks for more only if needed**

```
Agent: search_code("auth handler", limit=5)
  → Gets 5 results
  → Evaluates: "Not enough context about session handling"
  → Calls: search_code("session auth", limit=5)
  → Now has enough to respond

vs Traditional RAG: Always retrieves K=20, wastes tokens on irrelevant results
```

#### Realistic Token Estimates (per 100 queries per agent)

**BASELINE (no cache, no VectorGraphDB):**
```
Every query requires:
  • Query understanding:    ~300 tokens
  • Large raw context:    ~2,000 tokens (files, history, etc.)
  • Response generation:    ~700 tokens
  • TOTAL per query:      ~3,000 tokens

Librarian:   100 × 3,000 = 300,000 tokens
Archivalist: 100 × 2,500 = 250,000 tokens
Academic:    100 × 4,000 = 400,000 tokens
─────────────────────────────────────────
BASELINE TOTAL: 950,000 tokens
```

**WITH INTENT CACHE ONLY (no VectorGraphDB):**
```
Cache hits cost 0, misses cost full price:

Librarian:   70 hits × 0 + 30 misses × 3,000 =  90,000 tokens
Archivalist: 75 hits × 0 + 25 misses × 2,500 =  62,500 tokens
Academic:    60 hits × 0 + 40 misses × 4,000 = 160,000 tokens
─────────────────────────────────────────────────────────────
CACHE-ONLY TOTAL: 312,500 tokens (67% savings vs baseline)
```

**WITH INTENT CACHE + VECTORGRAPHDB:**
```
Cache hits: 0 tokens (same as above)
Cache misses: Agent runs with SMALLER context from VectorGraphDB

Per non-cached query:
  • Query understanding:    ~300 tokens
  • Curated VDB context:    ~800 tokens (60% smaller than raw)
  • Response generation:    ~700 tokens
  • TOTAL per query:      ~1,800 tokens (40% less than baseline)

Librarian:   70 hits × 0 + 30 misses × 1,800 =  54,000 tokens
Archivalist: 75 hits × 0 + 25 misses × 1,500 =  37,500 tokens
Academic:    60 hits × 0 + 40 misses × 2,400 =  96,000 tokens
Cross-domain: (new capability)                =  50,000 tokens
─────────────────────────────────────────────────────────────
VECTORGRAPHDB TOTAL: 237,500 tokens (75% savings vs baseline)
```

#### Savings Breakdown

| Agent | Baseline | Cache Only | +VectorGraphDB | Total Savings |
|-------|----------|------------|----------------|---------------|
| **Librarian** | 300,000 | 90,000 | 54,000 | **82%** |
| **Archivalist** | 250,000 | 62,500 | 37,500 | **85%** |
| **Academic** | 400,000 | 160,000 | 96,000 | **76%** |
| **Cross-Domain** | N/A | N/A | 50,000 | New capability |
| **TOTAL** | 950,000 | 312,500 | 237,500 | **75%** |

**Key insight**: Intent cache provides ~67% savings. VectorGraphDB adds ~24% more savings on top by reducing context size. Combined: **75% total savings**.

#### Monthly Cost Comparison

```
┌─────────────────────────────────────────────────────────────────────────────┐
│                      MONTHLY COST ANALYSIS                                  │
├─────────────────────────────────────────────────────────────────────────────┤
│                                                                             │
│  BASELINE (no cache, no VectorGraphDB):                                     │
│  ──────────────────────────────────────                                     │
│  950K tokens × 30 days × $0.01/1K = $285/month                             │
│                                                                             │
│  WITH CACHE ONLY:                                                           │
│  ────────────────                                                           │
│  312.5K tokens × 30 days × $0.01/1K = $94/month                            │
│                                                                             │
│  WITH CACHE + VECTORGRAPHDB:                                                │
│  ───────────────────────────                                                │
│  237.5K tokens × 30 days × $0.01/1K = $71/month                            │
│  Embedding overhead:                   $1/month                             │
│  ───────────────────────────────────────────                                │
│  TOTAL:                               $72/month                             │
│                                                                             │
│  SAVINGS vs baseline: $213/month (75% reduction)                            │
│  SAVINGS vs cache-only: $22/month (additional 24% reduction)                │
│                                                                             │
└─────────────────────────────────────────────────────────────────────────────┘
```

#### Cross-Domain Query Savings

Cross-domain queries are where VectorGraphDB really shines - previously required multiple agent calls:

```
┌─────────────────────────────────────────────────────────────────────────────┐
│  EXAMPLE: "What's the best practice for error handling and how do we do it?"│
├─────────────────────────────────────────────────────────────────────────────┤
│                                                                             │
│  WITHOUT VectorGraphDB (requires 3 separate agent calls):                   │
│  ────────────────────────────────────────────────────────                   │
│  1. User → Guide → Academic: "best practice"     4,000 tokens              │
│  2. User → Guide → Librarian: "our code"         3,000 tokens              │
│  3. User → Guide → Archivalist: "past decisions" 2,500 tokens              │
│  4. User synthesizes manually or asks again      3,000 tokens              │
│  ────────────────────────────────────────────────────                       │
│  TOTAL:                                         12,500 tokens              │
│                                                                             │
│  WITH VectorGraphDB (single agent, cross-domain edges):                     │
│  ───────────────────────────────────────────────────────                    │
│  1. User → Guide → Academic                                                │
│  2. Academic invokes:                                                       │
│     • find_best_practices("error handling")     → Academic domain          │
│     • get_code_for_academic(result_id)          → Code domain (edge)       │
│     • get_history_for_academic(result_id)       → History domain (edge)    │
│  3. Academic synthesizes with all context        4,000 tokens              │
│  ────────────────────────────────────────────────────                       │
│  TOTAL:                                          4,000 tokens              │
│                                                                             │
│  SAVINGS: 68% reduction                                                     │
│                                                                             │
└─────────────────────────────────────────────────────────────────────────────┘
```

#### Progressive Retrieval Savings

```
┌─────────────────────────────────────────────────────────────────────────────┐
│                  AGENTIC PROGRESSIVE RETRIEVAL                              │
├─────────────────────────────────────────────────────────────────────────────┤
│                                                                             │
│  TRADITIONAL RAG (fixed retrieval):                                         │
│  ──────────────────────────────────                                         │
│  Always retrieve K=20 results                                               │
│  Always include all in context                                              │
│  Cost: 20 × 500 tokens = 10,000 tokens context                             │
│                                                                             │
│  AGENTIC RAG (agent decides):                                               │
│  ────────────────────────────                                               │
│  Agent: search_code("auth", limit=5)                                       │
│    → Reviews 5 results (2,500 tokens)                                      │
│    → "This is sufficient" → stops                                          │
│  OR                                                                         │
│    → "Need more about sessions" → refines query                            │
│    → search_code("auth session", limit=5)                                  │
│    → Total: 10 results (5,000 tokens)                                      │
│                                                                             │
│  Average retrieval: 8 results = 4,000 tokens                               │
│  SAVINGS: 60% reduction vs fixed K=20                                       │
│                                                                             │
└─────────────────────────────────────────────────────────────────────────────┘
```

#### Skill Loading Overhead

```
┌─────────────────────────────────────────────────────────────────────────────┐
│                  SKILL LOADING ANALYSIS                                     │
├─────────────────────────────────────────────────────────────────────────────┤
│                                                                             │
│  21 VectorGraphDB skills across 3 agents                                    │
│  Each skill definition: ~200 tokens                                         │
│                                                                             │
│  WITHOUT Progressive Loading:                                               │
│  ────────────────────────────                                               │
│  All 21 skills in prompt = ~4,200 tokens overhead per query                │
│                                                                             │
│  WITH Progressive Loading:                                                  │
│  ─────────────────────────                                                  │
│  Tier 1 (core): 3 skills = ~600 tokens (always loaded)                     │
│  Tier 2 (triggered): 4-5 skills = ~800 tokens (keyword match)              │
│  Tier 3 (explicit): 0 tokens (user must request)                           │
│                                                                             │
│  Average per query: ~1,400 tokens                                          │
│  SAVINGS: ~2,800 tokens per query (67% reduction in skill overhead)        │
│                                                                             │
└─────────────────────────────────────────────────────────────────────────────┘
```

#### Token Allocation Summary

```
┌─────────────────────────────────────────────────────────────────────────────┐
│                    TOKEN ALLOCATION (per 100 queries)                       │
├─────────────────────────────────────────────────────────────────────────────┤
│                                                                             │
│  BASELINE: 950,000 tokens                                                   │
│  ════════════════════════                                                   │
│  ████████████████████████████████████████████████████████████████ 100%     │
│                                                                             │
│  WITH CACHE ONLY: 312,500 tokens                                            │
│  ═══════════════════════════════                                            │
│  █████████████████████░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░ 33%      │
│                                                                             │
│  WITH CACHE + VECTORGRAPHDB: 237,500 tokens                                 │
│  ═══════════════════════════════════════════                                │
│  ████████████████░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░ 25%      │
│                                                                             │
│  BREAKDOWN OF 237,500 TOKENS:                                               │
│  ├── Intent Cache Hits (68%):           0 tokens                           │
│  ├── Agent Query Processing (32%):  76,000 tokens (300 × 253 queries)     │
│  ├── VectorGraphDB Context (32%):  101,500 tokens (curated, not raw)      │
│  ├── Response Generation (32%):     60,000 tokens                          │
│  └── Embedding overhead:               ~50 tokens (negligible)             │
│                                                                             │
│  SAVINGS: 712,500 tokens (75%)                                              │
│  MONTHLY COST: $72 vs $285 = $213 saved                                     │
│                                                                             │
└─────────────────────────────────────────────────────────────────────────────┘
```

**Key Insight**: The agent always runs on cache misses - VectorGraphDB doesn't replace LLM reasoning, it **reduces context size** by providing curated, scored, pre-filtered results instead of raw files and grep output. Combined with intent caching, this achieves 75% total token savings.

### XOR Filters as Internal Optimization

XOR filters are used **internally within search skills**, not exposed as separate LLM tools. This is critical for actual token savings.

#### Why NOT Expose XOR Filters to LLMs

```
┌─────────────────────────────────────────────────────────────────────────────┐
│                    XOR AS SEPARATE TOOL = BAD                               │
├─────────────────────────────────────────────────────────────────────────────┤
│                                                                             │
│   Agent: exists_in_domain("retry", "code")     → 20 tokens                 │
│   Agent: (thinks) "It exists, I should search" → 10 tokens                 │
│   Agent: search_code("retry logic")            → 800 tokens                │
│   Total: 830 tokens                                                        │
│                                                                             │
│   vs WITHOUT XOR:                                                          │
│   Agent: search_code("retry logic")            → 800 tokens                │
│                                                                             │
│   XOR as separate tool is WORSE when content exists!                       │
│   Only saves tokens when content DOESN'T exist.                            │
│                                                                             │
│   If 80% of searches find something:                                       │
│   • Without XOR: 800 tokens                                                │
│   • With XOR tool: 0.8 × 830 + 0.2 × 20 = 668 tokens                      │
│   • Only ~16% savings, not worth the complexity                            │
│                                                                             │
└─────────────────────────────────────────────────────────────────────────────┘
```

#### Correct Approach: XOR as Internal Implementation

```
┌─────────────────────────────────────────────────────────────────────────────┐
│                    XOR AS INTERNAL OPTIMIZATION = GOOD                      │
├─────────────────────────────────────────────────────────────────────────────┤
│                                                                             │
│   LLM calls: search_code("retry")                                          │
│                                                                             │
│   INTERNALLY (hidden from LLM):                                            │
│   ─────────────────────────────                                            │
│   1. XOR filter check: topic exists in code domain?                        │
│      → If NO: return {"status": "no_matches", "hint": "..."} immediately  │
│      → If YES: continue to step 2                                          │
│                                                                             │
│   2. HNSW vector search for actual results                                 │
│                                                                             │
│   3. Return results to LLM                                                 │
│                                                                             │
│   LLM doesn't know XOR exists - it just gets faster "no matches"           │
│   responses when content doesn't exist (30 tokens vs 800 tokens).          │
│                                                                             │
│   NO EXTRA TOOL CALL OVERHEAD                                              │
│                                                                             │
└─────────────────────────────────────────────────────────────────────────────┘
```

#### Smart Search Skills with Internal XOR

```go
// search_code with internal XOR optimization
var SearchCodeSkill = skills.NewSkill("search_code").
    Description("Search code by semantic similarity. Returns results or 'no_matches' quickly.").
    Domain("code").
    StringParam("query", "What to search for", true).
    IntParam("limit", "Max results (default: 10)", false).
    Handler(func(ctx context.Context, input json.RawMessage) (any, error) {
        var params struct {
            Query string `json:"query"`
            Limit int    `json:"limit"`
        }
        json.Unmarshal(input, &params)

        // INTERNAL: XOR filter early exit (not exposed to LLM)
        topicHash := hashTopic(params.Query)
        if !xorFilters.Code.Contains(topicHash) {
            // Return minimal response - saves ~770 tokens
            return SearchResponse{
                Status: "no_matches",
                Hint:   "No code matches this query. Consider searching history or academic.",
            }, nil
        }

        // Content likely exists - do full search
        results, err := vdb.SimilarNodes(ctx, params.Query, params.Limit,
            &SearchFilter{Domain: DomainCode})
        if err != nil {
            return nil, err
        }

        return formatSearchResults(results), nil
    }).
    Build()
```

#### Multi-Domain Search (Reduces Tool Calls)

Instead of agent making 3 separate calls, one call searches all domains:

```go
// search_all - Single call for multi-domain search
var SearchAllSkill = skills.NewSkill("search_all").
    Description("Search across multiple domains in one call. Returns results grouped by domain.").
    Domain("search").
    StringParam("query", "What to search for", true).
    ArrayParam("domains", "Domains to search: code, history, academic (default: all)", "string", false).
    IntParam("limit_per_domain", "Max results per domain (default: 5)", false).
    Handler(func(ctx context.Context, input json.RawMessage) (any, error) {
        var params struct {
            Query          string   `json:"query"`
            Domains        []string `json:"domains"`
            LimitPerDomain int      `json:"limit_per_domain"`
        }
        json.Unmarshal(input, &params)

        if len(params.Domains) == 0 {
            params.Domains = []string{"code", "history", "academic"}
        }
        if params.LimitPerDomain == 0 {
            params.LimitPerDomain = 5
        }

        results := make(map[string]any)
        topicHash := hashTopic(params.Query)

        for _, domain := range params.Domains {
            // INTERNAL: XOR early exit per domain
            if !xorFilters.Get(domain).Contains(topicHash) {
                results[domain] = DomainResult{Status: "no_matches"}
                continue
            }

            domainResults, _ := vdb.SimilarNodes(ctx, params.Query,
                params.LimitPerDomain, &SearchFilter{Domain: Domain(domain)})
            results[domain] = formatDomainResults(domainResults)
        }

        return MultiDomainResponse{
            Query:   params.Query,
            Results: results,
        }, nil
    }).
    Build()
```

**Token savings:**
```
BEFORE (3 separate calls):
  search_code("retry")     → 800 tokens
  search_history("retry")  → 800 tokens
  search_academic("retry") → 800 tokens
  Total: 2,400 tokens

AFTER (1 call with internal XOR):
  search_all("retry", domains=["code", "history", "academic"])
    → code: 5 results (exists)
    → history: no_matches (XOR early exit - minimal tokens)
    → academic: 3 results (exists)
  Total: ~900 tokens (62% savings)
```

#### Search with Inline Domain Hints

Include hints about other domains in search results:

```go
type SearchResponse struct {
    Results      []SearchResult `json:"results"`
    AlsoExistsIn []string       `json:"also_exists_in,omitempty"`
    Suggestions  []string       `json:"suggestions,omitempty"`
}

func formatSearchWithHints(results []SearchResult, query string) SearchResponse {
    resp := SearchResponse{Results: results}

    // INTERNAL: XOR probe other domains (fast, no extra tool call)
    topicHash := hashTopic(query)
    if xorFilters.History.Contains(topicHash) {
        resp.AlsoExistsIn = append(resp.AlsoExistsIn, "history")
    }
    if xorFilters.Academic.Contains(topicHash) {
        resp.AlsoExistsIn = append(resp.AlsoExistsIn, "academic")
    }

    if len(resp.AlsoExistsIn) > 0 {
        resp.Suggestions = append(resp.Suggestions,
            fmt.Sprintf("Related content in: %s", strings.Join(resp.AlsoExistsIn, ", ")))
    }

    return resp
}
```

**Example response to LLM:**
```json
{
  "results": [
    {"id": "auth/handler.go:Login", "score": 0.92, "snippet": "..."},
    {"id": "auth/middleware.go:Validate", "score": 0.87, "snippet": "..."}
  ],
  "also_exists_in": ["history"],
  "suggestions": ["Related content in: history"]
}
```

Agent learns about related domains without extra tool calls.

#### Smart Search with Budget Control

Let the tool manage progressive retrieval internally:

```go
// smart_search - Budget-aware search with internal progressive retrieval
var SmartSearchSkill = skills.NewSkill("smart_search").
    Description("Intelligent search that retrieves until confident or budget exhausted.").
    Domain("search").
    StringParam("query", "What to search for", true).
    IntParam("token_budget", "Max tokens for results (default: 1000)", false).
    EnumParam("thoroughness", "Search depth", []string{"quick", "moderate", "thorough"}, false).
    Handler(func(ctx context.Context, input json.RawMessage) (any, error) {
        var params struct {
            Query        string `json:"query"`
            TokenBudget  int    `json:"token_budget"`
            Thoroughness string `json:"thoroughness"`
        }
        json.Unmarshal(input, &params)

        if params.TokenBudget == 0 {
            params.TokenBudget = 1000
        }

        // INTERNAL: XOR early exit
        if !xorFilters.HasAny(params.Query) {
            return SearchResponse{
                Status: "no_matches",
                Hint:   "No content matches this query in any domain.",
            }, nil
        }

        // Progressive retrieval within budget
        var results []SearchResult
        var tokensUsed int
        batchSize := 3

        for tokensUsed < params.TokenBudget {
            batch, _ := vdb.SimilarNodes(ctx, params.Query, batchSize, nil)
            if len(batch) == 0 {
                break
            }

            for _, r := range batch {
                result := formatResult(r)
                resultTokens := estimateTokens(result)

                if tokensUsed + resultTokens > params.TokenBudget {
                    break
                }

                results = append(results, result)
                tokensUsed += resultTokens
            }

            // Check confidence based on thoroughness
            if hasEnoughResults(results, params.Thoroughness) {
                break
            }

            batchSize += 2 // Expand search
        }

        return SearchResponse{
            Results:    results,
            TokensUsed: tokensUsed,
            Budget:     params.TokenBudget,
        }, nil
    }).
    Build()
```

#### XOR Filter Maintenance (Internal)

XOR filters are rebuilt in the background, with a pending set for consistency:

```go
type XORFilterManager struct {
    filters    map[string]*xorfilter.Xor8  // Immutable XOR filters
    pending    map[string]*bloom.Filter     // Mutable bloom for recent adds
    vdb        *VectorGraphDB
    rebuildMu  sync.Mutex
}

// Contains checks both XOR (stable) and pending (recent)
func (m *XORFilterManager) Contains(domain string, key uint64) bool {
    // Check immutable XOR filter (fast)
    if m.filters[domain] != nil && m.filters[domain].Contains(key) {
        return true
    }
    // Check pending bloom filter (recent additions)
    if m.pending[domain] != nil && m.pending[domain].Test(key) {
        return true
    }
    return false
}

// NotifyAdd adds to pending (called after VectorGraphDB write)
func (m *XORFilterManager) NotifyAdd(domain string, keys []uint64) {
    for _, key := range keys {
        m.pending[domain].Add(key)
    }
}

// Rebuild runs periodically in background
func (m *XORFilterManager) Rebuild(ctx context.Context) error {
    m.rebuildMu.Lock()
    defer m.rebuildMu.Unlock()

    for _, domain := range []string{"code", "history", "academic"} {
        keys, _ := m.vdb.CollectTopicKeys(ctx, domain)
        m.filters[domain], _ = xorfilter.Populate(keys)
        m.pending[domain] = bloom.NewWithEstimates(10000, 0.01) // Reset pending
    }

    return nil
}
```

#### Token Savings Summary with Internal XOR

| Optimization | Mechanism | Token Savings |
|--------------|-----------|---------------|
| **XOR early exit** | Skip HNSW when no matches | ~770 tokens/empty search |
| **Multi-domain search** | One call instead of 3 | ~1,500 tokens/cross-domain |
| **Inline domain hints** | No extra probe calls | ~20 tokens × avoided calls |
| **Budget-controlled search** | Tool manages retrieval | ~200 tokens/query |

**Updated total savings: ~80%** (up from 75% without XOR)

### Latency Analysis

```
┌─────────────────────────────────────────────────────────────────────────────┐
│                    LATENCY ANALYSIS                                          │
├─────────────────────────────────────────────────────────────────────────────┤
│                                                                             │
│  OPERATION                        │ LATENCY      │ NOTES                    │
│  ═════════════════════════════════╪══════════════╪══════════════════════════│
│                                   │              │                          │
│  SQLite Operations:               │              │                          │
│  ─────────────────                │              │                          │
│  Insert node                      │ <1ms         │ With WAL mode            │
│  Insert edge                      │ <1ms         │ With WAL mode            │
│  Get node by ID                   │ <1ms         │ Primary key lookup       │
│  Get edges (indexed)              │ 1-2ms        │ With composite index     │
│  Graph traversal (depth 3)        │ 5-20ms       │ Recursive CTE            │
│  Graph traversal (depth 5)        │ 20-50ms      │ Recursive CTE            │
│                                   │              │                          │
│  HNSW Operations:                 │              │                          │
│  ────────────────                 │              │                          │
│  Add node (10K nodes)             │ 2-5ms        │ O(log n) insertion       │
│  Add node (100K nodes)            │ 5-10ms       │ O(log n) insertion       │
│  Search k=10 (10K nodes)          │ 2-5ms        │ O(log n) search          │
│  Search k=10 (100K nodes)         │ 5-15ms       │ O(log n) search          │
│  Search k=10 with filter          │ 10-30ms      │ Extra filtering pass     │
│                                   │              │                          │
│  Combined Operations:             │              │                          │
│  ───────────────────              │              │                          │
│  Vector search + get nodes        │ 10-30ms      │ HNSW + SQLite            │
│  Graph traversal + enrichment     │ 20-50ms      │ Multiple joins           │
│  Cross-domain query               │ 30-80ms      │ Vector + graph + joins   │
│  Full query with scoring          │ 50-100ms     │ All mitigations applied  │
│                                   │              │                          │
│  Embedding Generation:            │              │                          │
│  ─────────────────────            │              │                          │
│  Query embedding (API call)       │ 50-200ms     │ Network latency          │
│  Query embedding (local model)    │ 10-50ms      │ If using local embedder  │
│                                   │              │                          │
│  End-to-End (Cache Miss):         │              │                          │
│  ─────────────────────            │              │                          │
│  Intent cache lookup              │ <1ms         │ In-memory                │
│  + Embedding generation           │ 50-200ms     │ API call                 │
│  + HNSW search                    │ 5-15ms       │ In-memory                │
│  + Node retrieval                 │ 10-30ms      │ SQLite                   │
│  + Quality scoring                │ 5-10ms       │ In-memory                │
│  + Conflict detection             │ 10-20ms      │ Comparisons              │
│  ═══════════════════════════════════════════════════════════════════════   │
│  TOTAL (graph-only response)      │ 80-280ms     │ No LLM needed            │
│                                   │              │                          │
│  End-to-End (LLM Required):       │              │                          │
│  ──────────────────────           │              │                          │
│  All above                        │ 80-280ms     │                          │
│  + LLM API call                   │ 500-3000ms   │ Depends on context size  │
│  ═══════════════════════════════════════════════════════════════════════   │
│  TOTAL (with LLM)                 │ 580-3280ms   │                          │
│                                   │              │                          │
│  ──────────────────────────────────────────────────────────────────────────│
│                                                                             │
│  COMPARISON:                                                                │
│  ═══════════                                                                │
│  Pure LLM (no graph):             │ 500-3000ms   │ Every query              │
│  Graph-only (70% of queries):     │ 80-280ms     │ 5-10x faster             │
│  Graph + LLM (30% of queries):    │ 580-3280ms   │ Similar to pure LLM      │
│                                                                             │
│  WEIGHTED AVERAGE:                                                          │
│  0.70 × 180ms + 0.30 × 1900ms = 126ms + 570ms = 696ms                      │
│  vs Pure LLM: 1900ms average                                               │
│  IMPROVEMENT: 63% faster average response time                             │
│                                                                             │
└─────────────────────────────────────────────────────────────────────────────┘
```

### Memory Impact Analysis

```
┌─────────────────────────────────────────────────────────────────────────────┐
│                    MEMORY IMPACT ANALYSIS                                    │
├─────────────────────────────────────────────────────────────────────────────┤
│                                                                             │
│  COMPONENT                    │ 10K NODES  │ 50K NODES  │ 100K NODES       │
│  ════════════════════════════╪════════════╪════════════╪══════════════════│
│                               │            │            │                  │
│  SQLite Database (on disk):   │            │            │                  │
│  ───────────────────────────  │            │            │                  │
│  Nodes table                  │ 5 MB       │ 25 MB      │ 50 MB            │
│  Edges table                  │ 2 MB       │ 10 MB      │ 20 MB            │
│  Vectors table (BLOBs)        │ 30 MB      │ 150 MB     │ 300 MB           │
│  Indexes                      │ 3 MB       │ 15 MB      │ 30 MB            │
│  Other tables                 │ 1 MB       │ 5 MB       │ 10 MB            │
│  ─────────────────────────────────────────────────────────────────────────│
│  SQLite Total (disk)          │ 41 MB      │ 205 MB     │ 410 MB           │
│                               │            │            │                  │
│  In-Memory (HNSW Index):      │            │            │                  │
│  ───────────────────────────  │            │            │                  │
│  Vectors (float32 × 768)      │ 30 MB      │ 150 MB     │ 300 MB           │
│  Magnitudes (float64)         │ 0.08 MB    │ 0.4 MB     │ 0.8 MB           │
│  Graph layers (adjacency)     │ 5 MB       │ 25 MB      │ 50 MB            │
│  Metadata maps                │ 2 MB       │ 10 MB      │ 20 MB            │
│  ─────────────────────────────────────────────────────────────────────────│
│  HNSW Total (RAM)             │ 37 MB      │ 185 MB     │ 371 MB           │
│                               │            │            │                  │
│  Intent Cache (In-Memory):    │            │            │                  │
│  ───────────────────────────  │            │            │                  │
│  Cached responses (1000)      │ 5 MB       │ 5 MB       │ 5 MB             │
│  Hot cache entries            │ 1 MB       │ 1 MB       │ 1 MB             │
│  ─────────────────────────────────────────────────────────────────────────│
│  Cache Total (RAM)            │ 6 MB       │ 6 MB       │ 6 MB             │
│                               │            │            │                  │
│  ═══════════════════════════════════════════════════════════════════════   │
│                               │            │            │                  │
│  TOTAL DISK                   │ 41 MB      │ 205 MB     │ 410 MB           │
│  TOTAL RAM                    │ 43 MB      │ 191 MB     │ 377 MB           │
│                               │            │            │                  │
│  ──────────────────────────────────────────────────────────────────────────│
│                                                                             │
│  SCALING NOTES:                                                             │
│  ═══════════════                                                            │
│  • RAM scales linearly with node count                                     │
│  • Disk scales linearly with node count                                    │
│  • 768-dim embeddings: ~3KB per node                                       │
│  • With 1536-dim embeddings: ~6KB per node (double the above)              │
│                                                                             │
│  MEMORY OPTIMIZATION OPTIONS:                                               │
│  ═════════════════════════════                                              │
│  1. Lazy loading: Only load active partitions into HNSW                    │
│  2. Quantization: int8 vectors = 75% memory reduction                      │
│  3. Sharding: Split by domain, load on demand                              │
│  4. Disk-based HNSW: Trade latency for memory (10x slower)                 │
│                                                                             │
│  RECOMMENDED LIMITS:                                                        │
│  ═══════════════════                                                        │
│  Small project (<10K functions):    No optimization needed                 │
│  Medium project (10-50K functions): Monitor RAM, consider lazy loading     │
│  Large project (50-100K functions): Use sharding + lazy loading            │
│  Monorepo (>100K functions):        Use quantization + sharding            │
│                                                                             │
│  ──────────────────────────────────────────────────────────────────────────│
│                                                                             │
│  STARTUP TIME:                                                              │
│  ═════════════                                                              │
│  10K nodes:  ~500ms  (load vectors + build HNSW)                           │
│  50K nodes:  ~2.5s   (load vectors + build HNSW)                           │
│  100K nodes: ~5s     (load vectors + build HNSW)                           │
│                                                                             │
│  With persisted HNSW:                                                       │
│  10K nodes:  ~200ms  (load vectors + load HNSW edges)                      │
│  50K nodes:  ~1s     (load vectors + load HNSW edges)                      │
│  100K nodes: ~2s     (load vectors + load HNSW edges)                      │
│                                                                             │
└─────────────────────────────────────────────────────────────────────────────┘
```

### Unified Query Resolution Flow

```
┌─────────────────────────────────────────────────────────────────────────────┐
│                    UNIFIED QUERY RESOLUTION FLOW                             │
├─────────────────────────────────────────────────────────────────────────────┤
│                                                                             │
│                              USER QUERY                                     │
│                                  │                                          │
│                                  ▼                                          │
│                         ┌────────────────┐                                  │
│                         │  GUIDE ROUTER  │                                  │
│                         │                │                                  │
│                         │ Intent: LOCATE │                                  │
│                         │ Subject: auth  │                                  │
│                         │ Target: lib    │                                  │
│                         └───────┬────────┘                                  │
│                                 │                                           │
│                                 ▼                                           │
│                    ┌────────────────────────┐                               │
│                    │    UNIFIED RESOLVER    │                               │
│                    └────────────┬───────────┘                               │
│                                 │                                           │
│            ┌────────────────────┼────────────────────┐                      │
│            │                    │                    │                      │
│            ▼                    ▼                    ▼                      │
│    ┌──────────────┐    ┌──────────────┐    ┌──────────────┐                │
│    │ L1: INTENT   │    │ L2: VECTOR   │    │ L3: LLM      │                │
│    │    CACHE     │    │    GRAPH     │    │    SYNTHESIS │                │
│    │              │    │              │    │              │                │
│    │ <1ms lookup  │    │ 80-280ms     │    │ 500-3000ms   │                │
│    │ 0 tokens     │    │ 0 tokens     │    │ 2000+ tokens │                │
│    └──────┬───────┘    └──────┬───────┘    └──────┬───────┘                │
│           │                   │                   │                         │
│           │ HIT               │ SUFFICIENT        │ REQUIRED                │
│           │                   │                   │                         │
│           ▼                   ▼                   ▼                         │
│    ┌─────────────────────────────────────────────────────────────┐         │
│    │                    MITIGATION LAYER                          │         │
│    │                                                              │         │
│    │  ┌────────────┐ ┌────────────┐ ┌────────────┐ ┌──────────┐ │         │
│    │  │ Freshness  │ │   Trust    │ │  Conflict  │ │ Quality  │ │         │
│    │  │  Tracker   │ │ Hierarchy  │ │  Detector  │ │  Scorer  │ │         │
│    │  └────────────┘ └────────────┘ └────────────┘ └──────────┘ │         │
│    │                                                              │         │
│    │  ┌────────────┐ ┌────────────┐ ┌────────────┐              │         │
│    │  │Provenance  │ │Hallucination│ │   LLM     │              │         │
│    │  │  Tracker   │ │  Firewall  │ │  Prompter │              │         │
│    │  └────────────┘ └────────────┘ └────────────┘              │         │
│    │                                                              │         │
│    └──────────────────────────┬───────────────────────────────────┘         │
│                               │                                             │
│                               ▼                                             │
│                    ┌────────────────────────┐                               │
│                    │   ANNOTATED RESPONSE   │                               │
│                    │                        │                               │
│                    │  • Source citations    │                               │
│                    │  • Trust levels        │                               │
│                    │  • Conflict warnings   │                               │
│                    │  • Freshness notes     │                               │
│                    │  • Provenance chain    │                               │
│                    └────────────────────────┘                               │
│                                                                             │
└─────────────────────────────────────────────────────────────────────────────┘
```

```go
// UnifiedResolver orchestrates all query resolution
type UnifiedResolver struct {
    vdb         *VectorGraphDB
    intentCache *IntentCache
    hnsw        *HNSWIndex
    embedder    Embedder

    // Mitigations
    firewall    *HallucinationFirewall
    freshness   *FreshnessTracker
    provenance  *ProvenanceTracker
    trust       *TrustHierarchy
    conflicts   *ConflictDetector
    scorer      *ContextQualityScorer
    prompter    *LLMContextBuilder

    // LLM client for synthesis
    llm LLMClient
}

// ResolveQuery handles a query through all resolution layers
func (ur *UnifiedResolver) ResolveQuery(ctx context.Context, query RoutedQuery) (*AnnotatedLLMResponse, error) {
    // L1: Intent Cache
    if cached := ur.intentCache.Get(query.Intent, query.Subject, query.SessionID); cached != nil {
        return &AnnotatedLLMResponse{
            Response:    string(cached.Response),
            SourcesUsed: cached.SourceNodes,
        }, nil
    }

    // L2: VectorGraphDB
    graphResult, err := ur.queryGraph(ctx, query)
    if err != nil {
        return nil, err
    }

    // Apply mitigations
    freshnessRanked := ur.freshness.RefreshRanking(ctx, graphResult.Similar)

    var nodes []*Node
    for _, sr := range freshnessRanked {
        node, _ := ur.vdb.GetNode(ctx, sr.NodeID)
        if node != nil {
            nodes = append(nodes, node)
        }
    }

    trustRanked := ur.trust.RankByTrust(ctx, nodes)
    conflicts, _ := ur.conflicts.DetectConflicts(ctx, nodes)

    scoredNodes := make([]ScoredNode, len(freshnessRanked))
    for i, sr := range freshnessRanked {
        scoredNodes[i] = sr.ScoredNode
    }

    scoredContext, _ := ur.scorer.SelectBestContext(ctx, query.Query, scoredNodes, DefaultQualityScorerConfig())

    // Check if graph-only response is sufficient
    if ur.isGraphSufficient(query, scoredContext, conflicts) {
        response := ur.formatGraphResponse(ctx, query, scoredContext, conflicts)

        // Cache for future
        ur.cacheResponse(query, response, scoredContext)

        return response, nil
    }

    // L3: LLM Synthesis
    prompt, _ := ur.prompter.BuildPrompt(ctx, query.Query, scoredContext, conflicts)

    llmResponse, err := ur.llm.Complete(ctx, prompt)
    if err != nil {
        return nil, err
    }

    // Parse and annotate response
    annotated := ur.prompter.ParseLLMResponse(llmResponse, scoredContext)
    annotated.Conflicts = conflicts

    // Verify before caching (hallucination firewall)
    if ur.shouldCache(annotated) {
        ur.cacheResponse(query, annotated, scoredContext)
    }

    return annotated, nil
}

// isGraphSufficient determines if graph results are enough
func (ur *UnifiedResolver) isGraphSufficient(query RoutedQuery, scored []ScoredContext, conflicts []Conflict) bool {
    // LOCATE queries are usually graph-sufficient
    if query.Intent == QueryIntentLocate {
        return true
    }

    // If we have high-confidence, high-trust results
    goodResults := 0
    for _, s := range scored {
        if s.Included && s.QualityScore > 0.8 && s.TrustLevel >= TrustLevelRecent {
            goodResults++
        }
    }

    // Need at least 2 good results and no unresolved conflicts
    if goodResults >= 2 && len(conflicts) == 0 {
        return true
    }

    // EXPLAIN and PATTERN usually need LLM
    if query.Intent == QueryIntentExplain || query.Intent == QueryIntentPattern {
        return false
    }

    return false
}
```

---

## VectorGraphDB System Integration

This section defines how VectorGraphDB integrates with the Sylk multi-agent system. The key principle: **Guide routes ALL messages**, and **only three knowledge agents access VectorGraphDB directly**.

### Architecture Overview

```
┌─────────────────────────────────────────────────────────────────────────────────┐
│                           SYLK SYSTEM WITH VECTORGRAPHDB                        │
├─────────────────────────────────────────────────────────────────────────────────┤
│                                                                                 │
│                              ┌──────────────┐                                   │
│                              │     USER     │                                   │
│                              └──────┬───────┘                                   │
│                                     │                                           │
│                                     ▼                                           │
│  ┌──────────────────────────────────────────────────────────────────────────┐  │
│  │                              GUIDE                                        │  │
│  │                                                                           │  │
│  │  • Intent Classification (via LLM or DSL rules)                          │  │
│  │  • Routes user queries to ANY agent based on intent                      │  │
│  │  • Extracts metadata during routing (intent, subject, session)           │  │
│  │  • Routes agent-to-agent messages                                        │  │
│  │  • ALL communication flows through Guide                                 │  │
│  │                                                                           │  │
│  └─────┬──────────┬──────────┬──────────┬──────────┬──────────┬────────────┘  │
│        │          │          │          │          │          │                 │
│        ▼          ▼          ▼          ▼          ▼          ▼                 │
│  ┌──────────┐ ┌──────────┐ ┌──────────┐ ┌──────────┐ ┌──────────┐ ┌──────────┐ │
│  │ARCHITECT │ │ORCHESTR- │ │ENGINEER  │ │INSPECTOR │ │ TESTER   │ │  OTHER   │ │
│  │          │ │ATOR      │ │(×N)      │ │          │ │          │ │  AGENTS  │ │
│  └────┬─────┘ └────┬─────┘ └────┬─────┘ └────┬─────┘ └────┬─────┘ └────┬─────┘ │
│       │            │            │            │            │            │        │
│       │            │            │            │            │            │        │
│       │ Summaries  │ Summaries  │ Summaries  │ Summaries  │ Summaries  │        │
│       │ via Guide  │ via Guide  │ via Guide  │ via Guide  │ via Guide  │        │
│       │     │      │     │      │     │      │     │      │     │      │        │
│       │     └──────┴─────┴──────┴─────┴──────┴─────┴──────┘     │      │        │
│       │                         │                               │      │        │
│       │                         ▼                               │      │        │
│       │          ┌──────────────────────────────┐               │      │        │
│       │          │      GUIDE (routing)         │               │      │        │
│       │          └──────────────┬───────────────┘               │      │        │
│       │                         │                               │      │        │
│       │                         ▼                               │      │        │
│  ─────┴─────────────────────────────────────────────────────────┴──────┴─────── │
│                                                                                 │
│     ╔═══════════════════════════════════════════════════════════════════════╗   │
│     ║                    KNOWLEDGE LAYER (VectorGraphDB)                    ║   │
│     ╠═══════════════════════════════════════════════════════════════════════╣   │
│     ║                                                                       ║   │
│     ║  User can directly query these via Guide's intent-based routing:     ║   │
│     ║                                                                       ║   │
│     ║  ┌───────────────┐  ┌───────────────┐  ┌───────────────┐            ║   │
│     ║  │  LIBRARIAN    │  │  ARCHIVALIST  │  │   ACADEMIC    │            ║   │
│     ║  │  (Sonnet 4.5) │  │  (Sonnet 4.5) │  │  (Opus 4.5)   │            ║   │
│     ║  │               │  │               │  │               │            ║   │
│     ║  │ • Code search │  │ • Store sums  │  │ • Research    │            ║   │
│     ║  │ • Symbol nav  │  │ • Query hist  │  │ • Academic    │            ║   │
│     ║  │ • Dep graph   │  │ • Patterns    │  │ • Papers      │            ║   │
│     ║  │ • Doc lookup  │  │ • Session     │  │ • Best pract  │            ║   │
│     ║  └───────┬───────┘  └───────┬───────┘  └───────┬───────┘            ║   │
│     ║          │                  │                  │                    ║   │
│     ║          │                  │                  │                    ║   │
│     ║          ▼                  ▼                  ▼                    ║   │
│     ║  ┌─────────────────────────────────────────────────────────────┐   ║   │
│     ║  │                      VectorGraphDB                          │   ║   │
│     ║  │                                                             │   ║   │
│     ║  │  ┌─────────────┐     ┌─────────────┐     ┌─────────────┐   │   ║   │
│     ║  │  │ CODE DOMAIN │◄───►│HISTORY DOM. │◄───►│ACADEMIC DOM.│   │   ║   │
│     ║  │  │             │     │             │     │             │   │   ║   │
│     ║  │  │ • Symbols   │     │ • Sessions  │     │ • Papers    │   │   ║   │
│     ║  │  │ • Files     │     │ • Decisions │     │ • Docs      │   │   ║   │
│     ║  │  │ • Deps      │     │ • Failures  │     │ • Patterns  │   │   ║   │
│     ║  │  │ • Docs      │     │ • Patterns  │     │ • Standards │   │   ║   │
│     ║  │  └─────────────┘     └─────────────┘     └─────────────┘   │   ║   │
│     ║  │         │                   │                   │          │   ║   │
│     ║  │         └───────────────────┼───────────────────┘          │   ║   │
│     ║  │                             │                              │   ║   │
│     ║  │                   Cross-Domain Edges                       │   ║   │
│     ║  │                                                             │   ║   │
│     ║  └─────────────────────────────────────────────────────────────┘   ║   │
│     ║                                                                       ║   │
│     ╚═══════════════════════════════════════════════════════════════════════╝   │
│                                                                                 │
└─────────────────────────────────────────────────────────────────────────────────┘
```

### Message Flow Patterns

#### Pattern 1: User Directly Queries Knowledge Agent

User can query ANY agent (including knowledge agents) via Guide's intent-based routing:

```
User: "Where is the authentication code?"
                    │
                    ▼
        ┌───────────────────────┐
        │        GUIDE          │
        │                       │
        │ Intent: LOCATE        │
        │ Subject: auth code    │
        │ Route → Librarian     │
        └───────────┬───────────┘
                    │
                    ▼
        ┌───────────────────────┐
        │      LIBRARIAN        │
        │                       │
        │ → VectorGraphDB.Query │
        │   (Code Domain)       │
        │                       │
        │ Returns: auth.go:45   │
        └───────────┬───────────┘
                    │
                    ▼
                  User
```

#### Pattern 2: User Queries Historical Context

```
User: "What patterns have we used for error handling?"
                    │
                    ▼
        ┌───────────────────────┐
        │        GUIDE          │
        │                       │
        │ Intent: PATTERN       │
        │ Subject: errors       │
        │ Route → Archivalist   │
        └───────────┬───────────┘
                    │
                    ▼
        ┌───────────────────────┐
        │     ARCHIVALIST       │
        │                       │
        │ → VectorGraphDB.Query │
        │   (History Domain)    │
        │                       │
        │ Returns: patterns +   │
        │ related code examples │
        └───────────┬───────────┘
                    │
                    ▼
                  User
```

#### Pattern 3: Agent Sends Summary for Storage

When agents complete work, they send summaries through Guide to Archivalist:

```
Engineer completes task:
"Fixed auth bug by adding null check"
                    │
                    ▼
        ┌───────────────────────┐
        │        GUIDE          │
        │                       │
        │ From: Engineer        │
        │ To: Archivalist       │
        │ Type: STORE_SUMMARY   │
        └───────────┬───────────┘
                    │
                    ▼
        ┌───────────────────────┐
        │     ARCHIVALIST       │
        │                       │
        │ → VectorGraphDB.Add   │
        │   (History Domain)    │
        │                       │
        │ Creates cross-domain  │
        │ edge to Code domain   │
        └───────────────────────┘
```

#### Pattern 4: Cross-Domain Query via Academic

```
User: "What's the best practice for retry logic?"
                    │
                    ▼
        ┌───────────────────────┐
        │        GUIDE          │
        │                       │
        │ Intent: RESEARCH      │
        │ Subject: retry logic  │
        │ Route → Academic      │
        └───────────┬───────────┘
                    │
                    ▼
        ┌───────────────────────┐
        │       ACADEMIC        │
        │      (Opus 4.5)       │
        │                       │
        │ 1. Query Academic dom │
        │ 2. Query Code domain  │
        │    (via Librarian)    │
        │ 3. Synthesize answer  │
        │                       │
        │ Returns: Best pract + │
        │ how codebase does it  │
        └───────────┬───────────┘
                    │
                    ▼
                  User
```

### Agent-to-VectorGraphDB Skills

Each knowledge agent has specific skills for VectorGraphDB access. These are defined using the fluent Builder API and registered with the agent's skill registry.

#### Librarian Skills (Code Domain)

```go
// =============================================================================
// Librarian Skills - Code search, navigation, and dependency analysis
// =============================================================================

// search_code - Vector similarity search for code
var SearchCodeSkill = skills.NewSkill("search_code").
    Description("Search the codebase for code similar to the query. Returns ranked results by semantic similarity.").
    Domain("code").
    Keywords("find", "search", "locate", "where", "code").
    Priority(100).
    StringParam("query", "Natural language description of the code to find", true).
    IntParam("limit", "Maximum number of results to return (default: 10)", false).
    EnumParam("symbol_type", "Filter by symbol type", []string{
        "function", "method", "class", "struct", "interface", "variable", "constant", "type", "any",
    }, false).
    StringParam("file_pattern", "Glob pattern to filter files (e.g., '*.go', 'src/**/*.ts')", false).
    Handler(func(ctx context.Context, input json.RawMessage) (any, error) {
        var params struct {
            Query      string `json:"query"`
            Limit      int    `json:"limit"`
            SymbolType string `json:"symbol_type"`
            FilePattern string `json:"file_pattern"`
        }
        if err := json.Unmarshal(input, &params); err != nil {
            return nil, err
        }
        if params.Limit == 0 {
            params.Limit = 10
        }

        filter := &SearchFilter{Domain: DomainCode}
        if params.SymbolType != "" && params.SymbolType != "any" {
            filter.NodeTypes = []NodeType{NodeType(params.SymbolType)}
        }
        if params.FilePattern != "" {
            filter.Metadata = map[string]any{"file_pattern": params.FilePattern}
        }

        results, err := librarian.vdb.SimilarNodes(ctx, params.Query, params.Limit, filter)
        if err != nil {
            return nil, err
        }
        return formatCodeResults(results), nil
    }).
    Build()

// get_symbol - Get detailed information about a specific symbol
var GetSymbolSkill = skills.NewSkill("get_symbol").
    Description("Get detailed information about a specific code symbol by its ID or fully-qualified name.").
    Domain("code").
    Keywords("symbol", "definition", "details", "info").
    Priority(90).
    StringParam("symbol_id", "The symbol ID or fully-qualified name (e.g., 'pkg/auth.Login')", true).
    BoolParam("include_source", "Include the full source code of the symbol", false).
    Handler(func(ctx context.Context, input json.RawMessage) (any, error) {
        var params struct {
            SymbolID      string `json:"symbol_id"`
            IncludeSource bool   `json:"include_source"`
        }
        if err := json.Unmarshal(input, &params); err != nil {
            return nil, err
        }

        node, err := librarian.vdb.GetNode(ctx, params.SymbolID)
        if err != nil {
            return nil, err
        }
        return formatSymbolDetails(node, params.IncludeSource), nil
    }).
    Build()

// get_dependencies - Get what a symbol depends on
var GetDependenciesSkill = skills.NewSkill("get_dependencies").
    Description("Get all symbols that the specified symbol depends on (imports, calls, references).").
    Domain("code").
    Keywords("depends", "dependencies", "imports", "uses", "calls").
    Priority(85).
    StringParam("symbol_id", "The symbol ID to get dependencies for", true).
    EnumParam("edge_type", "Type of dependency to follow", []string{
        "imports", "calls", "references", "extends", "implements", "all",
    }, false).
    IntParam("depth", "Maximum traversal depth (default: 1)", false).
    Handler(func(ctx context.Context, input json.RawMessage) (any, error) {
        var params struct {
            SymbolID string `json:"symbol_id"`
            EdgeType string `json:"edge_type"`
            Depth    int    `json:"depth"`
        }
        if err := json.Unmarshal(input, &params); err != nil {
            return nil, err
        }
        if params.Depth == 0 {
            params.Depth = 1
        }
        if params.EdgeType == "" {
            params.EdgeType = "all"
        }

        deps, err := librarian.vdb.TraverseEdges(ctx, params.SymbolID, EdgeType(params.EdgeType), "outgoing")
        if err != nil {
            return nil, err
        }
        return formatDependencies(deps), nil
    }).
    Build()

// get_dependents - Get what depends on a symbol
var GetDependentsSkill = skills.NewSkill("get_dependents").
    Description("Get all symbols that depend on the specified symbol (reverse dependencies).").
    Domain("code").
    Keywords("dependents", "used by", "called by", "references to").
    Priority(85).
    StringParam("symbol_id", "The symbol ID to get dependents for", true).
    EnumParam("edge_type", "Type of dependency to follow", []string{
        "imports", "calls", "references", "extends", "implements", "all",
    }, false).
    IntParam("depth", "Maximum traversal depth (default: 1)", false).
    Handler(func(ctx context.Context, input json.RawMessage) (any, error) {
        var params struct {
            SymbolID string `json:"symbol_id"`
            EdgeType string `json:"edge_type"`
            Depth    int    `json:"depth"`
        }
        if err := json.Unmarshal(input, &params); err != nil {
            return nil, err
        }
        if params.Depth == 0 {
            params.Depth = 1
        }

        dependents, err := librarian.vdb.TraverseEdges(ctx, params.SymbolID, EdgeType(params.EdgeType), "incoming")
        if err != nil {
            return nil, err
        }
        return formatDependents(dependents), nil
    }).
    Build()

// find_similar_symbols - Find symbols similar to another symbol
var FindSimilarSymbolsSkill = skills.NewSkill("find_similar_symbols").
    Description("Find code symbols that are semantically similar to a given symbol. Useful for finding related implementations or duplicates.").
    Domain("code").
    Keywords("similar", "like", "related", "duplicates").
    Priority(80).
    StringParam("symbol_id", "The symbol ID to find similar symbols for", true).
    IntParam("limit", "Maximum number of results (default: 10)", false).
    BoolParam("same_type_only", "Only return symbols of the same type", false).
    Handler(func(ctx context.Context, input json.RawMessage) (any, error) {
        var params struct {
            SymbolID     string `json:"symbol_id"`
            Limit        int    `json:"limit"`
            SameTypeOnly bool   `json:"same_type_only"`
        }
        if err := json.Unmarshal(input, &params); err != nil {
            return nil, err
        }
        if params.Limit == 0 {
            params.Limit = 10
        }

        filter := &SearchFilter{Domain: DomainCode}
        if params.SameTypeOnly {
            sourceNode, _ := librarian.vdb.GetNode(ctx, params.SymbolID)
            if sourceNode != nil {
                filter.NodeTypes = []NodeType{sourceNode.NodeType}
            }
        }

        similar, err := librarian.vdb.SimilarToNode(ctx, params.SymbolID, params.Limit, filter)
        if err != nil {
            return nil, err
        }
        return formatSimilarSymbols(similar), nil
    }).
    Build()

// get_file_symbols - Get all symbols in a file
var GetFileSymbolsSkill = skills.NewSkill("get_file_symbols").
    Description("Get all symbols defined in a specific file.").
    Domain("code").
    Keywords("file", "symbols in", "contents of").
    Priority(75).
    StringParam("file_path", "Path to the file (relative to project root)", true).
    EnumParam("symbol_type", "Filter by symbol type", []string{
        "function", "method", "class", "struct", "interface", "variable", "constant", "type", "any",
    }, false).
    Handler(func(ctx context.Context, input json.RawMessage) (any, error) {
        var params struct {
            FilePath   string `json:"file_path"`
            SymbolType string `json:"symbol_type"`
        }
        if err := json.Unmarshal(input, &params); err != nil {
            return nil, err
        }

        filter := &SearchFilter{
            Domain:   DomainCode,
            Metadata: map[string]any{"file_path": params.FilePath},
        }
        if params.SymbolType != "" && params.SymbolType != "any" {
            filter.NodeTypes = []NodeType{NodeType(params.SymbolType)}
        }

        // Query with empty string to get all matching the filter
        results, err := librarian.vdb.SimilarNodes(ctx, "", 100, filter)
        if err != nil {
            return nil, err
        }
        return formatFileSymbols(results, params.FilePath), nil
    }).
    Build()

// get_history_for_code - Cross-domain: get history for code
var GetHistoryForCodeSkill = skills.NewSkill("get_history_for_code").
    Description("Get the historical context for a piece of code - when it was added, modified, why changes were made.").
    Domain("code").
    Keywords("history", "changes", "why", "when", "modified").
    Priority(70).
    StringParam("symbol_id", "The symbol ID to get history for", true).
    IntParam("limit", "Maximum number of history entries (default: 10)", false).
    Handler(func(ctx context.Context, input json.RawMessage) (any, error) {
        var params struct {
            SymbolID string `json:"symbol_id"`
            Limit    int    `json:"limit"`
        }
        if err := json.Unmarshal(input, &params); err != nil {
            return nil, err
        }
        if params.Limit == 0 {
            params.Limit = 10
        }

        history, err := librarian.vdb.GetHistoryForCode(ctx, params.SymbolID)
        if err != nil {
            return nil, err
        }
        return formatHistoryForCode(history, params.Limit), nil
    }).
    Build()

// RegisterLibrarianSkills registers all Librarian skills
func RegisterLibrarianSkills(registry *skills.Registry) {
    registry.Register(SearchCodeSkill)
    registry.Register(GetSymbolSkill)
    registry.Register(GetDependenciesSkill)
    registry.Register(GetDependentsSkill)
    registry.Register(FindSimilarSymbolsSkill)
    registry.Register(GetFileSymbolsSkill)
    registry.Register(GetHistoryForCodeSkill)
}
```

#### Archivalist Skills (History Domain)

```go
// =============================================================================
// Archivalist Skills - Historical context, patterns, and session management
// =============================================================================

// store_summary - Store a summary from another agent
var StoreSummarySkill = skills.NewSkill("store_summary").
    Description("Store a work summary from an agent. Creates nodes in the history domain and links to referenced code.").
    Domain("history").
    Keywords("store", "save", "record", "log").
    Priority(100).
    StringParam("content", "The summary content to store", true).
    EnumParam("summary_type", "Type of summary", []string{
        "task_completion", "decision", "failure", "pattern", "refactoring", "bug_fix", "feature",
    }, true).
    StringParam("session_id", "Session ID this summary belongs to", true).
    ArrayParam("code_references", "IDs of code symbols referenced in this summary", "string", false).
    ArrayParam("tags", "Tags for categorization", "string", false).
    ObjectParam("metadata", "Additional metadata", map[string]*skills.Property{
        "agent":       {Type: "string", Description: "Agent that created this summary"},
        "task_id":     {Type: "string", Description: "Associated task ID"},
        "confidence":  {Type: "number", Description: "Confidence level 0-1"},
    }, false).
    Handler(func(ctx context.Context, input json.RawMessage) (any, error) {
        var params struct {
            Content        string         `json:"content"`
            SummaryType    string         `json:"summary_type"`
            SessionID      string         `json:"session_id"`
            CodeReferences []string       `json:"code_references"`
            Tags           []string       `json:"tags"`
            Metadata       map[string]any `json:"metadata"`
        }
        if err := json.Unmarshal(input, &params); err != nil {
            return nil, err
        }

        // Create history node
        node := &Node{
            ID:        generateID("history"),
            Domain:    DomainHistory,
            NodeType:  NodeType(params.SummaryType),
            Content:   params.Content,
            SessionID: params.SessionID,
            Metadata:  params.Metadata,
            Tags:      params.Tags,
            CreatedAt: time.Now(),
        }

        if err := archivalist.vdb.AddNode(ctx, node); err != nil {
            return nil, err
        }

        // Create cross-domain edges to referenced code
        for _, codeRef := range params.CodeReferences {
            archivalist.vdb.AddEdge(ctx, node.ID, codeRef, EdgeTypeReferencedIn, 0.9, nil)
        }

        return map[string]any{
            "stored":  true,
            "node_id": node.ID,
            "edges":   len(params.CodeReferences),
        }, nil
    }).
    Build()

// search_history - Search historical context
var SearchHistorySkill = skills.NewSkill("search_history").
    Description("Search historical summaries, decisions, and patterns by semantic similarity.").
    Domain("history").
    Keywords("history", "past", "previous", "before", "earlier").
    Priority(95).
    StringParam("query", "Natural language query to search history", true).
    IntParam("limit", "Maximum results (default: 10)", false).
    EnumParam("summary_type", "Filter by summary type", []string{
        "task_completion", "decision", "failure", "pattern", "refactoring", "bug_fix", "feature", "any",
    }, false).
    StringParam("session_id", "Limit to specific session (empty for all)", false).
    StringParam("since", "Only include entries after this time (RFC3339)", false).
    Handler(func(ctx context.Context, input json.RawMessage) (any, error) {
        var params struct {
            Query       string `json:"query"`
            Limit       int    `json:"limit"`
            SummaryType string `json:"summary_type"`
            SessionID   string `json:"session_id"`
            Since       string `json:"since"`
        }
        if err := json.Unmarshal(input, &params); err != nil {
            return nil, err
        }
        if params.Limit == 0 {
            params.Limit = 10
        }

        filter := &SearchFilter{Domain: DomainHistory}
        if params.SummaryType != "" && params.SummaryType != "any" {
            filter.NodeTypes = []NodeType{NodeType(params.SummaryType)}
        }
        if params.SessionID != "" {
            filter.SessionID = params.SessionID
        }
        if params.Since != "" {
            if t, err := time.Parse(time.RFC3339, params.Since); err == nil {
                filter.CreatedAfter = t
            }
        }

        results, err := archivalist.vdb.SimilarNodes(ctx, params.Query, params.Limit, filter)
        if err != nil {
            return nil, err
        }
        return formatHistoryResults(results), nil
    }).
    Build()

// find_patterns - Find recurring patterns in project history
var FindPatternsSkill = skills.NewSkill("find_patterns").
    Description("Find recurring patterns in how problems were solved, decisions made, or code structured.").
    Domain("history").
    Keywords("pattern", "patterns", "recurring", "common", "typical", "usually").
    Priority(90).
    StringParam("query", "Description of the pattern to find", true).
    IntParam("min_occurrences", "Minimum times pattern must appear (default: 2)", false).
    StringParam("category", "Pattern category to search", false).
    Handler(func(ctx context.Context, input json.RawMessage) (any, error) {
        var params struct {
            Query          string `json:"query"`
            MinOccurrences int    `json:"min_occurrences"`
            Category       string `json:"category"`
        }
        if err := json.Unmarshal(input, &params); err != nil {
            return nil, err
        }
        if params.MinOccurrences == 0 {
            params.MinOccurrences = 2
        }

        filter := &SearchFilter{
            Domain:    DomainHistory,
            NodeTypes: []NodeType{NodeTypePattern},
        }

        results, err := archivalist.vdb.SimilarNodes(ctx, params.Query, 50, filter)
        if err != nil {
            return nil, err
        }

        // Group and count patterns
        patterns := aggregatePatterns(results, params.MinOccurrences)
        return formatPatterns(patterns), nil
    }).
    Build()

// find_failures - Find past failures and their resolutions
var FindFailuresSkill = skills.NewSkill("find_failures").
    Description("Find past failures, errors, and how they were resolved. Useful for debugging similar issues.").
    Domain("history").
    Keywords("failure", "error", "bug", "issue", "problem", "failed", "broke", "fix").
    Priority(90).
    StringParam("query", "Description of the failure or error", true).
    IntParam("limit", "Maximum results (default: 10)", false).
    BoolParam("resolved_only", "Only show failures that have resolutions", false).
    Handler(func(ctx context.Context, input json.RawMessage) (any, error) {
        var params struct {
            Query        string `json:"query"`
            Limit        int    `json:"limit"`
            ResolvedOnly bool   `json:"resolved_only"`
        }
        if err := json.Unmarshal(input, &params); err != nil {
            return nil, err
        }
        if params.Limit == 0 {
            params.Limit = 10
        }

        filter := &SearchFilter{
            Domain:    DomainHistory,
            NodeTypes: []NodeType{NodeTypeFailure},
        }
        if params.ResolvedOnly {
            filter.Metadata = map[string]any{"resolved": true}
        }

        results, err := archivalist.vdb.SimilarNodes(ctx, params.Query, params.Limit, filter)
        if err != nil {
            return nil, err
        }
        return formatFailures(results), nil
    }).
    Build()

// get_session_history - Get history for a specific session
var GetSessionHistorySkill = skills.NewSkill("get_session_history").
    Description("Get the complete history of a session - all tasks, decisions, and outcomes.").
    Domain("history").
    Keywords("session", "this session", "current", "today").
    Priority(85).
    StringParam("session_id", "Session ID to get history for (empty for current)", false).
    BoolParam("chronological", "Order by time (default: true)", false).
    EnumParam("include", "What to include", []string{"all", "decisions", "tasks", "failures"}, false).
    Handler(func(ctx context.Context, input json.RawMessage) (any, error) {
        var params struct {
            SessionID     string `json:"session_id"`
            Chronological bool   `json:"chronological"`
            Include       string `json:"include"`
        }
        if err := json.Unmarshal(input, &params); err != nil {
            return nil, err
        }
        if params.SessionID == "" {
            params.SessionID = getCurrentSessionID(ctx)
        }

        filter := &SearchFilter{
            Domain:    DomainHistory,
            SessionID: params.SessionID,
        }

        if params.Include != "" && params.Include != "all" {
            filter.NodeTypes = []NodeType{NodeType(params.Include)}
        }

        results, err := archivalist.vdb.SimilarNodes(ctx, "", 100, filter)
        if err != nil {
            return nil, err
        }

        if params.Chronological {
            sortByTime(results)
        }
        return formatSessionHistory(results), nil
    }).
    Build()

// get_code_for_history - Cross-domain: get code for history entry
var GetCodeForHistorySkill = skills.NewSkill("get_code_for_history").
    Description("Get the code that was referenced or modified in a historical entry.").
    Domain("history").
    Keywords("code", "what code", "which code", "related code").
    Priority(75).
    StringParam("history_id", "The history entry ID", true).
    Handler(func(ctx context.Context, input json.RawMessage) (any, error) {
        var params struct {
            HistoryID string `json:"history_id"`
        }
        if err := json.Unmarshal(input, &params); err != nil {
            return nil, err
        }

        code, err := archivalist.vdb.GetCodeForHistory(ctx, params.HistoryID)
        if err != nil {
            return nil, err
        }
        return formatCodeForHistory(code), nil
    }).
    Build()

// get_decisions - Get architectural and design decisions
var GetDecisionsSkill = skills.NewSkill("get_decisions").
    Description("Get past architectural and design decisions, with rationale and context.").
    Domain("history").
    Keywords("decision", "decided", "chose", "why", "rationale", "architectural").
    Priority(85).
    StringParam("query", "Topic or area to find decisions about", true).
    IntParam("limit", "Maximum results (default: 10)", false).
    Handler(func(ctx context.Context, input json.RawMessage) (any, error) {
        var params struct {
            Query string `json:"query"`
            Limit int    `json:"limit"`
        }
        if err := json.Unmarshal(input, &params); err != nil {
            return nil, err
        }
        if params.Limit == 0 {
            params.Limit = 10
        }

        filter := &SearchFilter{
            Domain:    DomainHistory,
            NodeTypes: []NodeType{NodeTypeDecision},
        }

        results, err := archivalist.vdb.SimilarNodes(ctx, params.Query, params.Limit, filter)
        if err != nil {
            return nil, err
        }
        return formatDecisions(results), nil
    }).
    Build()

// RegisterArchivalistSkills registers all Archivalist skills
func RegisterArchivalistSkills(registry *skills.Registry) {
    registry.Register(StoreSummarySkill)
    registry.Register(SearchHistorySkill)
    registry.Register(FindPatternsSkill)
    registry.Register(FindFailuresSkill)
    registry.Register(GetSessionHistorySkill)
    registry.Register(GetCodeForHistorySkill)
    registry.Register(GetDecisionsSkill)
}
```

#### Academic Skills (Academic Domain)

```go
// =============================================================================
// Academic Skills - External research, papers, best practices (Opus 4.5)
// =============================================================================

// research - Deep research on a topic
var ResearchSkill = skills.NewSkill("research").
    Description("Conduct deep research on a technical topic. Searches academic papers, documentation, and best practices. Uses Opus 4.5 for complex reasoning.").
    Domain("academic").
    Keywords("research", "learn", "understand", "best practice", "how to", "should I").
    Priority(100).
    StringParam("query", "The research question or topic", true).
    IntParam("depth", "Research depth: 1=quick, 2=moderate, 3=comprehensive (default: 2)", false).
    ArrayParam("focus_areas", "Specific areas to focus on", "string", false).
    BoolParam("include_papers", "Include academic papers (default: true)", false).
    BoolParam("include_docs", "Include official documentation (default: true)", false).
    Handler(func(ctx context.Context, input json.RawMessage) (any, error) {
        var params struct {
            Query         string   `json:"query"`
            Depth         int      `json:"depth"`
            FocusAreas    []string `json:"focus_areas"`
            IncludePapers bool     `json:"include_papers"`
            IncludeDocs   bool     `json:"include_docs"`
        }
        if err := json.Unmarshal(input, &params); err != nil {
            return nil, err
        }
        if params.Depth == 0 {
            params.Depth = 2
        }

        // Calculate result limits based on depth
        limit := params.Depth * 10

        filter := &SearchFilter{Domain: DomainAcademic}
        var nodeTypes []NodeType
        if params.IncludePapers {
            nodeTypes = append(nodeTypes, NodeTypePaper)
        }
        if params.IncludeDocs {
            nodeTypes = append(nodeTypes, NodeTypeDocumentation)
        }
        if len(nodeTypes) > 0 {
            filter.NodeTypes = nodeTypes
        }

        results, err := academic.vdb.SimilarNodes(ctx, params.Query, limit, filter)
        if err != nil {
            return nil, err
        }

        // Use Opus 4.5 to synthesize research findings
        synthesis := academic.synthesizeResearch(ctx, params.Query, results, params.FocusAreas)
        return synthesis, nil
    }).
    Build()

// find_best_practices - Find established best practices
var FindBestPracticesSkill = skills.NewSkill("find_best_practices").
    Description("Find established best practices for a technology, pattern, or approach.").
    Domain("academic").
    Keywords("best practice", "recommended", "standard", "convention", "proper way").
    Priority(95).
    StringParam("topic", "The topic to find best practices for", true).
    StringParam("technology", "Specific technology context (e.g., 'Go', 'React')", false).
    IntParam("limit", "Maximum results (default: 5)", false).
    Handler(func(ctx context.Context, input json.RawMessage) (any, error) {
        var params struct {
            Topic      string `json:"topic"`
            Technology string `json:"technology"`
            Limit      int    `json:"limit"`
        }
        if err := json.Unmarshal(input, &params); err != nil {
            return nil, err
        }
        if params.Limit == 0 {
            params.Limit = 5
        }

        query := params.Topic
        if params.Technology != "" {
            query = fmt.Sprintf("%s %s best practices", params.Technology, params.Topic)
        }

        filter := &SearchFilter{
            Domain:    DomainAcademic,
            NodeTypes: []NodeType{NodeTypeBestPractice, NodeTypeDocumentation},
        }

        results, err := academic.vdb.SimilarNodes(ctx, query, params.Limit, filter)
        if err != nil {
            return nil, err
        }
        return formatBestPractices(results), nil
    }).
    Build()

// find_papers - Find relevant academic papers
var FindPapersSkill = skills.NewSkill("find_papers").
    Description("Find academic papers relevant to a topic. Returns abstracts and key findings.").
    Domain("academic").
    Keywords("paper", "papers", "academic", "research", "study", "publication").
    Priority(90).
    StringParam("query", "Topic to find papers about", true).
    IntParam("limit", "Maximum papers (default: 5)", false).
    IntParam("min_year", "Minimum publication year", false).
    ArrayParam("venues", "Preferred venues/conferences", "string", false).
    Handler(func(ctx context.Context, input json.RawMessage) (any, error) {
        var params struct {
            Query   string   `json:"query"`
            Limit   int      `json:"limit"`
            MinYear int      `json:"min_year"`
            Venues  []string `json:"venues"`
        }
        if err := json.Unmarshal(input, &params); err != nil {
            return nil, err
        }
        if params.Limit == 0 {
            params.Limit = 5
        }

        filter := &SearchFilter{
            Domain:    DomainAcademic,
            NodeTypes: []NodeType{NodeTypePaper},
        }
        if params.MinYear > 0 {
            filter.Metadata = map[string]any{"min_year": params.MinYear}
        }

        results, err := academic.vdb.SimilarNodes(ctx, params.Query, params.Limit, filter)
        if err != nil {
            return nil, err
        }
        return formatPapers(results), nil
    }).
    Build()

// compare_approaches - Compare different technical approaches
var CompareApproachesSkill = skills.NewSkill("compare_approaches").
    Description("Compare different technical approaches, patterns, or technologies based on academic research and best practices. Uses Opus 4.5 for complex analysis.").
    Domain("academic").
    Keywords("compare", "versus", "vs", "difference", "tradeoff", "which is better").
    Priority(90).
    ArrayParam("approaches", "The approaches to compare (2-4)", "string", true).
    StringParam("context", "The context for comparison (e.g., 'high-throughput API')", false).
    ArrayParam("criteria", "Specific criteria to compare on", "string", false).
    Handler(func(ctx context.Context, input json.RawMessage) (any, error) {
        var params struct {
            Approaches []string `json:"approaches"`
            Context    string   `json:"context"`
            Criteria   []string `json:"criteria"`
        }
        if err := json.Unmarshal(input, &params); err != nil {
            return nil, err
        }

        // Search for each approach
        var allResults []ScoredNode
        for _, approach := range params.Approaches {
            query := approach
            if params.Context != "" {
                query = fmt.Sprintf("%s for %s", approach, params.Context)
            }
            results, _ := academic.vdb.SimilarNodes(ctx, query, 5, &SearchFilter{Domain: DomainAcademic})
            allResults = append(allResults, results...)
        }

        // Use Opus 4.5 to synthesize comparison
        comparison := academic.synthesizeComparison(ctx, params.Approaches, allResults, params.Criteria)
        return comparison, nil
    }).
    Build()

// synthesize_with_codebase - Cross-domain: combine academic knowledge with codebase
var SynthesizeWithCodebaseSkill = skills.NewSkill("synthesize_with_codebase").
    Description("Synthesize academic knowledge with how things are actually implemented in the codebase. Shows theory vs practice.").
    Domain("academic").
    Keywords("how we do", "our implementation", "in practice", "codebase").
    Priority(85).
    StringParam("topic", "The topic to synthesize", true).
    BoolParam("show_gaps", "Highlight gaps between best practice and implementation", false).
    Handler(func(ctx context.Context, input json.RawMessage) (any, error) {
        var params struct {
            Topic    string `json:"topic"`
            ShowGaps bool   `json:"show_gaps"`
        }
        if err := json.Unmarshal(input, &params); err != nil {
            return nil, err
        }

        // Get academic knowledge
        academicResults, _ := academic.vdb.SimilarNodes(ctx, params.Topic, 5, &SearchFilter{Domain: DomainAcademic})

        // Get codebase implementations
        codeResults, _ := academic.vdb.SimilarNodes(ctx, params.Topic, 5, &SearchFilter{Domain: DomainCode})

        // Synthesize with Opus 4.5
        synthesis := academic.synthesizeTheoryAndPractice(ctx, params.Topic, academicResults, codeResults, params.ShowGaps)
        return synthesis, nil
    }).
    Build()

// get_code_for_academic - Cross-domain: get code implementing academic concept
var GetCodeForAcademicSkill = skills.NewSkill("get_code_for_academic").
    Description("Find code in the codebase that implements a specific academic concept or pattern.").
    Domain("academic").
    Keywords("implementation", "example", "code for").
    Priority(80).
    StringParam("academic_id", "The academic node ID", true).
    IntParam("limit", "Maximum code examples (default: 5)", false).
    Handler(func(ctx context.Context, input json.RawMessage) (any, error) {
        var params struct {
            AcademicID string `json:"academic_id"`
            Limit      int    `json:"limit"`
        }
        if err := json.Unmarshal(input, &params); err != nil {
            return nil, err
        }
        if params.Limit == 0 {
            params.Limit = 5
        }

        code, err := academic.vdb.GetCodeForAcademic(ctx, params.AcademicID)
        if err != nil {
            return nil, err
        }
        return formatCodeForAcademic(code, params.Limit), nil
    }).
    Build()

// get_history_for_academic - Cross-domain: get history related to academic concept
var GetHistoryForAcademicSkill = skills.NewSkill("get_history_for_academic").
    Description("Find historical decisions and discussions related to an academic concept.").
    Domain("academic").
    Keywords("history", "decisions about", "discussions").
    Priority(75).
    StringParam("academic_id", "The academic node ID", true).
    Handler(func(ctx context.Context, input json.RawMessage) (any, error) {
        var params struct {
            AcademicID string `json:"academic_id"`
        }
        if err := json.Unmarshal(input, &params); err != nil {
            return nil, err
        }

        history, err := academic.vdb.GetHistoryForAcademic(ctx, params.AcademicID)
        if err != nil {
            return nil, err
        }
        return formatHistoryForAcademic(history), nil
    }).
    Build()

// RegisterAcademicSkills registers all Academic skills
func RegisterAcademicSkills(registry *skills.Registry) {
    registry.Register(ResearchSkill)
    registry.Register(FindBestPracticesSkill)
    registry.Register(FindPapersSkill)
    registry.Register(CompareApproachesSkill)
    registry.Register(SynthesizeWithCodebaseSkill)
    registry.Register(GetCodeForAcademicSkill)
    registry.Register(GetHistoryForAcademicSkill)
}
```

#### Skill Summary Table

| Agent | Skill | Description | Cross-Domain |
|-------|-------|-------------|--------------|
| **Librarian** | `search_code` | Semantic search for code | No |
| | `get_symbol` | Get symbol details | No |
| | `get_dependencies` | What a symbol depends on | No |
| | `get_dependents` | What depends on a symbol | No |
| | `find_similar_symbols` | Find similar code | No |
| | `get_file_symbols` | All symbols in a file | No |
| | `get_history_for_code` | History for code | Yes → History |
| **Archivalist** | `store_summary` | Store agent summaries | Yes → Code |
| | `search_history` | Search past context | No |
| | `find_patterns` | Find recurring patterns | No |
| | `find_failures` | Find past failures | No |
| | `get_session_history` | Session history | No |
| | `get_code_for_history` | Code for history | Yes → Code |
| | `get_decisions` | Past decisions | No |
| **Academic** | `research` | Deep research (Opus) | No |
| | `find_best_practices` | Best practices | No |
| | `find_papers` | Academic papers | No |
| | `compare_approaches` | Compare options (Opus) | No |
| | `synthesize_with_codebase` | Theory vs practice (Opus) | Yes → Code |
| | `get_code_for_academic` | Code for concept | Yes → Code |
| | `get_history_for_academic` | History for concept | Yes → History |

### Cross-Domain Edge Creation

Knowledge agents create cross-domain edges when relationships are discovered:

```go
// When Archivalist stores a summary that references code
func (a *Archivalist) StoreSummary(ctx context.Context, summary AgentSummary) error {
    // 1. Store in History domain
    historyNode, err := a.vdb.AddNode(ctx, &Node{
        ID:        summary.ID,
        Domain:    DomainHistory,
        NodeType:  NodeTypeSession,
        Content:   summary.Content,
        Metadata:  summary.Metadata,
    })

    // 2. Find referenced code and create cross-domain edges
    for _, codeRef := range summary.CodeReferences {
        a.vdb.AddEdge(ctx, historyNode.ID, codeRef.ID, EdgeTypeReferencedIn, 0.9, nil)
    }

    return nil
}

// When Librarian discovers code implements an academic pattern
func (l *Librarian) LinkToAcademic(ctx context.Context, codeID string, academicID string) error {
    return l.vdb.AddEdge(ctx, codeID, academicID, EdgeTypeImplements, 0.85, nil)
}

// When Academic finds research relevant to historical decisions
func (ac *Academic) LinkToHistory(ctx context.Context, academicID string, historyID string) error {
    return ac.vdb.AddEdge(ctx, academicID, historyID, EdgeTypeInspiredBy, 0.8, nil)
}
```

### Summary Storage Flow

All agent summaries flow through Guide to Archivalist:

```
┌─────────────────────────────────────────────────────────────────────────────────┐
│                        SUMMARY STORAGE FLOW                                     │
├─────────────────────────────────────────────────────────────────────────────────┤
│                                                                                 │
│  ┌────────────┐  ┌────────────┐  ┌────────────┐  ┌────────────┐               │
│  │ Architect  │  │Orchestrator│  │  Engineer  │  │  Inspector │               │
│  └─────┬──────┘  └─────┬──────┘  └─────┬──────┘  └─────┬──────┘               │
│        │               │               │               │                        │
│        │ Summary       │ Summary       │ Summary       │ Summary                │
│        │               │               │               │                        │
│        └───────────────┴───────────────┴───────────────┘                        │
│                                  │                                              │
│                                  ▼                                              │
│                        ┌─────────────────┐                                      │
│                        │      GUIDE      │                                      │
│                        │                 │                                      │
│                        │ Routes to:      │                                      │
│                        │ archivalist/    │                                      │
│                        │ store           │                                      │
│                        └────────┬────────┘                                      │
│                                 │                                               │
│                                 ▼                                               │
│                        ┌─────────────────┐                                      │
│                        │   ARCHIVALIST   │                                      │
│                        │                 │                                      │
│                        │ 1. Store node   │                                      │
│                        │ 2. Create edges │                                      │
│                        │ 3. Index embed  │                                      │
│                        └────────┬────────┘                                      │
│                                 │                                               │
│                                 ▼                                               │
│                        ┌─────────────────┐                                      │
│                        │  VectorGraphDB  │                                      │
│                        │                 │                                      │
│                        │ History Domain  │                                      │
│                        └─────────────────┘                                      │
│                                                                                 │
└─────────────────────────────────────────────────────────────────────────────────┘
```

### LLM Selection by Agent

Knowledge agents use appropriate LLM models based on task complexity:

| Agent | Model | Rationale |
|-------|-------|-----------|
| Librarian | Sonnet 4.5 | Fast code search, symbol navigation - speed critical |
| Archivalist | Sonnet 4.5 | Pattern matching, history queries - speed critical |
| Academic | Opus 4.5 | Complex research, synthesis - reasoning critical |

```go
// Guide selects model based on routed agent
func (g *Guide) selectModel(routedAgent string) string {
    switch routedAgent {
    case "librarian", "archivalist":
        return "claude-sonnet-4-5"  // Fast, efficient
    case "academic":
        return "claude-opus-4-5"    // Complex reasoning
    default:
        return "claude-sonnet-4-5"
    }
}
```

### Agentic RAG: Progressive Retrieval

Knowledge agents use progressive retrieval - the LLM decides when and how much to retrieve:

```go
// AgenticRAGConfig for progressive retrieval
type AgenticRAGConfig struct {
    // Initial retrieval count
    InitialK int

    // Maximum retrieval depth
    MaxDepth int

    // Confidence threshold to stop retrieving
    ConfidenceThreshold float64

    // Maximum tokens to spend on retrieval
    MaxRetrievalTokens int
}

// Progressive retrieval loop
func (agent *KnowledgeAgent) progressiveRetrieve(ctx context.Context, query string) ([]Node, error) {
    var accumulated []Node
    depth := 0
    confidence := 0.0

    for depth < agent.config.MaxDepth && confidence < agent.config.ConfidenceThreshold {
        // Retrieve next batch
        results, err := agent.vdb.SimilarNodes(ctx, query, agent.config.InitialK, nil)
        if err != nil {
            return accumulated, err
        }

        // LLM evaluates: do we have enough?
        evaluation := agent.llm.Evaluate(ctx, query, accumulated, results)

        if evaluation.Sufficient {
            confidence = evaluation.Confidence
            break
        }

        // LLM suggests next query refinement
        query = evaluation.RefinedQuery
        accumulated = append(accumulated, evaluation.SelectedNodes...)
        depth++
    }

    return accumulated, nil
}
```

---

## Session Integration with Agents

### Guide Session Awareness

```go
// Guide session-aware routing
func (g *Guide) Route(ctx context.Context, request *RouteRequest) (*ForwardedRequest, error) {
    // Ensure session context
    if request.SessionID == "" {
        request.SessionID = g.sessionManager.GetActive().ID
    }

    // Validate session exists and is active
    session, exists := g.sessionManager.Get(request.SessionID)
    if !exists {
        return nil, fmt.Errorf("session %s not found", request.SessionID)
    }
    if session.State != SessionStateActive {
        return nil, fmt.Errorf("session %s is not active (state: %s)", request.SessionID, session.State)
    }

    // Execute pre-prompt hooks with session context
    hookCtx := context.WithValue(ctx, "session", session)
    hookCtx = context.WithValue(hookCtx, "session_id", request.SessionID)

    // ... routing logic ...

    // Inject session_id into forwarded request
    forwarded.SessionID = request.SessionID

    return forwarded, nil
}
```

### Archivalist Session Awareness

```go
// Archivalist session-aware queries
func (a *Archivalist) Query(ctx context.Context, query ArchiveQuery) ([]*Entry, error) {
    sessionID := ctx.Value("session_id").(string)

    // Apply session filtering based on query type
    if query.CurrentOnly {
        query.SessionIDs = []string{sessionID}
    } else if !query.CrossSession {
        // Default: current session only for non-promoted entries
        query.SessionIDs = []string{sessionID}
        // But include promoted entries from all sessions
        query.IncludePromoted = true
    }
    // CrossSession=true: query all sessions explicitly

    return a.store.Query(query)
}

// Archivalist session-aware storage
func (a *Archivalist) StoreEntry(ctx context.Context, entry *Entry) SubmissionResult {
    sessionID := ctx.Value("session_id").(string)

    // Tag entry with session
    entry.SessionID = sessionID

    // ... storage logic ...
}
```

### Orchestrator Session Awareness

```go
// Orchestrator session-scoped DAG execution
func (o *Orchestrator) Execute(ctx context.Context, dag *DAG) error {
    sessionID := ctx.Value("session_id").(string)
    session := o.sessionManager.Get(sessionID)

    // Update session with active DAG
    session.ActiveDAGID = dag.ID

    // Create session-scoped engineers
    for i := 0; i < o.config.MaxEngineers; i++ {
        engineer := o.createEngineer(sessionID, i)
        session.Context.ActiveAgents[engineer.ID] = &AgentState{
            ID:     engineer.ID,
            Status: "ready",
        }
    }

    // Execute with session context
    return o.executeDAG(ctx, dag, session)
}
```

---

## Agent Efficiency Techniques

This section describes techniques to help agents work more efficiently, avoid repeated mistakes, and maintain consistency. These address common inefficiencies in LLM-based coding agents.

### Agent Pain Points Addressed

```
┌─────────────────────────────────────────────────────────────────────────────┐
│                    AGENT INEFFICIENCIES ADDRESSED                           │
├─────────────────────────────────────────────────────────────────────────────┤
│                                                                             │
│   CONTEXT LOSS                        REPEATED WORK                         │
│   ─────────────                       ─────────────                         │
│   • Forgets what it learned           • Reads same files repeatedly        │
│   • Loses track of current task       • Searches for same things           │
│   • Re-gathers context it just had    • Rebuilds context unnecessarily     │
│                                                                             │
│   PATTERN BLINDNESS                   STYLE DRIFT                           │
│   ────────────────                    ───────────                           │
│   • Doesn't recognize similar work    • Inconsistent code style            │
│   • Reinvents solved problems         • Mismatched UI patterns             │
│   • Misses reusable components        • Naming inconsistencies             │
│                                                                             │
│   MISTAKE REPETITION                  USER PREFERENCE AMNESIA              │
│   ─────────────────                   ───────────────────────              │
│   • Makes same errors repeatedly      • Forgets stated preferences         │
│   • Doesn't learn from failures       • Re-asks clarifying questions       │
│   • Falls into known pitfalls         • Ignores implicit preferences       │
│                                                                             │
└─────────────────────────────────────────────────────────────────────────────┘
```

### Technique Summary

| Technique | Problem Solved | Token Savings | Complexity |
|-----------|---------------|---------------|------------|
| **Scratchpad Memory** | Context loss within session | ~15% | Low |
| **Style Inference** | Style drift, inconsistency | ~10% | Medium |
| **Component Registry** | Duplicate components | ~20% | Medium |
| **Mistake Memory** | Repeated errors | ~15% | Medium |
| **Diff Preview** | Wrong edits | ~5% | Low |
| **User Preferences** | Re-asking, wrong choices | ~10% | Medium |
| **File Snapshot Cache** | Re-reading files | ~10% | Low |
| **Dependency Awareness** | Duplicate deps | ~5% | Low |
| **Task Continuity** | Context rebuild on resume | ~20% | Medium |
| **Design Token Awareness** | UI inconsistency | ~5% | Low |

---

### 1. Scratchpad Memory (Working Memory)

Within-session memory that persists across tool calls. Agents write notes to themselves.

```go
// Scratchpad provides within-session working memory for agents
type Scratchpad struct {
    mu      sync.RWMutex
    notes   map[string]ScratchpadNote
    session string
}

type ScratchpadNote struct {
    Key       string    `json:"key"`
    Content   string    `json:"content"`
    CreatedAt time.Time `json:"created_at"`
    UpdatedAt time.Time `json:"updated_at"`
    TTL       time.Duration `json:"ttl,omitempty"`
}

// Skills for scratchpad access
var ScratchpadWriteSkill = skills.NewSkill("scratchpad_write").
    Description("Write a note to yourself for later reference in this session. Use for tracking progress, decisions, things tried, or user preferences.").
    Domain("memory").
    Keywords("remember", "note", "track", "save").
    Priority(100).
    StringParam("key", "Short key for this note (e.g., 'current_task', 'user_preference', 'tried_failed')", true).
    StringParam("note", "The note content", true).
    Handler(func(ctx context.Context, input json.RawMessage) (any, error) {
        var params struct {
            Key  string `json:"key"`
            Note string `json:"note"`
        }
        json.Unmarshal(input, &params)

        session := getSession(ctx)
        session.Scratchpad.Set(params.Key, params.Note)

        return map[string]any{
            "stored": true,
            "key":    params.Key,
        }, nil
    }).
    Build()

var ScratchpadReadSkill = skills.NewSkill("scratchpad_read").
    Description("Read a note you wrote earlier in this session.").
    Domain("memory").
    StringParam("key", "Key to read (or 'all' for all notes)", true).
    Handler(func(ctx context.Context, input json.RawMessage) (any, error) {
        var params struct {
            Key string `json:"key"`
        }
        json.Unmarshal(input, &params)

        session := getSession(ctx)

        if params.Key == "all" {
            return session.Scratchpad.GetAll(), nil
        }

        note, exists := session.Scratchpad.Get(params.Key)
        if !exists {
            return map[string]any{"exists": false}, nil
        }

        return map[string]any{
            "exists":  true,
            "content": note.Content,
        }, nil
    }).
    Build()
```

**Use cases:**
```
Agent: scratchpad_write("current_approach", "Using JWT with refresh tokens, user confirmed")
Agent: scratchpad_write("tried_failed", "Redis caching failed - version mismatch with existing redis 5.x")
Agent: scratchpad_write("user_prefers", "Functional components, minimal comments, early returns")
Agent: scratchpad_write("files_modified", "auth/handler.go, auth/middleware.go, auth/types.go")

Later in session...
Agent: scratchpad_read("current_approach")
  → "Using JWT with refresh tokens, user confirmed"
Agent: scratchpad_read("tried_failed")
  → "Redis caching failed - version mismatch with existing redis 5.x"
```

**Token savings**: Avoids re-asking user, avoids re-gathering context, tracks what was tried.

---

### 2. Code Style Inference

Automatically analyze and enforce codebase style without agent effort.

```go
// StyleInference analyzes codebase to infer coding conventions
type StyleInference struct {
    mu sync.RWMutex

    // Inferred styles by language
    styles map[string]*LanguageStyle

    // Last analysis time
    analyzedAt time.Time
}

type LanguageStyle struct {
    Language string `json:"language"`

    // Naming conventions
    Naming NamingStyle `json:"naming"`

    // Formatting
    Formatting FormattingStyle `json:"formatting"`

    // Patterns
    Patterns PatternStyle `json:"patterns"`

    // Examples from codebase
    Examples []StyleExample `json:"examples,omitempty"`
}

type NamingStyle struct {
    Functions   string `json:"functions"`    // camelCase, snake_case, PascalCase
    Variables   string `json:"variables"`
    Constants   string `json:"constants"`
    Types       string `json:"types"`
    Files       string `json:"files"`        // kebab-case, snake_case, camelCase
    Components  string `json:"components"`   // For React/Vue
}

type FormattingStyle struct {
    IndentStyle string `json:"indent_style"` // tabs, spaces
    IndentSize  int    `json:"indent_size"`
    QuoteStyle  string `json:"quote_style"`  // single, double
    Semicolons  bool   `json:"semicolons"`
    TrailingComma string `json:"trailing_comma"` // all, es5, none
}

type PatternStyle struct {
    ErrorHandling  string `json:"error_handling"`  // try-catch, Result, early-return
    AsyncStyle     string `json:"async_style"`     // async-await, promises, callbacks
    ComponentStyle string `json:"component_style"` // functional, class
    ExportStyle    string `json:"export_style"`    // named, default, mixed
    ImportOrder    []string `json:"import_order"`  // ["builtin", "external", "internal", "relative"]
}

// Skill to get style guide
var GetStyleGuideSkill = skills.NewSkill("get_style_guide").
    Description("Get the inferred code style guide for this project. Use BEFORE writing code to match existing patterns.").
    Domain("code").
    Keywords("style", "convention", "format", "naming").
    Priority(90).
    StringParam("language", "Language to get style for (or 'all')", false).
    Handler(func(ctx context.Context, input json.RawMessage) (any, error) {
        var params struct {
            Language string `json:"language"`
        }
        json.Unmarshal(input, &params)

        if params.Language == "" || params.Language == "all" {
            return styleInference.GetAll(), nil
        }

        return styleInference.Get(params.Language), nil
    }).
    Build()
```

**Example response:**
```json
{
  "language": "typescript",
  "naming": {
    "functions": "camelCase",
    "variables": "camelCase",
    "constants": "UPPER_SNAKE_CASE",
    "types": "PascalCase",
    "files": "kebab-case",
    "components": "PascalCase"
  },
  "formatting": {
    "indent_style": "spaces",
    "indent_size": 2,
    "quote_style": "single",
    "semicolons": false,
    "trailing_comma": "es5"
  },
  "patterns": {
    "error_handling": "try-catch with custom errors",
    "async_style": "async-await",
    "component_style": "functional with hooks",
    "export_style": "named exports",
    "import_order": ["react", "external", "internal", "relative", "styles"]
  },
  "examples": [
    {"pattern": "error_handling", "file": "src/utils/api.ts", "line": 45}
  ]
}
```

**Token savings**: Agent doesn't guess style, produces consistent code first time.

---

### 3. Component/Pattern Registry

Know what reusable components and patterns exist before writing new ones.

```go
// ComponentRegistry indexes reusable code artifacts
type ComponentRegistry struct {
    mu sync.RWMutex

    // Indexed components by type
    uiComponents []UIComponent
    hooks        []Hook
    utilities    []Utility
    patterns     []Pattern

    // Search index
    searchIndex *SearchIndex
}

type UIComponent struct {
    Name        string            `json:"name"`
    Path        string            `json:"path"`
    Description string            `json:"description"`
    Props       []PropDefinition  `json:"props"`
    UsageExample string           `json:"usage_example"`
    Tags        []string          `json:"tags"`
    Variants    []string          `json:"variants,omitempty"`
}

type Hook struct {
    Name        string   `json:"name"`
    Path        string   `json:"path"`
    Description string   `json:"description"`
    Parameters  []Param  `json:"parameters"`
    Returns     string   `json:"returns"`
    UsageExample string  `json:"usage_example"`
}

type Utility struct {
    Name        string   `json:"name"`
    Path        string   `json:"path"`
    Description string   `json:"description"`
    Signature   string   `json:"signature"`
    UsageExample string  `json:"usage_example"`
}

// Skill to search for existing components
var FindComponentSkill = skills.NewSkill("find_component").
    Description("Search for existing reusable components, hooks, or utilities BEFORE writing new ones. Prevents duplication.").
    Domain("code").
    Keywords("component", "hook", "utility", "reuse", "existing").
    Priority(95).
    StringParam("need", "What you need (e.g., 'modal dialog', 'date formatting', 'auth hook')", true).
    EnumParam("type", "Type to search for", []string{"ui", "hook", "utility", "pattern", "any"}, false).
    Handler(func(ctx context.Context, input json.RawMessage) (any, error) {
        var params struct {
            Need string `json:"need"`
            Type string `json:"type"`
        }
        json.Unmarshal(input, &params)

        if params.Type == "" {
            params.Type = "any"
        }

        matches := componentRegistry.Search(params.Need, params.Type)

        if len(matches) == 0 {
            return map[string]any{
                "found": false,
                "suggestion": "No existing component found. You may need to create one.",
            }, nil
        }

        return map[string]any{
            "found":   true,
            "matches": formatComponentMatches(matches),
        }, nil
    }).
    Build()
```

**Example usage:**
```
Agent: find_component("confirmation dialog", type="ui")
Response: {
  "found": true,
  "matches": [
    {
      "name": "ConfirmModal",
      "path": "src/components/modals/ConfirmModal.tsx",
      "description": "Reusable confirmation dialog with customizable actions",
      "props": [
        {"name": "title", "type": "string", "required": true},
        {"name": "message", "type": "string", "required": true},
        {"name": "onConfirm", "type": "() => void", "required": true},
        {"name": "onCancel", "type": "() => void", "required": false},
        {"name": "confirmText", "type": "string", "default": "Confirm"},
        {"name": "variant", "type": "'danger' | 'warning' | 'info'", "default": "warning"}
      ],
      "usage_example": "<ConfirmModal\n  title=\"Delete Item?\"\n  message=\"This cannot be undone.\"\n  onConfirm={handleDelete}\n  variant=\"danger\"\n/>"
    }
  ]
}
```

Agent uses existing `ConfirmModal` instead of creating a duplicate.

**Token savings**: Avoids writing duplicate components, ensures UI consistency.

---

### 4. Mistake Memory (Anti-Pattern Database)

Learn from failures and share knowledge across sessions.

```go
// MistakeMemory stores known anti-patterns and failures
type MistakeMemory struct {
    mu sync.RWMutex

    mistakes []Mistake
    index    *SearchIndex

    // Persistence
    storage Storage
}

type Mistake struct {
    ID          string    `json:"id"`
    Pattern     string    `json:"pattern"`      // What was tried
    Error       string    `json:"error"`        // What went wrong
    Fix         string    `json:"fix"`          // How to fix/avoid
    Context     string    `json:"context"`      // When this applies
    Tags        []string  `json:"tags"`
    Confidence  float64   `json:"confidence"`   // How reliable (0-1)
    Occurrences int       `json:"occurrences"`  // Times encountered
    CreatedAt   time.Time `json:"created_at"`
    LastSeenAt  time.Time `json:"last_seen_at"`
}

// Skill to check for known mistakes
var CheckMistakesSkill = skills.NewSkill("check_mistakes").
    Description("Check if an approach has known issues BEFORE trying it. Learns from past failures across all sessions.").
    Domain("memory").
    Keywords("mistake", "issue", "problem", "avoid", "pitfall").
    Priority(95).
    StringParam("approach", "What you're planning to do", true).
    Handler(func(ctx context.Context, input json.RawMessage) (any, error) {
        var params struct {
            Approach string `json:"approach"`
        }
        json.Unmarshal(input, &params)

        matches := mistakeMemory.Search(params.Approach)

        if len(matches) == 0 {
            return map[string]any{
                "known_issues": false,
                "safe_to_proceed": true,
            }, nil
        }

        return map[string]any{
            "known_issues": true,
            "warnings": formatMistakeWarnings(matches),
        }, nil
    }).
    Build()

// Skill to report a new mistake
var ReportMistakeSkill = skills.NewSkill("report_mistake").
    Description("Report a mistake or anti-pattern you discovered. Helps future sessions avoid it.").
    Domain("memory").
    Keywords("report", "learned", "discovered", "anti-pattern").
    Priority(80).
    StringParam("pattern", "What was tried", true).
    StringParam("error", "What went wrong", true).
    StringParam("fix", "How to fix or avoid this", true).
    StringParam("context", "When this applies (optional)", false).
    ArrayParam("tags", "Tags for categorization", "string", false).
    Handler(func(ctx context.Context, input json.RawMessage) (any, error) {
        var params struct {
            Pattern string   `json:"pattern"`
            Error   string   `json:"error"`
            Fix     string   `json:"fix"`
            Context string   `json:"context"`
            Tags    []string `json:"tags"`
        }
        json.Unmarshal(input, &params)

        mistake := &Mistake{
            ID:          generateID("mistake"),
            Pattern:     params.Pattern,
            Error:       params.Error,
            Fix:         params.Fix,
            Context:     params.Context,
            Tags:        params.Tags,
            Confidence:  0.7, // Initial confidence
            Occurrences: 1,
            CreatedAt:   time.Now(),
            LastSeenAt:  time.Now(),
        }

        mistakeMemory.Add(mistake)

        return map[string]any{
            "recorded": true,
            "id":       mistake.ID,
        }, nil
    }).
    Build()
```

**Example - checking before trying:**
```
Agent: check_mistakes("using localStorage for auth tokens")
Response: {
  "known_issues": true,
  "warnings": [
    {
      "pattern": "storing auth tokens in localStorage",
      "error": "XSS vulnerability - tokens accessible via JavaScript injection",
      "fix": "Use httpOnly cookies for sensitive tokens, or sessionStorage for less sensitive data",
      "context": "Web applications with authentication",
      "confidence": 0.95,
      "occurrences": 12
    }
  ]
}
Agent: "I'll use httpOnly cookies instead"
```

**Example - reporting a new mistake:**
```
Agent: report_mistake(
  pattern="using moment.js for new projects",
  error="moment.js is deprecated and has large bundle size",
  fix="Use date-fns or dayjs instead - smaller, tree-shakeable",
  context="JavaScript/TypeScript date handling",
  tags=["javascript", "dependencies", "performance"]
)
```

**Token savings**: Avoids making known mistakes, avoids debugging sessions.

---

### 5. Diff Preview

Show what will change before making changes. Catches errors early.

```go
// DiffPreview shows changes before applying them
type DiffPreview struct {
    Diff          string   `json:"diff"`
    LinesAdded    int      `json:"lines_added"`
    LinesRemoved  int      `json:"lines_removed"`
    FilesAffected int      `json:"files_affected"`
    Warnings      []string `json:"warnings,omitempty"`
    Reversible    bool     `json:"reversible"`
}

var PreviewEditSkill = skills.NewSkill("preview_edit").
    Description("Preview what an edit will look like BEFORE applying it. Shows diff and potential issues.").
    Domain("code").
    Keywords("preview", "diff", "check", "before").
    Priority(85).
    StringParam("file_path", "File to edit", true).
    StringParam("old_string", "Text to replace", true).
    StringParam("new_string", "Replacement text", true).
    Handler(func(ctx context.Context, input json.RawMessage) (any, error) {
        var params struct {
            FilePath  string `json:"file_path"`
            OldString string `json:"old_string"`
            NewString string `json:"new_string"`
        }
        json.Unmarshal(input, &params)

        // Read current file
        content, err := readFile(params.FilePath)
        if err != nil {
            return nil, err
        }

        // Check if old_string exists
        if !strings.Contains(content, params.OldString) {
            return map[string]any{
                "error": "old_string not found in file",
                "suggestion": "Check for whitespace differences or use a larger context",
            }, nil
        }

        // Check for multiple matches
        matchCount := strings.Count(content, params.OldString)

        // Generate preview
        newContent := strings.Replace(content, params.OldString, params.NewString, 1)
        diff := generateUnifiedDiff(params.FilePath, content, newContent)

        preview := DiffPreview{
            Diff:          diff,
            LinesAdded:    countLines(params.NewString),
            LinesRemoved:  countLines(params.OldString),
            FilesAffected: 1,
            Reversible:    true,
        }

        if matchCount > 1 {
            preview.Warnings = append(preview.Warnings,
                fmt.Sprintf("old_string matches %d times - only first will be replaced", matchCount))
        }

        return preview, nil
    }).
    Build()
```

**Example:**
```
Agent: preview_edit(
  file_path="src/auth/handler.ts",
  old_string="const token = localStorage.getItem('token')",
  new_string="const token = await getSecureToken()"
)
Response: {
  "diff": "--- src/auth/handler.ts\n+++ src/auth/handler.ts\n@@ -45,7 +45,7 @@\n-  const token = localStorage.getItem('token')\n+  const token = await getSecureToken()",
  "lines_added": 1,
  "lines_removed": 1,
  "files_affected": 1,
  "warnings": [],
  "reversible": true
}
```

**Token savings**: Catches errors before they happen, avoids fix-up cycles.

---

### 6. User Preference Learning

Automatically learn and apply user preferences.

```go
// UserPreferences tracks explicit and inferred preferences
type UserPreferences struct {
    mu sync.RWMutex

    // Explicitly stated by user
    Explicit map[string]string `json:"explicit"`

    // Inferred from behavior
    Implicit map[string]InferredPreference `json:"implicit"`

    // Session overrides (temporary)
    SessionOverrides map[string]string `json:"session_overrides,omitempty"`
}

type InferredPreference struct {
    Value       string   `json:"value"`
    Confidence  float64  `json:"confidence"`  // 0-1
    Evidence    []string `json:"evidence"`    // Why we think this
    ObservedAt  time.Time `json:"observed_at"`
}

// Learn from user feedback
func (p *UserPreferences) LearnFromFeedback(category, suggestion, response string) {
    p.mu.Lock()
    defer p.mu.Unlock()

    existing, exists := p.Implicit[category]

    if response == "accepted" {
        if exists && existing.Value == suggestion {
            // Reinforce
            existing.Confidence = min(existing.Confidence + 0.1, 1.0)
            existing.Evidence = append(existing.Evidence, "accepted: "+suggestion)
        } else {
            // New preference
            p.Implicit[category] = InferredPreference{
                Value:      suggestion,
                Confidence: 0.6,
                Evidence:   []string{"accepted: " + suggestion},
                ObservedAt: time.Now(),
            }
        }
    } else if response == "rejected" {
        if exists && existing.Value == suggestion {
            // Diminish confidence
            existing.Confidence = max(existing.Confidence - 0.2, 0)
        }
    }
}

// Skill to get preferences
var GetPreferencesSkill = skills.NewSkill("get_preferences").
    Description("Get known user preferences to inform your approach. Includes explicit and inferred preferences.").
    Domain("memory").
    Keywords("preference", "like", "prefer", "style").
    Priority(90).
    StringParam("category", "Category: code_style, ui, workflow, tools, all", false).
    Handler(func(ctx context.Context, input json.RawMessage) (any, error) {
        var params struct {
            Category string `json:"category"`
        }
        json.Unmarshal(input, &params)

        if params.Category == "" || params.Category == "all" {
            return userPreferences.GetAll(), nil
        }

        return userPreferences.GetCategory(params.Category), nil
    }).
    Build()

// Skill to set explicit preference
var SetPreferenceSkill = skills.NewSkill("set_preference").
    Description("Record an explicit user preference.").
    Domain("memory").
    StringParam("category", "Preference category", true).
    StringParam("preference", "The preference value", true).
    Handler(func(ctx context.Context, input json.RawMessage) (any, error) {
        var params struct {
            Category   string `json:"category"`
            Preference string `json:"preference"`
        }
        json.Unmarshal(input, &params)

        userPreferences.SetExplicit(params.Category, params.Preference)

        return map[string]any{"stored": true}, nil
    }).
    Build()
```

**Example response:**
```json
{
  "explicit": {
    "framework": "React with TypeScript",
    "styling": "Tailwind CSS",
    "testing": "Prefer integration tests over unit tests",
    "comments": "Only for complex logic"
  },
  "implicit": {
    "error_handling": {
      "value": "early return pattern",
      "confidence": 0.85,
      "evidence": ["accepted in auth.ts", "accepted in api.ts", "used in user code"]
    },
    "component_size": {
      "value": "small, single responsibility",
      "confidence": 0.78,
      "evidence": ["broke up large component when suggested", "approved split"]
    },
    "variable_naming": {
      "value": "descriptive, avoid abbreviations",
      "confidence": 0.72,
      "evidence": ["renamed 'usr' to 'user'", "renamed 'btn' to 'button'"]
    }
  }
}
```

**Token savings**: Avoids re-asking user, makes choices that match expectations.

---

### 7. File Snapshot Cache

Avoid re-reading files that haven't changed within a session.

```go
// FileSnapshotCache caches file contents within a session
type FileSnapshotCache struct {
    mu        sync.RWMutex
    snapshots map[string]*FileSnapshot
    watcher   *fsnotify.Watcher
    maxAge    time.Duration
}

type FileSnapshot struct {
    Path       string    `json:"path"`
    Content    string    `json:"-"`  // Not serialized
    Hash       string    `json:"hash"`
    Size       int64     `json:"size"`
    ModTime    time.Time `json:"mod_time"`
    ReadAt     time.Time `json:"read_at"`
    TokenCount int       `json:"token_count"`
    HitCount   int       `json:"hit_count"`
}

// GetOrRead returns cached content or reads fresh
func (c *FileSnapshotCache) GetOrRead(ctx context.Context, path string) (*FileSnapshot, bool, error) {
    c.mu.RLock()
    snapshot, exists := c.snapshots[path]
    c.mu.RUnlock()

    if exists {
        // Check if file has changed
        stat, err := os.Stat(path)
        if err == nil && stat.ModTime().Equal(snapshot.ModTime) && stat.Size() == snapshot.Size {
            // Cache hit
            c.mu.Lock()
            snapshot.HitCount++
            c.mu.Unlock()
            return snapshot, true, nil
        }
    }

    // Cache miss - read file
    content, err := os.ReadFile(path)
    if err != nil {
        return nil, false, err
    }

    stat, _ := os.Stat(path)

    snapshot = &FileSnapshot{
        Path:       path,
        Content:    string(content),
        Hash:       hashContent(content),
        Size:       stat.Size(),
        ModTime:    stat.ModTime(),
        ReadAt:     time.Now(),
        TokenCount: estimateTokens(string(content)),
        HitCount:   0,
    }

    c.mu.Lock()
    c.snapshots[path] = snapshot
    c.mu.Unlock()

    return snapshot, false, nil
}

// Integrated into Read skill handler
func enhancedReadHandler(ctx context.Context, input json.RawMessage) (any, error) {
    var params struct {
        FilePath string `json:"file_path"`
    }
    json.Unmarshal(input, &params)

    snapshot, cacheHit, err := fileCache.GetOrRead(ctx, params.FilePath)
    if err != nil {
        return nil, err
    }

    return FileReadResponse{
        Content:   snapshot.Content,
        CacheHit:  cacheHit,
        TokenCount: snapshot.TokenCount,
    }, nil
}
```

**Token savings**: Avoids re-reading unchanged files (common in iterative work).

---

### 8. Dependency Awareness

Know what's available before suggesting new dependencies.

```go
// DependencyRegistry tracks project dependencies and their capabilities
type DependencyRegistry struct {
    mu sync.RWMutex

    // Parsed from package.json, go.mod, requirements.txt, etc.
    dependencies map[string]*Dependency

    // Capability index
    capabilities map[string][]*Dependency  // capability -> deps that provide it
}

type Dependency struct {
    Name         string            `json:"name"`
    Version      string            `json:"version"`
    Type         string            `json:"type"`  // runtime, dev, peer
    Capabilities []string          `json:"capabilities"`
    UsageExamples map[string]string `json:"usage_examples"`
}

// Skill to check for existing capabilities
var CheckDependencySkill = skills.NewSkill("check_dependency").
    Description("Check if a capability is already available via existing dependencies BEFORE suggesting a new one.").
    Domain("code").
    Keywords("dependency", "package", "library", "install").
    Priority(90).
    StringParam("need", "What capability you need (e.g., 'date formatting', 'HTTP client', 'form validation')", true).
    Handler(func(ctx context.Context, input json.RawMessage) (any, error) {
        var params struct {
            Need string `json:"need"`
        }
        json.Unmarshal(input, &params)

        existing := dependencyRegistry.FindCapability(params.Need)

        if len(existing) > 0 {
            return map[string]any{
                "already_available": true,
                "packages": formatDependencies(existing),
            }, nil
        }

        // Suggest alternatives
        suggestions := dependencyRegistry.SuggestPackages(params.Need)

        return map[string]any{
            "already_available": false,
            "suggestions":       suggestions,
        }, nil
    }).
    Build()
```

**Example:**
```
Agent: check_dependency("date formatting")
Response: {
  "already_available": true,
  "packages": [
    {
      "name": "date-fns",
      "version": "2.30.0",
      "type": "runtime",
      "capabilities": ["date formatting", "date parsing", "date manipulation"],
      "usage_examples": {
        "format": "import { format } from 'date-fns'\nformat(new Date(), 'yyyy-MM-dd')",
        "parse": "import { parse } from 'date-fns'\nparse('2024-01-15', 'yyyy-MM-dd', new Date())"
      }
    }
  ]
}
Agent: "I'll use the existing date-fns instead of adding moment.js"
```

**Token savings**: Avoids adding duplicate dependencies, knows how to use existing ones.

---

### 9. Task Continuity (Checkpoint/Resume)

Resume interrupted work without re-gathering context.

```go
// TaskCheckpoint saves progress for later resumption
type TaskCheckpoint struct {
    ID          string          `json:"id"`
    SessionID   string          `json:"session_id"`
    Description string          `json:"description"`
    Status      string          `json:"status"`  // in_progress, blocked, paused, completed

    // Progress tracking
    Progress    []string        `json:"progress"`     // Steps completed
    NextSteps   []string        `json:"next_steps"`   // What's remaining

    // Context preservation
    Context     map[string]any  `json:"context"`      // Gathered context
    FilesRead   []string        `json:"files_read"`   // Files already read
    FilesModi   []string        `json:"files_modified"` // Files changed
    Decisions   []Decision      `json:"decisions"`    // Decisions made

    // Scratchpad at time of checkpoint
    Scratchpad  map[string]string `json:"scratchpad"`

    CreatedAt   time.Time       `json:"created_at"`
    UpdatedAt   time.Time       `json:"updated_at"`
}

type Decision struct {
    Question string `json:"question"`
    Choice   string `json:"choice"`
    Reason   string `json:"reason"`
}

// Skill to save checkpoint
var SaveCheckpointSkill = skills.NewSkill("save_checkpoint").
    Description("Save your current progress so you can resume later if interrupted. Include what you've done and what's next.").
    Domain("memory").
    Keywords("save", "checkpoint", "pause", "resume later").
    Priority(85).
    StringParam("summary", "Brief summary of what you've done so far", true).
    ArrayParam("completed", "Steps you've completed", "string", true).
    ArrayParam("next_steps", "Steps remaining", "string", true).
    StringParam("status", "Status: in_progress, blocked, paused", false).
    Handler(func(ctx context.Context, input json.RawMessage) (any, error) {
        var params struct {
            Summary   string   `json:"summary"`
            Completed []string `json:"completed"`
            NextSteps []string `json:"next_steps"`
            Status    string   `json:"status"`
        }
        json.Unmarshal(input, &params)

        session := getSession(ctx)

        checkpoint := &TaskCheckpoint{
            ID:          generateID("checkpoint"),
            SessionID:   session.ID,
            Description: params.Summary,
            Status:      params.Status,
            Progress:    params.Completed,
            NextSteps:   params.NextSteps,
            FilesRead:   session.FilesRead,
            FilesModi:   session.FilesModified,
            Scratchpad:  session.Scratchpad.GetAll(),
            CreatedAt:   time.Now(),
            UpdatedAt:   time.Now(),
        }

        checkpointStore.Save(checkpoint)

        return map[string]any{
            "saved": true,
            "id":    checkpoint.ID,
            "hint":  "Use resume_task with this ID to continue later",
        }, nil
    }).
    Build()

// Skill to resume from checkpoint
var ResumeTaskSkill = skills.NewSkill("resume_task").
    Description("Resume a previously saved task with full context restored.").
    Domain("memory").
    Keywords("resume", "continue", "pick up").
    Priority(100).
    StringParam("task_id", "Checkpoint ID to resume (or 'latest' for most recent)", true).
    Handler(func(ctx context.Context, input json.RawMessage) (any, error) {
        var params struct {
            TaskID string `json:"task_id"`
        }
        json.Unmarshal(input, &params)

        var checkpoint *TaskCheckpoint
        if params.TaskID == "latest" {
            checkpoint = checkpointStore.GetLatest()
        } else {
            checkpoint = checkpointStore.Get(params.TaskID)
        }

        if checkpoint == nil {
            return map[string]any{
                "found": false,
                "error": "No checkpoint found",
            }, nil
        }

        // Restore scratchpad
        session := getSession(ctx)
        for k, v := range checkpoint.Scratchpad {
            session.Scratchpad.Set(k, v)
        }

        return TaskResume{
            ID:            checkpoint.ID,
            Summary:       checkpoint.Description,
            Completed:     checkpoint.Progress,
            NextSteps:     checkpoint.NextSteps,
            FilesRead:     checkpoint.FilesRead,
            FilesModified: checkpoint.FilesModi,
            Scratchpad:    checkpoint.Scratchpad,
            Hint:          fmt.Sprintf("Continue with: %s", checkpoint.NextSteps[0]),
        }, nil
    }).
    Build()
```

**Example - saving:**
```
Agent: save_checkpoint(
  summary="Implementing JWT auth - middleware done, need handler",
  completed=["Created auth types", "Implemented JWT middleware", "Added config"],
  next_steps=["Implement login handler", "Implement refresh endpoint", "Add tests"],
  status="paused"
)
Response: {"saved": true, "id": "checkpoint_abc123"}
```

**Example - resuming:**
```
Agent: resume_task("checkpoint_abc123")
Response: {
  "id": "checkpoint_abc123",
  "summary": "Implementing JWT auth - middleware done, need handler",
  "completed": ["Created auth types", "Implemented JWT middleware", "Added config"],
  "next_steps": ["Implement login handler", "Implement refresh endpoint", "Add tests"],
  "files_read": ["auth/types.go", "auth/middleware.go", "config/config.go"],
  "files_modified": ["auth/types.go", "auth/middleware.go"],
  "scratchpad": {
    "user_prefers": "Early returns, minimal error wrapping",
    "token_expiry": "15 minutes access, 7 days refresh - user confirmed"
  },
  "hint": "Continue with: Implement login handler"
}
```

**Token savings**: Avoids re-gathering context after interruption.

---

### 10. Design Token Awareness

For UI work, know the design system and use consistent values.

```go
// DesignSystem stores design tokens and patterns
type DesignSystem struct {
    mu sync.RWMutex

    // Core tokens
    Colors     map[string]string `json:"colors"`
    Spacing    map[string]string `json:"spacing"`
    Typography map[string]TypographyToken `json:"typography"`
    Shadows    map[string]string `json:"shadows"`
    Radii      map[string]string `json:"radii"`

    // Component patterns
    ComponentPatterns map[string]ComponentPattern `json:"component_patterns"`

    // Source files
    Sources    []string `json:"sources"`  // Where tokens are defined
}

type TypographyToken struct {
    FontFamily string `json:"font_family"`
    FontSize   string `json:"font_size"`
    FontWeight string `json:"font_weight"`
    LineHeight string `json:"line_height"`
}

type ComponentPattern struct {
    Name        string   `json:"name"`
    Description string   `json:"description"`
    Tokens      []string `json:"tokens"`  // Design tokens used
    Example     string   `json:"example"`
}

// Skill to get design tokens
var GetDesignTokensSkill = skills.NewSkill("get_design_tokens").
    Description("Get the project's design tokens. Use these values instead of hardcoding colors, spacing, etc.").
    Domain("ui").
    Keywords("design", "token", "color", "spacing", "theme").
    Priority(90).
    EnumParam("category", "Token category", []string{"colors", "spacing", "typography", "shadows", "radii", "all"}, false).
    Handler(func(ctx context.Context, input json.RawMessage) (any, error) {
        var params struct {
            Category string `json:"category"`
        }
        json.Unmarshal(input, &params)

        if params.Category == "" || params.Category == "all" {
            return designSystem.GetAll(), nil
        }

        return designSystem.GetCategory(params.Category), nil
    }).
    Build()

// Skill to find token by value
var FindDesignTokenSkill = skills.NewSkill("find_design_token").
    Description("Find the design token name for a hardcoded value. Helps replace magic values with tokens.").
    Domain("ui").
    StringParam("value", "The value to find (e.g., '#3B82F6', '16px')", true).
    Handler(func(ctx context.Context, input json.RawMessage) (any, error) {
        var params struct {
            Value string `json:"value"`
        }
        json.Unmarshal(input, &params)

        matches := designSystem.FindByValue(params.Value)

        if len(matches) == 0 {
            return map[string]any{
                "found": false,
                "suggestion": "No matching token found. Consider adding one or using a similar existing token.",
            }, nil
        }

        return map[string]any{
            "found":   true,
            "matches": matches,
        }, nil
    }).
    Build()
```

**Example - getting tokens:**
```
Agent: get_design_tokens(category="colors")
Response: {
  "colors": {
    "primary": "#3B82F6",
    "primary-hover": "#2563EB",
    "primary-light": "#DBEAFE",
    "secondary": "#6B7280",
    "error": "#EF4444",
    "error-light": "#FEE2E2",
    "success": "#10B981",
    "warning": "#F59E0B",
    "text": "#1F2937",
    "text-muted": "#6B7280",
    "background": "#FFFFFF",
    "background-secondary": "#F9FAFB",
    "border": "#E5E7EB"
  }
}
```

**Example - finding a token:**
```
Agent: find_design_token(value="#3B82F6")
Response: {
  "found": true,
  "matches": [
    {"category": "colors", "name": "primary", "value": "#3B82F6"}
  ]
}
Agent: "I should use 'colors.primary' instead of hardcoding '#3B82F6'"
```

**Token savings**: Ensures UI consistency, avoids hardcoded values that break themes.

---

### Agent-to-Technique Mapping

Each efficiency technique is applicable to specific agents based on their responsibilities:

```
┌─────────────────────────────────────────────────────────────────────────────────────────────────────────┐
│                              AGENT EFFICIENCY TECHNIQUE MATRIX                                           │
├────────────────────────┬────────┬──────────┬──────────┬──────────┬────────┬──────────┬─────────────────┤
│ Technique              │ Router │ Architect│ Engineer │ Inspector│ Tester │ Designer │ Knowledge RAGs  │
├────────────────────────┼────────┼──────────┼──────────┼──────────┼────────┼──────────┼─────────────────┤
│ Scratchpad Memory      │   -    │    ✓     │    ✓     │    ✓     │   ✓    │    ✓     │       -         │
│ Style Inference        │   -    │    -     │    ✓     │    ✓     │   ✓    │    -     │       -         │
│ Component Registry     │   -    │    -     │    ✓     │    -     │   -    │    ✓     │       -         │
│ Mistake Memory         │   -    │    -     │    ✓     │    ✓     │   ✓    │    -     │  Archivalist*   │
│ Diff Preview           │   -    │    -     │    ✓     │    -     │   -    │    ✓     │       -         │
│ User Preferences       │   ✓    │    ✓     │    ✓     │    -     │   -    │    ✓     │       -         │
│ File Snapshot Cache    │   -    │    -     │    ✓     │    ✓     │   ✓    │    -     │   Librarian*    │
│ Dependency Awareness   │   -    │    -     │    ✓     │    ✓     │   ✓    │    -     │   Academic*     │
│ Task Continuity        │   -    │    ✓     │    ✓     │    -     │   ✓    │    -     │       -         │
│ Design Token Awareness │   -    │    -     │    ✓     │    ✓     │   -    │    ✓     │       -         │
├────────────────────────┼────────┼──────────┼──────────┼──────────┼────────┼──────────┼─────────────────┤
│ TOTAL TECHNIQUES       │   1    │    3     │   10     │    6     │   6    │    5     │      3*         │
└────────────────────────┴────────┴──────────┴──────────┴──────────┴────────┴──────────┴─────────────────┘

* Knowledge RAG agents (Librarian, Archivalist, Academic) serve as SOURCES for some techniques:
  - Archivalist: SOURCE for Mistake Memory (provides historical error data)
  - Librarian: SOURCE for File Snapshot Cache (indexes code files)
  - Academic: SOURCE for Dependency Awareness (provides best practices)
```

#### Agent-Specific Rationale

**Router (Guide)**
- **User Preferences only** - Can route based on preferred workflow
- No code writing, no memory needs, no task state

**Architect**
- **Scratchpad** - Track requirements, decisions, blockers during planning
- **User Preferences** - Adapt verbosity, detail level to user style
- **Task Continuity** - Multi-step planning can be interrupted

**Engineer**
- **All 10 techniques** - Primary code writer benefits from everything
- Critical: Style Inference, Component Registry, Mistake Memory, Diff Preview

**Inspector**
- **Scratchpad** - Track issues found during review
- **Style Inference** - Must know style to review against
- **Mistake Memory** - Flag known anti-patterns
- **File Snapshot** - Efficient reading during review
- **Dependency Awareness** - Flag deprecated API usage
- **Design Tokens** - Flag raw values instead of tokens

**Tester**
- **Scratchpad** - Track test cases to cover
- **Style Inference** - Tests should match project style
- **Mistake Memory** - Know common failure patterns
- **File Snapshot** - Efficient test file reading
- **Dependency Awareness** - Mock dependencies correctly
- **Task Continuity** - Large test suites can be interrupted

**Designer**
- **Scratchpad** - Track design decisions, user feedback
- **Component Registry** - Know existing components to reuse
- **Diff Preview** - Preview UI changes
- **User Preferences** - Design style preferences
- **Design Tokens** - Primary user of design system

**Knowledge RAGs (Librarian, Archivalist, Academic)**
- Primarily serve as data SOURCES for other agents
- Archivalist indexes and retrieves historical mistakes
- Librarian provides cached file snapshots
- Academic provides dependency best practices

---

### Token Savings Summary

```
┌─────────────────────────────────────────────────────────────────────────────┐
│                    AGENT EFFICIENCY TECHNIQUES - SAVINGS                    │
├─────────────────────────────────────────────────────────────────────────────┤
│                                                                             │
│   Technique                    │ Primary Benefit           │ Token Savings │
│   ════════════════════════════╪═══════════════════════════╪═══════════════│
│   Scratchpad Memory           │ Context preservation      │     ~15%     │
│   Style Inference             │ Consistent code style     │     ~10%     │
│   Component Registry          │ No duplicate components   │     ~20%     │
│   Mistake Memory              │ Avoid known pitfalls      │     ~15%     │
│   Diff Preview                │ Catch errors early        │      ~5%     │
│   User Preferences            │ Match expectations        │     ~10%     │
│   File Snapshot Cache         │ Avoid re-reading          │     ~10%     │
│   Dependency Awareness        │ Use existing packages     │      ~5%     │
│   Task Continuity             │ Resume without re-context │     ~20%     │
│   Design Token Awareness      │ Consistent UI             │      ~5%     │
│   ──────────────────────────────────────────────────────────────────────── │
│                                                                             │
│   Note: Savings are not additive - they apply to different scenarios.     │
│   Estimated combined impact: 15-25% additional savings on top of          │
│   VectorGraphDB + XOR optimizations.                                       │
│                                                                             │
│   TOTAL ESTIMATED SAVINGS:                                                  │
│   • Baseline:                     0% savings                               │
│   • + Intent Cache:              67% savings                               │
│   • + VectorGraphDB:             75% savings                               │
│   • + XOR Internal:              80% savings                               │
│   • + Agent Efficiency:          85-88% savings                            │
│                                                                             │
└─────────────────────────────────────────────────────────────────────────────┘
```

---

## Implementation Guide

### Session Management
- `core/session/session.go`: Session data model
- `core/session/manager.go`: Session lifecycle management
- `core/session/context.go`: Session context (isolated state)
- `core/session/snapshot.go`: Session preservation/restoration
- `core/session/manager_test.go`: Session manager tests

### Guide and Bus
- `agents/guide/guide.go`: Central routing, session-aware correlation
- `agents/guide/bus.go`: Bus interface and topic constants
- `agents/guide/channel_bus.go`: In-process bus (sharded for multi-session scalability)

### Routing and Classification
- `agents/guide/agent_router.go`: Tiered routing (DSL → cache → LLM)
- `agents/guide/classification.go`: LLM-based routing
- `agents/guide/route_cache.go`: Cached route results

### Resilience
- `agents/guide/retry.go`: Retry queue
- `agents/guide/dead_letter.go`: Dead letter queue
- `agents/guide/circuit_breaker.go`: Circuit breaker
- `agents/guide/health.go`: Health monitoring

### Message Envelope
- `core/messaging/message.go`: Envelope with session_id field

### Skills
- `core/skills/skills.go`: Skill registry and loading
- `core/skills/loader.go`: Progressive skill loading
- `core/skills/hooks.go`: Hook registry and execution

### Agents (to be implemented)
- `agents/academic/`: External knowledge RAG
- `agents/architect/`: Planning and coordination
- `agents/orchestrator/`: DAG execution (session-aware)
- `agents/engineer/`: Task execution (session-scoped)
- `agents/designer/`: UI/UX implementation (components, styling, accessibility, design systems)
- `agents/librarian/`: Local codebase RAG
- `agents/archivalist/`: Historical RAG (session-aware storage, cross-session queries)
- `agents/inspector/`: Code validation
- `agents/tester/`: Test planning and execution

---

## Plan Mode Architecture

Plan Mode is a structured workflow for complex tasks requiring user approval before implementation. Plans are stored **per-session** and **local to the project** to ensure isolation and reproducibility.

### Plan Mode Overview

```
┌─────────────────────────────────────────────────────────────────────────────────────┐
│                              PLAN MODE OVERVIEW                                       │
├─────────────────────────────────────────────────────────────────────────────────────┤
│                                                                                     │
│  WHAT IS PLAN MODE?                                                                 │
│  A structured workflow where agents explore, design, and get user approval          │
│  before implementing complex changes. Prevents wasted effort and ensures            │
│  alignment between user intent and implementation approach.                         │
│                                                                                     │
│  STORAGE MODEL:                                                                     │
│  ├── Per-Session: Each session has isolated plan state                             │
│  ├── Per-Project: Plans stored in .sylk/plans/{session_id}/                         │
│  ├── Persistent: Plans survive session restarts                                    │
│  └── Versioned: Plan modifications create new versions                             │
│                                                                                     │
│  PLAN FILE LOCATION:                                                                │
│  .sylk/                                                                             │
│  └── plans/                                                                         │
│      └── {session_id}/                                                              │
│          ├── current_plan.md          # Active plan                                 │
│          ├── plan_history/            # Previous versions                           │
│          │   ├── plan_v1.md                                                         │
│          │   └── plan_v2.md                                                         │
│          └── todos.json               # Task tracking                               │
│                                                                                     │
└─────────────────────────────────────────────────────────────────────────────────────┘
```

### Plan State Machine

```
┌─────────────────────────────────────────────────────────────────────────────────────┐
│                              PLAN STATE MACHINE                                       │
├─────────────────────────────────────────────────────────────────────────────────────┤
│                                                                                     │
│                          ┌──────────────┐                                           │
│                          │    IDLE      │                                           │
│                          │  (no plan)   │                                           │
│                          └──────┬───────┘                                           │
│                                 │                                                   │
│                                 │ enter_plan_mode                                   │
│                                 ▼                                                   │
│                          ┌──────────────┐                                           │
│                    ┌────►│  EXPLORING   │◄────┐                                     │
│                    │     │              │     │                                     │
│                    │     └──────┬───────┘     │                                     │
│                    │            │             │                                     │
│                    │            │ plan ready  │ needs more exploration              │
│                    │            ▼             │                                     │
│                    │     ┌──────────────┐     │                                     │
│                    │     │   DRAFTING   │─────┘                                     │
│                    │     │              │                                           │
│                    │     └──────┬───────┘                                           │
│                    │            │                                                   │
│                    │            │ exit_plan_mode                                    │
│                    │            ▼                                                   │
│                    │     ┌──────────────┐                                           │
│                    │     │  AWAITING    │                                           │
│       user_modify  │     │  APPROVAL    │                                           │
│                    │     └──────┬───────┘                                           │
│                    │            │                                                   │
│                    │     ┌──────┴──────┐                                            │
│                    │     │             │                                            │
│                    │   reject      approve                                          │
│                    │     │             │                                            │
│                    │     ▼             ▼                                            │
│                    │  ┌──────┐   ┌──────────────┐                                   │
│                    └──│MODIFY│   │  APPROVED    │                                   │
│                       └──────┘   └──────┬───────┘                                   │
│                                         │                                           │
│                                         │ dispatch to Orchestrator                  │
│                                         ▼                                           │
│                                  ┌──────────────┐                                   │
│                            ┌────►│  EXECUTING   │◄────┐                             │
│                            │     └──────┬───────┘     │                             │
│                            │            │             │                             │
│                       step_fail    all_complete   step_complete                     │
│                            │            │             │                             │
│                            │            ▼             │                             │
│                            │     ┌──────────────┐     │                             │
│                            │     │  COMPLETED   │     │                             │
│                            │     └──────────────┘     │                             │
│                            │                                                        │
│                            │            OR                                          │
│                            │                                                        │
│                            │     ┌──────────────┐                                   │
│                            └────►│   FAILED     │                                   │
│                                  │ (re-plan?)   │                                   │
│                                  └──────────────┘                                   │
│                                                                                     │
└─────────────────────────────────────────────────────────────────────────────────────┘
```

### Plan State Definition

```go
// core/plan/state.go

package plan

import (
    "time"
)

// PlanState represents the current state of plan mode
type PlanState string

const (
    PlanStateIdle            PlanState = "idle"             // No active plan
    PlanStateExploring       PlanState = "exploring"        // Gathering context
    PlanStateDrafting        PlanState = "drafting"         // Writing plan
    PlanStateAwaitingApproval PlanState = "awaiting_approval" // User decision pending
    PlanStateApproved        PlanState = "approved"         // Ready for execution
    PlanStateExecuting       PlanState = "executing"        // In progress
    PlanStateCompleted       PlanState = "completed"        // Successfully finished
    PlanStateFailed          PlanState = "failed"           // Execution failed
)

// PlanContext holds all plan-related state for a session
type PlanContext struct {
    SessionID       string            `json:"session_id"`
    State           PlanState         `json:"state"`
    CurrentPlan     *Plan             `json:"current_plan,omitempty"`
    PlanHistory     []PlanVersion     `json:"plan_history,omitempty"`
    Todos           []TodoItem        `json:"todos,omitempty"`
    ExecutionState  *ExecutionState   `json:"execution_state,omitempty"`
    CreatedAt       time.Time         `json:"created_at"`
    UpdatedAt       time.Time         `json:"updated_at"`
}

// Plan represents a structured implementation plan
type Plan struct {
    ID              string            `json:"id"`
    Title           string            `json:"title"`
    Overview        string            `json:"overview"`
    FilesToModify   []FileChange      `json:"files_to_modify"`
    Steps           []PlanStep        `json:"steps"`
    Considerations  []string          `json:"considerations"`
    AllowedPrompts  []AllowedPrompt   `json:"allowed_prompts,omitempty"`
    Version         int               `json:"version"`
    CreatedAt       time.Time         `json:"created_at"`
    CreatedBy       string            `json:"created_by"` // Agent that created
}

// FileChange describes a file modification
type FileChange struct {
    Path        string `json:"path"`
    Action      string `json:"action"` // "create", "modify", "delete"
    Description string `json:"description"`
}

// PlanStep is an atomic unit of work
type PlanStep struct {
    Index       int            `json:"index"`
    Description string         `json:"description"`
    Agent       string         `json:"agent"`      // Which agent executes
    DependsOn   []int          `json:"depends_on"` // Step indices this depends on
    Layer       int            `json:"layer"`      // Topological layer (computed)
    Status      PlanStepStatus `json:"status"`     // Execution status (updated by Architect)
    Estimated   string         `json:"estimated"`  // Optional complexity hint
}

// PlanStepStatus tracks individual step completion
type PlanStepStatus string

const (
    StepStatusPending    PlanStepStatus = "pending"
    StepStatusInProgress PlanStepStatus = "in_progress"
    StepStatusCompleted  PlanStepStatus = "completed"
    StepStatusFailed     PlanStepStatus = "failed"
    StepStatusSkipped    PlanStepStatus = "skipped"
)

// TopologicalLayer groups steps that can execute concurrently
type TopologicalLayer struct {
    Layer int        `json:"layer"`
    Steps []PlanStep `json:"steps"`
}

// ComputeTopologicalLayers organizes steps into concurrent execution layers.
// Steps in the same layer have no dependencies on each other and can run in parallel.
// Layer 0 contains steps with no dependencies.
// Layer N contains steps whose dependencies are all in layers < N.
func ComputeTopologicalLayers(steps []PlanStep) []TopologicalLayer {
    if len(steps) == 0 {
        return nil
    }

    // Build index lookup
    stepByIndex := make(map[int]*PlanStep, len(steps))
    for i := range steps {
        stepByIndex[steps[i].Index] = &steps[i]
    }

    // Compute layer for each step using Kahn's algorithm variant
    computed := make(map[int]int, len(steps))

    for _, step := range steps {
        computeStepLayer(step.Index, stepByIndex, computed)
    }

    // Update steps with computed layers
    for i := range steps {
        steps[i].Layer = computed[steps[i].Index]
    }

    // Group by layer
    maxLayer := 0
    for _, layer := range computed {
        if layer > maxLayer {
            maxLayer = layer
        }
    }

    layers := make([]TopologicalLayer, maxLayer+1)
    for i := range layers {
        layers[i] = TopologicalLayer{Layer: i}
    }

    for _, step := range steps {
        layer := computed[step.Index]
        layers[layer].Steps = append(layers[layer].Steps, step)
    }

    return layers
}

// computeStepLayer recursively computes layer for a step
func computeStepLayer(idx int, steps map[int]*PlanStep, computed map[int]int) int {
    if layer, ok := computed[idx]; ok {
        return layer
    }

    step := steps[idx]
    if step == nil || len(step.DependsOn) == 0 {
        computed[idx] = 0
        return 0
    }

    maxDepLayer := 0
    for _, depIdx := range step.DependsOn {
        depLayer := computeStepLayer(depIdx, steps, computed)
        if depLayer >= maxDepLayer {
            maxDepLayer = depLayer + 1
        }
    }

    computed[idx] = maxDepLayer
    return maxDepLayer
}

// AllowedPrompt defines a bash permission for the plan
type AllowedPrompt struct {
    Tool   string `json:"tool"`
    Prompt string `json:"prompt"`
}

// PlanVersion stores historical plan versions
type PlanVersion struct {
    Version   int       `json:"version"`
    Plan      Plan      `json:"plan"`
    Reason    string    `json:"reason"` // Why this version was created
    CreatedAt time.Time `json:"created_at"`
}

// TodoItem tracks progress on plan steps
type TodoItem struct {
    Content    string     `json:"content"`
    ActiveForm string     `json:"active_form"`
    Status     TodoStatus `json:"status"`
    StepIndex  int        `json:"step_index,omitempty"`
}

type TodoStatus string

const (
    TodoStatusPending    TodoStatus = "pending"
    TodoStatusInProgress TodoStatus = "in_progress"
    TodoStatusCompleted  TodoStatus = "completed"
)

// ExecutionState tracks DAG execution progress
type ExecutionState struct {
    DAGID           string                 `json:"dag_id"`
    CompletedSteps  []int                  `json:"completed_steps"`
    CurrentStep     int                    `json:"current_step"`
    FailedStep      *int                   `json:"failed_step,omitempty"`
    FailureReason   string                 `json:"failure_reason,omitempty"`
    StartedAt       time.Time              `json:"started_at"`
    StepResults     map[int]StepResult     `json:"step_results"`
}

type StepResult struct {
    StepIndex   int       `json:"step_index"`
    Status      string    `json:"status"` // "success", "failure", "skipped"
    Output      string    `json:"output,omitempty"`
    Error       string    `json:"error,omitempty"`
    CompletedAt time.Time `json:"completed_at"`
}
```

### Plan Storage Implementation

Plans are stored in user space (`~/.sylk/`) indexed by project hash, ensuring:
- Plans persist across sessions
- No pollution of project directory
- User-private storage (not committed to git)
- Project-aware organization

#### Directory Structure

```
~/.sylk/
├── sessions/{session_id}/              ← Ephemeral session data
│   ├── wal/
│   ├── checkpoints/
│   └── active_plan.txt                 ← Pointer to current plan slug
│
├── projects/{project_hash}/            ← Persistent per-project data
│   ├── project_info.json               ← Project path, name, created_at
│   ├── plans/
│   │   ├── implement-user-auth/        ← Plan slug (derived from title)
│   │   │   ├── metadata.json           ← Version history, change sets
│   │   │   ├── context.json            ← Current execution context
│   │   │   ├── todos.json              ← Todo list state
│   │   │   ├── current.md              ← Symlink/copy to HEAD version
│   │   │   ├── v1.md                   ← Version 1
│   │   │   ├── v2.md                   ← Version 2
│   │   │   ├── v3.md                   ← Version 3 (HEAD)
│   │   │   └── checkpoints/            ← Execution state snapshots
│   │   │       ├── v1_complete.json
│   │   │       └── v2_step3.json
│   │   │
│   │   └── refactor-database/
│   │       ├── metadata.json
│   │       ├── context.json
│   │       ├── current.md → v1.md
│   │       └── v1.md
│   │
│   └── archived/                       ← Completed/abandoned plans
│       └── fix-login-bug/
│           ├── metadata.json
│           ├── v1.md
│           ├── v2.md
│           └── outcome.json
│
└── shared/
    └── sessions.db
```

#### Example File Paths

```
Plan version file:
~/.sylk/projects/a1b2c3d4/plans/implement-user-auth/v3.md

Breakdown:
~/.sylk/              → User sylk root
projects/             → Per-project persistent data
a1b2c3d4/             → ProjectHash("/Users/ada/myproject") - first 8 bytes of SHA256
plans/                → Plans directory
implement-user-auth/  → Plan slug (slugified from title)
v3.md                 → Version 3 of the plan

Other paths:
~/.sylk/projects/a1b2c3d4/plans/implement-user-auth/metadata.json
~/.sylk/projects/a1b2c3d4/plans/implement-user-auth/context.json
~/.sylk/projects/a1b2c3d4/plans/implement-user-auth/checkpoints/v2_step3.json
~/.sylk/projects/a1b2c3d4/archived/fix-login-bug/outcome.json
```

#### Storage Implementation

```go
// core/plan/storage.go

package plan

import (
    "crypto/sha256"
    "encoding/hex"
    "encoding/json"
    "fmt"
    "os"
    "path/filepath"
    "regexp"
    "strings"
    "time"
)

// Storage handles plan persistence in user space
type Storage struct {
    sylkRoot    string // ~/.sylk
    projectHash string // Hash of project root path
    projectRoot string // Original project path (for reference)
}

// NewStorage creates a storage instance for a project
func NewStorage(projectRoot string) (*Storage, error) {
    home, err := os.UserHomeDir()
    if err != nil {
        return nil, fmt.Errorf("get home dir: %w", err)
    }

    sylkRoot := filepath.Join(home, ".sylk")
    projectHash := ProjectHash(projectRoot)

    s := &Storage{
        sylkRoot:    sylkRoot,
        projectHash: projectHash,
        projectRoot: projectRoot,
    }

    // Initialize project directory with info
    if err := s.initProjectDir(); err != nil {
        return nil, err
    }

    return s, nil
}

// ProjectHash generates consistent hash for project path
func ProjectHash(projectRoot string) string {
    absPath, _ := filepath.Abs(projectRoot)
    hash := sha256.Sum256([]byte(absPath))
    return hex.EncodeToString(hash[:8]) // 16 chars
}

// Slugify converts a plan title to a filesystem-safe slug
func Slugify(title string) string {
    // Lowercase
    slug := strings.ToLower(title)
    // Replace spaces and special chars with hyphens
    reg := regexp.MustCompile(`[^a-z0-9]+`)
    slug = reg.ReplaceAllString(slug, "-")
    // Trim leading/trailing hyphens
    slug = strings.Trim(slug, "-")
    // Limit length
    if len(slug) > 50 {
        slug = slug[:50]
    }
    return slug
}

// projectDir returns the base directory for this project's data
func (s *Storage) projectDir() string {
    return filepath.Join(s.sylkRoot, "projects", s.projectHash)
}

// planDir returns the directory for a specific plan
func (s *Storage) planDir(planSlug string) string {
    return filepath.Join(s.projectDir(), "plans", planSlug)
}

// archivedDir returns the directory for archived plans
func (s *Storage) archivedDir() string {
    return filepath.Join(s.projectDir(), "archived")
}

// initProjectDir creates project directory and stores project info
func (s *Storage) initProjectDir() error {
    dir := s.projectDir()
    if err := os.MkdirAll(filepath.Join(dir, "plans"), 0755); err != nil {
        return fmt.Errorf("create project dir: %w", err)
    }
    if err := os.MkdirAll(filepath.Join(dir, "archived"), 0755); err != nil {
        return fmt.Errorf("create archived dir: %w", err)
    }

    // Write project info if not exists
    infoPath := filepath.Join(dir, "project_info.json")
    if _, err := os.Stat(infoPath); os.IsNotExist(err) {
        info := map[string]any{
            "project_path": s.projectRoot,
            "project_hash": s.projectHash,
            "created_at":   time.Now(),
        }
        data, _ := json.MarshalIndent(info, "", "  ")
        os.WriteFile(infoPath, data, 0644)
    }

    return nil
}

// EnsurePlanDir creates directory structure for a plan
func (s *Storage) EnsurePlanDir(planSlug string) error {
    dir := s.planDir(planSlug)
    if err := os.MkdirAll(dir, 0755); err != nil {
        return fmt.Errorf("create plan dir: %w", err)
    }
    checkpointDir := filepath.Join(dir, "checkpoints")
    return os.MkdirAll(checkpointDir, 0755)
}

// SaveContext persists the full plan context
func (s *Storage) SaveContext(ctx *PlanContext) error {
    if ctx.CurrentPlan == nil {
        return fmt.Errorf("no current plan")
    }

    planSlug := Slugify(ctx.CurrentPlan.Title)
    if err := s.EnsurePlanDir(planSlug); err != nil {
        return err
    }

    ctx.UpdatedAt = time.Now()

    // Save context.json
    contextPath := filepath.Join(s.planDir(planSlug), "context.json")
    data, err := json.MarshalIndent(ctx, "", "  ")
    if err != nil {
        return fmt.Errorf("marshal context: %w", err)
    }
    if err := os.WriteFile(contextPath, data, 0644); err != nil {
        return fmt.Errorf("write context: %w", err)
    }

    // Save current version markdown
    if err := s.saveVersionMarkdown(planSlug, ctx.CurrentPlan); err != nil {
        return err
    }

    // Update current.md symlink/copy
    if err := s.updateCurrentPointer(planSlug, ctx.CurrentPlan.Version); err != nil {
        return err
    }

    // Save todos.json
    return s.saveTodos(planSlug, ctx.Todos)
}

// LoadContext loads the plan context for a plan slug
func (s *Storage) LoadContext(planSlug string) (*PlanContext, error) {
    contextPath := filepath.Join(s.planDir(planSlug), "context.json")

    data, err := os.ReadFile(contextPath)
    if os.IsNotExist(err) {
        return nil, fmt.Errorf("plan not found: %s", planSlug)
    }
    if err != nil {
        return nil, fmt.Errorf("read context: %w", err)
    }

    var ctx PlanContext
    if err := json.Unmarshal(data, &ctx); err != nil {
        return nil, fmt.Errorf("unmarshal context: %w", err)
    }

    return &ctx, nil
}

// ListPlans returns all plan slugs for this project
func (s *Storage) ListPlans() ([]string, error) {
    plansDir := filepath.Join(s.projectDir(), "plans")
    entries, err := os.ReadDir(plansDir)
    if err != nil {
        if os.IsNotExist(err) {
            return nil, nil
        }
        return nil, err
    }

    var slugs []string
    for _, e := range entries {
        if e.IsDir() {
            slugs = append(slugs, e.Name())
        }
    }
    return slugs, nil
}

// saveVersionMarkdown writes plan as versioned markdown file
func (s *Storage) saveVersionMarkdown(planSlug string, plan *Plan) error {
    filename := fmt.Sprintf("v%d.md", plan.Version)
    path := filepath.Join(s.planDir(planSlug), filename)

    md := formatPlanMarkdown(plan)
    header := fmt.Sprintf("<!-- Plan: %s | Version: %d | Created: %s -->\n\n",
        plan.Title, plan.Version, time.Now().Format(time.RFC3339))

    return os.WriteFile(path, []byte(header+md), 0644)
}

// updateCurrentPointer updates current.md to point to HEAD version
func (s *Storage) updateCurrentPointer(planSlug string, version int) error {
    currentPath := filepath.Join(s.planDir(planSlug), "current.md")
    versionPath := filepath.Join(s.planDir(planSlug), fmt.Sprintf("v%d.md", version))

    // Remove existing current.md
    os.Remove(currentPath)

    // Copy version file to current.md (symlinks can be problematic on some systems)
    data, err := os.ReadFile(versionPath)
    if err != nil {
        return err
    }
    return os.WriteFile(currentPath, data, 0644)
}

// SaveMetadata saves the version history and change sets
func (s *Storage) SaveMetadata(planSlug string, meta *PlanMetadata) error {
    path := filepath.Join(s.planDir(planSlug), "metadata.json")
    data, err := json.MarshalIndent(meta, "", "  ")
    if err != nil {
        return fmt.Errorf("marshal metadata: %w", err)
    }
    return os.WriteFile(path, data, 0644)
}

// LoadMetadata loads version history and change sets
func (s *Storage) LoadMetadata(planSlug string) (*PlanMetadata, error) {
    path := filepath.Join(s.planDir(planSlug), "metadata.json")
    data, err := os.ReadFile(path)
    if os.IsNotExist(err) {
        return &PlanMetadata{Slug: planSlug}, nil
    }
    if err != nil {
        return nil, err
    }

    var meta PlanMetadata
    if err := json.Unmarshal(data, &meta); err != nil {
        return nil, err
    }
    return &meta, nil
}

// LoadVersion loads a specific version of a plan
func (s *Storage) LoadVersion(planSlug string, version int) (string, error) {
    path := filepath.Join(s.planDir(planSlug), fmt.Sprintf("v%d.md", version))
    data, err := os.ReadFile(path)
    if err != nil {
        return "", err
    }
    return string(data), nil
}

// SaveCheckpoint saves execution state at a point in time
func (s *Storage) SaveCheckpoint(planSlug string, version int, stepIndex int, state *ExecutionState) error {
    filename := fmt.Sprintf("v%d_step%d.json", version, stepIndex)
    path := filepath.Join(s.planDir(planSlug), "checkpoints", filename)

    data, err := json.MarshalIndent(state, "", "  ")
    if err != nil {
        return err
    }
    return os.WriteFile(path, data, 0644)
}

// ArchivePlan moves a completed/abandoned plan to archived/
func (s *Storage) ArchivePlan(planSlug string, outcome *PlanOutcome) error {
    srcDir := s.planDir(planSlug)
    dstDir := filepath.Join(s.archivedDir(), planSlug)

    // Move plan directory
    if err := os.Rename(srcDir, dstDir); err != nil {
        return fmt.Errorf("archive plan: %w", err)
    }

    // Write outcome
    outcomePath := filepath.Join(dstDir, "outcome.json")
    data, err := json.MarshalIndent(outcome, "", "  ")
    if err != nil {
        return err
    }
    return os.WriteFile(outcomePath, data, 0644)
}

func (s *Storage) saveTodos(planSlug string, todos []TodoItem) error {
    path := filepath.Join(s.planDir(planSlug), "todos.json")
    data, err := json.MarshalIndent(todos, "", "  ")
    if err != nil {
        return fmt.Errorf("marshal todos: %w", err)
    }
    return os.WriteFile(path, data, 0644)
}

// PlanOutcome records final state of a completed/failed plan
type PlanOutcome struct {
    Result        string    `json:"result"` // "success", "failure", "abandoned"
    CompletedAt   time.Time `json:"completed_at"`
    StepsComplete int       `json:"steps_complete"`
    StepsTotal    int       `json:"steps_total"`
    FailureReason string    `json:"failure_reason,omitempty"`
    Duration      string    `json:"duration"`
}

// formatPlanMarkdown converts a Plan to markdown format with topological layers
func formatPlanMarkdown(plan *Plan) string {
    var md string

    md += fmt.Sprintf("# Plan: %s\n\n", plan.Title)
    md += fmt.Sprintf("## Overview\n\n%s\n\n", plan.Overview)

    md += "## Files to Modify\n\n"
    for _, f := range plan.FilesToModify {
        md += fmt.Sprintf("- `%s` (%s): %s\n", f.Path, f.Action, f.Description)
    }
    md += "\n"

    // Compute topological layers for concurrent execution visualization
    layers := ComputeTopologicalLayers(plan.Steps)

    md += "## Execution Plan\n\n"
    md += "Steps are organized by **execution layer**. "
    md += "Steps within the same layer have no interdependencies and **can execute concurrently**.\n\n"

    // Compute progress statistics
    completed, total := countCompletedSteps(plan.Steps)
    md += fmt.Sprintf("**Progress: %d/%d steps completed**\n\n", completed, total)

    md += "```\n"
    md += "Execution Flow:\n"
    for i, layer := range layers {
        if i > 0 {
            md += "    │\n"
            md += "    ▼\n"
        }
        md += fmt.Sprintf("┌─ Layer %d ", layer.Layer)
        if len(layer.Steps) > 1 {
            md += fmt.Sprintf("(parallel: %d tasks) ", len(layer.Steps))
        }
        md += fmt.Sprintf("[%s] ─┐\n", layerStatus(layer.Steps))
        for _, step := range layer.Steps {
            md += fmt.Sprintf("│  %s Step %d: [%s]       │\n",
                statusIcon(step.Status), step.Index, step.Agent)
        }
        md += "└────────────────────────┘\n"
    }
    md += "```\n\n"

    md += "### Detailed Steps by Layer\n\n"
    for _, layer := range layers {
        concurrency := "sequential"
        if len(layer.Steps) > 1 {
            concurrency = fmt.Sprintf("**%d concurrent**", len(layer.Steps))
        }
        md += fmt.Sprintf("#### Layer %d (%s)\n\n", layer.Layer, concurrency)

        for _, step := range layer.Steps {
            checkbox := statusCheckbox(step.Status)
            deps := ""
            if len(step.DependsOn) > 0 {
                deps = fmt.Sprintf("\n   - *Depends on*: steps %v", step.DependsOn)
            }
            md += fmt.Sprintf("- %s **Step %d** [%s]: %s%s\n",
                checkbox, step.Index, step.Agent, step.Description, deps)
        }
        md += "\n"
    }

    md += "### Sequential Step Reference\n\n"
    for _, step := range plan.Steps {
        checkbox := statusCheckbox(step.Status)
        deps := ""
        if len(step.DependsOn) > 0 {
            deps = fmt.Sprintf(" (depends on: %v)", step.DependsOn)
        }
        md += fmt.Sprintf("%s %d. [Layer %d] [%s] %s%s\n",
            checkbox, step.Index, step.Layer, step.Agent, step.Description, deps)
    }
    md += "\n"

    if len(plan.Considerations) > 0 {
        md += "## Considerations\n\n"
        for _, c := range plan.Considerations {
            md += fmt.Sprintf("- %s\n", c)
        }
        md += "\n"
    }

    if len(plan.AllowedPrompts) > 0 {
        md += "## Required Permissions\n\n"
        for _, p := range plan.AllowedPrompts {
            md += fmt.Sprintf("- %s: %s\n", p.Tool, p.Prompt)
        }
    }

    return md
}

// statusCheckbox returns a markdown checkbox based on step status
func statusCheckbox(status PlanStepStatus) string {
    switch status {
    case StepStatusCompleted:
        return "[x]"
    case StepStatusFailed:
        return "[!]"
    case StepStatusSkipped:
        return "[-]"
    case StepStatusInProgress:
        return "[~]"
    default:
        return "[ ]"
    }
}

// statusIcon returns an icon for the execution flow diagram
func statusIcon(status PlanStepStatus) string {
    switch status {
    case StepStatusCompleted:
        return "✓"
    case StepStatusFailed:
        return "✗"
    case StepStatusSkipped:
        return "⊘"
    case StepStatusInProgress:
        return "►"
    default:
        return "○"
    }
}

// layerStatus returns aggregate status for a layer
func layerStatus(steps []PlanStep) string {
    completed := 0
    failed := 0
    inProgress := 0
    for _, s := range steps {
        switch s.Status {
        case StepStatusCompleted:
            completed++
        case StepStatusFailed:
            failed++
        case StepStatusInProgress:
            inProgress++
        }
    }
    if failed > 0 {
        return "FAILED"
    }
    if completed == len(steps) {
        return "DONE"
    }
    if inProgress > 0 || completed > 0 {
        return fmt.Sprintf("%d/%d", completed, len(steps))
    }
    return "PENDING"
}

// countCompletedSteps returns (completed, total) step counts
func countCompletedSteps(steps []PlanStep) (int, int) {
    completed := 0
    for _, s := range steps {
        if s.Status == StepStatusCompleted {
            completed++
        }
    }
    return completed, len(steps)
}

// Example plan output with topological layers:
//
// # Plan: Implement User Authentication
//
// ## Overview
//
// Add JWT-based authentication with login, logout, and session management.
//
// ## Files to Modify
//
// - `core/auth/jwt.go` (create): JWT token generation and validation
// - `core/auth/middleware.go` (create): Auth middleware for protected routes
// - `api/handlers/auth.go` (create): Login/logout HTTP handlers
// - `api/routes.go` (modify): Add auth routes
// - `tests/auth_test.go` (create): Authentication tests
//
// ## Execution Plan
//
// Steps are organized by **execution layer**. Steps within the same layer
// have no interdependencies and **can execute concurrently**.
//
// ```
// Execution Flow:
// ┌─ Layer 0 (parallel: 2 tasks) ─┐
// │  Step 1: [engineer]           │
// │  Step 2: [engineer]           │
// └────────────────────────────────┘
//     │
//     ▼
// ┌─ Layer 1 (parallel: 2 tasks) ─┐
// │  Step 3: [engineer]           │
// │  Step 4: [engineer]           │
// └────────────────────────────────┘
//     │
//     ▼
// ┌─ Layer 2 ─┐
// │  Step 5: [engineer]           │
// └────────────────────────────────┘
//     │
//     ▼
// ┌─ Layer 3 ─┐
// │  Step 6: [tester]             │
// └────────────────────────────────┘
// ```
//
// ### Detailed Steps by Layer
//
// #### Layer 0 (**2 concurrent**)
//
// - **Step 1** [engineer]: Create JWT token utilities in core/auth/jwt.go
// - **Step 2** [engineer]: Create auth middleware in core/auth/middleware.go
//
// #### Layer 1 (**2 concurrent**)
//
// - **Step 3** [engineer]: Create login handler in api/handlers/auth.go
//   - *Depends on*: steps [1]
// - **Step 4** [engineer]: Create logout handler in api/handlers/auth.go
//   - *Depends on*: steps [1]
//
// #### Layer 2 (sequential)
//
// - **Step 5** [engineer]: Update api/routes.go with auth endpoints
//   - *Depends on*: steps [3, 4]
//
// #### Layer 3 (sequential)
//
// - **Step 6** [tester]: Write comprehensive auth tests
//   - *Depends on*: steps [1, 2, 3, 4, 5]
//
// ### Sequential Step Reference
//
// 1. [Layer 0] [engineer] Create JWT token utilities in core/auth/jwt.go
// 2. [Layer 0] [engineer] Create auth middleware in core/auth/middleware.go
// 3. [Layer 1] [engineer] Create login handler (depends on: [1])
// 4. [Layer 1] [engineer] Create logout handler (depends on: [1])
// 5. [Layer 2] [engineer] Update routes (depends on: [3, 4])
// 6. [Layer 3] [tester] Write tests (depends on: [1, 2, 3, 4, 5])
//
// ## Considerations
//
// - JWT secret must be loaded from environment variable
// - Consider refresh token implementation for future iteration
// - Rate limiting on login endpoint recommended
//
// ## Required Permissions
//
// - Bash: run tests
// - Bash: install dependencies
```

### Plan File Synchronization on Step Completion

When pipelines signal completion of their respective tasks, the Architect agent **must** update the plan file to reflect current progress. This ensures the plan file remains the source of truth for execution state.

#### Synchronization Flow

```
┌─────────────────────────────────────────────────────────────────────────────┐
│                    PLAN FILE SYNCHRONIZATION FLOW                            │
├─────────────────────────────────────────────────────────────────────────────┤
│                                                                             │
│  ┌──────────────┐    step_completed     ┌──────────────┐                   │
│  │  Pipeline    │ ──────────────────► │  Orchestrator │                   │
│  │  (Engineer)  │    {step_index,       │              │                   │
│  └──────────────┘     status, result}   └──────┬───────┘                   │
│                                                │                            │
│                                                │ update_step_status         │
│                                                │ {step_index, status}       │
│                                                ▼                            │
│                                         ┌──────────────┐                   │
│                                         │   Architect  │                   │
│                                         │              │                   │
│                                         └──────┬───────┘                   │
│                                                │                            │
│                            ┌───────────────────┼───────────────────┐       │
│                            │                   │                   │       │
│                            ▼                   ▼                   ▼       │
│                     ┌────────────┐      ┌────────────┐      ┌──────────┐  │
│                     │ Update     │      │ Update     │      │ Regenerate│  │
│                     │ Plan.Steps │      │ Todos      │      │ Markdown  │  │
│                     │ [].Status  │      │ [].Status  │      │ File      │  │
│                     └────────────┘      └────────────┘      └──────────┘  │
│                                                                             │
│                                         ┌──────────────┐                   │
│                                         │ .sylk/plans/ │                   │
│                                         │ {session}/   │                   │
│                                         │ current_     │                   │
│                                         │ plan.md      │                   │
│                                         └──────────────┘                   │
└─────────────────────────────────────────────────────────────────────────────┘
```

#### Step Completion Signal

When a pipeline completes a step, it emits a `StepCompletedSignal`:

```go
// core/signals/step_completed.go

type StepCompletedSignal struct {
    SessionID  string         `json:"session_id"`
    PlanID     string         `json:"plan_id"`
    StepIndex  int            `json:"step_index"`
    Status     PlanStepStatus `json:"status"`
    Output     string         `json:"output,omitempty"`
    Error      string         `json:"error,omitempty"`
    Duration   time.Duration  `json:"duration"`
    AgentID    string         `json:"agent_id"`
    Timestamp  time.Time      `json:"timestamp"`
}
```

#### Architect Step Update Handler

The Architect agent subscribes to step completion signals and updates the plan:

```go
// agents/architect/step_handler.go

// HandleStepCompleted processes step completion signals from pipelines
func (a *ArchitectAgent) HandleStepCompleted(sig *StepCompletedSignal) error {
    // Load current plan context
    ctx, err := a.storage.LoadContext(sig.SessionID)
    if err != nil {
        return fmt.Errorf("load context: %w", err)
    }

    if ctx.CurrentPlan == nil || ctx.CurrentPlan.ID != sig.PlanID {
        return fmt.Errorf("plan mismatch: expected %s, got %s", sig.PlanID, ctx.CurrentPlan.ID)
    }

    // Update step status
    if err := updateStepStatus(ctx.CurrentPlan, sig.StepIndex, sig.Status); err != nil {
        return err
    }

    // Update corresponding todo
    updateTodoForStep(ctx.Todos, sig.StepIndex, sig.Status)

    // Update execution state
    updateExecutionState(ctx.ExecutionState, sig)

    // Persist changes (regenerates markdown file)
    return a.storage.SaveContext(ctx)
}

func updateStepStatus(plan *Plan, stepIndex int, status PlanStepStatus) error {
    for i := range plan.Steps {
        if plan.Steps[i].Index == stepIndex {
            plan.Steps[i].Status = status
            return nil
        }
    }
    return fmt.Errorf("step %d not found", stepIndex)
}

func updateTodoForStep(todos []TodoItem, stepIndex int, status PlanStepStatus) {
    for i := range todos {
        if todos[i].StepIndex == stepIndex {
            switch status {
            case StepStatusCompleted:
                todos[i].Status = TodoStatusCompleted
            case StepStatusInProgress:
                todos[i].Status = TodoStatusInProgress
            case StepStatusFailed, StepStatusSkipped:
                todos[i].Status = TodoStatusPending // Allow retry
            }
            return
        }
    }
}

func updateExecutionState(state *ExecutionState, sig *StepCompletedSignal) {
    if state == nil {
        return
    }

    state.StepResults[sig.StepIndex] = StepResult{
        StepIndex:   sig.StepIndex,
        Status:      string(sig.Status),
        Output:      sig.Output,
        Error:       sig.Error,
        CompletedAt: sig.Timestamp,
    }

    if sig.Status == StepStatusCompleted {
        state.CompletedSteps = append(state.CompletedSteps, sig.StepIndex)
    }
}
```

#### Automatic Plan File Regeneration

When `SaveContext` is called, the plan markdown file is automatically regenerated with updated status:

```go
// In storage.go SaveContext method:

// Save current_plan.md if plan exists
if ctx.CurrentPlan != nil {
    // formatPlanMarkdown now includes status checkboxes
    if err := s.savePlanMarkdown(ctx.SessionID, ctx.CurrentPlan); err != nil {
        return err
    }
}
```

#### Example: Plan File Before and After Step Completion

**Before (step 1 in progress):**
```markdown
**Progress: 0/6 steps completed**

### Sequential Step Reference

[ ] 1. [Layer 0] [engineer] Create JWT token utilities
[~] 2. [Layer 0] [engineer] Create auth middleware (in progress)
[ ] 3. [Layer 1] [engineer] Create login handler
[ ] 4. [Layer 1] [engineer] Create logout handler
[ ] 5. [Layer 2] [engineer] Update routes
[ ] 6. [Layer 3] [tester] Write tests
```

**After (steps 1 and 2 completed):**
```markdown
**Progress: 2/6 steps completed**

### Sequential Step Reference

[x] 1. [Layer 0] [engineer] Create JWT token utilities
[x] 2. [Layer 0] [engineer] Create auth middleware
[ ] 3. [Layer 1] [engineer] Create login handler
[ ] 4. [Layer 1] [engineer] Create logout handler
[ ] 5. [Layer 2] [engineer] Update routes
[ ] 6. [Layer 3] [tester] Write tests
```

#### Orchestrator Integration

The Orchestrator subscribes to step completion from executing pipelines and forwards to Architect:

```go
// agents/orchestrator/executor.go

func (o *OrchestratorAgent) executeStep(ctx context.Context, step PlanStep, planCtx *PlanContext) error {
    // Mark step in progress
    o.emitStepSignal(planCtx, step.Index, StepStatusInProgress, "", "")

    // Execute via appropriate agent
    result, err := o.dispatchToAgent(ctx, step)

    if err != nil {
        o.emitStepSignal(planCtx, step.Index, StepStatusFailed, "", err.Error())
        return err
    }

    // Mark step completed
    o.emitStepSignal(planCtx, step.Index, StepStatusCompleted, result, "")
    return nil
}

func (o *OrchestratorAgent) emitStepSignal(planCtx *PlanContext, stepIndex int, status PlanStepStatus, output, errMsg string) {
    sig := &StepCompletedSignal{
        SessionID:  planCtx.SessionID,
        PlanID:     planCtx.CurrentPlan.ID,
        StepIndex:  stepIndex,
        Status:     status,
        Output:     output,
        Error:      errMsg,
        Timestamp:  time.Now(),
        AgentID:    o.ID(),
    }

    // Route to Architect via bus
    o.bus.Publish("plan.step.completed", sig)
}
```

### Plan Archival on Approval

When a user approves a plan, the Architect **must** submit the plan to the Archivalist for long-term storage. This enables:
- Future reference for similar tasks
- Historical pattern analysis
- Knowledge retrieval for new sessions

#### Archival Flow

```
┌─────────────────────────────────────────────────────────────────────────────┐
│                        PLAN ARCHIVAL FLOW                                    │
├─────────────────────────────────────────────────────────────────────────────┤
│                                                                             │
│  User Approval                                                              │
│       │                                                                     │
│       ▼                                                                     │
│  ┌──────────────┐    archive_plan      ┌──────────────┐                    │
│  │   Architect  │ ──────────────────► │  Archivalist │                    │
│  │              │    {plan, metadata}  │              │                    │
│  └──────────────┘                      └──────┬───────┘                    │
│                                               │                             │
│                                               │ store in category           │
│                                               │ "approved_plans"            │
│                                               ▼                             │
│                                        ┌──────────────┐                    │
│                                        │  Historical  │                    │
│                                        │  Knowledge   │                    │
│                                        │  Store       │                    │
│                                        └──────────────┘                    │
│                                                                             │
│  Plan Completion                                                            │
│       │                                                                     │
│       ▼                                                                     │
│  ┌──────────────┐    archive_outcome   ┌──────────────┐                    │
│  │   Architect  │ ──────────────────► │  Archivalist │                    │
│  │              │    {plan, results,   │              │                    │
│  └──────────────┘     success/failure} └──────────────┘                    │
│                                                                             │
└─────────────────────────────────────────────────────────────────────────────┘
```

#### Archive on Approval

```go
// agents/architect/archival.go

// ArchivePlanOnApproval submits approved plan to Archivalist
func (a *ArchitectAgent) ArchivePlanOnApproval(ctx *PlanContext) error {
    if ctx.State != PlanStateApproved {
        return fmt.Errorf("can only archive approved plans")
    }

    entry := &archivalist.Entry{
        Category:  "approved_plans",
        SessionID: ctx.SessionID,
        Content:   formatPlanForArchival(ctx.CurrentPlan),
        Metadata: map[string]any{
            "plan_id":      ctx.CurrentPlan.ID,
            "title":        ctx.CurrentPlan.Title,
            "step_count":   len(ctx.CurrentPlan.Steps),
            "files_count":  len(ctx.CurrentPlan.FilesToModify),
            "created_by":   ctx.CurrentPlan.CreatedBy,
            "approved_at":  time.Now(),
            "layers":       countLayers(ctx.CurrentPlan.Steps),
            "tags":         extractPlanTags(ctx.CurrentPlan),
        },
    }

    return a.archivalist.Store(entry)
}

func formatPlanForArchival(plan *Plan) string {
    // Include full plan markdown for searchability
    return formatPlanMarkdown(plan)
}

func countLayers(steps []PlanStep) int {
    maxLayer := 0
    for _, s := range steps {
        if s.Layer > maxLayer {
            maxLayer = s.Layer
        }
    }
    return maxLayer + 1
}

func extractPlanTags(plan *Plan) []string {
    tags := []string{}
    // Extract tags from file paths (e.g., "auth", "api", "tests")
    for _, f := range plan.FilesToModify {
        dir := filepath.Dir(f.Path)
        if dir != "." && dir != "/" {
            tags = append(tags, filepath.Base(dir))
        }
    }
    // Deduplicate
    return uniqueStrings(tags)
}
```

#### Archive on Completion (Success or Failure)

```go
// ArchivePlanOutcome records final plan outcome for future learning
func (a *ArchitectAgent) ArchivePlanOutcome(ctx *PlanContext) error {
    if ctx.State != PlanStateCompleted && ctx.State != PlanStateFailed {
        return fmt.Errorf("can only archive completed/failed plans")
    }

    outcome := "success"
    if ctx.State == PlanStateFailed {
        outcome = "failure"
    }

    // Compute execution statistics
    stats := computeExecutionStats(ctx)

    entry := &archivalist.Entry{
        Category:  "plan_outcomes",
        SessionID: ctx.SessionID,
        Content:   formatOutcomeForArchival(ctx, stats),
        Metadata: map[string]any{
            "plan_id":           ctx.CurrentPlan.ID,
            "title":             ctx.CurrentPlan.Title,
            "outcome":           outcome,
            "completed_steps":   stats.CompletedSteps,
            "failed_steps":      stats.FailedSteps,
            "skipped_steps":     stats.SkippedSteps,
            "total_steps":       stats.TotalSteps,
            "duration_seconds":  stats.Duration.Seconds(),
            "failure_reason":    ctx.ExecutionState.FailureReason,
            "completed_at":      time.Now(),
        },
    }

    return a.archivalist.Store(entry)
}

type ExecutionStats struct {
    CompletedSteps int
    FailedSteps    int
    SkippedSteps   int
    TotalSteps     int
    Duration       time.Duration
}

func computeExecutionStats(ctx *PlanContext) ExecutionStats {
    stats := ExecutionStats{TotalSteps: len(ctx.CurrentPlan.Steps)}

    for _, step := range ctx.CurrentPlan.Steps {
        switch step.Status {
        case StepStatusCompleted:
            stats.CompletedSteps++
        case StepStatusFailed:
            stats.FailedSteps++
        case StepStatusSkipped:
            stats.SkippedSteps++
        }
    }

    if ctx.ExecutionState != nil && !ctx.ExecutionState.StartedAt.IsZero() {
        stats.Duration = time.Since(ctx.ExecutionState.StartedAt)
    }

    return stats
}

func formatOutcomeForArchival(ctx *PlanContext, stats ExecutionStats) string {
    var sb strings.Builder

    sb.WriteString(fmt.Sprintf("# Plan Outcome: %s\n\n", ctx.CurrentPlan.Title))
    sb.WriteString(fmt.Sprintf("**Result**: %s\n\n", ctx.State))
    sb.WriteString(fmt.Sprintf("**Statistics**:\n"))
    sb.WriteString(fmt.Sprintf("- Completed: %d/%d steps\n", stats.CompletedSteps, stats.TotalSteps))
    sb.WriteString(fmt.Sprintf("- Failed: %d steps\n", stats.FailedSteps))
    sb.WriteString(fmt.Sprintf("- Skipped: %d steps\n", stats.SkippedSteps))
    sb.WriteString(fmt.Sprintf("- Duration: %s\n\n", stats.Duration))

    if ctx.State == PlanStateFailed && ctx.ExecutionState != nil {
        sb.WriteString(fmt.Sprintf("**Failure Reason**: %s\n\n", ctx.ExecutionState.FailureReason))
        if ctx.ExecutionState.FailedStep != nil {
            failedIdx := *ctx.ExecutionState.FailedStep
            for _, step := range ctx.CurrentPlan.Steps {
                if step.Index == failedIdx {
                    sb.WriteString(fmt.Sprintf("**Failed Step**: %d - %s\n\n", step.Index, step.Description))
                    break
                }
            }
        }
    }

    sb.WriteString("## Full Plan\n\n")
    sb.WriteString(formatPlanMarkdown(ctx.CurrentPlan))

    return sb.String()
}
```

### Failure Handling and Plan File Updates

When a step fails, the plan file is updated to clearly indicate the failure:

#### Failure Status Indicators

| Status | Checkbox | Icon | Meaning |
|--------|----------|------|---------|
| pending | `[ ]` | ○ | Not started |
| in_progress | `[~]` | ► | Currently executing |
| completed | `[x]` | ✓ | Successfully done |
| failed | `[!]` | ✗ | Execution failed |
| skipped | `[-]` | ⊘ | Skipped (dependency failed) |

#### Example: Plan File with Failed Step

```markdown
**Progress: 2/6 steps completed**

```
Execution Flow:
┌─ Layer 0 (parallel: 2 tasks) [DONE] ─┐
│  ✓ Step 1: [engineer]                │
│  ✓ Step 2: [engineer]                │
└────────────────────────────────────────┘
    │
    ▼
┌─ Layer 1 (parallel: 2 tasks) [FAILED] ─┐
│  ✗ Step 3: [engineer]                  │
│  ⊘ Step 4: [engineer]                  │
└────────────────────────────────────────┘
    │
    ▼
┌─ Layer 2 [PENDING] ─┐
│  ○ Step 5: [engineer]                  │
└────────────────────────────────────────┘
```

### Sequential Step Reference

[x] 1. [Layer 0] [engineer] Create JWT token utilities
[x] 2. [Layer 0] [engineer] Create auth middleware
[!] 3. [Layer 1] [engineer] Create login handler - **FAILED: compilation error in auth.go:45**
[-] 4. [Layer 1] [engineer] Create logout handler - *skipped due to step 3 failure*
[ ] 5. [Layer 2] [engineer] Update routes
[ ] 6. [Layer 3] [tester] Write tests
```

#### Failure Recovery

When a step fails, the Architect can:
1. **Retry**: Reset step status to `pending` and re-execute
2. **Skip**: Mark step as `skipped` and continue with independent steps
3. **Abort**: Mark plan as `failed` and archive outcome

```go
// agents/architect/recovery.go

func (a *ArchitectAgent) HandleStepFailure(ctx *PlanContext, failedStep int, err error) error {
    // Update step status
    updateStepStatus(ctx.CurrentPlan, failedStep, StepStatusFailed)

    // Mark dependent steps as skipped
    for i := range ctx.CurrentPlan.Steps {
        step := &ctx.CurrentPlan.Steps[i]
        if containsInt(step.DependsOn, failedStep) {
            step.Status = StepStatusSkipped
        }
    }

    // Add failure reason to execution state
    if ctx.ExecutionState == nil {
        ctx.ExecutionState = &ExecutionState{}
    }
    ctx.ExecutionState.FailedStep = &failedStep
    ctx.ExecutionState.FailureReason = err.Error()

    // Persist updated plan file
    return a.storage.SaveContext(ctx)
}
```

### Agent Plan Mode Integration

```
┌─────────────────────────────────────────────────────────────────────────────────────┐
│                        AGENT PLAN MODE COMPATIBILITY                                 │
├─────────────────────────────────────────────────────────────────────────────────────┤
│                                                                                     │
│  ┌─────────────────────────────────────────────────────────────────────────────┐   │
│  │ PRIMARY PLAN MODE USERS (Create and manage plans)                           │   │
│  ├─────────────────────────────────────────────────────────────────────────────┤   │
│  │                                                                             │   │
│  │  ARCHITECT                                                                  │   │
│  │  ├── Role: Plan Mode Owner                                                  │   │
│  │  ├── Skills: enter_plan_mode, exit_plan_mode, update_plan_file              │   │
│  │  ├── Actions:                                                               │   │
│  │  │   • Enters plan mode for non-trivial tasks                               │   │
│  │  │   • Consults knowledge agents during exploration                         │   │
│  │  │   • Writes structured implementation plans                               │   │
│  │  │   • Requests user approval                                               │   │
│  │  │   • Hands approved plans to Orchestrator                                 │   │
│  │  └── State transitions: IDLE → EXPLORING → DRAFTING → AWAITING_APPROVAL    │   │
│  │                                                                             │   │
│  │  TESTER                                                                     │   │
│  │  ├── Role: Test Plan Mode                                                   │   │
│  │  ├── Skills: enter_test_plan_mode, exit_test_plan_mode                      │   │
│  │  ├── Actions:                                                               │   │
│  │  │   • Creates test strategy plans                                          │   │
│  │  │   • Defines coverage targets and edge cases                              │   │
│  │  │   • Gets approval before test execution                                  │   │
│  │  └── Uses same state machine, different plan type                           │   │
│  │                                                                             │   │
│  │  DESIGNER                                                                   │   │
│  │  ├── Role: Design Plan Mode                                                 │   │
│  │  ├── Skills: enter_design_plan_mode, exit_design_plan_mode                  │   │
│  │  ├── Actions:                                                               │   │
│  │  │   • Creates design system change plans                                   │   │
│  │  │   • Component architecture proposals                                     │   │
│  │  │   • Visual/UX decision documentation                                     │   │
│  │  └── Uses same state machine, design-focused content                        │   │
│  │                                                                             │   │
│  └─────────────────────────────────────────────────────────────────────────────┘   │
│                                                                                     │
│  ┌─────────────────────────────────────────────────────────────────────────────┐   │
│  │ PLAN MODE EXECUTORS (Receive and execute approved plans)                    │   │
│  ├─────────────────────────────────────────────────────────────────────────────┤   │
│  │                                                                             │   │
│  │  ORCHESTRATOR                                                               │   │
│  │  ├── Role: Plan-to-DAG Converter                                            │   │
│  │  ├── Receives: Approved Plan from Architect                                 │   │
│  │  ├── Actions:                                                               │   │
│  │  │   • Converts PlanStep[] → DAG nodes                                      │   │
│  │  │   • Respects step dependencies                                           │   │
│  │  │   • Dispatches to Engineers                                              │   │
│  │  │   • Reports progress to Architect                                        │   │
│  │  └── Never creates plans, only executes                                     │   │
│  │                                                                             │   │
│  │  ENGINEER                                                                   │   │
│  │  ├── Role: Step Executor                                                    │   │
│  │  ├── Receives: Single step from Orchestrator                                │   │
│  │  ├── Actions:                                                               │   │
│  │  │   • Has read-only access to full plan for context                        │   │
│  │  │   • Executes assigned step                                               │   │
│  │  │   • Reports completion/failure                                           │   │
│  │  │   • Clarification requests bubble up through Orchestrator                │   │
│  │  └── No plan mode skills, just execution                                    │   │
│  │                                                                             │   │
│  │  INSPECTOR                                                                  │   │
│  │  ├── Role: Plan Compliance Validator                                        │   │
│  │  ├── Receives: Completed work for validation                                │   │
│  │  ├── Actions:                                                               │   │
│  │  │   • Validates implementation matches plan intent                         │   │
│  │  │   • Flags deviations: "Plan said X, implementation does Y"               │   │
│  │  │   • May trigger re-planning for significant issues                       │   │
│  │  └── Read-only plan access                                                  │   │
│  │                                                                             │   │
│  └─────────────────────────────────────────────────────────────────────────────┘   │
│                                                                                     │
│  ┌─────────────────────────────────────────────────────────────────────────────┐   │
│  │ PLAN MODE CONSULTANTS (Inform planning decisions)                           │   │
│  ├─────────────────────────────────────────────────────────────────────────────┤   │
│  │                                                                             │   │
│  │  LIBRARIAN                                                                  │   │
│  │  ├── Role: Codebase Context Provider                                        │   │
│  │  ├── Consulted during: EXPLORING phase                                      │   │
│  │  ├── Provides:                                                              │   │
│  │  │   • Existing patterns for similar functionality                          │   │
│  │  │   • Code structure and conventions                                       │   │
│  │  │   • Dependency information                                               │   │
│  │  └── Query-response only, no plan skills                                    │   │
│  │                                                                             │   │
│  │  ARCHIVALIST                                                                │   │
│  │  ├── Role: Historical Context Provider                                      │   │
│  │  ├── Consulted during: EXPLORING phase                                      │   │
│  │  ├── Provides:                                                              │   │
│  │  │   • Past attempts at similar tasks                                       │   │
│  │  │   • Previous failures and their causes                                   │   │
│  │  │   • Successful approaches for similar problems                           │   │
│  │  ├── Also:                                                                  │   │
│  │  │   • Stores completed plans for future reference                          │   │
│  │  │   • Records plan outcomes (success/failure)                              │   │
│  │  └── Query-response + storage, no plan skills                               │   │
│  │                                                                             │   │
│  │  ACADEMIC                                                                   │   │
│  │  ├── Role: External Knowledge Provider                                      │   │
│  │  ├── Consulted during: EXPLORING phase                                      │   │
│  │  ├── Provides:                                                              │   │
│  │  │   • Best practices from external sources                                 │   │
│  │  │   • Security considerations                                              │   │
│  │  │   • Performance implications                                             │   │
│  │  └── Query-response only, no plan skills                                    │   │
│  │                                                                             │   │
│  └─────────────────────────────────────────────────────────────────────────────┘   │
│                                                                                     │
│  ┌─────────────────────────────────────────────────────────────────────────────┐   │
│  │ PLAN MODE INFRASTRUCTURE (Routing and state management)                     │   │
│  ├─────────────────────────────────────────────────────────────────────────────┤   │
│  │                                                                             │   │
│  │  GUIDE                                                                      │   │
│  │  ├── Role: Plan State Router                                                │   │
│  │  ├── Responsibilities:                                                      │   │
│  │  │   • Tracks plan state per session                                        │   │
│  │  │   • Routes plan-related messages                                         │   │
│  │  │   • Blocks non-plan actions during AWAITING_APPROVAL                     │   │
│  │  │   • Notifies user of plan state changes                                  │   │
│  │  ├── Message types:                                                         │   │
│  │  │   • PLAN_ENTER, PLAN_EXIT                                                │   │
│  │  │   • PLAN_APPROVAL_REQUEST, PLAN_APPROVED, PLAN_REJECTED                  │   │
│  │  │   • PLAN_MODIFIED, PLAN_EXECUTION_UPDATE                                 │   │
│  │  └── No plan skills, routing infrastructure only                            │   │
│  │                                                                             │   │
│  └─────────────────────────────────────────────────────────────────────────────┘   │
│                                                                                     │
└─────────────────────────────────────────────────────────────────────────────────────┘
```

### Complete Plan Mode Flow

```
┌─────────────────────────────────────────────────────────────────────────────────────┐
│                           COMPLETE PLAN MODE FLOW                                    │
├─────────────────────────────────────────────────────────────────────────────────────┤
│                                                                                     │
│  USER REQUEST                                                                       │
│       │                                                                             │
│       │ "Add user authentication with OAuth"                                        │
│       ▼                                                                             │
│  ┌─────────────────────────────────────────────────────────────────────────────┐   │
│  │  GUIDE: Intent Classification                                               │   │
│  │  ├── Complexity: HIGH (multiple files, architectural decision)              │   │
│  │  ├── Route to: ARCHITECT                                                    │   │
│  │  └── Recommendation: PLAN_MODE                                              │   │
│  └─────────────────────────────────────────────────────────────────────────────┘   │
│       │                                                                             │
│       ▼                                                                             │
│  ┌─────────────────────────────────────────────────────────────────────────────┐   │
│  │  ARCHITECT: enter_plan_mode                                                 │   │
│  │  State: IDLE → EXPLORING                                                    │   │
│  │  Creates: .sylk/plans/{session_id}/                                         │   │
│  └─────────────────────────────────────────────────────────────────────────────┘   │
│       │                                                                             │
│       │ Consultation Phase                                                          │
│       ▼                                                                             │
│  ┌─────────────────────────────────────────────────────────────────────────────┐   │
│  │  EXPLORING PHASE                                                            │   │
│  │                                                                             │   │
│  │  Architect → Guide → Librarian:                                             │   │
│  │    Q: "What authentication patterns exist in this codebase?"                │   │
│  │    A: "Found JWT middleware in pkg/auth/jwt.go, session store in            │   │
│  │        pkg/session/store.go. No OAuth implementation exists."               │   │
│  │                                                                             │   │
│  │  Architect → Guide → Archivalist:                                           │   │
│  │    Q: "Any past OAuth implementations or attempts?"                         │   │
│  │    A: "Session sess-abc attempted OAuth 3 months ago, abandoned due to      │   │
│  │        callback URL issues. Recommendation: use established library."       │   │
│  │                                                                             │   │
│  │  Architect → Guide → Academic:                                              │   │
│  │    Q: "Best practices for Go OAuth2 implementation?"                        │   │
│  │    A: "Recommended: golang.org/x/oauth2 package. Security considerations:   │   │
│  │        state parameter, PKCE for public clients, secure token storage."     │   │
│  │                                                                             │   │
│  └─────────────────────────────────────────────────────────────────────────────┘   │
│       │                                                                             │
│       │ State: EXPLORING → DRAFTING                                                 │
│       ▼                                                                             │
│  ┌─────────────────────────────────────────────────────────────────────────────┐   │
│  │  DRAFTING PHASE                                                             │   │
│  │                                                                             │   │
│  │  Architect writes to .sylk/plans/{session}/current_plan.md:                 │   │
│  │                                                                             │   │
│  │  ┌───────────────────────────────────────────────────────────────────────┐  │   │
│  │  │ # Plan: Add OAuth Authentication                                     │  │   │
│  │  │                                                                       │  │   │
│  │  │ ## Overview                                                           │  │   │
│  │  │ Implement OAuth 2.0 authentication using golang.org/x/oauth2         │  │   │
│  │  │ with Google as the initial provider. Integrate with existing          │  │   │
│  │  │ session management in pkg/session/.                                   │  │   │
│  │  │                                                                       │  │   │
│  │  │ ## Files to Modify                                                    │  │   │
│  │  │ - `pkg/auth/oauth.go` (create): OAuth provider configuration          │  │   │
│  │  │ - `pkg/auth/callback.go` (create): Callback handler                   │  │   │
│  │  │ - `pkg/auth/middleware.go` (modify): Add OAuth token validation       │  │   │
│  │  │ - `cmd/server/routes.go` (modify): Add OAuth routes                   │  │   │
│  │  │ - `config/config.go` (modify): Add OAuth config fields                │  │   │
│  │  │                                                                       │  │   │
│  │  │ ## Implementation Steps                                               │  │   │
│  │  │ 1. [Engineer] Add OAuth config fields to config struct               │  │   │
│  │  │ 2. [Engineer] Create OAuth provider setup in pkg/auth/oauth.go       │  │   │
│  │  │ 3. [Engineer] Implement callback handler (depends on: 1, 2)          │  │   │
│  │  │ 4. [Engineer] Update middleware for OAuth tokens (depends on: 2)     │  │   │
│  │  │ 5. [Engineer] Add routes to server (depends on: 3, 4)                │  │   │
│  │  │ 6. [Tester] Write integration tests (depends on: 5)                  │  │   │
│  │  │ 7. [Inspector] Security audit (depends on: 5)                        │  │   │
│  │  │                                                                       │  │   │
│  │  │ ## Considerations                                                     │  │   │
│  │  │ - State parameter required for CSRF protection                       │  │   │
│  │  │ - Token refresh needs background goroutine                           │  │   │
│  │  │ - Consider PKCE for mobile/SPA clients in future                     │  │   │
│  │  │                                                                       │  │   │
│  │  │ ## Required Permissions                                               │  │   │
│  │  │ - Bash: run tests                                                     │  │   │
│  │  │ - Bash: install dependencies                                          │  │   │
│  │  └───────────────────────────────────────────────────────────────────────┘  │   │
│  │                                                                             │   │
│  └─────────────────────────────────────────────────────────────────────────────┘   │
│       │                                                                             │
│       │ exit_plan_mode                                                              │
│       │ State: DRAFTING → AWAITING_APPROVAL                                         │
│       ▼                                                                             │
│  ┌─────────────────────────────────────────────────────────────────────────────┐   │
│  │  USER APPROVAL                                                              │   │
│  │                                                                             │   │
│  │  Guide displays plan to user with options:                                  │   │
│  │  ├── [Approve] - Proceed with execution                                     │   │
│  │  ├── [Modify] - Request changes to plan                                     │   │
│  │  └── [Reject] - Cancel and return to IDLE                                   │   │
│  │                                                                             │   │
│  │  BLOCKING: No other actions allowed until user responds                     │   │
│  │                                                                             │   │
│  └─────────────────────────────────────────────────────────────────────────────┘   │
│       │                                                                             │
│       │ User: "Approve"                                                             │
│       │ State: AWAITING_APPROVAL → APPROVED                                         │
│       ▼                                                                             │
│  ┌─────────────────────────────────────────────────────────────────────────────┐   │
│  │  ARCHITECT → ORCHESTRATOR: Dispatch Plan                                    │   │
│  │                                                                             │   │
│  │  Message: {                                                                 │   │
│  │    type: "PLAN_DISPATCH",                                                   │   │
│  │    plan_id: "plan-123",                                                     │   │
│  │    session_id: "sess-456",                                                  │   │
│  │    steps: [...],                                                            │   │
│  │    allowed_prompts: [...]                                                   │   │
│  │  }                                                                          │   │
│  │                                                                             │   │
│  │  State: APPROVED → EXECUTING                                                │   │
│  │                                                                             │   │
│  └─────────────────────────────────────────────────────────────────────────────┘   │
│       │                                                                             │
│       ▼                                                                             │
│  ┌─────────────────────────────────────────────────────────────────────────────┐   │
│  │  ORCHESTRATOR: DAG Execution                                                │   │
│  │                                                                             │   │
│  │  Converts steps to DAG:                                                     │   │
│  │                                                                             │   │
│  │       [Step 1]──┬──[Step 2]──┬──[Step 3]                                    │   │
│  │                 │            │                                              │   │
│  │                 │            └──[Step 4]──┬──[Step 5]──[Step 6]             │   │
│  │                 │                         │            │                    │   │
│  │                 └─────────────────────────┘            └──[Step 7]          │   │
│  │                                                                             │   │
│  │  Dispatches to Engineers (parallel where possible)                          │   │
│  │  Updates todo list as steps complete                                        │   │
│  │  Reports progress to Architect                                              │   │
│  │                                                                             │   │
│  └─────────────────────────────────────────────────────────────────────────────┘   │
│       │                                                                             │
│       │ All steps complete                                                          │
│       │ State: EXECUTING → COMPLETED                                                │
│       ▼                                                                             │
│  ┌─────────────────────────────────────────────────────────────────────────────┐   │
│  │  COMPLETION                                                                 │   │
│  │                                                                             │   │
│  │  Architect → User: "OAuth authentication implementation complete.          │   │
│  │                     All 7 steps executed successfully."                     │   │
│  │                                                                             │   │
│  │  Archivalist: Store plan outcome for future reference                       │   │
│  │                                                                             │   │
│  │  State: COMPLETED → IDLE (ready for next task)                              │   │
│  │                                                                             │   │
│  └─────────────────────────────────────────────────────────────────────────────┘   │
│                                                                                     │
└─────────────────────────────────────────────────────────────────────────────────────┘
```

### Plan Mode Skills Implementation

```go
// agents/architect/skills_plan.go

package architect

import (
    "context"
    "encoding/json"
    "time"

    "github.com/adalundhe/sylk/core/plan"
    "github.com/adalundhe/sylk/core/skills"
)

// Plan mode skills for Architect agent

func (a *Architect) registerPlanModeSkills() {
    a.skills.Register(enterPlanModeSkill(a))
    a.skills.Register(exitPlanModeSkill(a))
    a.skills.Register(updatePlanFileSkill(a))
    a.skills.Register(todoWriteSkill(a))
    a.skills.Register(askUserQuestionSkill(a))
}

func enterPlanModeSkill(a *Architect) *skills.Skill {
    return skills.NewSkill("enter_plan_mode").
        Description("Enter plan mode for complex tasks requiring user approval before implementation").
        Domain("planning").
        Keywords("plan", "complex", "design", "architecture", "multi-step").
        StringParam("task_description", "Description of the task to plan", true).
        StringParam("plan_file", "Optional custom path for plan file", false).
        Handler(func(ctx context.Context, input json.RawMessage) (any, error) {
            var params struct {
                TaskDescription string `json:"task_description"`
                PlanFile        string `json:"plan_file"`
            }
            if err := json.Unmarshal(input, &params); err != nil {
                return nil, err
            }

            // Get session from context
            sessionID := ctx.Value("session_id").(string)

            // Initialize plan context
            planCtx := &plan.PlanContext{
                SessionID: sessionID,
                State:     plan.PlanStateExploring,
                CurrentPlan: &plan.Plan{
                    ID:        generatePlanID(),
                    Title:     params.TaskDescription,
                    Version:   1,
                    CreatedAt: time.Now(),
                    CreatedBy: "architect",
                },
                CreatedAt: time.Now(),
                UpdatedAt: time.Now(),
            }

            // Persist plan context
            if err := a.planStorage.SaveContext(planCtx); err != nil {
                return nil, err
            }

            // Update session state
            a.sessionState.SetPlanContext(sessionID, planCtx)

            return map[string]any{
                "status":     "entered_plan_mode",
                "session_id": sessionID,
                "plan_id":    planCtx.CurrentPlan.ID,
                "state":      planCtx.State,
                "message":    "Now in exploration phase. Query Librarian, Archivalist, and Academic for context.",
            }, nil
        }).
        Build()
}

func exitPlanModeSkill(a *Architect) *skills.Skill {
    return skills.NewSkill("exit_plan_mode").
        Description("Exit plan mode and request user approval for the current plan").
        Domain("planning").
        Keywords("approve", "ready", "proceed", "submit").
        ArrayParam("allowed_prompts", "Bash permissions needed for execution", false).
        Handler(func(ctx context.Context, input json.RawMessage) (any, error) {
            var params struct {
                AllowedPrompts []plan.AllowedPrompt `json:"allowed_prompts"`
            }
            json.Unmarshal(input, &params) // Optional params

            sessionID := ctx.Value("session_id").(string)

            // Get current plan context
            planCtx, err := a.planStorage.LoadContext(sessionID)
            if err != nil {
                return nil, err
            }

            if planCtx.State != plan.PlanStateDrafting && planCtx.State != plan.PlanStateExploring {
                return nil, fmt.Errorf("cannot exit plan mode from state: %s", planCtx.State)
            }

            // Update allowed prompts
            if len(params.AllowedPrompts) > 0 {
                planCtx.CurrentPlan.AllowedPrompts = params.AllowedPrompts
            }

            // Transition to awaiting approval
            planCtx.State = plan.PlanStateAwaitingApproval
            planCtx.UpdatedAt = time.Now()

            if err := a.planStorage.SaveContext(planCtx); err != nil {
                return nil, err
            }

            // Send approval request through Guide
            a.sendPlanApprovalRequest(sessionID, planCtx.CurrentPlan)

            return map[string]any{
                "status":  "awaiting_approval",
                "plan_id": planCtx.CurrentPlan.ID,
                "message": "Plan submitted for user approval. Waiting for response.",
            }, nil
        }).
        Build()
}

func updatePlanFileSkill(a *Architect) *skills.Skill {
    return skills.NewSkill("update_plan_file").
        Description("Update the current plan with new content").
        Domain("planning").
        Keywords("update", "revise", "modify", "plan").
        StringParam("overview", "Plan overview/summary", false).
        ArrayParam("files_to_modify", "Files that will be changed", false).
        ArrayParam("steps", "Implementation steps", false).
        ArrayParam("considerations", "Trade-offs and risks", false).
        Handler(func(ctx context.Context, input json.RawMessage) (any, error) {
            var params struct {
                Overview       string            `json:"overview"`
                FilesToModify  []plan.FileChange `json:"files_to_modify"`
                Steps          []plan.PlanStep   `json:"steps"`
                Considerations []string          `json:"considerations"`
            }
            if err := json.Unmarshal(input, &params); err != nil {
                return nil, err
            }

            sessionID := ctx.Value("session_id").(string)
            planCtx, err := a.planStorage.LoadContext(sessionID)
            if err != nil {
                return nil, err
            }

            // Update plan fields if provided
            if params.Overview != "" {
                planCtx.CurrentPlan.Overview = params.Overview
            }
            if len(params.FilesToModify) > 0 {
                planCtx.CurrentPlan.FilesToModify = params.FilesToModify
            }
            if len(params.Steps) > 0 {
                planCtx.CurrentPlan.Steps = params.Steps
            }
            if len(params.Considerations) > 0 {
                planCtx.CurrentPlan.Considerations = params.Considerations
            }

            // Move to drafting if still exploring
            if planCtx.State == plan.PlanStateExploring {
                planCtx.State = plan.PlanStateDrafting
            }

            planCtx.UpdatedAt = time.Now()

            if err := a.planStorage.SaveContext(planCtx); err != nil {
                return nil, err
            }

            return map[string]any{
                "status":  "plan_updated",
                "plan_id": planCtx.CurrentPlan.ID,
                "state":   planCtx.State,
            }, nil
        }).
        Build()
}

func todoWriteSkill(a *Architect) *skills.Skill {
    return skills.NewSkill("todo_write").
        Description("Manage task todo list for tracking progress").
        Domain("tracking").
        Keywords("todo", "task", "track", "progress").
        ArrayParam("todos", "Array of {content, status, activeForm}", true).
        Handler(func(ctx context.Context, input json.RawMessage) (any, error) {
            var params struct {
                Todos []plan.TodoItem `json:"todos"`
            }
            if err := json.Unmarshal(input, &params); err != nil {
                return nil, err
            }

            sessionID := ctx.Value("session_id").(string)
            planCtx, err := a.planStorage.LoadContext(sessionID)
            if err != nil {
                return nil, err
            }

            planCtx.Todos = params.Todos
            planCtx.UpdatedAt = time.Now()

            if err := a.planStorage.SaveContext(planCtx); err != nil {
                return nil, err
            }

            return map[string]any{
                "status":      "todos_updated",
                "todo_count":  len(params.Todos),
                "in_progress": countByStatus(params.Todos, plan.TodoStatusInProgress),
                "completed":   countByStatus(params.Todos, plan.TodoStatusCompleted),
            }, nil
        }).
        Build()
}

func askUserQuestionSkill(a *Architect) *skills.Skill {
    return skills.NewSkill("ask_user_question").
        Description("Ask user for clarification or decision during planning").
        Domain("coordination").
        Keywords("ask", "clarify", "question", "decide").
        ArrayParam("questions", "Questions with options to present", true).
        Handler(func(ctx context.Context, input json.RawMessage) (any, error) {
            var params struct {
                Questions []UserQuestion `json:"questions"`
            }
            if err := json.Unmarshal(input, &params); err != nil {
                return nil, err
            }

            sessionID := ctx.Value("session_id").(string)

            // Send questions to user through Guide
            responseID := a.sendUserQuestions(sessionID, params.Questions)

            return map[string]any{
                "status":      "questions_sent",
                "response_id": responseID,
                "message":     "Waiting for user response to continue planning.",
            }, nil
        }).
        Build()
}

// Helper types

type UserQuestion struct {
    Question    string           `json:"question"`
    Header      string           `json:"header"`
    Options     []QuestionOption `json:"options"`
    MultiSelect bool             `json:"multi_select"`
}

type QuestionOption struct {
    Label       string `json:"label"`
    Description string `json:"description"`
}

func countByStatus(todos []plan.TodoItem, status plan.TodoStatus) int {
    count := 0
    for _, t := range todos {
        if t.Status == status {
            count++
        }
    }
    return count
}
```

### Guide Plan Mode Routing

```go
// agents/guide/plan_routing.go

package guide

import (
    "context"
    "fmt"

    "github.com/adalundhe/sylk/core/plan"
)

// Plan mode message types
const (
    MessageTypePlanEnter           MessageType = "plan_enter"
    MessageTypePlanExit            MessageType = "plan_exit"
    MessageTypePlanApprovalRequest MessageType = "plan_approval_request"
    MessageTypePlanApproved        MessageType = "plan_approved"
    MessageTypePlanRejected        MessageType = "plan_rejected"
    MessageTypePlanModified        MessageType = "plan_modified"
    MessageTypePlanDispatch        MessageType = "plan_dispatch"
    MessageTypePlanProgress        MessageType = "plan_progress"
    MessageTypePlanCompleted       MessageType = "plan_completed"
    MessageTypePlanFailed          MessageType = "plan_failed"
)

// PlanStateTracker manages plan state per session
type PlanStateTracker struct {
    storage *plan.Storage
    states  map[string]plan.PlanState // sessionID -> state
}

func NewPlanStateTracker(storage *plan.Storage) *PlanStateTracker {
    return &PlanStateTracker{
        storage: storage,
        states:  make(map[string]plan.PlanState),
    }
}

// GetState returns the current plan state for a session
func (t *PlanStateTracker) GetState(sessionID string) plan.PlanState {
    if state, ok := t.states[sessionID]; ok {
        return state
    }

    // Load from storage
    ctx, err := t.storage.LoadContext(sessionID)
    if err != nil {
        return plan.PlanStateIdle
    }

    t.states[sessionID] = ctx.State
    return ctx.State
}

// SetState updates the plan state for a session
func (t *PlanStateTracker) SetState(sessionID string, state plan.PlanState) {
    t.states[sessionID] = state
}

// IsBlocking returns true if plan mode blocks other operations
func (t *PlanStateTracker) IsBlocking(sessionID string) bool {
    state := t.GetState(sessionID)
    return state == plan.PlanStateAwaitingApproval
}

// CanRoute checks if a message can be routed given plan state
func (t *PlanStateTracker) CanRoute(sessionID string, msgType MessageType) error {
    state := t.GetState(sessionID)

    // Allow plan-related messages always
    if isPlanMessage(msgType) {
        return nil
    }

    // Block non-plan messages during approval wait
    if state == plan.PlanStateAwaitingApproval {
        return fmt.Errorf("awaiting plan approval - respond to plan before continuing")
    }

    return nil
}

func isPlanMessage(msgType MessageType) bool {
    switch msgType {
    case MessageTypePlanEnter,
        MessageTypePlanExit,
        MessageTypePlanApprovalRequest,
        MessageTypePlanApproved,
        MessageTypePlanRejected,
        MessageTypePlanModified,
        MessageTypePlanDispatch,
        MessageTypePlanProgress,
        MessageTypePlanCompleted,
        MessageTypePlanFailed:
        return true
    }
    return false
}

// HandlePlanApproval processes user response to plan approval request
func (g *Guide) HandlePlanApproval(ctx context.Context, sessionID string, approved bool, modifications string) error {
    planCtx, err := g.planStorage.LoadContext(sessionID)
    if err != nil {
        return err
    }

    if planCtx.State != plan.PlanStateAwaitingApproval {
        return fmt.Errorf("not awaiting approval, current state: %s", planCtx.State)
    }

    if approved {
        // Transition to approved
        planCtx.State = plan.PlanStateApproved

        if err := g.planStorage.SaveContext(planCtx); err != nil {
            return err
        }

        // Notify Architect to dispatch to Orchestrator
        msg := NewMessage(MessageTypePlanApproved, map[string]any{
            "session_id": sessionID,
            "plan_id":    planCtx.CurrentPlan.ID,
        })
        g.bus.Publish(TopicArchitect, msg)

        g.planTracker.SetState(sessionID, plan.PlanStateApproved)

    } else if modifications != "" {
        // User wants modifications
        planCtx.State = plan.PlanStateExploring // Back to exploration

        // Save current as version
        if err := g.planStorage.SavePlanVersion(sessionID, planCtx.CurrentPlan, "user_modification_request"); err != nil {
            return err
        }

        planCtx.CurrentPlan.Version++

        if err := g.planStorage.SaveContext(planCtx); err != nil {
            return err
        }

        // Notify Architect of modification request
        msg := NewMessage(MessageTypePlanModified, map[string]any{
            "session_id":    sessionID,
            "plan_id":       planCtx.CurrentPlan.ID,
            "modifications": modifications,
        })
        g.bus.Publish(TopicArchitect, msg)

        g.planTracker.SetState(sessionID, plan.PlanStateExploring)

    } else {
        // Rejected entirely
        planCtx.State = plan.PlanStateIdle

        if err := g.planStorage.SaveContext(planCtx); err != nil {
            return err
        }

        msg := NewMessage(MessageTypePlanRejected, map[string]any{
            "session_id": sessionID,
            "plan_id":    planCtx.CurrentPlan.ID,
        })
        g.bus.Publish(TopicArchitect, msg)

        g.planTracker.SetState(sessionID, plan.PlanStateIdle)
    }

    return nil
}
```

### Integration with Session Management

```go
// core/session/plan_integration.go

package session

import (
    "github.com/adalundhe/sylk/core/plan"
)

// SessionPlanIntegration adds plan context to sessions
type SessionPlanIntegration struct {
    storage *plan.Storage
}

// InitPlanContext initializes plan context for a new session
func (s *SessionPlanIntegration) InitPlanContext(sessionID string, projectRoot string) error {
    storage := plan.NewStorage(projectRoot)
    return storage.EnsureDir(sessionID)
}

// GetPlanContext retrieves plan context for a session
func (s *SessionPlanIntegration) GetPlanContext(sessionID string) (*plan.PlanContext, error) {
    return s.storage.LoadContext(sessionID)
}

// CleanupPlanContext removes plan data for a session (optional)
func (s *SessionPlanIntegration) CleanupPlanContext(sessionID string, keepHistory bool) error {
    if keepHistory {
        // Archive to Archivalist before cleanup
        return s.archivePlanHistory(sessionID)
    }
    // Remove plan directory
    return s.storage.RemoveSession(sessionID)
}

func (s *SessionPlanIntegration) archivePlanHistory(sessionID string) error {
    ctx, err := s.storage.LoadContext(sessionID)
    if err != nil {
        return err
    }

    // Store plan and outcome in Archivalist for cross-session learning
    // Implementation depends on Archivalist API
    return nil
}
```

### Plan Mode Skill Definitions (SKILL.md Format)

```
agents/architect/skillfiles/
├── enter-plan-mode/
│   └── SKILL.md
├── exit-plan-mode/
│   └── SKILL.md
├── update-plan-file/
│   └── SKILL.md
├── todo-write/
│   └── SKILL.md
└── ask-user-question/
    └── SKILL.md
```

**agents/architect/skillfiles/enter-plan-mode/SKILL.md:**

```markdown
---
name: enter-plan-mode
description: Enter plan mode for complex tasks requiring user approval. Use when task involves multiple files, architectural decisions, or unclear requirements.
---

# Enter Plan Mode

## When to Use
- New feature implementation (non-trivial)
- Multiple valid approaches exist
- Architectural decisions required
- Multi-file changes expected (>3 files)
- Unclear requirements need exploration

## When NOT to Use
- Single-line fixes or typos
- Adding comments or documentation
- Simple bug fixes with obvious solution
- User gave very specific instructions

## Parameters
- `task_description` (required): Clear description of what needs to be planned
- `plan_file` (optional): Custom path for plan file

## Workflow After Entering
1. Query Librarian for existing patterns
2. Query Archivalist for past attempts
3. Query Academic for best practices
4. Draft plan with update_plan_file
5. Exit with exit_plan_mode for approval

## Example
```json
{
  "task_description": "Add OAuth authentication with Google provider"
}
```
```

**agents/architect/skillfiles/exit-plan-mode/SKILL.md:**

```markdown
---
name: exit-plan-mode
description: Exit plan mode and request user approval. Call after plan is complete and ready for review.
---

# Exit Plan Mode

## When to Use
- Plan is fully drafted with all steps
- Files to modify are identified
- Considerations are documented
- Ready for user review

## Parameters
- `allowed_prompts` (optional): Bash permissions plan needs
  - Example: `[{"tool": "Bash", "prompt": "run tests"}]`

## What Happens
1. Plan state changes to AWAITING_APPROVAL
2. User sees plan and approval options
3. Session blocks until user responds
4. On approve: Plan dispatched to Orchestrator
5. On modify: Return to exploration with feedback
6. On reject: Return to idle state

## Important
- Ensure plan is complete before calling
- User MUST respond before work continues
- Use ask_user_question for pre-approval clarifications
```

---

## Research Paper Architecture

The Academic agent produces **research papers** - comprehensive technical documents that capture investigation findings, architectural recommendations, and implementation guidance. These papers serve as the bridge between exploratory research and actionable plans.

### Research Paper Overview

```
┌─────────────────────────────────────────────────────────────────────────────────┐
│                    RESEARCH PAPER LIFECYCLE                                      │
├─────────────────────────────────────────────────────────────────────────────────┤
│                                                                                 │
│  USER                    ACADEMIC                  ARCHITECT                    │
│    │                        │                          │                        │
│    │  "Research how to      │                          │                        │
│    │   implement caching"   │                          │                        │
│    │───────────────────────►│                          │                        │
│    │                        │                          │                        │
│    │                        │  [Investigates]          │                        │
│    │                        │  • Web searches          │                        │
│    │                        │  • Reads docs            │                        │
│    │                        │  • Analyzes patterns     │                        │
│    │                        │  • Consults Librarian    │                        │
│    │                        │                          │                        │
│    │◄───────────────────────│                          │                        │
│    │  "I found 3 approaches │                          │                        │
│    │   with tradeoffs..."   │                          │                        │
│    │                        │                          │                        │
│    │  "Let's go with Redis" │                          │                        │
│    │───────────────────────►│                          │                        │
│    │                        │                          │                        │
│    │  "Ready to start work" │                          │                        │
│    │───────────────────────►│                          │                        │
│    │                        │                          │                        │
│    │                        │  [write_research_paper]  │                        │
│    │                        │  Sends PROPOSAL message  │                        │
│    │                        │  via Guide ─────────────►│                        │
│    │                        │                          │                        │
│    │                        │                          │  [read_research_paper] │
│    │                        │                          │  Triggered by PROPOSAL │
│    │                        │                          │  message type          │
│    │                        │                          │                        │
│    │◄─────────────────────────────────────────────────│                        │
│    │  "Based on the research, here's my proposed plan" │                        │
│    │                                                   │                        │
└─────────────────────────────────────────────────────────────────────────────────┘
```

### Research Paper Storage

Research papers follow the same project-hash storage pattern as plans:

```
~/.sylk/projects/{project_hash}/
├── project_info.json
│
├── plans/                              ← Architect's domain
│   └── {plan_slug}/
│       └── ...
│
├── research/                           ← Academic's domain
│   ├── implement-redis-caching/        ← Research slug
│   │   ├── metadata.json               ← Research metadata & version history
│   │   ├── context.json                ← Current research context
│   │   ├── decisions.json              ← User decisions on recommendations
│   │   ├── sources.json                ← URLs, docs referenced
│   │   ├── current.md                  ← Copy of HEAD version
│   │   ├── v1.md                       ← Version 1
│   │   ├── v2.md                       ← Version 2
│   │   └── attachments/                ← Diagrams, code samples
│   │       ├── architecture.mermaid
│   │       └── benchmark_results.json
│   │
│   └── evaluate-auth-libraries/
│       └── ...
│
└── archived/
    ├── plans/...
    └── research/...                    ← Completed/superseded research
```

**Example paths:**
```
~/.sylk/projects/a1b2c3d4/research/implement-redis-caching/paper.md
~/.sylk/projects/a1b2c3d4/research/implement-redis-caching/v2.md
~/.sylk/projects/a1b2c3d4/research/implement-redis-caching/decisions.json
```

### Research Paper Document Structure

```markdown
<!-- Research Paper: Implement Redis Caching | Version: 2 | Status: READY_FOR_IMPLEMENTATION -->
<!-- Academic: academic_agent_abc123 | Session: session_xyz789 -->

# Research Paper: Implement Redis Caching

## Executive Summary

Brief overview of findings and recommended approach.

- **Primary Recommendation**: Redis with read-through caching pattern
- **Complexity**: Medium
- **Estimated Scope**: 4-6 files, 3 topological layers
- **Key Dependencies**: redis-go v9, connection pooling

---

## Problem Statement

### Context
What problem are we solving? Why does it matter?

### Requirements
- [ ] Cache API responses with configurable TTL
- [ ] Support cache invalidation on data mutations
- [ ] Handle cache failures gracefully (fallback to DB)
- [ ] Provide cache hit/miss metrics

### Constraints
- Must work with existing PostgreSQL setup
- Cannot exceed 512MB memory budget
- Must support horizontal scaling

---

## Research Findings

### Approach A: Redis (Recommended)

#### Description
Distributed in-memory cache with persistence options.

#### Architecture
```
┌─────────────┐     ┌─────────────┐     ┌─────────────┐
│   Client    │────►│   API       │────►│   Redis     │
│             │     │   Server    │     │   Cluster   │
└─────────────┘     └──────┬──────┘     └─────────────┘
                          │                    │
                          │ cache miss         │ cache hit
                          ▼                    │
                   ┌─────────────┐             │
                   │  PostgreSQL │◄────────────┘
                   └─────────────┘   write-through
```

#### Implementation Pattern
```go
// Cache-aside pattern with fallback
func (s *Service) GetUser(ctx context.Context, id string) (*User, error) {
    // Try cache first
    cached, err := s.cache.Get(ctx, userKey(id))
    if err == nil {
        return cached.(*User), nil
    }

    // Cache miss - fetch from DB
    user, err := s.db.GetUser(ctx, id)
    if err != nil {
        return nil, err
    }

    // Populate cache (async)
    go s.cache.Set(ctx, userKey(id), user, s.ttl)

    return user, nil
}
```

#### Pros
- Battle-tested, widely adopted
- Excellent performance (sub-millisecond reads)
- Rich data structures
- Built-in clustering

#### Cons
- Additional infrastructure
- Memory-bound
- Requires connection pooling

### Approach B: In-Memory (go-cache)

[... similar structure ...]

### Approach C: PostgreSQL Materialized Views

[... similar structure ...]

---

## Recommended Architecture

### System Design
[Detailed architecture diagram]

### Data Flow
[Step-by-step flow description]

### State Machine
[State diagram for relevant components]

---

## Implementation Guide

### Files to Create/Modify

| File | Action | Purpose |
|------|--------|---------|
| `core/cache/client.go` | create | Redis client wrapper |
| `core/cache/config.go` | create | Cache configuration |
| `core/cache/middleware.go` | create | HTTP middleware |
| `services/user/service.go` | modify | Add cache integration |

### Implementation Steps (Suggested Order)

1. **[Layer 0]** Create cache client and config
2. **[Layer 0]** Create key generation utilities
3. **[Layer 1]** Create cache middleware (depends on client)
4. **[Layer 2]** Integrate into user service (depends on middleware)
5. **[Layer 3]** Write integration tests

---

## Acceptance Criteria

### Functional
- [ ] Cache hit returns response in < 5ms (p99)
- [ ] Cache miss falls back to database successfully
- [ ] Cache invalidation occurs within 100ms of mutation

### Non-Functional
- [ ] Memory usage stays under 512MB
- [ ] Graceful degradation when Redis unavailable

---

## Edge Cases & Potential Problems

### ⚠️ EC-1: Cache Stampede
**Problem**: When a popular cache key expires, many requests simultaneously hit the database.

**Mitigation Options**:
```
[x] ACCEPTED   - Implement distributed locking (singleflight pattern)
[ ] REJECTED   - Use probabilistic early expiration
[ ] DEFERRED   - Add request coalescing at load balancer
```

**Implementation Note**: Use `golang.org/x/sync/singleflight`

### ⚠️ EC-2: Thundering Herd on Startup
**Problem**: Cold cache after deployment causes all requests to hit database.

**Mitigation Options**:
```
[x] ACCEPTED   - Implement cache warming on startup
[ ] IGNORED    - Accept temporary latency spike
```

### ⚠️ EC-3: Stale Data After Partial Failure
**Problem**: Database write succeeds but cache invalidation fails.

**Mitigation Options**:
```
[x] ACCEPTED   - Use short TTL as safety net
[x] ACCEPTED   - Implement retry with exponential backoff
[ ] REJECTED   - Use transactional outbox pattern (too complex)
```

---

## Open Questions

1. **TTL Duration**: What's the acceptable staleness? (Suggested: 5 minutes)
2. **Cache Scope**: Cache at API response level or entity level?

---

## References

- [Redis Best Practices](https://redis.io/docs/manual/patterns/)
- Internal: `services/session/cache.go` (existing pattern)
```

### Research Paper Metadata

```go
// core/research/types.go

// ResearchStatus tracks paper lifecycle
type ResearchStatus string

const (
    ResearchStatusDraft               ResearchStatus = "DRAFT"
    ResearchStatusReadyForReview      ResearchStatus = "READY_FOR_REVIEW"
    ResearchStatusReadyForImplementation ResearchStatus = "READY_FOR_IMPLEMENTATION"
    ResearchStatusApproved            ResearchStatus = "APPROVED"
    ResearchStatusSuperseded          ResearchStatus = "SUPERSEDED"
    ResearchStatusArchived            ResearchStatus = "ARCHIVED"
)

// ResearchMetadata stored in metadata.json
type ResearchMetadata struct {
    PaperID         string            `json:"paper_id"`
    Slug            string            `json:"slug"`
    Title           string            `json:"title"`
    Status          ResearchStatus    `json:"status"`
    Head            int               `json:"head"`              // Current version
    CreatedAt       time.Time         `json:"created_at"`
    UpdatedAt       time.Time         `json:"updated_at"`
    AcademicAgent   string            `json:"academic_agent"`
    SessionID       string            `json:"session_id"`
    ProjectHash     string            `json:"project_hash"`
    Tags            []string          `json:"tags"`
    ApproachesEval  int               `json:"approaches_evaluated"`
    Recommended     string            `json:"recommended_approach"`
    EdgeCasesCount  int               `json:"edge_cases_count"`
    OpenQuestions   int               `json:"open_questions_count"`
    LinkedPlan      string            `json:"linked_plan,omitempty"`
    Versions        []VersionEntry    `json:"versions"`
    ChangeSets      map[string]ChangeSet `json:"change_sets,omitempty"`
}

// ResearchDecisions stored in decisions.json
type ResearchDecisions struct {
    PaperID   string     `json:"paper_id"`
    Decisions []Decision `json:"decisions"`
    OpenQuestionsResolved []ResolvedQuestion `json:"open_questions_resolved"`
}

type Decision struct {
    ID          string           `json:"id"`           // "EC-1", "EC-2", etc.
    Title       string           `json:"title"`
    Options     []DecisionOption `json:"options"`
    DecidedAt   time.Time        `json:"decided_at"`
    DecidedBy   string           `json:"decided_by"`   // "user" or agent ID
}

type DecisionOption struct {
    ID          string `json:"id"`
    Description string `json:"description"`
    Status      string `json:"status"`  // "ACCEPTED", "REJECTED", "DEFERRED", "IGNORED"
}

type ResolvedQuestion struct {
    Question   string    `json:"question"`
    Answer     string    `json:"answer"`
    ResolvedAt time.Time `json:"resolved_at"`
}
```

### PROPOSAL Message Flow

When Academic completes research, it sends a PROPOSAL message to Architect via Guide:

```
┌─────────────────────────────────────────────────────────────────────────────────┐
│                    PROPOSAL MESSAGE ROUTING                                      │
├─────────────────────────────────────────────────────────────────────────────────┤
│                                                                                 │
│  ACADEMIC                      GUIDE                      ARCHITECT             │
│     │                           │                            │                  │
│     │  write_research_paper     │                            │                  │
│     │  skill completes          │                            │                  │
│     │                           │                            │                  │
│     │  SendMessage({            │                            │                  │
│     │    to: "guide",           │                            │                  │
│     │    type: "ROUTE_REQUEST", │                            │                  │
│     │    payload: {             │                            │                  │
│     │      target: "architect", │                            │                  │
│     │      channel: "requests", │                            │                  │
│     │      message: {           │                            │                  │
│     │        type: "PROPOSAL",  │                            │                  │
│     │        research_slug,     │                            │                  │
│     │        paper_path,        │                            │                  │
│     │        version,           │                            │                  │
│     │        summary            │                            │                  │
│     │      }                    │                            │                  │
│     │    }                      │                            │                  │
│     │  })                       │                            │                  │
│     │ ─────────────────────────►│                            │                  │
│     │                           │                            │                  │
│     │                           │  Route to architect.requests                  │
│     │                           │                            │                  │
│     │                           │  PublishTo("architect.requests", {            │
│     │                           │    type: "PROPOSAL",       │                  │
│     │                           │    from: "academic",       │                  │
│     │                           │    research_slug: "...",   │                  │
│     │                           │    paper_path: "...",      │                  │
│     │                           │    version: 2,             │                  │
│     │                           │    summary: "..."          │                  │
│     │                           │  })                        │                  │
│     │                           │ ───────────────────────────►                  │
│     │                           │                            │                  │
│     │                           │                            │  handleRequest() │
│     │                           │                            │  checks type     │
│     │                           │                            │  == "PROPOSAL"   │
│     │                           │                            │                  │
│     │                           │                            │  Triggers        │
│     │                           │                            │  read_research_  │
│     │                           │                            │  paper skill     │
│     │                           │                            │                  │
└─────────────────────────────────────────────────────────────────────────────────┘
```

### Message Types

```go
// core/messages/types.go

type MessageType string

const (
    // Existing types
    MessageTypeQuery      MessageType = "QUERY"
    MessageTypeResponse   MessageType = "RESPONSE"
    MessageTypeCommand    MessageType = "COMMAND"

    // Research handoff
    MessageTypeProposal   MessageType = "PROPOSAL"

    // Revision triggers
    MessageTypeTaskModifiedByUser     MessageType = "TASK_MODIFIED_BY_USER"
    MessageTypeWorkflowChangeRequired MessageType = "WORKFLOW_CHANGE_REQUIRED"
)

// ProposalMessage sent when Academic completes research
type ProposalMessage struct {
    Type          MessageType `json:"type"`           // "PROPOSAL"
    From          string      `json:"from"`           // "academic"
    SessionID     string      `json:"session_id"`
    ProjectHash   string      `json:"project_hash"`
    ResearchSlug  string      `json:"research_slug"`
    PaperPath     string      `json:"paper_path"`     // Full path to current.md
    Version       int         `json:"version"`
    Summary       string      `json:"summary"`        // Brief description
    Timestamp     time.Time   `json:"timestamp"`
}
```

### Architect Request Handler

```go
// agents/architect/handler.go

func (a *ArchitectAgent) handleRequest(msg *messages.Message) error {
    switch msg.Type {
    case messages.MessageTypeProposal:
        // Research paper ready from Academic
        return a.handleProposal(msg)

    case messages.MessageTypeTaskModifiedByUser:
        // Pipeline agent reports user modified their task
        return a.handleUserTaskModification(msg)

    case messages.MessageTypeWorkflowChangeRequired:
        // Pipeline agent needs workflow change
        return a.handleWorkflowChangeRequest(msg)

    case messages.MessageTypeCommand:
        return a.handleCommand(msg)

    default:
        return a.handleDefault(msg)
    }
}

func (a *ArchitectAgent) handleProposal(msg *messages.Message) error {
    var proposal ProposalMessage
    if err := json.Unmarshal(msg.Payload, &proposal); err != nil {
        return err
    }

    // Trigger read_research_paper skill
    return a.skills.Execute("read_research_paper", map[string]any{
        "research_slug": proposal.ResearchSlug,
        "paper_path":    proposal.PaperPath,
        "version":       proposal.Version,
    })
}
```

### Research Paper Skills

#### Academic: write_research_paper

```go
// agents/academic/skills/write_research_paper.go

// Trigger conditions:
// - User says "I'm ready to start implementing"
// - User says "Let's build this"
// - User says "Write up your findings"
// - User says "Create the research paper"

type WriteResearchPaperSkill struct {
    storage    *research.Storage
    archivalist *archivalist.Client
    bus        *messaging.Bus
}

func (s *WriteResearchPaperSkill) Execute(ctx context.Context, params map[string]any) error {
    // 1. Aggregate research findings from conversation context
    findings := s.aggregateFindings(ctx)

    // 2. Structure into standard paper format
    paper := s.structurePaper(findings)

    // 3. Generate diagrams
    paper.Diagrams = s.generateDiagrams(findings)

    // 4. Compile edge cases with user decisions
    paper.EdgeCases = s.compileEdgeCases(findings)

    // 5. Determine version number
    meta, _ := s.storage.LoadMetadata(paper.Slug)
    paper.Version = meta.Head + 1

    // 6. Persist to storage
    if err := s.storage.SavePaper(paper); err != nil {
        return err
    }

    // 7. Archive version to Archivalist
    if err := s.archivePaperVersion(paper); err != nil {
        return err
    }

    // 8. Send PROPOSAL to Architect via Guide
    proposal := &ProposalMessage{
        Type:         messages.MessageTypeProposal,
        From:         "academic",
        SessionID:    ctx.Value("session_id").(string),
        ProjectHash:  s.storage.ProjectHash(),
        ResearchSlug: paper.Slug,
        PaperPath:    s.storage.PaperPath(paper.Slug),
        Version:      paper.Version,
        Summary:      paper.Summary,
        Timestamp:    time.Now(),
    }

    return s.bus.RouteToAgent("architect", "requests", proposal)
}
```

#### Architect: read_research_paper

```go
// agents/architect/skills/read_research_paper.go

// Triggered by: PROPOSAL message type on architect.requests channel

type ReadResearchPaperSkill struct {
    storage     *plan.Storage
    researchDB  *research.Storage
    librarian   *consultation.Client
    vectorDB    *vectordb.Client
}

func (s *ReadResearchPaperSkill) Execute(ctx context.Context, params map[string]any) error {
    slug := params["research_slug"].(string)
    version := params["version"].(int)

    // 1. Fetch the research paper
    paper, err := s.researchDB.FetchVersion(slug, &version)
    if err != nil {
        return err
    }

    // 2. Parse structured sections
    parsed := s.parsePaper(paper.Content)

    // 3. Load decisions
    decisions, _ := s.researchDB.LoadDecisions(slug)

    // 4. Consult Librarian to verify file paths
    verified := s.verifyFilePaths(parsed.FilesToModify)

    // 5. Convert to Plan structure
    plan := s.convertToPlan(parsed, decisions, verified)
    plan.Metadata["source_research"] = slug
    plan.Metadata["research_version"] = version

    // 6. Enter plan mode with pre-populated draft
    planCtx := &PlanContext{
        SessionID:   ctx.Value("session_id").(string),
        State:       PlanStateDrafting,
        CurrentPlan: plan,
        CreatedAt:   time.Now(),
    }

    if err := s.storage.SaveContext(planCtx); err != nil {
        return err
    }

    // 7. Present to user for review
    return s.presentPlanToUser(ctx, plan, paper.Summary)
}

func (s *ReadResearchPaperSkill) convertToPlan(parsed *ParsedPaper, decisions *ResearchDecisions, verified []VerifiedFile) *Plan {
    steps := []PlanStep{}

    // Convert implementation steps
    for i, implStep := range parsed.ImplementationSteps {
        step := PlanStep{
            Index:       i + 1,
            Description: implStep.Description,
            Agent:       s.inferAgent(implStep),
            DependsOn:   s.parseDependencies(implStep.Layer, steps),
            Layer:       implStep.Layer,
            Status:      StepStatusPending,
        }

        // Add acceptance criteria if available
        if criteria := parsed.AcceptanceCriteria[implStep.ID]; criteria != nil {
            step.AcceptanceCriteria = criteria
        }

        steps = append(steps, step)
    }

    // Add edge case mitigations as steps
    for _, ec := range parsed.EdgeCases {
        for _, opt := range decisions.GetDecision(ec.ID).Options {
            if opt.Status == "ACCEPTED" {
                steps = append(steps, PlanStep{
                    Index:       len(steps) + 1,
                    Description: fmt.Sprintf("Implement %s mitigation: %s", ec.ID, opt.Description),
                    Agent:       "engineer",
                    Source:      fmt.Sprintf("research:%s:%s:%s", parsed.Slug, ec.ID, opt.ID),
                })
            }
        }
    }

    // Recompute layers
    ComputeTopologicalLayers(steps)

    return &Plan{
        ID:            generatePlanID(),
        Title:         parsed.Title,
        Overview:      parsed.Summary,
        FilesToModify: verified,
        Steps:         steps,
        Considerations: parsed.Considerations,
    }
}
```

### fetch_version Skill (Both Agents)

Both Academic and Architect have a `fetch_version` skill for retrieving document versions:

```go
// Shared interface
type VersionedDocumentFetcher interface {
    FetchVersion(slug string, version *int) (*VersionedDocument, error)
    FetchLatest(slug string) (*VersionedDocument, error)
    ListVersions(slug string) ([]int, error)
}

type VersionedDocument struct {
    Slug      string         `json:"slug"`
    Version   int            `json:"version"`
    Content   string         `json:"content"`
    Path      string         `json:"path"`
    Metadata  map[string]any `json:"metadata"`
    CreatedAt time.Time      `json:"created_at"`
}
```

#### Academic: fetch_version (Research Papers)

```go
// agents/academic/skills/fetch_version.go

func (a *AcademicAgent) FetchVersion(slug string, version *int) (*VersionedDocument, error) {
    baseDir := filepath.Join(a.storage.ProjectDir(), "research", slug)

    if version == nil {
        return a.fetchLatestVersion(baseDir, slug)
    }
    return a.fetchSpecificVersion(baseDir, slug, *version)
}

func (a *AcademicAgent) fetchLatestVersion(baseDir, slug string) (*VersionedDocument, error) {
    // Quick file search: find all v*.md files
    pattern := filepath.Join(baseDir, "v*.md")
    matches, err := filepath.Glob(pattern)
    if err != nil || len(matches) == 0 {
        return nil, fmt.Errorf("no versions found for research: %s", slug)
    }

    // Find highest version number
    maxVersion := 0
    for _, match := range matches {
        filename := filepath.Base(match)
        var v int
        if _, err := fmt.Sscanf(filename, "v%d.md", &v); err == nil {
            if v > maxVersion {
                maxVersion = v
            }
        }
    }

    return a.fetchSpecificVersion(baseDir, slug, maxVersion)
}

func (a *AcademicAgent) fetchSpecificVersion(baseDir, slug string, version int) (*VersionedDocument, error) {
    path := filepath.Join(baseDir, fmt.Sprintf("v%d.md", version))

    content, err := os.ReadFile(path)
    if err != nil {
        return nil, fmt.Errorf("version %d not found for research: %s", version, slug)
    }

    // Load metadata
    metaPath := filepath.Join(baseDir, "metadata.json")
    var metadata map[string]any
    if data, err := os.ReadFile(metaPath); err == nil {
        json.Unmarshal(data, &metadata)
    }

    info, _ := os.Stat(path)

    return &VersionedDocument{
        Slug:      slug,
        Version:   version,
        Content:   string(content),
        Path:      path,
        Metadata:  metadata,
        CreatedAt: info.ModTime(),
    }, nil
}
```

#### Architect: fetch_version (Plans)

```go
// agents/architect/skills/fetch_version.go

func (a *ArchitectAgent) FetchVersion(slug string, version *int) (*VersionedDocument, error) {
    baseDir := filepath.Join(a.storage.ProjectDir(), "plans", slug)  // Different path

    if version == nil {
        return a.fetchLatestVersion(baseDir, slug)
    }
    return a.fetchSpecificVersion(baseDir, slug, *version)
}

// Implementation identical to Academic's, just different base path
```

---

## Vector DB Ingestion

Approved documents are chunked and embedded into the vector DB for semantic search. Only **new content** is ingested on version updates.

### Ingestion Flow

```
┌─────────────────────────────────────────────────────────────────────────────────┐
│                    VECTOR DB INCREMENTAL INGESTION                               │
├─────────────────────────────────────────────────────────────────────────────────┤
│                                                                                 │
│  DOCUMENT APPROVAL                                                              │
│       │                                                                         │
│       ▼                                                                         │
│  ┌─────────────────────────────────────────────────────────────────────────┐   │
│  │                     INGESTION PIPELINE                                   │   │
│  ├─────────────────────────────────────────────────────────────────────────┤   │
│  │                                                                          │   │
│  │  1. CHUNK DOCUMENT BY SECTION                                            │   │
│  │     ┌────────────────────────────────────────────────────────────┐      │   │
│  │     │  paper.md / plan.md                                         │      │   │
│  │     │  ════════════════════                                       │      │   │
│  │     │  ## Executive Summary  ──────► Chunk 1                      │      │   │
│  │     │  ## Problem Statement  ──────► Chunk 2                      │      │   │
│  │     │  ## Research Findings  ──────► Chunk 3, 4, 5 (by approach) │      │   │
│  │     │  ## Edge Cases  ─────────────► Chunk 6, 7, 8               │      │   │
│  │     └────────────────────────────────────────────────────────────┘      │   │
│  │                                                                          │   │
│  │  2. COMPUTE CHUNK HASHES                                                 │   │
│  │     chunk_hash = SHA256(chunk_content)                                   │   │
│  │                                                                          │   │
│  │  3. LOAD EXISTING HASHES FROM VECTOR DB METADATA                         │   │
│  │     doc_id: "research:implement-redis-caching"                           │   │
│  │     existing_hashes: [hash1, hash2, hash3]                               │   │
│  │                                                                          │   │
│  │  4. COMPUTE DIFF                                                         │   │
│  │     new_chunks     = current_hashes - existing_hashes                    │   │
│  │     removed_chunks = existing_hashes - current_hashes                    │   │
│  │     unchanged      = current_hashes ∩ existing_hashes                    │   │
│  │                                                                          │   │
│  │  5. INCREMENTAL UPDATE                                                   │   │
│  │     • Embed ONLY new_chunks (save compute)                               │   │
│  │     • Delete vectors for removed_chunks                                  │   │
│  │     • Skip unchanged chunks                                              │   │
│  │     • Update metadata with current_hashes                                │   │
│  │                                                                          │   │
│  └─────────────────────────────────────────────────────────────────────────┘   │
│                                                                                 │
└─────────────────────────────────────────────────────────────────────────────────┘
```

### Document Chunk Structure

```go
// core/vectordb/document.go

type DocumentChunk struct {
    // Identity
    ChunkID      string `json:"chunk_id"`      // "{doc_type}:{slug}:{section}:{index}"
    DocumentID   string `json:"document_id"`   // "{doc_type}:{slug}"
    DocumentType string `json:"document_type"` // "research_paper" | "plan"

    // Content
    Content      string `json:"content"`
    ContentHash  string `json:"content_hash"`  // SHA256 for dedup

    // Metadata for filtering
    ProjectHash  string `json:"project_hash"`
    Slug         string `json:"slug"`
    Version      int    `json:"version"`
    Section      string `json:"section"`       // "executive_summary", "edge_cases", etc.

    // Embedding
    Embedding    []float32 `json:"embedding"`

    // Timestamps
    IngestedAt   time.Time `json:"ingested_at"`
}

// Example chunk IDs:
// "research_paper:implement-redis-caching:executive_summary:0"
// "research_paper:implement-redis-caching:edge_cases:2"
// "plan:implement-redis-caching:step:5"
```

### Incremental Ingestion Implementation

```go
// core/vectordb/ingestion.go

func (v *VectorDB) IngestDocument(doc *ApprovedDocument) error {
    docID := fmt.Sprintf("%s:%s", doc.Type, doc.Slug)

    // 1. Chunk the document by section
    chunks := chunkDocument(doc)

    // 2. Compute hashes for each chunk
    currentHashes := make(map[string]string)
    for _, chunk := range chunks {
        chunk.ContentHash = sha256Hash(chunk.Content)
        currentHashes[chunk.ChunkID] = chunk.ContentHash
    }

    // 3. Load existing hashes
    existingHashes, _ := v.getDocumentHashes(docID)

    // 4. Compute diff
    var toAdd []DocumentChunk
    var toRemove []string

    for chunkID, hash := range currentHashes {
        existingHash, exists := existingHashes[chunkID]
        if !exists || existingHash != hash {
            // New or modified chunk - needs embedding
            toAdd = append(toAdd, findChunk(chunks, chunkID))
        }
    }

    for chunkID := range existingHashes {
        if _, exists := currentHashes[chunkID]; !exists {
            // Chunk was removed
            toRemove = append(toRemove, chunkID)
        }
    }

    // 5. Apply incremental update
    if len(toRemove) > 0 {
        v.deleteChunks(toRemove)
    }

    if len(toAdd) > 0 {
        // Embed only new chunks
        for i := range toAdd {
            toAdd[i].Embedding = v.embedder.Embed(toAdd[i].Content)
            toAdd[i].IngestedAt = time.Now()
        }
        v.insertChunks(toAdd)
    }

    // 6. Update document metadata
    return v.updateDocumentHashes(docID, currentHashes)
}
```

### Ingestion Trigger Points

| Event | Action |
|-------|--------|
| Research paper approved | Archive + Ingest (incremental) |
| Research paper revised (v2, v3...) | Archive + Ingest (only new/changed chunks) |
| Plan approved | Archive + Ingest (incremental) |
| Plan revised | Archive + Ingest (only new/changed chunks) |

---

## Revision Triggers

### Academic: Research Paper Revision

The Academic **only** creates a new version when the user **explicitly** requests modification:

```
┌─────────────────────────────────────────────────────────────────────────────────┐
│                    ACADEMIC REVISION TRIGGERS                                    │
├─────────────────────────────────────────────────────────────────────────────────┤
│                                                                                 │
│  ✓ TRIGGERS REVISION (Explicit User Request Only)                              │
│  ════════════════════════════════════════════════                               │
│                                                                                 │
│  • "Update the research paper to include X"                                    │
│  • "Add a section about Y to the proposal"                                     │
│  • "Remove the materialized views approach"                                    │
│  • "Change the recommendation from Redis to Memcached"                         │
│  • "Revise the edge cases section"                                             │
│  • "I want to modify the research document"                                    │
│                                                                                 │
│  ───────────────────────────────────────────────────────────────────────────   │
│                                                                                 │
│  ✗ DOES NOT TRIGGER REVISION                                                   │
│  ═══════════════════════════                                                    │
│                                                                                 │
│  • Casual discussion about the research                                        │
│  • Questions about the findings                                                │
│  • User thinking out loud ("interesting", "okay")                              │
│  • Academic providing clarification                                            │
│                                                                                 │
└─────────────────────────────────────────────────────────────────────────────────┘
```

### Architect: Plan Revision

The Architect creates a new version under **two scenarios**:

```
┌─────────────────────────────────────────────────────────────────────────────────┐
│                    ARCHITECT REVISION TRIGGERS                                   │
├─────────────────────────────────────────────────────────────────────────────────┤
│                                                                                 │
│  SCENARIO A: USER-INITIATED CHANGES                                             │
│  ═══════════════════════════════════                                            │
│                                                                                 │
│  1. Direct user request to modify plan:                                        │
│     • "Add a step for database migration"                                      │
│     • "Remove step 4 from the plan"                                            │
│     • "I want to modify the workflow"                                          │
│                                                                                 │
│  2. Pipeline agent reports USER modified their task:                           │
│                                                                                 │
│     Message type: TASK_MODIFIED_BY_USER                                        │
│     {                                                                           │
│       type: "TASK_MODIFIED_BY_USER",                                           │
│       from: "engineer_pipeline_3",                                             │
│       step_index: 5,                                                           │
│       modification: "scope_expanded",                                          │
│       description: "User asked to also handle edge case X"                     │
│     }                                                                           │
│                                                                                 │
│  ───────────────────────────────────────────────────────────────────────────   │
│                                                                                 │
│  SCENARIO B: AGENT-INITIATED WORKFLOW CHANGES                                   │
│  ════════════════════════════════════════════                                   │
│                                                                                 │
│  Pipeline agent discovers they NEED to modify workflow:                        │
│                                                                                 │
│  • "I need to add a prerequisite step"                                         │
│  • "This task should be split into two steps"                                  │
│  • "Step 6 depends on step 3, not step 2"                                      │
│                                                                                 │
│     Message type: WORKFLOW_CHANGE_REQUIRED                                     │
│     {                                                                           │
│       type: "WORKFLOW_CHANGE_REQUIRED",                                        │
│       from: "engineer_pipeline_5",                                             │
│       step_index: 7,                                                           │
│       change_type: "add_prerequisite",                                         │
│       reason: "Discovered shared utility needed",                              │
│       proposed_change: {                                                       │
│         action: "insert_step",                                                 │
│         before_step: 7,                                                        │
│         new_step: { description: "Create shared utility", ... }               │
│       },                                                                        │
│       blocking: true                                                           │
│     }                                                                           │
│                                                                                 │
│  For WORKFLOW_CHANGE_REQUIRED:                                                 │
│  1. Architect PAUSES execution (if blocking)                                   │
│  2. Architect presents change to user for approval                             │
│  3. If approved → Create new version, resume                                   │
│  4. If rejected → Notify agent to find alternative                             │
│                                                                                 │
└─────────────────────────────────────────────────────────────────────────────────┘
```

### Revision Message Types

```go
// core/messages/revision.go

// TaskModifiedByUserMessage - sent when user changes a task mid-execution
type TaskModifiedByUserMessage struct {
    Type          MessageType `json:"type"`  // "TASK_MODIFIED_BY_USER"
    From          string      `json:"from"`
    SessionID     string      `json:"session_id"`
    PlanSlug      string      `json:"plan_slug"`
    StepIndex     int         `json:"step_index"`
    Modification  string      `json:"modification"`  // "scope_expanded", "requirements_changed"
    Description   string      `json:"description"`
    OriginalTask  string      `json:"original_task"`
    ModifiedTask  string      `json:"modified_task"`
    Timestamp     time.Time   `json:"timestamp"`
}

// WorkflowChangeRequiredMessage - sent when agent needs workflow change
type WorkflowChangeRequiredMessage struct {
    Type           MessageType    `json:"type"`  // "WORKFLOW_CHANGE_REQUIRED"
    From           string         `json:"from"`
    SessionID      string         `json:"session_id"`
    PlanSlug       string         `json:"plan_slug"`
    StepIndex      int            `json:"step_index"`
    ChangeType     string         `json:"change_type"`  // "add_step", "split_step", "change_deps"
    Reason         string         `json:"reason"`
    ProposedChange ProposedChange `json:"proposed_change"`
    Blocking       bool           `json:"blocking"`
    Timestamp      time.Time      `json:"timestamp"`
}

type ProposedChange struct {
    Action       string    `json:"action"`  // "insert_step", "delete_step", "modify_step"
    TargetStep   int       `json:"target_step"`
    NewStep      *PlanStep `json:"new_step,omitempty"`
    ModifiedStep *PlanStep `json:"modified_step,omitempty"`
    NewDeps      []int     `json:"new_deps,omitempty"`
}
```

### Workflow Change Handling

```go
// agents/architect/revision.go

func (a *ArchitectAgent) handleWorkflowChangeRequest(msg *messages.Message) error {
    var req WorkflowChangeRequiredMessage
    json.Unmarshal(msg.Payload, &req)

    // 1. Pause execution if blocking
    if req.Blocking {
        a.orchestrator.PauseExecution(req.PlanSlug)
    }

    // 2. Evaluate impact
    impact := a.evaluateChangeImpact(req.ProposedChange)

    // 3. Present to user
    approval := a.presentChangeToUser(req, impact)

    if approval.Approved {
        // 4. Create new plan version
        newPlan := a.applyChange(req.ProposedChange)
        a.storage.SaveContext(newPlan)

        // 5. Archive and ingest (incremental)
        a.archivePlanVersion(newPlan)
        a.vectorDB.IngestDocument(newPlan)

        // 6. Resume
        if req.Blocking {
            a.orchestrator.ResumeExecution(req.PlanSlug, newPlan)
        }
    } else {
        // Notify agent to find alternative
        a.notifyChangeRejected(req.From, req.StepIndex, approval.Reason)
        if req.Blocking {
            a.orchestrator.ResumeExecution(req.PlanSlug, nil)
        }
    }

    return nil
}
```

### Complete Handoff Sequence

```
┌─────────────────────────────────────────────────────────────────────────────────┐
│                    COMPLETE RESEARCH → PLAN → EXECUTION FLOW                     │
├─────────────────────────────────────────────────────────────────────────────────┤
│                                                                                 │
│  RESEARCH PHASE                                                                 │
│  ══════════════                                                                 │
│  User: "Research how to add caching"                                           │
│  Guide → Academic                                                              │
│  Academic: [investigates, presents options]                                    │
│  User: [makes decisions on edge cases]                                         │
│  User: "I'm ready to implement"                                                │
│                                                                                 │
│  HANDOFF                                                                        │
│  ════════                                                                       │
│  Academic: [write_research_paper skill]                                        │
│    • Persists paper to ~/.sylk/projects/{hash}/research/{slug}/v1.md           │
│    • Archives to Archivalist                                                   │
│    • Sends PROPOSAL via Guide → architect.requests                             │
│                                                                                 │
│  Architect: [receives PROPOSAL, triggers read_research_paper]                  │
│    • Fetches paper via fetch_version skill                                     │
│    • Parses sections, loads decisions                                          │
│    • Consults Librarian for file verification                                  │
│    • Converts to Plan structure                                                │
│    • Enters plan mode with draft                                               │
│                                                                                 │
│  PLAN APPROVAL                                                                  │
│  ═════════════                                                                  │
│  Architect: [presents plan to user]                                            │
│  User: "Looks good"                                                            │
│  Architect:                                                                     │
│    • Archives plan to Archivalist                                              │
│    • Ingests to Vector DB (incremental)                                        │
│    • Links plan to research: source_research = paper_id                        │
│    • Archives research as APPROVED                                             │
│    • Ingests research to Vector DB                                             │
│                                                                                 │
│  EXECUTION                                                                      │
│  ═════════                                                                      │
│  Orchestrator: [dispatches to Engineers]                                       │
│                                                                                 │
│  [If Engineer discovers workflow change needed]                                │
│  Engineer → WORKFLOW_CHANGE_REQUIRED → architect.requests                      │
│  Architect: [pauses, presents to user]                                         │
│  User: [approves/rejects]                                                      │
│  Architect: [creates v2 if approved, resumes]                                  │
│                                                                                 │
└─────────────────────────────────────────────────────────────────────────────────┘
```

---

## Summary

Sylk combines **DAG-based orchestration** with a **Guide-centered universal routing bus** and **LLM skill planning**.

Key architectural principles:

1. **Guide routes user messages and ambiguous requests** - Direct consultation skills bypass Guide for known targets (token efficiency)
2. **Sessions are the unit of isolation** - Context pollution prevention
3. **Shared historical knowledge** - Cross-session learning via Archivalist
4. **Architect is the user's primary interface** - All status, plans, and questions flow through Architect
5. **Engineers are invisible to users** - Managed by Orchestrator, clarifications route through Architect
6. **Three knowledge RAGs** - Librarian (local code), Archivalist (history), Academic (external)
7. **Quality loop** - Inspector validates, Tester tests, fixes loop back through Architect
8. **Progressive skill disclosure** - Skills loaded on demand to minimize tokens
9. **Hooks for extensibility** - Pre/post hooks for prompts and tool calls

The system is designed for large-scale concurrency across many sessions and subagents, with strong resilience, operational visibility, and token efficiency.
